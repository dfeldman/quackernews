[
  {
    "title": "The Illustrated Transformer (jalammar.github.io)",
    "points": 244,
    "submitter": "auraham",
    "submit_time": "2025-12-22T19:15:56 1766430956",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=46357675",
    "comments": [
      "I read this article back when I was learning the basics of transformers; the visualizations were really helpful. Although in retrospect knowing how a transformer works wasn't very useful at all in my day job applying LLMs, except as a sort of deep background for reassurance that I had some idea of how the big black box producing the tokens was put together, and to give me the mathematical basis for things like context size limitations etc.I would strongly caution anyone who thinks that they will be able to understand or explain LLM behavior better by studying the architecture closely. That is a trap. Big SotA models these days exhibit so much nontrivial emergent phenomena (in part due to the massive application of reinforcement learning techniques) that give them capabilities very few people expected to ever see when this architecture first arrived. Most of us confidently claimed even back in 2023 that, based on LLM architecture and training algorithms, LLMs would never be able to perform well on novel coding or mathematics tasks. We were wrong. That points towards some caution and humility about using network architecture alone to reason about how LLMs work and what they can do. You'd really need to be able to poke at the weights inside a big SotA model to even begin to answer those kinds of questions, but unfortunately that's only really possible if you're a \"mechanistic interpretability\" researcher at one of the major labs.Regardless, this is a nice article, and this stuff is worth learning because it's interesting for its own sake! Right now I'm actually spending some vacation time implementing a transformer in PyTorch just to refresh my memory of it all. It's a lot of fun! If anyone else wants to get started with that I would highly recommend Sebastian Raschka's book and youtube videos as way into the subject: https://github.com/rasbt/LLMs-from-scratch .Has anyone read TFA author Jay Alammar's book (published Oct 2024) and would they recommend it for a more up-to-date picture?reply",
      "I think the biggest problem is that most tutorials use words to illustrate how the attention mechanism works. In reality, there are no word-associated tokens inside a Transformer. Tokens != word parts. An LLM does not perform language processing inside the Transformer blocks, and a Vision Transformer does not perform image processing. Words and pixels are only relevant at the input. I think this misunderstanding was a root cause of underestimating their capabilities.reply",
      "Nice video o mechanical interpretability from Welch Labs:https://youtu.be/D8GOeCFFby4?si=2rWnwv4M2bjkpEocreply",
      "How was reinforcement learning used as a gamechanger?What happens to an LLM without reinforcement learning?reply",
      "The essence of it is that after the \"read the whole internet and predict the next token\" pre-training step (and the chat fine-tuning), SotA LLMs now have a training step where they solve huge numbers of tasks that have verifiable answers (especially programming and math). The model therefore gets the very broad general knowledge and natural language abilities from pre-training and gets good at solving actual problems (problems that can't be bullshitted or hallucinated through because they have some verifiable right answer) from the RL step. In ways that still aren't really understood, it develops internal models of mathematics and coding that allow it to generalize to solve things it hasn't seen before. That is why LLMs got so much better at coding in 2025; the success of tools like Claude Code (to pick just one example) is built upon it. Of course, the LLMs still have a lot of limitations (the internal models are not perfect and aren't like how humans think at all), but RL has taken us pretty far.Unfortunately the really interesting details of this are mostly secret sauce stuff locked up inside the big AI labs. But there are still people who know far more than I do who do post about it, e.g. Andrej Karpathy discusses RL a bit in his 2025 LLMs Year in Review: https://karpathy.bearblog.dev/year-in-review-2025/reply",
      "Do you have the answer to the second question? Is an LLM trained on the internet just GPT-3?reply",
      "> Most of us confidently claimed even back in 2023 that, based on LLM architecture and training algorithms, LLMs would never be able to perform well on novel coding or mathematics tasks.I feel like there are three groups of people:1. Those who think that LLMs are stupid slop-generating machines which couldn't ever possibly be of any use to anybody, because there's some problem that is simple for humans but hard for LLMs, which makes them unintelligent by definition.2. Those who think we have already achieved AGI and don't need human programmers any more.3. Those who believe LLMs will destroy the world in the next 5 years.I feel like the composition of these three groups is pretty much constant since the release of Chat GPT, and like with most political fights, evidence doesn't convince people either way.reply",
      "Those three positions are all extreme viewpoints. There are certainly people who hold them, and they tend to be loud and confident and have an outsize presence in HN and other places online.But a lot of us have a more nuanced take! It's perfectly possible to believe simultaneously that 1) LLMs are more than stochastic parrots 2) LLMs are useful for software development 3) LLMs have all sorts of limitations and risks (you can produce unmaintainable slop with them, and many people will, there are massive security issues, I can go on and on...) 4) We're not getting AGI or world-destroying super-intelligence anytime soon, if ever 5) We're in a bubble and it's going to pop and cause a big mess 6) This tech is still going to be transformative long term, on a similar level to the web and smartphones.Don't let the noise from the extreme people who formed their opinions back when ChatGPT came out drown out serious discussion! A lot of us try and walk a middle course with this and have been and still are open to changing our minds.reply",
      "It is almost like understanding wood at a molecular level and being a carpenter. It also may help the carpentery, but you cam be a great one without it. And a bad one with the knowledge.reply",
      "Kudos also to Transformer Explainer team for putting some amazing visualizations https://poloclub.github.io/transformer-explainer/\nIt really clicked to me after reading this two and watching 3blue1brown videosreply"
    ],
    "link": "https://jalammar.github.io/illustrated-transformer/",
    "first_paragraph": "Visualizing machine learning one concept at a time.Read our book, Hands-On Large Language Models and follow me on LinkedIn, Bluesky, Substack, X,YouTube Discussions:\nHacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)\n\n\nTranslations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese\n\nWatch: MIT\u2019s Deep Learning State of the Art lecture referencing this post\n\nFeatured in courses at Stanford, Harvard, MIT, Princeton, CMU and othersIn the previous post, we looked at Attention \u2013 a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer \u2013 a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. Th"
  },
  {
    "title": "It's Always TCP_NODELAY (brooker.co.za)",
    "points": 124,
    "submitter": "eieio",
    "submit_time": "2025-12-22T21:09:59 1766437799",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=46359120",
    "comments": [
      "The Nagle algorithm was created back in the day of multi-point networking.  Multiple hosts were all tied to the same communications (Ethernet) channel, so they would use CSMA (https://en.wikipedia.org/wiki/Carrier-sense_multiple_access_...) to avoid collisions.  CSMA is no longer necessary on Ethernet today because all modern connections are point-to-point with only two \"hosts\" per channel.  (Each host can have any number of \"users.\")  In fact, most modern (copper) (Gigabit+) Ethernet connections have both ends both transmitting and receiving AT THE SAME TIME ON THE SAME WIRES.  A hybrid is used on the PHY at each end to subtract what is being transmitted from what is being received.  Older (10/100 Base-T) can do the same thing because each end has dedicated TX/RX pairs.  Fiber optic Ethernet can use either the same fiber with different wavelengths, or separate TX/RX fibers.  I haven't seen a 10Base-2 Ethernet/DECnet interface for more than 25 years.  If any are still operating somewhere, they are still using CSMA.  CSMA is also still used for digital radio systems (WiFi and others).  CSMA includes a \"random exponential backoff timer\" which does the (poor) job of managing congestion.  (More modern congestion control methods exist today.)  Back in the day, disabling the random backoff timer was somewhat equivalent to setting TCP_NODELAY.Dumping the Nagle algorithm (by setting TCP_NODELAY) almost always makes sense and should be enabled by default.reply",
      "Nagle is quite sensible when your application isn't taking any care to create sensibly-sized packets, and isn't so sensitive to latency. It avoids creating stupidly small packets unless your network is fast enough to handle them.reply",
      "At this point, this is an application level problem and not something the kernel should be silently doing for you IMO.  An option for legacy systems or known problematic hosts fine, but off by default and probably not a per SOCKOPT.Every modern language has buffers in their stdlib.  Anyone writing character at a time to the wire lazily or unintentionally should fix their application.reply",
      "Just to add, ethernet uses csma/cd, WiFi uses csma/ca.Upgraded our DC switches to new ones around 2014 and needed to keep a few old ones because the new ones didn't support 10Mbit half duplex.reply",
      "Thanks for the clarification.  They're so close to being the same thing that I always call it CSMA/CD.  Avoiding a collision is far more preferable than just detecting one.Yeah, many enterprise switches don't even support 100Base-T or 10Base-T anymore.  I've had to daisy chain an old switch that supports 100Base-T onto a modern one a few times myself.  If you drop 10/100 support, you can also drop HD (simplex) support.  In my junk drawer, I still have a few old 10/100 hubs (not switches), which are by definition always HD.reply",
      "I found this article while debugging some networking delays for a game that I'm working on.It turns out that in my case it wasn't TCP_NODELAY - my backend is written in go, and go sets TCP_NODELAY by default!But I still found the article - and in particular Nagle's acknowledgement of the issues! - to be interesting.There's a discussion from two years ago here: https://news.ycombinator.com/item?id=40310896 - but I figured it'd been long enough that others might be interested in giving this a read too.reply",
      "There is also a good write-up [0] by Julia Evans. We ran into this with DICOM storescp, which is a chatty protocol and TCP_NODELAY=1 makes the throughput significantly better. Since DICOM is often used in a LAN, that default just makes it unnecessarily worse.[0]: https://jvns.ca/blog/2015/11/21/why-you-should-understand-a-...[1]: https://news.ycombinator.com/item?id=10607422reply",
      "Oh! Thank you for this! I love Julia\u2019s writing but haven\u2019t read this post.reply",
      "Any details on the game you\u2019ve been working on? I\u2019ve been really enjoying Ebitengine and Golang for game dev so would love to read about what you\u2019ve been up to!reply",
      "The problem is actually that nobody uses the generic solution to these classes of problems and then everybody complains that the special-case for one set of parameters works poorly for a different set of parameters.Nagle\u2019s algorithm is just a special case solution of the generic problem of choosing when and how long to batch. We want to batch because batching usually allows for more efficient batched algorithms, locality, less overhead etc. You do not want to batch because that increases latency, both when collecting enough data to batch and because you need to process the whole batch.One class of solution is \u201cWork or Time\u201d. You batch up to a certain amount of work or up to a certain amount of time, whichever comes first. You choose your amount of time as your desired worst case latency. You choose your amount of work as your efficient batch size (it should be less than max throughput * latency, otherwise you will always hit your timer first).Nagle\u2019s algorithm is \u201cWork\u201d being one packet (~1.5 KB) with \u201cTime\u201d being the time until all data gets a ack (you might already see how this degree of dynamism in your timeout might pose a problem already) which results in the fallback timer of 500 ms when delayed ack is on. It should be obvious that is a terrible set of parameters for modern connections. The problem is that Nagle\u2019s algorithm only deals with the \u201cWork\u201d component, but punts on the \u201cTime\u201d component allowing for nonsense like delayed ack helpfully \u201cconfiguring\u201d your effective \u201cTime\u201d component to a eternity resulting in \u201cstuck\u201d buffers which is what the timeout is supposed to avoid. I will decline to discuss the other aspect which is choosing when to buffer and how much of which Nagle\u2019s algorithm is again a special case.Delayed ack is, funnily enough, basically the exact same problem but done on the receive side. So both sides set timeouts based on the other side going first which is obviously a recipe for disaster. They both set fixed \u201cWork\u201d, but no fixed \u201cTime\u201d resulting in the situation where both drivers are busy trying to wave the other person through the intersection.What should be done is use the generic solutions that are parameterized by your system and channel properties which holistically solve these problems which would take too long to describe in depth here.reply"
    ],
    "link": "https://brooker.co.za/blog/2024/05/09/nagle.html",
    "first_paragraph": "It's not the 1980s anymore, thankfully.The first thing I check when debugging latency issues in distributed systems is whether TCP_NODELAY is enabled. And it\u2019s not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.First, let\u2019s be clear about what we\u2019re talking about. There\u2019s no better source than John Nagle\u2019s RFC896 from 19841. First, the problem statement:There is a special problem associated with small  packets.   When TCP  is  used  for  the transmission of single-character messages originating at a keyboard, the typical result  is  that  41  byte packets (one  byte  of data, 40 bytes of header) are transmitted for each byte of useful data.  This 4000%  overhead  is  annoying but tolerable on lightly loaded networks.In short, Nagle was interested in better amortizing the cost of TCP headers, to get better t"
  },
  {
    "title": "Ultrasound Cancer Treatment: Sound Waves Fight Tumors (ieee.org)",
    "points": 166,
    "submitter": "rbanffy",
    "submit_time": "2025-12-22T19:37:34 1766432254",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=46357945",
    "comments": [
      "What are the chances that breaking up a tumor this way seeds cancer elsewhere in the body? 2024 meta analysis of seeding I didn't see ultrasound in there: https://pubmed.ncbi.nlm.nih.gov/39605885/Here is a study on AEs specifically from this type of ultrasound: https://journals.plos.org/plosone/article?id=10.1371/journal...Quote: \"Cavitation detaches cancer cells/emboli from the primary site and thereby releases them into the circulation, leading to metastasis\"reply",
      "We simply won\u2019t know until they do the inevitable phase2/3 RCTs. They will need to show that this method helps people survive longer or with better quality of life than the current standard of care.reply",
      "HistoSonics has studies published with 50 patients. Their upcoming study with 5000 liver patients obviously will give more information, but we already have some.And with that said, these studies are more relevant than the top of thread linking to a review from 2011 looking at papers from 2005-2006 for ultrasound cavitation causing metastases.reply",
      ">>> ... the study found that removing the parachute prior to jumping led to a shocking increase in mortality among skydivers.When there's a clear causal mechanism, additional research that doesn't propose a clear resolution to the underlying problem doesn't negate the clear causal mechanism. Releasing a bunch of loose cancer into the body is a clear causal mechanism, so unless you're filtering it or killing the loose cancer somehow, I'm not sure what those studies could tell you that overcomes the underlying problem. And until they address that problem, it's going to be limited to a quality of life type application - stopping the tumor from killing you now with the certainty of metastasis killing you later.reply",
      "So what's the problem? The vast majority of cancer treatments seek only to put the condition into remission for a while. Realistically that's often all that can be done.reply",
      "Putting it into remission is basically the opposite of causing it to metastasize.reply",
      "Chemo post-histrophy would remove any lingering cancer cells effectively. Cancer cells need lots of fuel or they stop replicating, and this is what traditional chemo is great at stopping.reply",
      "Is the idea that you would need less chemo after the tumor is broken up to remove any remaining cancer cells versus just starting out with chemo to remove the tumor?reply",
      "> What are the chances that breaking up a tumor this way seeds cancer elsewhere in the body?that's discussed in the articlereply",
      "It seems they are initially focused on pancreatic cancer, which has a very low survival rate ~14% [1].In theory, this may mean that metastisizing this tumour could destroy it in the pancreas, but allow the cells to spread to more treatable locations?1 - https://www.canceraustralia.gov.au/cancer-types/pancreatic-c...reply"
    ],
    "link": "https://spectrum.ieee.org/ultrasound-cancer-treatment",
    "first_paragraph": "HistoSonics turns its tumor-liquifying tech against pancreatic cancerHistoSonics\u2019 Edison system uses a water-filled membrane to transmit focused ultrasound into the body. The resulting bubbles expand and collapse within the tumor, producing mechanical stress that destroys cancer cells and liquefies the tumor.For many years, doctors and technicians who performed medical ultrasound procedures viewed bubbles with wary concern. The phenomenon of cavitation\u2014the formation and collapse of tiny gas bubbles due to changes in pressure\u2014was considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.The trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. Zhen Xu, who was working on a Ph.D. in biomedical engineering at the time, was bombarding pig heart tissue "
  },
  {
    "title": "Flock Exposed Its AI-Powered Cameras to the Internet. We Tracked Ourselves (404media.co)",
    "points": 353,
    "submitter": "chaps",
    "submit_time": "2025-12-22T16:31:40 1766421100",
    "num_comments": 330,
    "comments_url": "https://news.ycombinator.com/item?id=46355548",
    "comments": [
      "Was fortunate to talk to a security lead who built the data-driven policing network for a major American city that was an early adopter. ALPR vendors like Flock either heavily augment and/or anchor the tech setups.What was notable to me is the following, and it\u2019s why I think a career spent on either security researching, or going to law school and suing, these vendors into the ground over 20 years would be the ultimate act of civil service:1. It\u2019s not just Flock cams. It\u2019s the data eng into these networks - 18 wheeler feed cams, flock cams, retail user nest cams, traffic cams, ISP data sales2. All in one hub, all searchable by your local PD and also the local PD across state lines who doesn\u2019t like your abortion/marijuana/gun/whatever laws, and relying on:3. The PD to setup and maintain proper RBAC in a nationwide surveillance network that is 100%, for sure, no doubt about it (wait how did that Texas cop track the abortion into Indiana/Illinois\u2026?), configured for least privilege.4. Or if the PD doesn\u2019t want flock in town, they reinstall cameras against the ruling (Illinois iirc?) or just say \u201cwe have the feeds for the DoT cameras in/out of town and the truckers through town so might as well have control over it, PD!\u201dLayer the above with the current trend in the US, and 2025 model Nissan uploading stop-by-stop geolocation and telematics to cloud (then, sold into flock? Does even knowing for sure if it does or doesn\u2019t even matter?)Very bad line of companies. Again all is from primary sources who helped implement it over the years. If you spend enough time at cybersecurity conferences you\u2019ll meet people with these jobs.reply",
      "Now you have scale with ai hardware becoming cheaper and software incentives aligning.reply",
      "Flock or their defenders will lock in on the excuse that \u201coh these are misconfigured\u201d or \u201cyeah hacking is illegal, only cops should have this data\u201d. The issue is neither of the above. The issue is the collection and collation of this footage in the first place! I don\u2019t want hackers watching me all the time, sure, but I DEFINITELY don\u2019t trust the state or megacorps to watch me all the time. Hackers concern me less, actually. I\u2019m glad that Benn Jordan and others are giving this the airtime it needs, but they\u2019re focusing the messaging on security vulnerabilities and not state surveillance. Thus Flock can go \u201cok we will do better about security\u201d and the bureaucrats, average suburbanites, and law enforcement agencies will go \u201cok good they fixed the vulnerabilities I\u2019m happy now\u201dreply",
      "Yes and the biggest problem with this kind of ALPRs are they bypass the due process. Most of the time police can just pull up data without any warrant and there has been instances where this was abused (I think some cops used this for stalking their exes [1]) and also the most worrying Flock seems to really okay with giving ICE unlimited access to this data [2] [3] (which I speculate for loose regulations).[1]: https://lookout.co/georgia-police-chief-arrested-for-using-f...\n[2]: https://www.404media.co/emails-reveal-the-casual-surveillanc...\n[3]: https://www.404media.co/ice-taps-into-nationwide-ai-enabled-...reply",
      "I'm sure the 40 percent of cops who are domestic abusers and the white supremacists militias recruited wholesale into ICE will use this power responsibly.reply",
      "When you give access to any system that collects the personal information including location data for people in the US to the police, a percentage of the police will always use those systems for stalking their exes.reply",
      "What is not only true for police but for every sufficiently big group of people.reply",
      "Cops do have some unique tendencies but I think the real issue is the cops are able to leverage the power of the government in ways other large groups cannot.reply",
      "I keep an unofficial record of instances where police and similar authorities have abused their access to these types of systems. The list is long. It's almost exclusively men stalking ex-partners or attractive women they don't know, but have seen in public.What's frightening is it's not rare, it actually happens constantly, and this is just within the systems which have a high level of internal logging/user-tracking.So now with Flock and data brokers we have authorities having access to information that was originally held behind a judge's signature. Often with little oversight, and frequently for unofficial, abusive purposes.This reality also ties back to the discussion about providing the \"good guys\" encryption backdoors. The reality is that there are no \"good guys\", everyone exists in shades of grey, and I dare say there are people in forces whom are attracted to the power the role provides, rather than any desire for public service.In conclusion it's a fundamental design flaw to rely on the operator being a \"good guy\", and that's before we get into the problem of leaks, bugs, and flaws in the security model, or in this case: complete open access to the public web - laughable, farcical, and horrifying.reply",
      "> What's frightening is it's not rare, it actually happens constantly, and this is just within the systems which have a high level of internal logging/user-tracking.Would not be surprised if these types of abuse serve to obfuscate other abusive uses as well and are thus part of the system operating as it should. Flood the internal logging with all kinds of this \"low-level\" stuff, hiding the high-level warrantless tracking.reply"
    ],
    "link": "https://www.404media.co/flock-exposed-its-ai-powered-cameras-to-the-internet-we-tracked-ourselves/",
    "first_paragraph": "I am standing on the corner of Harris Road and Young Street outside of the Crossroads Business Park in Bakersfield, California, looking up at a Flock surveillance camera bolted high above a traffic signal. On my phone, I am watching myself in real time as the camera records and livestreams me\u2014without any password or login\u2014to the open internet. I wander into the intersection, stare at the camera and wave. On the livestream, I can see myself clearly. Hundreds of miles away, my colleagues are remotely watching me too through the exposed feed.Flock left livestreams and administrator control panels for at least 60 of its AI-enabled Condor cameras around the country exposed to the open internet, where anyone could watch them, download 30 days worth of video archive, and change settings, see log files, and run diagnostics.\u00a0Unlike many of Flock\u2019s cameras, which are designed to capture license plates as people drive by, Flock\u2019s Condor cameras are pan-tilt-zoom (PTZ) cameras designed to record a"
  },
  {
    "title": "GLM-4.7: Advancing the Coding Capability (z.ai)",
    "points": 231,
    "submitter": "pretext",
    "submit_time": "2025-12-22T18:46:32 1766429192",
    "num_comments": 95,
    "comments_url": "https://news.ycombinator.com/item?id=46357287",
    "comments": [
      "My quickie: MoE model heavily optimized for coding agents, complex reasoning, and tool use. 358B/32B active. vLLM/SGLang only supported on the main branch of these engines, not the stable releases. Supports tool calling in OpenAI-style format. Multilingual English/Chinese primary. Context window: 200k. Claims Claude 3.5 Sonnet/GPT-5 level performance. 716GB in FP16, probably ca 220GB for Q4_K_M.My most important takeaway is that, in theory, I could get a \"relatively\" cheap Mac Studio and run this locally, and get usable coding assistance without being dependent on any of the large LLM providers. Maybe utilizing Kimik2 in addition. I like that open-weight models are nipping at the feet of the proprietary models.reply",
      "I bought a second\u2011hand Mac Studio Ultra M1 with 128 GB of RAM, intending to run an LLM locally for coding. Unfortunately, it's just way too slow.For instance, an 4\u2011bit quantized model of GLM 4.6 runs very slowly on my Mac. It's not only about tokens per second speed but also input processing, tokenization, and prompt loading; it takes so much time that it's testing my patience. People often mention about the TPS numbers, but they neglect to mention the input loading times.reply",
      "I've been running the 'frontier' open-weight LLMs (mainly deepseek r1/v3) at home, and I find that they're best for asynchronous interactions. Give it a prompt and come back in 30-45 minutes to read the response. I've been running on a dual-socket 36-core Xeon with 768GB of RAM and it typically gets 1-2 tokens/sec. Great for research questions or coding prompts, not great for text auto-complete while programming.reply",
      "Given the cost of the system, how long would it take to be less expensive than, for example, a $200/mo Claude Max subscription with Opus running?reply",
      "It's not really an apples-to-apples comparison - I enjoy playing around with LLMs, running different models, etc, and I place a relatively high premium on privacy. The computer itself was $2k about two years ago (and my employer reimbursed me for it), and 99% of my usage is for research questions which have relatively high output per input token. Using one for a coding assistant seems like it can run through a very high number of tokens with relatively few of them actually being used for anything. If I wanted a real-time coding assistant, I would probably be using something that fit in the 24GB of VRAM and would have very different cost/performance tradeoffs.reply",
      "Have you tried Qwen3 Next 80B? It may run a lot faster, though I don't know how well it does coding tasks.reply",
      "Anything except a 3bit quant of GLM 4.6 will exceed those 128 GB of RAM you mentioned, so of course it's slow for you. If you want good speeds, you'll at least need to store the entire thing in memory.reply",
      "> Supports tool calling in OpenAI-style formatSo Harmony? Or something older? Since Z.ai also claim the thinking mode does tool calling and reasoning interwoven, would make sense it was straight up OpenAI's Harmony.> in theory, I could get a \"relatively\" cheap Mac Studio and run this locallyIn practice, it'll be incredible slow and you'll quickly regret spending that much money on it instead of just using paid APIs until proper hardware gets cheaper / models get smaller.reply",
      "> In practice, it'll be incredible slow and you'll quickly regret spending that much money on it instead of just using paid APIs until proper hardware gets cheaper / models get smaller.Yes, as someone who spent several thousand $ on a multi-GPU setup, the only reason to run local codegen inference right now is privacy or deep integration with the model itself.It\u2019s decidedly more cost efficient to use frontier model APIs. Frontier models trained to work with their tightly-coupled harnesses are worlds ahead of quantized models with generic harnesses.reply",
      "Yeah, I think without a setup that costs 10k+ you can't even get remotely close in performance to something like claude code with opus 4.5.reply"
    ],
    "link": "https://z.ai/blog/glm-4.7",
    "first_paragraph": "GLM-4.7, your new coding partner, is coming with the following features:You can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.Benchmark Performance. More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.Coding: AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it *feels*. True intelligence isn't just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-\"coding\" this time.GLM-4.7 enhances Interleaved Thinking, a feature introduced since GLM-4.5, and further introduces Preserved Thinking and Turn-level Thinking. By thinking bet"
  },
  {
    "title": "FPGAs Need a New Future (allaboutcircuits.com)",
    "points": 63,
    "submitter": "thawawaycold",
    "submit_time": "2025-12-19T10:29:33 1766140173",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=46324269",
    "comments": [
      "One big issue with FPGAs is how annoying it is to learn how to use them. I did a course on embedded systems a few years ago and nobody  could truly get to enjoy it because we spent most of our time downloading and installing huge toolchains, waiting for synthesis and PnR to complete and debugging weird IDE issues. We need to open up the space to allow people to develop better solutions than what these companies are forcing down our throats.There already exist fantastic open source tools such as Yosys, Nextpnr, iverilog, OpenFPGALoader, ... that together implement most features that a typical hardware dev would want to use. But chip support is unfortunately limited, so fewer people are using these tools.We decided to build a VSCode extension that wraps these open source tools (https://edacation.github.io for the interested) to combat this problem. Students are already using it during the course and are generally very positive about the experience. It's by no means a full IDE, but if you're just getting started with HDL it's great to get familiar with it. Instead of a mess of a toolchain that nobody truly knows how to use, you now get a few buttons to visualize and (soon) program onto an FPGA.There's also Lushay Code for the slightly more advanced users. But we need more of these initiatives to really get the ball rolling and make an impact, so I'd highly recommend people to check out and contribute to projects like this.reply",
      "The issue with the software team using an FPGA is that software developers generally aren't very good at doing things in parallel. They generally do a poor job in implementing hardware. I previously taught undergraduates VHDL, the software students generally struggles with the dealing with things running in parallel.VHDL and Verilog are used because they are excellent languages to describe hardware. The tools don't really hold anyone back. Lack of training or understanding might.Consistently the issue with FPGA development for many years was that by the time you could get your hands on the latest devices, general purpose CPUs were good enough. The reality is that if you are going to build a custom piece of hardware then you are going to have to write the driver's and code yourself. It's achievable, however, it requires more skill than pure software programming.Again, thanks to low power an slow cost arm processors a class of problems previously handled by FPGAs have been picked up by cheap but fast processors.The reality is that for major markets custom hardware tends to win as you can make it smaller, faster and cheaper. The probability is someone will have built and tested it on an FPGA first.reply",
      "VHDL is ok, Verilog is a sin.The issue isn't the languages, it's the horrible tooling around them. I'm not going to install a multi GB proprietary IDE that needs a GUI for everything and doesn't operate with any of my existing tools. An IDE that costs money, even though I already bought the hardware. Or requires an NDA. F** that.I want to be able to do `cargo add risc-v` if I need a small cpu IP, and not sacrifice a goat.reply",
      "Well really, the language _is_ the difficulty of much of hardware design, both Verilog and VHDL are languages that were designed for simulation of hardware, and not synthesis of hardware. Both languages have of similar-but-not-quite ways of writing things, like blocking/nonblocking assigns causing incorrect behavior that's incredibly difficult to spot on the waveform, not being exhaustive in assigns in always blocks causing latches, maybe-synthesizeable for loops, etc. Most of this comes from their paradigm of an event loop, handling all events and the events that those events trigger, etc, until all are done, and advancing time until the next event. They simulate how the internal state of a chip changes every clock cycle, but not to actually do the designing of said chip itself.I'm tooting my own horn with this, as I'm building my own language for doing the actual designing. It's called SUS.Simple things look pretty much like C:  module add :\n    int#(FROM:-8, TO: 8) a,\n    int#(FROM: 2, TO: 20) b -> \n    int c {\n    c = a+b\n  }\n\nIt automatically compensates for pipelining registers you add, and allows you to use this pipelining information in the type system.It's a very young language, but me, a few of my colleagues, and some researchers in another university are already using it. Check it out => https://github.com/pc2/sus-compilerreply",
      "Or you could do the right thing, ignore the GUI for 99% of what you\u2019re doing, and treat the FPGA tools as command line tools that are invoked by running \u201cmake\u201d\u2026reply",
      "You can pretty much do everything in Vivado from the command line as long as you know Tcl...Also, modern Verilog (AKA Systemverilog) fixes a bunch of the issues you might have had. There isn't much advantage to VHDL these days unless perhaps you are in Europe or work in certain US defense companies.reply",
      "The main advantage to VHDL is the style of thinking it enforces. If you write your Verilog or SystemVerilog like it's VHDL, everything works great. If you write your VHDL like it's Verilog, you'll get piles of synthesis errors... and many of them will be real problems.So if you learn VHDL first, you'll be on a solid footing.reply",
      "# Here's the general flow for Vivado TCL projects that takes you from source code to a bit-file with no interaction.  Read UG835 for details.create_project -in_memory -part ${PART}set_property target_language VHDL [ current_project ]read_vhdl \"my_hdl_file.vhd\"synth_design -top my_hdl_top_module_name -part ${PART}opt_designplace_designroute_designcheck_timing -file my_timing.txtreport_utilization -file my_util.txtwrite_checkpoint my_routed_design.dcpwrite_bitstream my_bitfile.bitreply",
      "Yeah I agree it is a lack of understanding on how to use the tools. The main issue I ran into in my undergrad FPGA class as a CS student was a lack of understanding on how to use the IDE. We jumped right into trying to get something running on the board instead of taking time to get everything set up. IMO it would have been way easier if my class used an IDE that was as simple as Arduino instead of everyone trying to run a virtual machine on their macbooks to run Quartus Prime.reply",
      "FPGAs need their \"Arduino moment\". There have been so, so, so many projects where I've wanted just a little bit of moderately-complicated glue logic. Something pretty easy to dash off in VHDL or whatever. But the damn things require so much support infrastructure: they're complicated to put down on boards, they're complicated to load bitstreams in to, they're complicated to build those bitstreams for, and they're complicated to manage the software projects for.As soon as they reach the point where it's as easy to put down an FPGA as it is an old STM32 or whatever, they'll get a lot more interesting.reply"
    ],
    "link": "https://www.allaboutcircuits.com/industry-articles/fpgas-need-a-new-future/",
    "first_paragraph": ""
  },
  {
    "title": "The Garbage Collection Handbook (gchandbook.org)",
    "points": 136,
    "submitter": "andsoitis",
    "submit_time": "2025-12-22T19:30:18 1766431818",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=46357870",
    "comments": [
      "My favorite story about garbage collection: https://devblogs.microsoft.com/oldnewthing/20180228-00/?p=98...reply",
      "I would call that a region-based memory allocator... Only that it has a single region, ever.reply",
      "They do that in other places.As I heard the tale, on the Standard Missile, they don't recirculate the hydraulic fluid, they just spit out as the missile flies. It's a wonderful engineering solution.reply",
      "And on the Falcon 9, the hydrocarbon fuel is used as hydraulic fluid, then just dumped back into the fuel tank.reply",
      "now that is what i call the ultimate in garbage collection technologyreply",
      "I wish the author section provided what production garbage collectors the authors worked on. There's plenty of nonintuitive things you can learn in the real world, so a book including those would be both interesting and useful.reply",
      "I had Hosking as a professor.  Iirc, it was an okay experience.  Compilers course I believe.When the handbook came out, I bought it because \"hey, I know that guy\".  Ultimately, I don't think it's necessary, but having a more in depth knowledge of garbage collection and the problems in the space occasionally comes in handy.For example, what implication do finalizers have on garbage collection design?  Reading about that was kind of an eye opener.reply",
      "I have this, it is very well written and thorough. Highly recommend!reply",
      "Great book. Previous discussion:\nhttps://news.ycombinator.com/item?id=35492307(387 points, 166 comments)reply"
    ],
    "link": "https://gchandbook.org/index.html",
    "first_paragraph": "Richard Jones\u2019s Garbage Collection (Wiley, 1996) was a milestone book in the area of automatic memory management. \n    Its widely acclaimed successor,\n    The Garbage Collection Handbook: The Art of Automatic Memory Management\n    captured the state of the field in 2012. However, technology developments have made memory management more challenging, interesting and important than ever. This second edition updates the handbook, bringing together a wealth of knowledge gathered by automatic memory management researchers and developers over the past sixty years. The authors compare the most important approaches and state-of-the-art techniques in a single, accessible framework.The book addresses new challenges to garbage collection made by recent advances in hardware and software, and the environments in which programs are executed. It explores the consequences of these changes for designers and implementers of high performance garbage collectors. Along with simple and traditional algorithms"
  },
  {
    "title": "Claude Code gets native LSP support (github.com/anthropics)",
    "points": 300,
    "submitter": "JamesSwift",
    "submit_time": "2025-12-22T15:59:01 1766419141",
    "num_comments": 160,
    "comments_url": "https://news.ycombinator.com/item?id=46355165",
    "comments": [
      "I really can't understand why JetBrains hasn't integrated its refactoring tools into the AI system. Really missed the boat on making their platform transformational for AI coding. Imagine how much smaller the context would be for a tool that renames a function than editing hundreds of files. This LSP support is a good start but without the mutation functions it is still pretty lackluster. Plus LSPs aren't as good as JetBrains generally.reply",
      "Jetbrains seems a bit lost these days. Look at that very recent screw up [0].I thought about moving after 10+ years when they abandoned the commit modal, and jacked up the plan prices, but I barely understand how to commit things in Vscode anyway. Let's see in 2026.[0] https://blog.jetbrains.com/datagrip/2025/12/18/query-console...reply",
      "Jetbrainz needs to give up on Junie and their in house ai and focus on integrating with the established tools. If they don\u2019t, VS code will consume them.reply",
      "They've already done that. After the Junie fiasco, they pivoted to \"AI Assistant\", where Junie is just another provider alongside Anthropic and OpenAI. In theory, you have Claude Code inside Jetbrains IDEs now.What's incredible is just how bad it works. I nearly always work with projects that mount multiple folders, and the IDE's MCP doesn't support that. So it doesn't understand what folders are open and can't interact with them. Junie the same issue, and the AI Assistant appears to have inherited it. The issue has been open for ages and ignored by Jetbrains.I also tried out their full line completion, and it's incomprehensibly bad, at least for Go, even with \"cloud\" completion enabled. I'm back to using Augment, which is Claude-based autocompletion.reply",
      "They already kinda did. They brough ACP support which allows you to somewhat integrate Claude Code, Gemini CLI or OpenCode they also recently brought BYOK support so you can use an existing provider and don't pay extra subscription for it.reply",
      "ACP seems super under the radar. It has some support, but it got merged into A2A, which I don't hear anyone talking about, so it seems like it's going to die on the vine.reply",
      "> It has some support, but it got merged into A2A, which I don't hear anyone talking about, so it seems like it's going to die on the vine.I'm not sure this is true, do you have a source? Maybe conflating this with the recent Agentic AI Foundation & MCP news?reply",
      "This is really too bad, as editors should be able to plug and play with AI tooling in the same way that editors <> LSP can plug and play with language tooling.reply",
      "I think you may be confusing Agent Client Protocol with Agent Communication Protocol.reply",
      "I mean I tried Zeds implementation with OpenCode was working fine but yeah the whole standards part is really complicated right now. I can't keep track of it. I hear about A2A but did not know it was merged with ACP.reply"
    ],
    "link": "https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page."
  },
  {
    "title": "NIST was 5 \u03bcs off UTC after last week's power cut (jeffgeerling.com)",
    "points": 178,
    "submitter": "jtokoph",
    "submit_time": "2025-12-22T17:01:28 1766422888",
    "num_comments": 90,
    "comments_url": "https://news.ycombinator.com/item?id=46355949",
    "comments": [
      "I found the most interesting part of the NIST outage post [1] is NIST's special Time Over Fiber (TOF) program [2] that \"provides high-precision time transfer by other service arrangements; some direct fiber-optic links were affected and users will be contacted separately.\"I've never heard of this! Very cool service, presumably for \u2026 quant / HFT / finance firms (maybe for compliance with FINRA Rule 4590 [3])? Telecom providers synchronizing 5G clocks for time-division duplexing [4]? Google/hyperscalers as input to Spanner or other global databases?Seriously fascinating to me -- who would be a commercial consumer of NIST TOF?[1] https://groups.google.com/a/list.nist.gov/g/internet-time-se...[2] https://www.nist.gov/pml/time-and-frequency-division/time-se...[3] https://www.finra.org/rules-guidance/rulebooks/finra-rules/4...[4] https://www.ericsson.com/en/blog/2019/8/what-you-need-to-kno...reply",
      "I never saw a need for this in HFT. In my experience, GPS was used instead, but there was never any critical need for microsecond accuracy in live systems. Sub-microsecond latency, yes, but when that mattered it was in order to do something as soon as possible rather than as close as possible to Wall Clock Time X.Still useful for post-trade analysis; perhaps you can determine that a competitor now has a faster connection than you.The regulatory requirement you linked (and other typical requirements from regulators)  allows a tolerance of one second, so it doesn't call for this kind of technology.reply",
      "> I never saw a need for this in HFT. In my experience, GPS was used instead, but there was never any critical need for microsecond accuracy in live systems.mifid ii (uk/eu) minimum is 1us granularityhttps://eur-lex.europa.eu/legal-content/EN/TXT/?uri=uriserv:...reply",
      "It's 1 us granularity, which means you should report your timestamps with six figures after the decimal point.The required accuracy (Tables 1 and 2 in that document) is 100 us or 1000 us depending on the system.reply",
      "> The required accuracy (Tables 1 and 2 in that document)no, Tables 1 and 2 say divergence, not accuracyaccuracy is a mix of both granularity and divergenceregardless, your statement before:> The regulatory requirement you linked (and other typical requirements from regulators) allows a tolerance of one second, so it doesn't call for this kind of technology.is not truereply",
      "My guess would be scientific experiments where they need to correlate or sequence data over large regions. Things like correlating gravitational waves with radio signals and gamma ray bursts.reply",
      "those are GPS based too. You typically would have a circuit you trained off off 1PPS and hopefully had a 10 or so satellites in view.You can get 50ns with this. Of course, you would verify at NIST.reply",
      "> a commercial consumerWhere does it say these are commercial consumers?https://en.wikipedia.org/wiki/Schriever_Space_Force_Base#Rol...> Building 400 at Schriever SFB is the main control point for the Global Positioning System (GPS).reply",
      "> I've never heard of this! Very cool service, presumably for \u2026 quant / HFT / finance firms (maybe for compliance with FINRA Rule 4590 [3])?To start with, probably for scientific stuff, \u00e0 la:* https://en.wikipedia.org/wiki/White_Rabbit_ProjectBut fibre-based time is important in case of GNSS time signal loss:* https://www.gpsworld.com/china-finishing-high-precision-grou...reply",
      "SIGINT as a source clock for others in a network doing super accurate TDOA for example.reply"
    ],
    "link": "https://www.jeffgeerling.com/blog/2025/nist-was-5-\u03bcs-utc-after-last-weeks-power-cut",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: C-compiler to compile TCC for live-bootstrap (github.com/fransfaase)",
    "points": 20,
    "submitter": "fjfaase",
    "submit_time": "2025-12-17T23:34:51 1766014491",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=46307099",
    "comments": [
      "Bootstapping is so complex, here's 181-step document to compile Linux: from \"commented opcodes\" in hex0 to assembler with labels, from simple C compiler to ~c89 compiler to tinycc ~c99 compiler, from simple shell to scheme interpreter to Fiwix unix-like kernel.https://github.com/fosslinux/live-bootstrap/blob/master/part...reply",
      "At this point I believe running Common Lisp/Scheme from SectorLisp wouldn't be that far offreply",
      "don't threaten me with a good timereply"
    ],
    "link": "https://github.com/FransFaase/MES-replacement",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Investigation into replacing the MES compiler\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.The goal of this project is to simplify stage0 of\nlive-bootstrap,\nwhich involves implementing a replacement for the\nGNU Mes compiler\nby implementing a C-compiler in C that can compile the\nTiny C Compiler version 0.9.26.The motivation for this project is given in the presentation\nReviewing live-bootstrap.For blog article related to reviewing the live bootstrap project and this\nproject see the section 'Live-bootstrap' on this page.The first stage of this project is to implement said C-compiler for i386.\nThe source of the C-compiler is the file tcc_cc.c.\nThis compiler produces intermediate code for a stack based language\ncalled Stack-C. The compiler also includes the f"
  },
  {
    "title": "Scaling LLMs to Larger Codebases (kierangill.xyz)",
    "points": 205,
    "submitter": "kierangill",
    "submit_time": "2025-12-22T15:38:37 1766417917",
    "num_comments": 85,
    "comments_url": "https://news.ycombinator.com/item?id=46354970",
    "comments": [
      "As the models have progressively improved (able to handle more complex code bases, longer files, etc) I\u2019ve started using this simple framework on repeat which seems to work pretty well at one shorting complex fixes or new features.[Research] ask the agent to explain current functionality as a way to load the right files into context.[Plan] ask the agent to brainstorm the best practices way to implement a new feature or refactor. Brainstorm seems to be a keyword that triggers a better questioning loop for the agent. Ask it to write a detailed implementation plan to an md file.[clear] completely clear the context of the agent \u2014- better results than just compacting the conversation.[execute plan] ask the agent to review the specific plan again, sometimes it will ask additional questions which repeats the planning phase again. This loads only the plan into context and then have it implement the plan.[review & test] clear the context again and ask it to review the plan to make sure everything was implemented. This is where I add any unit or integration tests if needed. Also run test suites, type checks, lint, etc.With this loop I\u2019ve often had it run for 20-30 minutes straight and end up with usable results. It\u2019s become a game of context management and creating a solid testing feedback loop instead of trying to purely one-shot issues.reply",
      "As of Dec 2025, Sonnet/Opus and GPTCodex are both trained and most good agent tools (ie. opencode, claude-code, codex) have prompts to fire off subagents during an exploration (use the word explore) and you should be able to Research without needing the extra steps of writing plans and resetting context. I'd save that expense unless you need some huge multi-step verifiable plan implemented.The biggest gotcha I found is that these LLMs love to assume that code is C/Python but just in your favorite language of choice. Instead of considering that something should be written encapsulated into an object to maintain state, it will instead write 5 functions, passing the state as parameters between each function. It will also consistently ignore most of the code around it, even if it could benefit from reading it to know what specifically could be reused. So you end up with copy-pasta code, and unstructured copy-pasta at best.The other gotcha is that claude usually ignores CLAUDE.md. So for me, I first prompt it to read it and then I prompt it to next explore. Then, with those two rules, it usually does a good job following my request to fix, or add a new feature, or whatever, all within a single context. These recent agents do a much better job of throwing away useless context.I do think the older models and agents get better results when writing things to a plan document, but I've noticed recent opus and sonnet usually end up just writing the same code to the plan document anyway. That usually ends up confusing itself because it can't connect it to the code around the changes as easily.reply",
      "Interesting, for me they almost always assume/write TS.reply",
      ">Instead of considering that something should be written encapsulated into an object to maintain state, it will instead write 5 functions, passing the state as parameters between each function.Sounds very functional, testable, and clean. Sign me up.reply",
      "I know this is tongue in cheek, but writing functional code in an object oriented language, or even worse just taking a giant procedural trail of tears and spreading it across a few files like a roomba through a pile of dog doo is ... well.. a code smell at best.I have a user prompt saved called clean code to make a pass through the changes and remove unused, DRY and refactor - literally the high points of uncle bob's Clean Code. It works shockingly well at taking AI code and making it somewhat maintainable.reply",
      "Does its output follow the \"no comments needed\" principle of the uncle Bob?reply",
      "Care to share the prompt? Sounds useful!reply",
      "This is effectively what I'm doing, inspired by HumanLayer's Advanced Context Engineering guidelines: https://github.com/humanlayer/advanced-context-engineering-f...We've taken those prompts, tweaked them to be more relevant to us and our stack, and have pulled them in as custom commands that can be executed in Claude Code, i.e. `/research_codebase`, `/create_plan`, and `/implement_plan`.It's working exceptionally well for me, it helps that I'm very meticulous about reviewing the output and correcting it during the research and planning phase. Aside from a few use cases with mixed results, it hasn't really taken off throughout our team unfortunately.reply",
      "I don't do any of that. I find with GitHub copilot and Claude sonnet 4.5 if I'm clear enough about the what and where it'll sort things out pretty well, and then there's only reiteration of code styling or reuse of functionality. At that point it has enough context to keep going. The only time I might clear that whole thing is if I'm working on an entirely new feature where the context is too large and it gets stuck in summarising the history. Otherwise it's good. But this in codespaces. I find the Tasks feature much harder. Almost a write-off when trying to do something big. Twice I've had it go off on some strange tangent and build the most absurd thing. You really need to keep your eyes on it.reply",
      "Yeah I found that for daily work, current models like Sonnet/Opus 4.5, Gemini 3.0 Pro (and even Flash) work really well without planning as long as I divide and conquer larger tasks into smaller ones. Just like I would do if I was programming myself.For planning large tasks like \"setup playwright tests in this project with some demo tests\" I spend some time chatting with Gemini 3 or Opus 4.5 to figure out the most idiomatic easy-wins and possible pitfalls. Like: separate database for playwright tests. Separate users in playwright tests. Skipping login flow for most tests. And so on.I suspect that devs who use a formal-plan-first approach tend to tackle larger tasks and even vibe code large features at a time.reply"
    ],
    "link": "https://blog.kierangill.xyz/oversight-and-guidance",
    "first_paragraph": "Where to focus investments to best leverage AI toolingThis is the third part of a series on LLMs in software engineering.First we learned what LLMs and genetics have in common. (part 1) LLMs don't improve all facets of engineering. So, understanding which areas LLMs do improve (part 2) is important for knowing how to focus our investments. (part 3)How do we scale LLMs to larger codebases? Nobody knows yet. But by understanding how LLMs contribute to engineering, we realize that investments in guidance and oversight are worthwhile.When an LLM can generate a working high-quality implementation in a single try, that is called one-shotting. This is the most efficient form of LLM programming.The opposite of one-shotting is rework. This is when you fail to get a usable output from the LLM and must manually intervene.2 This often takes longer than just doing the work yourself.So how do we create more opportunities for one-shotting? Better guidance.LLMs are choice generators. Every set of toke"
  },
  {
    "title": "Lotusbail npm package found to be harvesting WhatsApp messages and contacts (koi.ai)",
    "points": 204,
    "submitter": "sohkamyung",
    "submit_time": "2025-12-22T22:35:45 1766442945",
    "num_comments": 132,
    "comments_url": "https://news.ycombinator.com/item?id=46359996",
    "comments": [
      "Just to talk about a different direction here for a second:Something that I find to be a frustrating side effect of malware issues like this is that it seems to result in well-intentioned security teams locking down the data in apps.The justification is quite plausible -- in this case WhatsApp messages were being stolen! But the thing is... that if this isn't what they steal they'll steal something else.Meanwhile locking down those apps so the only apps with a certain signature can read from your WhatsApp means that if you want to back up your messages or read them for any legitimate purpose you're now SOL, or reliant on a usually slow, non-automatable UI-only flow.I'm glad that modern computers are more secure than they have been, but I think that defense in depth by locking down everything and creating more silos is a problem of its own.reply",
      "The OS should be mediating such access where it explicitly asks your permission for an app to access data belonging to another publisher.reply",
      "I could certainly see the value in this in principle but sadly the labyrinthine mess that is the Apple permission system (in which they learned none of the lessons of early UAC) illustrates the kind of result that seems to arise from this.A great microcosm illustration of this is automation permission on macOS right now: there's a separate allow dialog for every single app. If you try to use a general purpose automation app it needs to request permission for every single app on your computer individually the first time you use it. Having experienced that in practice it... absolutely sucks.At this point it makes me feel like we need something like an async audit API. Maybe the OS just tracks and logs all of your apps' activity and then:1) You can view it of course.2) The OS monitors for deviations from expected patterns for that app globally (kinda like Microsoft's SmartScreen?)3) Your own apps can get permission to read this audit log if you want to analyze it your own way and/or be more secure. If you're more paranoid maybe you could use a variant that kills an app in a hurry if it's misbehaving.Sadly you can't even implement this as a third party thing on macOS at this point because the security model prohibits you from monitoring other apps. You can't even do it with the user's permission because tracing apps requires you to turn SIP off.reply",
      "This just sounds like another security nightmare.We're not in 1980 anymore. Most people need zero, and even power users need at most one or two apps that need that full access to the disk.In macOS, for example, the sandbox and the file dialog already allow opening any file, bundle or folder on the disk. I haven't really come across any app that does better browsing than this dialog, but if there's any, it should be a special case. Funny enough, WhatsApp on iOS is an app that reimplements the photo browser, as a dark pattern to force users to either give full permission to photos or suffer.The only time where the OS file dialog becomes limited is when a file is actually \"multiple files\". Which is 1) solvable by bundles or folders and 2) a symptom of developers not giving a shit about usability.reply",
      "> Maybe the OS just tracks and logs all of your apps' activityThe problem here, is that like so many social-media apps, the first thing the app will do is scrape as much as it possibly can from the device, lest it lose access later, at which point auditing it and restricting its permissions is already too late.Give an inch, and they\u2019ll take a mile. Better to make them justify every millimetre instead.reply",
      "This sounds great on paper, but what happens when the OS isn't working for the user like Windows?reply",
      "Switch OS.reply",
      "I mean this was an app for accessing WhatsApp data, you would approve it and go on... the problem is with it sending data off to a 3rd party.reply",
      "I think you miss understood. If the OS becomes the arbiter of what can and cannot be accessed; it's a slippery slope to the OS becoming a walled garden that only approved apps and developers are allowed to operate. Of course that is a pretty large generalization, but we already see it with mobile devices and are starting to see it with windows and Mac OS.I don't think we should be handing more power to OS makers and away from users. There has to be a middle ground between wall gardens and open systems. It would be much better for node & npm to come up with a solution than locking down access.reply",
      "The arbiter of what can be accessed should be the user, and always the user. The OS should be merely the enforcer.Currently OSs are a free-for-all, where the user must blindly trust third-party apps, or they enforce it clumsily like in macOS.This was fine in 1980 but isn't anymore.reply"
    ],
    "link": "https://www.koi.ai/blog/npm-package-with-56k-downloads-malware-stealing-whatsapp-messages",
    "first_paragraph": "BackTuval Admoni,,December 21, 2025IntroThe lotusbail npm package presents itself as a WhatsApp Web API library - a fork of the legitimate @whiskeysockets/baileys package. With over 56,000 downloads and functional code that actually works as advertised, it's the kind of dependency developers install without a second thought. The package has been available on npm for 6 months and is still live at the time of writing.Behind that working functionality: sophisticated malware that steals your WhatsApp credentials, intercepts every message, harvests your contacts, installs a persistent backdoor, and encrypts everything before sending it to the threat actor's server.What gets captured:Most malicious npm packages reveal themselves quickly - they're typosquats, they don't work, or they're obviously sketchy. This one actually functions as a WhatsApp API. It's based on the legitimate Baileys library and provides real, working functionality for sending and receiving WhatsApp messages.Obvious malwa"
  },
  {
    "title": "Cecot \u2013 60 Minutes (archive.org)",
    "points": 118,
    "submitter": "lawlessone",
    "submit_time": "2025-12-23T00:36:27 1766450187",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=46361024",
    "comments": [
      "I'm reminded of the Letter on Justice and Open Debate[1] that Bari Weiss signed only a few years ago, now she's spiking stories like this one on CECOT for showing the current administration in a negative light.I also wonder if this story will get the type of leeway to stay on HN to collect the 200+ upvotes and 300+ comments of that previous example or if it will be flagged off the front page within minutes like so many other similar stories.[1] - https://news.ycombinator.com/item?id=23759283reply",
      "She was hired following the acquisition of Paramount to do things exactly like this. She's not a journalist.reply",
      "There are other links here as well: https://www.404media.co/archivists-posted-the-60-minutes-cec...reply",
      "404media is shadowbanned from HN for nebulous reasons. The mods should really revisit this policy: they've been doing some great reporting recently.reply",
      "Has there been any mention of reasoning behind it?reply",
      "There's nothing nebulous; there's no workaround for 404media's articles.Tell HN: Paywalls with workarounds are OK; paywall complaints are off topic - https://news.ycombinator.com/item?id=10178989 - Sept 2015 (160 comments)reply",
      "It seems to work for me? https://archive.ph/sr0sdreply",
      "I have a feeling this will get DMCA-ed off of Internet Archive in an attempt to suppress it. Here's the infohash of the archive.org torrent download for future reference, this should allow the file to be retrieved in any torrent client as long as someone in the world is seeding it still.8105370ed7dba50dc7ec659fd67550569b4dd8a0reply",
      "here it is, in magnet link form:    magnet:?xt=urn:btih:734abc77f48d11c78543c52004b6f57db71d6d92&dn=60minutes-cecotsegment&xl=1483256352&tr=http%3A%2F%2Fbt1.archive.org%3A6969%2Fannounce&tr=http%3A%2F%2Fbt2.archive.org%3A6969%2Fannounce&ws=http://ia601703.us.archive.org/32/items/&ws=http://ia801703.us.archive.org/32/items/&ws=https://archive.org/download/\n\n(exported from my currently-seeding torrent client, then pasted into a separate torrent client, to verify that it works correctly)reply",
      "How long before Hackernews takes this one down?reply"
    ],
    "link": "https://archive.org/details/insidececot",
    "first_paragraph": "Ask the publishers to restore access to 500,000+ books.\n10 Days Left: The year is almost over\u2014help us finish strong in 2025!            \n                By submitting, you agree to receive donor-related emails from the Internet Archive.\n                Your privacy is important to us. We do not sell or trade your information with anyone.\n              \n                          Search the history of more than 1 trillion\n                          web pages.\n                          Capture a web page as it appears now for use as a trusted\n                          citation in the future.Please enter a valid web address\n\n0\n\n        Views      \n13\nFavorites\n\n        Uploaded by\n                  \n            Colin Haskins          \n        \n                  on December 23, 2025\n"
  },
  {
    "title": "Universal Reasoning Model (53.8% pass 1 ARC1 and 16.0% ARC 2) (arxiv.org)",
    "points": 61,
    "submitter": "marojejian",
    "submit_time": "2025-12-22T18:59:22 1766429962",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=46357458",
    "comments": [
      "Sounds like a further improvement in the spirit of HRM & TRM models.Decent comment via x:\nhttps://x.com/r0ck3t23/status/2002383378566303745I continue to be fascinated by these architectures that: \n- Build in recurrence / inference scaling to transformers more natively.\n- Don't use full recurrent gradient traces, and succeed not just despite, but because of that.reply",
      "Interesting. \nInstead of running the model once (flash) or multiple times (thinking/pro) in its entirety, this approach seems to apply the same principle within one run, looping back internally.Instead of big models that \u201cbrute force\u201d the right answer by knowing a lot of possible outcomes, this model seems to come to results with less knowledge but more wisdom.Kind of like having a database of most possible frames in a video game and blending between them instead of rendering the scene.reply",
      "Isn\u2019t this in a sense an RNN built out of a slice of an LLM? Which if true means it might have the same drawbacks, namely slowness to train but also benefits such as an endless context window (in theory)reply",
      "It's sort of an RNN, but it's also basically a transformer with shared layer weights. Each step is equivalent to one transformer layer, the computation for n steps is the same as the computation for a transformer with n layers.The notion of context window applies to the sequence, it doesn't really affect that, each iteration sees and attends over the whole sequence.reply",
      "I'm surprised more attention isn't paid to this research direction, that nobody has tried to generalize it for example by combining the recurrence concept with next token prediction. \nThat said despite the considerable gains this seems to just be some hyperparameter tweaking rather than a foundational improvement.reply",
      "Not just hyper parameter tweaking. Not foundational research either. But rather engineering improvements that compound with each other (conswiglu layers, muon optimizer)reply"
    ],
    "link": "https://arxiv.org/abs/2512.14693",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n"
  },
  {
    "title": "How the RESISTORS put computing into 1960s counter-culture (ieee.org)",
    "points": 30,
    "submitter": "rbanffy",
    "submit_time": "2025-12-17T19:19:00 1765999140",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=46304195",
    "comments": [
      "This is an excerpt from the book \u201cREADME\nA Bookish History of Computing from Electronic Brains to Everything Machines\u201d by W. Patrick McCray.MIT Press: https://mitpress.mit.edu/9780262553483/readme/Amazon: https://www.amazon.com/README-Computing-Electronic-Everythin...reply",
      "Trac64 implementation:https://git.luxferre.top/nntrac/reply",
      "> They borrowed an acoustic coupler\u2014a forerunner of the computer modem\u2014and connected it to a nearby pay phoneThe acoustic coupler is mounted on a modem, and is just the cradle where you rest a handset. The device is not a forerunner of a modem, it is a modem.reply",
      "Almost. A modem sometimes had a phone jack as well as a coupler, for those cases when the handset was hardwired into the phone and the phone was hardwired into the wall.We tapped where we could and we were happy. Bonus points if the rotary phone had a lock on it and you dialed out by pulsing the hangup switch.reply",
      "Web site is still up, resistors.org .  It looks like John and Margy Levine (first generation Resistors) are running it now.  I think Dave Fox (2nd generation I guess) took care of it before.  The linked article looks pretty good.  There were a bunch of paper archives kept around that are probably still interesting.  I don't know who has them now or if they still exist.I didn't know about Trac64 or that Trac even really had the concept of bits.  It was all string operations, including string arithmetic in arbitrary precision, I thought.  But I never used it much.  It could be seen as a weird take on both Forth and Lisp.#(ps,#(rs))reply"
    ],
    "link": "https://spectrum.ieee.org/teenage-hackers",
    "first_paragraph": "Teenage nerds in New Jersey were hacking before the PC and the InternetThe RESISTORS bonded over their shared love of computers and often met at the home of their mentor, Claude Kagan. The group shown here includes Chuck Ehrlich, Gail Warner, Daryl Bailey (at keyboard), and Barry Klein (back to camera).In late April of 1968, a computer conference in Atlantic City, N.J., got off to a rocky start. A strike by telephone operators prevented exhibitors from linking their terminals to off-site computers, as union-sympathetic workers refused to wire up the necessary connections. Companies\u2019 displays were effectively dead.  This article is an adapted excerpt from W. Patrick McCray\u2019s README: A Bookish History of Computing From Electronic Brains to Everything Machines (The MIT Press, 2025).MIT PressBut a small cohort of teenage computer enthusiasts from the Princeton, N.J., area flaunted a clever work-around: They borrowed an acoustic coupler\u2014a forerunner of the computer modem\u2014and connected it to"
  },
  {
    "title": "Satellites reveal heat leaking from largest US cryptocurrency mining center (space.com)",
    "points": 35,
    "submitter": "troglo-byte",
    "submit_time": "2025-12-22T23:23:12 1766445792",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=46360465",
    "comments": [
      "\"leaking\" is the wrong word here - it implies some sort of inefficiency, process which is not working as well as it needs to. Leaky bucket, leaky faucet...That's not the case here, that center is __dumping__ heat into environment - it is by design, all that electricity is being converted into the heat. By design, it's enormous electric heater.reply",
      "Technically it is inefficiency. The electricity should be doing computer things. Heat is wasted electricity. Just there's not much the data centre could do about it.reply",
      "There\u2019s a minimum level of energy consumption (and thus heat) that has to be produced by computation, just because of physics.  However, modern computers generate billions of times more heat than this minimum level.https://en.wikipedia.org/wiki/Landauer's_principlereply",
      "It'd be super fun to take that as an axiom of physics then to see how far upwards one could build from that. Above my skills by far.reply",
      "Heat is not by itself waste. It's what electricity turns into after it's done doing computer things. Efficiency is a separate question - how many computer things you got done per unit electricity turned into heat.reply",
      "I wonder if there's enough heat being produced for it to act as a district heating plant.reply",
      "There absolutely is, but of course it's nonzero cost to capture.reply",
      "Not sure what your point is. With POW inefficiency is by design.reply",
      "You got the point. It's \"by design\" - you've both said it.reply",
      "a home can leak heat to the environment because of bad insulation. a datacenter doesn't leak heat because leaking is normatively bad.reply"
    ],
    "link": "https://www.space.com/space-exploration/satellites/satellites-reveal-heat-leaking-from-largest-us-cryptocurrency-mining-center",
    "first_paragraph": " \"The world needs better ways to understand what's actually happening on the ground.\"\nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nOne of the world's largest Bitcoin mining facilities is seen leaking heat into the environment in a new image captured from orbit by a heat-seeking satellite that was recently released by the U.K.-based company SatVu.The image reveals the thermal footprint of a major Bitcoin-mining data center in Rockdale, Texas, which has been widely criticized for its electricity consumption and carbon footprint.SatVu didn't disclose which specific facility is in the image, but Rockdale is home to the Riot Platforms Bitcoin mine. The facility, considered the largest in the U.S., has an energy consumption of 700 megawatts, requiring about as much electricity as 300,000 homes.The satellite image reveals in a resolution of 11.5 feet (3.5 meters) where and how much heat leaks into the environment from the plant. SatVu "
  },
  {
    "title": "The biggest CRT ever made: Sony's PVM-4300 (homeip.net)",
    "points": 219,
    "submitter": "giuliomagnifico",
    "submit_time": "2025-12-22T12:54:29 1766408069",
    "num_comments": 140,
    "comments_url": "https://news.ycombinator.com/item?id=46353777",
    "comments": [
      "Don't sleep on that Shank Mods video linked at the end, it's insane that he managed to pull that off.He also made a second video (not linked) which shows off more of the actual hardware.https://www.youtube.com/watch?v=Dgkw3uu19V8reply",
      "Personal anecdotes from my early mid teen years1. Touching the circuit board on the back of the CRT tube by mistake trying to troubleshoot image issues, \u201cfortunately\u201d it was a \u201clow\u201d voltage as it was a B&W monitor\u2026.2. Throwing a big big stone to an abandoned next to the trashcan CRT TV while I had it placed normally because it didn\u2019t break when I threw it facing up and the next thing I remember after opening my eyes which I closed from the bang was my friends who were further down the road looking at me as it I were a ghost since big big chunks for the CRT glass flew just right next to me.CRTs were dangerous in many aspects!EDIT: I meant to reply to the other thread with the dangers of CRTsreply",
      "I'll never forget the feeling of the whoosh when I was working as a furniture mover in the early 2000s and felt the implosion when a cardboard box collapsed and dumped a large CRT TV face-down on the driveway, blowing our hair back. When the boss asked what happened to the TV, I said it fell, and our lead man (who had set it on the box) later thanked me for putting it so diplomatically.That was nothing compared to the time the CAT scan machine fell face down off the lift gate on the back of the delivery truck because our driver pushed the wrong button and tipped it instead of lowering it, but I missed the flack from that because I was on a move somewhere thankfully. Afterwords he was forever known as the quarter million dollar man.reply",
      "Oof, a whole CAT scanner is insane! Did insurance cover it?reply",
      "I still have a piece of glass in back of the palm of my right hand. Threw a rock at an old CRT and it exploded, after a couple of hours I noticed a little blood coming out of that part of hand. Many, many years later was doing xray for a broken finger and doctor asked what is that object doing there? I shrugged, doc said, well it looks like it's doing just fine, so might as well stay there. How lucky I am to have both eyes.reply",
      "> Touching the circuit board on the back of the CRT tube by mistake trying to troubleshoot image issues, \u201cfortunately\u201d it was a \u201clow\u201d voltage as it was a B&W monitor\u2026.My father ran his own TV repair shop for many years. When I was a teen he helped me make a Tesla coil out of a simple oscillator and the flyback transformer from a scrapped TV. It would make a spark 2 or 3 inches long and could illuminate a florescent light from several feet away. It definitely produced higher voltage than normally exists in a TV, but not orders of magnitude more. The high voltage circuits in CRTs are dangerous as hell.reply",
      "In high school we used them as a high voltage source to make lifters: https://youtu.be/jrfBrrDfdEAreply",
      "It was high voltage but low current. I touched high-voltage circuit in the back of TV accidentally while poking in it as a teen, and while it was quite unpleasant, all it did was burn a hole in the skin of my finger. It eventually healed.reply",
      "I used the CRT HV powersupply to make a little electronic hovering thing back in high school http://jnaudin.free.fr/lifters/main.htmreply",
      "What's wild is this TV was not mass produced, which added to the cost, plus the shipping costs. Not only did he get the TV but he got the premium model too, I think Sony intentionally gave the restaurant that model so they could take some marketing photos, and sure enough, that was it.reply"
    ],
    "link": "https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/",
    "first_paragraph": ""
  },
  {
    "title": "Tc \u2013 Theodore Calvin's language-agnostic testing framework (github.com/ahoward)",
    "points": 12,
    "submitter": "mooreds",
    "submit_time": "2025-12-22T22:03:07 1766440987",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=46359683",
    "comments": [
      "While I can\u2019t really comment on how good this specific implementation is simple diff based testing frameworks like this dont get enough press. My first job involved working on an internal programming language. The test suite was just a ton of statements that were executed top down and their output was compared against a single file. Yes that\u2019s a bit absurd but it worked remarkably well. If they\u2019d bothered to add a bit more structure around it I think it would have been perfect.reply",
      "It's not absurd at all (in my view). A test checks that some obtained result matches the expected result - and if that obtained result is something that got printed out and redirected to a file, and that expected result is something that was produced the same way from a known good run (that was determined to be good by somebody looking at it with their eyes), and the match is performed by comparing the two output files... then there you go.This is how basically all of the useful tests I've written have ended up working. (Including, yes, tests for an internal programming language.) The language is irrelevant, and the target system is irrelevant. All you need to be able to do is run something and capture its output somehow.(You're not wrong to note that the first draft basic approach can still be improved. I've had a lot of mileage from adding stuff: producing additional useful output files (image diffs in particular are very helpful), copying input and output files around so they're conveniently accessible when sizing up failures, poking at test runner setup so it scales will with core count, more of the same so that it's easy to re-run a specific problem test in the debugger - and so on. But the basic principle is always the same: does actual output match expected output, yes (success)/no (fail).)reply",
      "Agreed, this is the default testing methodology I reach for. Other methodologies are useful in some situations, but those are the minority.reply",
      "I wrote https://github.com/mmastrac/clitest because I needed a more complex testing harness for CLI tests that does something similar. It's not exactly the same, but it's definitely in the same universe.One-file-per testcase like `tc` does works, but it tends to fall apart a bit at large scale in my experience.reply"
    ],
    "link": "https://github.com/ahoward/tc",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        tc - theodore calvin's language-agnostic testing framework \ud83d\ude81 | simple, portable, unix-style testing for any language\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.language-agnostic testing for unix hackers\n\n\ntheodore \"tc\" calvin - helicopter pilot, testing framework namesake, legend\nWhat: Language-agnostic test framework. Write tests once, run against any language (bash, python, rust, go, whatever).How: Tests are directories. Your code reads input.json from stdin, writes expected.json to stdout. That's it.Get Started:That's it. See full docs for advanced features.tc conflicts with the Unix traffic control command. You MUST add this project's tc to your PATH.Verify:tc is a dead-simple testing framework that lets you:simple \u2022 portable \u2022 language-agnostic \u2022 u"
  },
  {
    "title": "Ask HN: How are most people converting HEIC to jpg?",
    "points": 9,
    "submitter": "par",
    "submit_time": "2025-12-19T14:31:57 1766154717",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=46326261",
    "comments": [
      "You can choose on the share sheet, right?I\u2019m referring to this: \n - select something in Photos, then Share.\n - tap \u201cOptions\u201d under \u201c1 photo selected\u201d top of sheet.There first choice there is:Format:Automatic (X)CurrentMost Compatiblereply",
      "this is my workflowreply",
      "I save my iPhone photos to my linux desktop. The default image viewing software in the Gnome 3 window manager can display HEIC images.Conversions on the command line are simple enough with imagemagick. Prior to conversion, I use exiftool to rename the files from IMG_0123.HEIC to the date the image or video was taken, followed by a truncated sha256 digest of the file, e.g. 2025-12-22-1732-f8b7302.HEIC. Otherwise, you can get a nasty condition where the IMG_XXXX filenames collide when consolidated into the same directory.reply",
      "The greybeards would do it with imagemagick, vips, or even ffmpeg. Gives you full control over the quality and you can script it, parallelize it, and more.reply",
      "I use VIPS cli https://www.libvips.orgI think converting HEIC into jpg would be simple  vips copy -i src.hif out.jpg\n\nAlthough I have not used it for a while so not sure it is exactly that. It also does not support batches, you must run one image at a time, but that can be scripted. It also does not support multiple operations, so you might need to use the .v extensions as intermediary between multiple presets(for example sepia + resize + crop).reply",
      "It's just `vips copy src.heic dst.jpg`.reply",
      "ah right, ffmpeg requires the -i, not vips.reply",
      "Hi.ffmpeg -i input.heic output.jpgThis works on all platforms. You can automate it too. I did for a cloud platform.Best of luck.reply",
      "When one eludes my ban ... (see other posts how to ban it inside the phone, but it exists outside (people send me messages)) [0] samples to play with.On the desktop, Preview app (and lots of others) will open and export as ...On the phone (Apple, sometimes you bewilder me), You can convert in Files, not Photos. 1. Save a photo to FILES from camera roll or web (This works with webp, as well) 2. click and hold the THUMBNAIL, do not open the image. 3. Quick Actions -- Convert image. 4. You can now \"save\" the image (open, do not click and hold) to your camera roll.This is BONKERSAs others have noted, \"There's an app to do it\".Worst for me in daily life, when you get info on an image (in the camera roll, pull up on the image) WEBP does not even show as a file type. HEIC does.ios 18, not 26.[0] https://toolsfairy.com/tools/image-test/sample-heic-filesreply",
      "To make it perfectly clear,  no browser support outside Appleworldhttps://caniuse.com/?search=HEICI mostly am a DSLR photographer but for the occasional iPhone shothttps://mastodon.social/@UP8/115740936297822037I use Photoshop.  (Where's Cindy when I need her?)reply"
    ],
    "link": "item?id=46326261",
    "first_paragraph": ""
  },
  {
    "title": "There Is No Future for Online Safety Without Privacy and Security (itsfoss.com)",
    "points": 40,
    "submitter": "abdelhousni",
    "submit_time": "2025-12-22T22:43:22 1766443402",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=46360067",
    "comments": [
      "Let me be blunt with what I think happened here.For years, technical people insisted it was the parent\u2019s job to monitor Internet safety. Parents, especially with the advent of social media, hardcore pornography, and every childhood friend having a device, correctly said this was unreasonable and impossible.Technical people had a chance for two decades to solve this problem on their terms. Instead we collectively decided that anyone articulating this point of view must be a morally panicked maniac, and that this is a problem with \u201cno reasonable solutions\u201d that we would tolerate. Now we don\u2019t get to dictate the terms, because everyone has had enough, and nobody has patience left for kids watching BDSM on their friend\u2019s phones at 12.Because of our industry\u2019s refusal to take those concerns seriously, we lost our voice, we lost the grounds of sounding reasonable, and the floodgates are now open - for everything. Nobody is listening anymore to our point of view and arguably correctly so.reply",
      "> watching BDSM on their friend\u2019s phones at 12.\n\nWhy does a 12 year old have a smartphone that's connected to the internet?When I was 12 our computer was in the living room and shared by everyone. It could access porn but you had to wait till you were the only one in the home.There's a pretty simple solution here if you don't already see it and I'm not sure why it isn't more acceptable. It solves all the problems you mention. You don't want kids being sucked into social media? Sucked into porn? Constantly staring at their screens?Have you ever considered not giving your kids smartphones?Or have you decided the benefits are worth the costs?Let's be honest here, even with strong government intervention this is always a cat and mouse game. The play of \"no smartphone\" is going to be far stronger than anything the government can ever do. Why is this not an option?reply",
      "This frames the issue in a fundamentally incorrect way.Since the dawn of pseudoanonymous communication, politicians have been trying to get their nasty little claws into it. See Clipper Chip in the 90's. They've tried many avenues to deanonymize and centralize. Going after the parents is just their latest - they've discovered they could use convincing language like this to trick a bunch of people who previously had no reason to care about The Internet to now suddenly \"realize\" oh gosh it's scary out there, what can we do to help.Unfortunately their latest tactic is working. They figured out how to recruit a (possibly) well-intentioned bloc into supporting efforts that undermine privacy in an irreversible way.> Because of our industry\u2019s refusal to take those concerns seriously, we lost our voice,Fighting against demands to censor, unmask, and neuter the closest thing we've got to a global platform of freedom is a valiant effort. Not entertaining these bureaucrats isn't some moral failing of our industry, in the same sense that ignoring a persistent busker on the street entitles him to your money after some uninvolved observer has arbitrarily decided he's made the same demand enough that somehow it's starting to make sense because the victim hasn't yelled at him with a good enough argument against it.In other words: yep, still the parents' job, yep, internet was still there when I grew up, yep, I turned out fine, yep, politicians have been trying to take away our privacy for 30 years (and unfortunately, they're finding more creative and convincing ways to disguise it). Hint: it's never about the kidsreply",
      "The reality is those parents have tried nothing and are all out of ideas, or, in the vast majority of cases, simply don't care.This is not at all about children by the way, because all millenials have grown while consuming porn and social media, and haven't turned into degenerates. It is 100% an excuse to spy on citizens, and nothing else.reply",
      "While we\u2019re posting honest opinions that run against the consensus on this site:In my view, freedom of speech is a natural right that no government can take away. But no such guarantee exists that you can do so anonymously.Now before you immediately flame me to death, please read a bit further:We already have a hodgepodge of laws in basically every jurisdiction around logging IP addresses, cooperating with law enforcement when there\u2019s a warrant, being able to track down who is (say) organizing violence, posting csam, etc. Like it or not, the government is entitled to search and seizure if there is a warrant signed by a judge to do so, and if you run an online service where can people can post things publicly, you better damn well keep logs of who\u2019s posting things and you better cooperate if a law enforcement officer with a warrant asks you to.So what I propose is that we streamline all of this. At age 16 you get a digital ID that works something like a FIDO chip that can be used to prove your identity to a government authentication server. Sub in/out whatever tech you want, it can be a passkey (blech), something resembling a yubikey, etc.  You get them at your local post office, where you can actually prove your identity in person. There\u2019s post offices everywhere, and they\u2019re already meant to serve everyone in the country.But critically, this key isn\u2019t used to auth to any sites except a government-run signin service. The service itself would be a modified form of OAuth/OIDC that preserves privacy from the site you\u2019re making an account on. They don\u2019t know who you are, they just get a signed payload from the government signin site saying \u201cthis is a user over the age of 16\u201d, and via a pre-established relationship between the website in question and the government auth site, a UUID is minted for that legal person.  It will be the same UUID for that website for the same person, so you can\u2019t just pretend you\u2019re multiple people when you create multiple accounts.With this system, and using Reddit as an example site that may leverage this:- Reddit can\u2019t know who you actually are, they only get a UUID and a signed payload indicating you\u2019re over 16 (or whatever other set of properties are legally salient for the account.) In the event of a breach, all you\u2019d get is a list of Reddit-specific UUID\u2019s. You\u2019d have to also hack the government auth service to know who these people actually are.- The government doesn\u2019t know who owns your username on Reddit, they only know the list of citizens that have Reddit accounts at all.- In the event of a crime with a warrant, the government can compel Reddit to inform them which UUID corresponds to some account. Reddit continues to not know who the account belongs to.- Every site using this system gets a completely different UUID for the same legal person and has no ability to correlate them- Every legal person using this system has no idea what their UUID is for any site- Every site using this doesn\u2019t have to worry at all about proving identity. They get working auth for every legal person in $country, streamlining signups and onboarding, and doesn\u2019t have to worry about asking the user to prove they\u2019re over 16.- You still get pseudo-anonymity in that you can use an alias (as many as the site allows, too), the site can remain blissfully ignorant as to who you really are, as well as everyone who reads your posts, etc.- The government can find out who owns an account, but only with a warrant. They don\u2019t have a list of account/UUID mappings anywhere.This system is probably the most closely aligned to how I would do things if I was somehow \u201cin charge\u201d\u2026 you have a right to pseudo-anonymity, but you don\u2019t have a right to cover your tracks so thoroughly that the government can\u2019t track you down with a proper warrant.With such a system, saying \u201csocial media is for ages 16 and up\u201d is a simple checkbox in the signup flow. Done.You can argue all day about whether a government should be able to uncloak your accounts with a warrant, but to me that question is already settled: yes, they absolutely can do that, they do so today all the time. Except today we have messy data breaches where everyone\u2019s identity gets leaked because every site has to reinvent their own form of proving your legal identity (in the case of Facebook/etc) or simply proving you\u2019re a certain age (uploading ID, etc.). I\u2019d take a centralized government-run approach to what we have today any day.You could ask \u201cbut should government be in the business of electronic authentication and identity?\u201d And my answer is: \u201cYES.\u201d It\u2019s basically the primary function of a working government! We trust them to issue passports for chrissake. To me this is basic table stakes in the 21st century. If we did government all over again, having the government provide a service to prove online identity is basically right up there with \u201ccollects tax revenue.\u201dNow, in the current government up to the challenge of doing this, and not fucking it up? Yeeesh, probably not. You got me there. But one can dream\u2026reply",
      "Treating users like adults and allowing them full control over setting system capability and app launch restrictions on devices (and even implementing fully optional, widely blocking restrictions as \"parental safety options\") was the industry taking it seriously. You considering the freedom of choice to the user as disastrous and the lack of heavily restricted lockdowns by default as \"refusal to take concerns seriously\" is just a reflection on your attitudes about other people, not any real argument for improving privacy or safety. You ARE a morally panicked maniac if your only grossly offensive things you want to keep pretending are horrifying examples have absolutely nothing to do with the invasion of privacy of children and more to do with puritanical outrage on children accessing adult material.reply",
      "I guess, but there's a philosophy that goes something along the lines of:  Government is not the answer to every problem.  It's a very divisive issue, clearly.  Technical people were never going to solve the problem of kids having access to adult materials online because tech people are only interested in making gobs of money, not in dealing with delicate social issues.  Government isn't going to fix anything, either, mind you, because the moment the crackdowns begin, the technologists will walk right in with a solution.  Maybe we'll finally have the distributed, peer-to-peer network we always thought we were getting but instead got centrally controlled nodes featuring mass surveillance.I stopped worrying about it as a parent.  My kids want to look at porn?  Let them.  If they want to see horrible, violent shit that gives them nightmares and they can't un-see it, let them.  They'll either figure it out or they will be lifetime children looking for big daddy government to solve all their stupid problems for them and society will collapse.reply",
      "Do you have kids? Do you give them unfiltered Internet access? Do you have any experience with PTSD caused by viewing the worst output of humanity?reply",
      "I don't know whats more damaging - watching some beheading video when 12 or having controlling parents that do not leave you any autonomy even in the digital space.Personally i prefer the beheading video as its not far from whats already on TV.reply",
      "It's still moralist pearl-clutching in service of totalitarian horseshit no matter how you'd like to justify it.I don't even think you're strictly 100% incorrect here. That said, I refuse to entertain the \"think of the children\" horseshit when parents happily park their kid in front of an iPad for hours a day because it shuts them up, not with kink content they shouldn't see, but with AI/algorithmically generated garbage, or for that matter, human generated garbage, that rots their brains far more than any gimp costume ever could.For fucks sake, 12 kids in America die PER DAY due to mass shootings, and we can't even pass common sense gun regulations that the vast majority of gun owners are completely fine with. We don't give a fuck about our kids here. This has from jump, and continues to be, astro-turfed puritan whining not merely to pornography of whatever preferred kind of the moment, but to the existence of queer people as a whole.It IS parents' fucking responsibility, and maybe that is an onerous burden, but instead of even attempting to meet it, the majority of parents have abdicated it. And it isn't porn fucking their kids up, it's non-stop screen exposure leaving them with no attention span and no ability to simply BE BORED.reply"
    ],
    "link": "https://itsfoss.com/news/alexander-linton-interview/",
    "first_paragraph": "Session is an open source encrypted messaging app that requires no phone number or email address to sign up. Instead of routing messages through centralized servers, Session uses a decentralized network of over 2,000 nodes running the onion routing protocol, similar to Tor, ensuring that no single server knows both the message origin and destination.The Session Technology Foundation took over stewardship of Session back in October 2024, succeeding the Australia-based Oxen Privacy Tech Foundation (OPTF).The transition wasn't purely administrative; it was triggered by Australian authorities' probes into Session's operations and the threat of anti-encryption laws that could compel backdoors.Alexander Linton, who worked as a journalist before joining the Session project, now serves as President of the Session Technology Foundation.In an email interview, we discussed his transition from journalism to privacy advocacy, Session's approach to trust and safety without centralized moderation, an"
  }
]