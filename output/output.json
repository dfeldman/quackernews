[
  {
    "title": "How I use Claude Code: Separation of planning and execution (boristane.com)",
    "points": 93,
    "submitter": "vinhnx",
    "submit_time": "2026-02-22T00:29:05 1771720145",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=47106686",
    "comments": [
      "I go a bit further than this and have had incredible success with 3 doc types and 2 skills:- Specs: these are generally static, but updatable as the project evolves. And they're broken out to an index file that gives a project overview, a high-level arch file, and files for all the main modules. Roughly ~1k lines of spec for 10k lines of code, and try to limit any particular spec file to 300 lines- Plans: these are the output of a planning session with an LLM. They point to the associated specs. These tend to be 100-300 lines and 3 to 5 phases.- Working memory files: I use both a status.md, which points to a latest plan, and. a project_status, which tracks the current state of the project- A planner skill I use w/ Gemini Pro to generate new plans. It essentially explains the specs/plans dichotomy, the role of the status files, and to review everything in the pertinent areas of code and give me a handful of high-level next set of features to address based on shortfalls in the specs or things noted in the project_status file. Based on what it presents, I select a feature or improvement to generate. Then it proceeds to generate a plan, updates a clean status.md that points to the plan, and adjusts project_status based on the state of the prior completed plan.- An implementer skill in Codex that goes to town on a spec file. It's fairly simple, it just looks at status.md, which points to the plan, and of course the plan points to the relevant specs so it loads up context pretty efficiently.I've tried the two main spec generation libraries, which were way overblown, and then I gave superpowers a shot... which was fine, but still too much. The above is all homegrown, and I have much better success because it keeps the context lean and focused.reply",
      "Looks good. Question - is it always better to use a monorepo in this new AI world? Vs breaking your app into separate repos? At my company we have like 6 repos all separate nextjs apps for the same user base. Trying to consolidate to one as it should make life easier overall.reply",
      "> Notice the language: \u201cdeeply\u201d, \u201cin great details\u201d, \u201cintricacies\u201d, \u201cgo through everything\u201d. This isn\u2019t fluff. Without these words, Claude will skim. It\u2019ll read a file, see what a function does at the signature level, and move on. You need to signal that surface-level reading is not acceptable.This makes no sense to my intuition of how an LLM works. It's not that I don't believe this works, but my mental model doesn't capture why asking the model to read the content \"more deeply\" will have any impact on whatever output the LLM generates.reply",
      "It\u2019s actually really common. If you look at Claude Code\u2019s own system prompts written by Anthropic, they\u2019re littered with \u201cCRITICAL (RULE 0):\u201d type of statements, and other similar prompting styles.reply",
      "Its a wild time to be in software development. Nobody(1) actually knows what causes LLMs to do certain things, we just pray the prompt moves the probabilities the right way enough such that it mostly does what we want. This used to be a field that prided itself on deterministic behavior and reproducibility.Now? We have AGENTS.md files that look like a parent talking to a child with all the bold all-caps, double emphasis, just praying that's enough to be sure they run the commands you want them to be running(1 Outside of some core ML developers at the big model companies)reply",
      "these sort-of-lies might help:think of the latent space inside the model like a topological map, and when you give it a prompt, you're dropping a ball at a certain point above the ground, and gravity pulls it along the surface until it settles.caveat though, thats nice per-token, but the signal gets messed up by picking a token from a distribution, so each token you're regenerating and re-distorting the signal. leaning on language that places that ball deep in a region that you want to be makes it less likely that those distortions will kick it out of the basin or valley you may want to end up in.if the response you get is 1000 tokens long, the initial trajectory needed to survive 1000 probabilistic filters to get there.or maybe none of that is right lol but thinking that it is has worked for me, which has been good enoughreply",
      "The disconnect might be that there is a separation between \"generating the final answer for the user\" and \"researching/thinking to get information needed for that answer\". Saying \"deeply\" prompts it to read more of the file (as in, actually use the `read` tool to grab more parts of the file into context), and generate more \"thinking\" tokens (as in, tokens that are not shown to the user but that the model writes to refine its thoughts and improve the quality of its answer).reply",
      "The author is referring to how the framing of your prompt informs the attention mechanism. You are essentially hinting to the attention mechanism that the function's implementation details have important context as well.reply",
      "Yeah, it's definitely a strange new world we're in, where I have to \"trick\" the computer into cooperating. The other day I told Claude \"Yes you can\", and it went off and did something it just said it couldn't do!reply",
      "You bumped the token predictor into the latent space where it knew what it was doing : )reply"
    ],
    "link": "https://boristane.com/blog/how-i-use-claude-code/",
    "first_paragraph": ""
  },
  {
    "title": "Are compilers deterministic? (onepatchdown.net)",
    "points": 32,
    "submitter": "fragmede",
    "submit_time": "2026-02-22T00:21:05 1771719665",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=47106626",
    "comments": [
      "> The computer science answer: a compiler is deterministic as a function of its full input state. Engineering answer: most real builds do not control the full input state, so outputs drift.To me that implies the input isn't deterministic, not the compiler itselfreply",
      "You're not wrong but I think the point is to differentiate between the computer science \"academic\" answer and the engineering \"pragmatic\" answer. The former is concerned about correctly describing all possible behavior of the compiler, whereas the latter is concerned about what the actual experience is when using the compiler in practice.You might argue that this is redefining the question in a way that changes the answer, but I'd argue that's also an academic objection; pragmatically, the important thing isn't the exact language but the intent behind the question, and for an engineer being asked this question, it's a lot more likely that the person asking has context for asking that cares about more than just the literal phrasing of \"are compilers deterministic?\"reply",
      "It matters a lot. For instance, many compilers will put time stamps in their output streams. This can mess up the downstream if your goal is a bit-by-bit identical piece of output across multiple environments.And that's just one really low hanging fruit type of example, there are many more for instance selecting a different optimization path when memory pressure is high and so on.reply",
      "> To me that implies the input isn't deterministic, not the compiler itselfor the system upon which the compiler is built (as well as the compiler itself) has made some practical trade offs.the source file contents are usually deterministic. the order in which they're read and combined and build-time metadata injections often are not (and can be quite difficult to make so).reply",
      "It's not uncommon to have a regression test for compilers that are written in their own language (e.g. some C compilers): compile each new version with itself, then use that to compile itself again, then use the result on unit tests or whatever, which should yield the same results as before.The point being that determinism of a particular form is expected and required in the instances where they do that.(I'm not arguing for or against that, I'm simply saying I've seen it in real life projects over the years.)reply",
      "> This comes up now as \u201cis vibecoding sane if LLMs are nondeterministic?\u201d Again: do you want the CS answer, or the engineering answer?Determinism would help you.  With a bit of engineering, you could make LLMs deterministic: basically, fix the random seed for the PRNG and make sure none of the other sources of entropy mentioned earlier in the article contribute.But that barely impact any of the issues people bring up with LLMs.reply",
      "Dumb.Compilers aren't deterministic in small ways, timestamps, encoding paths into debug information, etc. These are trivial, annoyances to reproducible build people and little else.You cannot take these trivial reproducibility issues and extrapolate out to \"determinism doesn't matter therefore LLMs are fine\". You cannot throw a ball in the air, determine it is trivial to launch an object a few feet, and thus conclude a trip the moon is similarly easy.The magnitude matters, not merely the category. Handwaving magnitude is a massive red flag a speaker has no idea what they're talking about.reply",
      "And that result of that magnitude is the paradigm of operation is just completely different. Good programmers create inputs, check outputs, and build up a mental model of the system. When the input -> output is not well defined you can't use those same skills.reply",
      "If the output has problems, do you usually rerun the compilation with the same input (that you control)? I don't usually.What is included in the 'verify' step? Does it involve changing the generated code? If not, how do you ensure things like code quality, architectural constraints, efficiency and consistency? It's difficult, if not (economically) impossible, to write tests for these things. What if the LLM does not follow the guidelines outlined in your prompt? This is still happening. If this is not included, I would call it 'brute forcing'. How much do you pay for tokens?reply",
      "I\u2019ve felt like a good response to the vibe coding thing is that customers, product managers, etc ask for features and don\u2019t read the code. You don\u2019t need to read the code of something to build a level of trust about what it does and whether that matches your expectations. It is not that wild that you can have a setup where you get an application and without reading the code decide if it solves your problem to your satisfaction.reply"
    ],
    "link": "https://blog.onepatchdown.net/2026/02/22/are-compilers-deterministic-nerd-version/",
    "first_paragraph": ""
  },
  {
    "title": "Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU (github.com/xaskasdf)",
    "points": 100,
    "submitter": "xaskasdf",
    "submit_time": "2026-02-21T20:57:30 1771707450",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=47104667",
    "comments": [
      "0.2 tok/s is fine for experimentation, but it is not interactive in any meaningful sense. For many use cases, a well-quantized 8B or 13B that stays resident will simply deliver a better latency-quality tradeoffreply",
      "I can imagine a couple scenarios in which a high-quality, large model would be much preferred over lower latency models, primarily when you need the quality.reply",
      "yeah, actually I wanted to see if this was possible at all. I managed to get around 3000 tokens/s on a ps2 with classic transformers, since the emotion engine is capable of 32 bit addresses, but it has like 32gb of ram. So I ran into the question of why was that fast and I couldn't get that speed even with small models, and the deal is that the instructions went right of the memory to the gpu and that's the main difference that does when a regular computer does inference: it has to request the instructions to the cpu every time. As I mentioned too, on professional cards you can avoid these problems naturally, since they got instructions precisely for this, but sadly I don't have 30k bucks to spare on a gpu :(reply",
      "*32MB of RAM (plus 4MB of video RAM and a little sound and IOP memory).reply",
      "I didn't really understand the performance table until I saw the top ones were 8B models.But 5 seconds / token is quite slow yeah. I guess this is for low ram machines? I'm pretty sure my 5950x with 128 gb ram can run this faster on the CPU with some layers / prefill on the 3060 gpu I have.I also see that they claim the process is compute bound at 2 seconds/token, but that doesn't seem correct with a 3090?reply",
      "LLM speed is roughly <memory_bandwidth> / <model_size> tok/s.DDR4 tops out about 27GbsDDR5 can do around 40GbsSo for 70B model at 8 bit quant, you will get around 0.3-0.5 tokens per second using RAM alone.reply",
      "Channels matter a lot, quad channel ddr4 is going to beat ddr5 in dual channel most of the time.reply",
      "Four channels of DDR4-3200 vs two channels of DDR5-6400 (four subchannels) should come out pretty close. I don't see any reason why the DDR4 configuration would be consistently faster; you might have more bank groups on DDR4, but I'm not sure that would outweigh other factors like the topology and bandwidth of the interconnects between the memory controller and the CPU cores.reply",
      "yeah, actually, I'm bottlenecked af since my mobo got pcie3 only :(reply",
      "DRAM speeds is one thing, but you should also account for the data rate of the PCIe bus (and/or VRAM speed). But yes, holding it \"lukewarm\" in DRAM rather than on NVMe storage is obviously faster.reply"
    ],
    "link": "https://github.com/xaskasdf/ntransformer",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        High-efficiency LLM inference engine in C++/CUDA. Run Llama 70B on RTX 3090.\n      High-efficiency C++/CUDA LLM inference engine. Runs Llama 70B on a single RTX 3090 (24GB VRAM) by streaming model layers through GPU memory via PCIe, with optional NVMe direct I/O that bypasses the CPU entirely.3-tier adaptive caching auto-sizes from hardware: VRAM-resident layers (zero I/O) + pinned RAM (H2D only) + NVMe/mmap fallback. Achieves 83x speedup over mmap baseline for 70B on consumer hardware (RTX 3090 + 48 GB RAM).Bottleneck is PCIe H2D bandwidth at Gen3 x8 (~6.5 GB/s). Q4_K_M fits 10 more layers in VRAM (36 vs 26), reducing tier B transfers. Layer skip (cosine similarity calibration) eliminates 20/80 layers per token with minimal quality loss.Running ntransformer with NVMe direct I/O requires system-level modifications. An automated setu"
  },
  {
    "title": "Evidence of the bouba-kiki effect in na\u00efve baby chicks (science.org)",
    "points": 68,
    "submitter": "suddenlybananas",
    "submit_time": "2026-02-21T21:51:58 1771710718",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=47105198",
    "comments": [
      "This is just one micro-instance of a much larger thing. Brain encodes structural similarity across modalities. Corollary: language is far from arbitrary labels for things.reply",
      "No, language is still pretty close to arbitrary labels. The handful of tenuous common threads like the bouba-kiki effect don't change the overall picture that much. The simple fact that language varies as much as it does is sufficient to prove that it's only loosely bound to anything universal.reply",
      "https://en.wikipedia.org/wiki/True_namereply",
      ">language is far from arbitrary labels for thingsI think this is a misunderstanding of the arbitrariness of the sign. Arbitrary doesn't mean \"random\" or \"uniformly sampled.\" The fact there are systematic tendencies among languages in how things are called doesn't negate the arbitrariness of the sign, they could have been called other things. We can also decide to refer to things by another name and we can use any arbitrary name we like! There is no limits on what names we can use (besides silly physiological constraints like having a word with 50 000 consonants). But, of course, there's much more to language than just labels!For me, the interesting thing in this paper vis-\u00e0-vis language is that it shows how much innate structure in cognition must shape our language.reply",
      "Arbitrariness of the sign is a principle that requires so many epicycles to present as \"true\" that it's more of a warning against overgeneralization than an insight with any significant predictive power in its own right.reply",
      "Is this not reducible to whether a speech sound contains fricatives and stops or not? They produce spiky soundsBut I guess it's about why so we associate those with spiky shapes, though surely it's because they represent sharp immediate changes in frequency?I'd be interested on results of shapes imagined when you take the source as musical or other non speech sounds.reply",
      "Preprint: https://www.biorxiv.org/content/10.1101/2024.05.17.594640v1....reply",
      "I think it\u2019s natural to think of this in terms of frequencies so the kiki shape has a higher visual frequency. As does the word have a higher audio frequencies within in than bouba so that is naturally associated with the lower frequency undulating line of that shape.reply",
      "What's the N value of this studyreply",
      "I don\u2019t know, but it really should be in units of N dozen.reply"
    ],
    "link": "https://www.science.org/doi/10.1126/science.adq7188",
    "first_paragraph": ""
  },
  {
    "title": "Parse, Don't Validate and Type-Driven Design in Rust (harudagondi.space)",
    "points": 125,
    "submitter": "todsacerdoti",
    "submit_time": "2026-02-21T19:40:06 1771702806",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=47103931",
    "comments": [
      "Note that the division-by-zero example used in this article is not the best example to demonstrate \"Parse, Don't Validate,\" because it relies on encapsulation. The principle of \"Parse, Don't Validate\" is best embodied by functions that transform untrusted data into some data type which is correct by construction.Alexis King, the author of the original \"Parse, Don't Validate\" article, also published a follow-up, \"Names are not type safety\" [0] clarifying that the \"newtype\" pattern (such as hiding a nonzero integer in a wrapper type) provide weaker guarantees than correctness by construction. Her original \"Parse, Don't Validate\" article also includes the following caveat:> Use abstract datatypes to make validators \u201clook like\u201d parsers. Sometimes, making an illegal state truly unrepresentable is just plain impractical given the tools Haskell provides, such as ensuring an integer is in a particular range. In that case, use an abstract newtype with a smart constructor to \u201cfake\u201d a parser from a validator.So, an abstract data type that protects its inner data is really a \"validator\" that tries to resemble a \"parser\" in cases where the type system itself cannot encode the invariant.The article's second example, the non-empty vec, is a better example, because it encodes within the type system the invariant that one element must exist. The crux of Alexis King's article is that programs should be structured so that functions return data types designed to be correct by construction, akin to a parser transforming less-structured data into more-structured data.[0] https://lexi-lambda.github.io/blog/2020/11/01/names-are-not-...reply",
      "Even the newtype-based \"parse, don't validate\" is tremendously useful in practice, though. The big thing is that if you have a bare string, you don't know \"where it's been\". It doesn't carry with it information whether it's already been validated. Even if a newtype can't provide you full correctness by construction, it's vastly easier to be convinced of the validity of an encapsulated value compared to a naked one.For full-on parse-don't-validate, you essentially need a dependent type system. As a more light-weight partial solution, Rust has been prototyping pattern types, which are types constrained by patterns. For instance a range-restricted integer type could be simply spelled `i8 is 0..100`, or a nonempty slice as `[T] is [_, ..]`. Such a feature would certainly make correctness-by-construction easier in many cases.The non-empty list implemented as a (T, Vec<T>) is, btw, a nice example of the clash between practicality and theoretical purity. It can't offer you a slice (consecutive view) of its elements without storing the first element twice (which requires immutability and that T: Clone, unlike normal Vec<T>), which makes it fairly useless as a vector. It's okay if you consider it just an abstract list with a more restricted interface.reply",
      "You can also search for \"make invalid states impossible/unrepresentable\" [0] to find more info on related practices. See \"domain modeling made functional\" [0] as a nice example[0] https://geeklaunch.io/blog/make-invalid-states-unrepresentab...[1] https://www.youtube.com/watch?v=2JB1_e5wZmUreply",
      "The phrasing that I hear more often is \"make illegal states unrepresentable\"; both the submitted article and Alexis King's original article use this phrase. At least according to https://fsharpforfunandprofit.com/posts/designing-with-types..., it originates from Yaron Minsky (a programmer at Jane Street who is prominent in the OCaml community).EDIT: Parent comment was edited to amend the \"impossible/unrepresentable\" wordingreply",
      "Yes, sorry. I thought to add some resources to it, or it would be a too vague comment and found the better phrasing.reply",
      "Recent and related: Parse, Don't Validate (2019) - https://news.ycombinator.com/item?id=46960392 - Feb 2026 (172 comments)also:Parse, Don\u2019t Validate \u2013 Some C Safety Tips - https://news.ycombinator.com/item?id=44507405 - July 2025 (73 comments)Parse, Don't Validate (2019) - https://news.ycombinator.com/item?id=41031585 - July 2024 (102 comments)Parse, don't validate (2019) - https://news.ycombinator.com/item?id=35053118 - March 2023 (219 comments)Parse, Don't Validate (2019) - https://news.ycombinator.com/item?id=27639890 - June 2021 (270 comments)Parsix: Parse Don't Validate - https://news.ycombinator.com/item?id=27166162 - May 2021 (107 comments)Parse, Don\u2019t Validate - https://news.ycombinator.com/item?id=21476261 - Nov 2019 (230 comments)Parse, Don't Validate - https://news.ycombinator.com/item?id=21471753 - Nov 2019 (4 comments)(p.s. these links are just to satisfy extra-curious readers - no criticism is intended! I add this because people sometimes assume otherwise)reply",
      "The alternative is one type, with many functions that can operate on that type.Like how clojure basically uses maps everywhere and the whole standard library allows you to manipulate them in various ways.The main problem with the many type approach is several same it worse similar types, all incompatible.reply",
      "Yeah, there's something of a tension between the Perlis quote \"It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures\" and Parse, don't validate.The way I've thought about it, though, is that it's possible to design a program well either by encoding your important invariants in your types or in your functions (especially simple functions).  In dynamically typed languages like Clojure, my experience is that there's a set of design practices that have a lot of the same effects as \"Parse, Don't Validate\" without statically enforced types. And, ultimately, it's a question of mindset which style you prefer.reply",
      "There's probably a case for both. Core logic might benefit from hard types deep in the bowels of unchanging engine.The real world often changes though, and more often than not the code has to adapt, regardless of how elegant are systems are designed.reply",
      "Coalton ( https://coalton-lang.github.io ) is the sort of thing I like: a Haskell-style language hosted inside a very dynamic one with good interop.reply"
    ],
    "link": "https://www.harudagondi.space/blog/parse-dont-validate-and-type-driven-design-in-rust/",
    "first_paragraph": "Photo by the Tingley Injury Law Firm.In the Rust Programming Language Community Server, there\u2019s tag named -parse-dont-validate which links to an article about the concept of avoiding validation functions and encoding invariants in the type level instead. I usually recommend it to beginners/intermediates to Rust who are struggling with designing APIs.The only problem is that it uses Haskell to explain its concepts.Yeah, it\u2019s fine, but for beginners unfamiliar with the functional paradigm, it might not be so approachable. And so I wanted so write a blog post about this pattern but in a rather Rust-centric way. So let\u2019s start!One basic example I can give is a function that divides a number by another number.This is fine, but unfortunately it can panic when b has the value of zero:This gives an error:That\u2019s fine and dandy if we want erroneous values to fail loudly at runtime, but what if we want stronger guarantees? This is especially important when some operations don\u2019t fail loudly, like "
  },
  {
    "title": "How far back in time can you understand English? (deadlanguagesociety.com)",
    "points": 363,
    "submitter": "spzb",
    "submit_time": "2026-02-18T14:56:58 1771426618",
    "num_comments": 209,
    "comments_url": "https://news.ycombinator.com/item?id=47061614",
    "comments": [
      "There are towns in England and America where I can't understand them today.reply",
      "In Christian circles some people are KJV-only, only reading from the 1611 KJV. But articles like this demonstrate that languages change dramatically over time.Thus I regard KJV-onlyism to be a passing fad; for if another 400 years passes, the writing in the 1611 will go from being strange to our eyes, to being unreadable in the future by anyone but trained scholars.reply",
      "Should be \"how far back in time can you read English?\" The language itself is what is spoken and the writing, while obviously related, is its own issue. Spelling is conventional and spelling and alphabet changes don't necessarily correspond to anything meaningful in the spoken language; meanwhile there can be large changes in pronunciation and comprehensibility that are masked by an orthography that doesn't reflect them.reply",
      "Indeed, I remember being in Oxford in the 90s and an older man approached me and spoke to me in English and I couldn\u2019t understand a word he said. My ex-wife, who\u2019s an ESL speaker who speaks fluently and without an accent has trouble with English accents in general. Similarly, in Spanish, I find it\u2019s generally easier for me to understand Spanish speakers than Mexican speakers even though I learned Mexican Spanish in school and it\u2019s been my primary exposure to the language. Likewise, I generally have an easier time understanding South American speakers than Caribbean speakers and both sound little like Mexican Spanish. (The Spanish I understand most easily is the heavily accented Spanish of non-native Spanish speakers.)Accents have diverged a lot over time and as I recall, American English (particularly the mid-Atlantic seaboard variety) is closer to what Shakespeare and his cohort spoke than the standard BBC accent employed in most contemporary Shakespeare productions).reply",
      "I live in London, I can drive a little over an hour from where I live and hardly understand the people working at the petrol station. A few more hours and they start to speak French.reply",
      "I have had to interpret between an Ulsterman and a South African, who were both speaking English. I think those accents have vowel shifted in opposite directions.I was also taught a bit of Chaucer (died 1400) in English at school. Although not any of the naughty bits.reply",
      "Having interpreted for a guy speaking with a broad Glaswegian accent on the east coast main line, I can totally believe this.reply",
      "> older man approached me and spoke to me in English and I couldn\u2019t understand a word he saidlike this https://www.youtube.com/watch?v=Hs-rgvkRfwc ?reply",
      "I was expecting the hooligans from Eurotrip.reply",
      "You can try this video to see how far back you can understand spoken English: https://www.youtube.com/watch?v=842OX2_vCicreply"
    ],
    "link": "https://www.deadlanguagesociety.com/p/how-far-back-in-time-understand-english",
    "first_paragraph": ""
  },
  {
    "title": "zclaw: personal AI assistant in under 888 KB, running on an ESP32 (github.com/tnm)",
    "points": 97,
    "submitter": "tosh",
    "submit_time": "2026-02-21T12:37:52 1771677472",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=47100232",
    "comments": [
      "This is a great example of how silly this whole thing is. There\u2019s next to nothing to these claws. Turns out that if you give an llm the ability to call APIs they will.reply",
      "What\u2019s most shocking to me about the whole OpenClaw hype is how little people in tech seem to know about computers\u2026It\u2019s like most of the industry hasn\u2019t ever looked any deeper than their node_modules folder.reply",
      "If it turns out that there is significant value in everyone having their own personal agent running 24/7, we might end up needing a lot more compute than anticipated.(It\u2019s a big if! I\u2019m not convinced about that myself, but it\u2019s worth considering that possibility.)reply",
      "I am using a claw. I am not ready to give it access to much but web and cron plus a chat channel is useful and feels more light touch than typical AI sessions and UIsreply",
      "\"LLM backends: Anthropic, OpenAI, OpenRouter.\"And here I was hoping that this was local inference :)reply",
      "Sure. Why purchase a H200 if you can go with an ESP32 ^^reply",
      "Blowing more than 800kb on essentially an http api wrapper is actually kinda bad. The original Doom binary was 700kb and had vastly more complexity. This is in C after all, so by stripping out nonessential stuff and using the right compiler options, I'd expect something like this to come in under 100kb.reply",
      "Doom had the benefit of an OS that included a lot of low-level bits like a net stack. This doesn\u2019t! That 800kB includes everything it would need from an OS too.reply",
      "yah my back of the envelope math..the \u201capp logic\u201d/wrapper pieces come out to about 25kbWiFi is 350\nTls is 120\nand certs are 90!reply",
      "> vastly more complexity.Doom is ingenious, but it is not terribly complex IMHO, not compared to a modern networking stack including WiFi driver.\nThe Doom renderer charm is in its overall simplicity. The AI is effective but not sophisticated.reply"
    ],
    "link": "https://github.com/tnm/zclaw",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Your personal AI assistant at all-in 888KiB (~25KB in app code). Running on an ESP32. GPIO, cron, memory, and more.\n      The smallest possible AI personal assistant for ESP32.zclaw is written in C and runs on ESP32 boards with a strict all-in firmware budget target of <= 888 KiB on the default build. It supports scheduled tasks, GPIO control, persistent memory, and custom tool composition through natural language.The 888 KiB cap is all-in firmware size, not just app code.\nIt includes zclaw logic plus ESP-IDF/FreeRTOS runtime, Wi-Fi/networking, TLS/crypto, and cert bundle overhead.Fun to use, fun to hack on.\nUse the docs site for complete guides and reference.One-line bootstrap (macOS/Linux):Already cloned?Non-interactive install:Important setup notes:Tested targets: ESP32-C3, ESP32-S3, and ESP32-C6.\nOther ESP32 variants should work"
  },
  {
    "title": "EDuke32 \u2013 Duke Nukem 3D (Open-Source) (eduke32.com)",
    "points": 152,
    "submitter": "reconnecting",
    "submit_time": "2026-02-21T20:10:13 1771704613",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=47104185",
    "comments": [
      "Blast from the past \u2014 I made the EDuke32 logo when I was teenager back in 2004. (I still have the PSD sitting around somewhere...) Back then there was quite an active community on the now defunct 3drealm's forums and I spent a lot of time contributing icons, logos, or web dev help to different Duke Nukem projects.I don't think I ever properly played Duke 3D until recently, picking up the \"Cursed Randy Version\" version on Switch. But as a kid I was hooked on the level  editor (and pixelated nudity.) Duke 3D's custom maps scene never eclipsed the popularity and duration of Doom or Quake, but there were some fantastic creations that really stirred the imagination and kept me in that editor for hours.(There is also a port of the Duke Nukem 64 version, which whilst almost identical, does have a few interesting variations which makes it worth the try for a series fan.)reply",
      "That early 2000's web design style, love it!reply",
      "Duke Nukem 3D was probably one of the earlier FPS games that really encouraged modding because of Ken Silverman's Build Engine.Even the enemy AI could be modified (albeit relatively limited) by editing the text CON files.Anyone else remember playing over LAN with friends, dropping a Duke hologram in an elevator along with a bunch of pipe bombs hidden at its feet?reply",
      "It was a wonderful collection of rage inducing weapons: pipe bombs, laser trip mines, shrink ray (then step on them for the kill), freeze gun (any hit shatters for the kill), and the BFG.We had LAN parties and would play for hours on end with custom maps we had built or downloaded.reply",
      "Same! We used to host \"Jetpack Freeze Ray\" duels which ended when somebody was frozen causing them to plummet out of the sky and shatter when they hit the ground~~reply",
      "> then step on them for the killI heard the sound effect of that when I read you're comment.reply",
      "Yeah it sounded like what I imagine pressing on a cardboard of eggs would.reply",
      "Hail to the king, baby!reply",
      "Back when I worked at the AG Group (famous for etherpeek) we'd play late at night we could hear each other screaming from our offices, and I'd walk out of my office terrified. The laser trips were the best. This game truly holds a special place in my heart.We also had a really good LAN there.reply",
      "Nice. The laser-trip alarm effect that would play right before it detonated in your face is forever emblazoned in my memory.reply"
    ],
    "link": "https://www.eduke32.com/",
    "first_paragraph": "Per-pixel dynamic lighting and realtime shadows... groovy!  Polymer renderer requires a bad-ass video card.More Polymer greatness.Hollywood Holocaust with classic texturesCome get some!EDuke32 is an awesome, free homebrew game engine and source port of the classic PC first person shooter Duke Nukem 3D\u2014 Duke3D for short\u2014to Windows, Linux, macOS, FreeBSD, several handhelds, your family toaster, and your girlfriend's vibrator.  We've added thousands of cool and useful features and upgrades for regular players and additional editing capabilities and scripting extensions for homebrew developers and mod creators. EDuke32 is open source software that is completely free to use for all non-commercial purposes.Created by Duke4.net community leader Richard \"TerminX\" Gobeille and a team of elite ninja programmers including Evan \"Hendricks266\" Ramos, Pierre-Loup \"Plagman\" Griffais, and Philipp \"Helixhorned\" Kutin (based on work by Todd Replogle/Ken Silverman/Jonathon Fowler/Matt Saettler), EDuke32 "
  },
  {
    "title": "Forward propagation of errors through time (nicolaszucchet.github.io)",
    "points": 6,
    "submitter": "iNic",
    "submit_time": "2026-02-19T09:23:53 1771493033",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://nicolaszucchet.github.io/Forward-propagation-errors-through-time/",
    "first_paragraph": "Nicolas Zucchet1, Guillaume Pourcel2 and Maxence Ernoult31Stanford University, 2University of Groningen, 3Google DeepMindFebruary 17, 2026TL;DR\nWe investigate a fundamental question in recurrent neural network training: why is backpropagation through time always ran backwards? We show, by deriving an exact gradient-based algorithm that propagates error forward in time (in multiple phases), that this does not necessarily need to be the case! However, while the math holds up, it suffers from critical numerical stability issues as the network forgets information faster.\nThis post details the derivation, the successful experiments, an analysis of why this promising idea suffers numerically, and the reasons why we did not investigate it further.Do we necessarily need to calculate error signals in backward time (as in backpropagation through time) when training recurrent neural networks? Surprisingly, no.In this post, we derive a method to propagate errors forward in time. By using a \u201cwarm-u"
  },
  {
    "title": "CXMT has been offering DDR4 chips at about half the prevailing market rate (koreaherald.com)",
    "points": 155,
    "submitter": "phront",
    "submit_time": "2026-02-21T14:32:16 1771684336",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=47101171",
    "comments": [
      "As a outside observer, NAND and DRAM prices have skyrocket ed with the AI infrastructure boom just as the China-based fabs are coming online.It is wise for these Chinese fabs to eventually use a very aggressive  dumping strategy to price well below cost push out other players forever, especially in DRAM.But right now it seems they can max out their supply capacity without selling below cost.Appears to me like China's endless state led (often unproductive) investment in semiconductor manufacturing subsidies (for decades) is about to pay off with some industry dominance soon.Like the electric vehicle sector.reply",
      "It's amazing what can be achieved when you can plan 5 years in advance, instead of just making the line go up for the next quarter.reply",
      "History is littered with the corpses of those slaughtered by the millions in the name of great leader\u2019s 5 year plans.https://en.wikipedia.org/wiki/Great_Leap_Forwardreply",
      "Not five, but likely twenty-five. Not specific plans but attention: situation changes all the time, but the interest remains.reply",
      "> It is wise for these Chinese fabs to eventually use a very aggressive dumping strategy to price well below cost push out other players forever, especially in DRAM.Crucial's departure from the consumer market left such a gaping hole, that CXMT doesn't even need to push other players out to gain a footing.reply",
      "It's more like everyone else abandoned the market, and CXMT realised it was free real estate.reply",
      "How's it dumping below cost when hey can simply sell for 100% margins instead of western makers selling for 400%.reply",
      "It's easy to misread, but they're not arguing that. Note the \"eventually\" and \"but right now\".reply",
      "Because only western companies are allowed to make massive profits at the expense of entire nations, it's not greed when they do it apparently.reply",
      "Is it not the case that they're raising price in response to demand? Ie. if they kept the price low, they'd be perpetually out of supply?reply"
    ],
    "link": "https://www.koreaherald.com/article/10679206",
    "first_paragraph": ""
  },
  {
    "title": "Claws are now a new layer on top of LLM agents (twitter.com/karpathy)",
    "points": 196,
    "submitter": "Cyphase",
    "submit_time": "2026-02-21T00:56:29 1771635389",
    "num_comments": 647,
    "comments_url": "https://news.ycombinator.com/item?id=47096253",
    "comments": [
      "All: quite a few comments in this thread (and another one we merged hither - https://news.ycombinator.com/item?id=47099160) have contained personal attacks. Hopefully most of them are [flagged] and/or [dead] now.On HN, please don't cross into personal attack no matter how strongly you feel about someone or disagree with them. It's destructive of what the site is for, and we moderate and/or ban accounts that do it.If you haven't recently, please review https://news.ycombinator.com/newsguidelines.html and make sure that you're using the site as intended when posting here.reply",
      "[flagged]",
      "> Weirdly, he doesn\u2019t care about the toxicity problems on Twitter/X either,> Yet he has the nerve to call HN toxic.That's not weird or paradoxical. They don't have to delineate every online platform they disagree with in order to criticize HN; \"orange site bad\" is a pretty common sentiment in my experience.reply",
      "I hear about X being hateful but nobody wants to talk about how toxic BlueSky actually is. I had to stop going on there permanently. I dont think I have ever quit a social media platform as quickly and permanently as BlueSky.reply",
      "This is also a strawman. Both platforms are irrelevant to this discussion.reply",
      "The chutzpah of complaining about the level here when most tech threads on X are primitive billionaire marketing. Why don't they complain about Musk's X?It isn't enough that Karpathy is rich---we also need to admire him. That dynamic was satirized in the Silicon Valley show with Gavin Belson.Also relevant:https://www.mcsweeneys.net/articles/please-dont-say-mean-thi...reply",
      "[flagged]",
      "Being rude isn't helpful. It's not their fault, it's the unavoidable reality of treating complex social signalling as one-dimensional. At minimum Hacker News would need to separate approval/disapproval signals from assessments of whether a comment is constructive. That\u2019s not a simple change given the obvious abuse vectors. It would require reliably distinguishing good-faith participants from bad actors. It can be done, but it's not easy.The main reason sites avoid this approach is institutional rather than technical. Adding algorithmic mediation invites accusations of algorithmic bias whenever results are unpopular.[0] Simple manual interventions are often sufficient to nudge community behaviour so that majority outcomes broadly align with the moderators\u2019 priors, without the visibility or accountability costs of a more complex system.[0] Case in point being X. People routinely accuse the new management of \"juicing\" the algorithm to favour their politics, when outcomes are adequately explained by the exodus of contributors on the other side. Isolating innate community bias from algorithms is a philosophically impossible problem.reply",
      "It's not 'downvote abuse' if it's working exactly as intended. The community decides what's 'perfectly fine and neutral.' If your comments follow the guidelines, at least they won't get deleted.reply",
      "This is pretty obviously false? I get downvoted quite frequently on HN for posting comments that go against what people typically think. For instance, I find it quite difficult to discuss the productivity gains of AI because any comment I make saying that AI makes me more productive immediately gets downvotes. I am not making inflammatory comments - my comments with a similar tone about other things that boost my productivity, like Rust or whatever, never get downvoted.reply"
    ],
    "link": "https://twitter.com/karpathy/status/2024987174077432126",
    "first_paragraph": "We\u2019ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.Help Center\nTerms of Service\nPrivacy Policy\nCookie Policy\nImprint\nAds info\n      \u00a9 2026 X Corp.\n    "
  },
  {
    "title": "Finding forall-exists Hyperbugs using Symbolic Execution (acm.org)",
    "points": 16,
    "submitter": "todsacerdoti",
    "submit_time": "2026-02-17T01:57:40 1771293460",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://dl.acm.org/doi/full/10.1145/3689761",
    "first_paragraph": ""
  },
  {
    "title": "Canvas_ity: A tiny, single-header <canvas>-like 2D rasterizer for C++ (github.com/a-e-k)",
    "points": 61,
    "submitter": "PaulHoule",
    "submit_time": "2026-02-21T18:50:19 1771699819",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=47103506",
    "comments": [
      "The list of \"recommended reading\" from one of the issues looks great:https://github.com/a-e-k/canvas_ity/issues/11#issuecomment-2...reply",
      "Author here.  What a pleasant surprise to see this trending on the front page!(I did post a Show HN at the time of the original release, https://news.ycombinator.com/item?id=33148540, but it never gained traction.)Just to answer some comments that I see:1. This was absolutely not vibecoded!I'd originally started with a different version control system and was still getting used to Git and GitHub at the time that I'd released this.  (I was a latecomer to Git just because I hated the CLI so much.)  It was easiest for me just to drop the whole thing as a snapshot in a single commit.But my private repo for it actually started in May 2017, and it had 320 commits leading up to its release, all human-written.For the v2.0 that I have in mind, I'm thinking of force-pushing to migrate the full development history to the public repo.And finally I'll add that I'm a graphics engineer by education and career.  Where would the fun be in vibe-coding this? :-)  Oh, and this compiles down to just ~36KiB of object code on x86-64 last I checked.  Good luck vibe-coding that constraint.2. Why a single header with `#define CANVAS_ITY_IMPLEMENTATION`?I was inspired by the STB header ibraries (https://github.com/nothings/stb) and by libraries inspired by those, all of which I've found very convenient.  In particular, I like their convenience for small utilities written in a single .cpp file where I can just `g++ -O3 -o prog prog.cpp` or such to compile without even bothering with a makefile or CMake.Since the implementation here is all within a single #ifdef block, I had figured that anyone who truly preferred separate .cpp and .h files could easily split it themselves in just a few minutes.But anyway, I thought this would be a fun way of \"giving back\" to the STB header ecosystem and filling what looked to me like an obvious gap among the available header libraries.  It started as something that I'd wished I'd had before, for doing some lightweight drawing on top of images, and it just kind of grew from there.  (Yes, there was Skia and Cairo, but both seemed way heavier weight than they ought to be, and even just building Skia was an annoying chore.)----Since I mentioned a v2.0, I do have a roadmap in mind with a few things for it: beside the small upgrades mentioned in the GitHub issues to support parts of newer <canvas> API specs (alternate fill rules, conic gradients, elliptical arcs, round rectangles) and text kerning, I'm thinking about porting it to a newer C++ standard such as C++20 (I intentionally limited v1.0 to C++03 so that it could be used in as many places as possible), possibly including a small optional library on top of it to parse and rasterize a subset of SVG, and an optional Python binding.reply",
      "And thus random 2D drawing APIs begat Cairo, and then Cairo begat the Canvas, and thus the Canvas begat Canvas_ity, which looked almost like it's grandparent, and yet was very much it's own self.reply",
      "The project is great. The HN comments are embarrassing. Isn\u2019t it ironic to imply laziness by chiming in with \u201cvibe coded\u201d which in itself is such a lazy reaction.reply",
      "It would be interesting to compile to WASM to compare side by side for performance and accuracy.reply",
      "Author here.  I have a JavaScript port of my automated test suite (https://github.com/a-e-k/canvas_ity/blob/main/test/test.html) that I used to compare my library against browser <canvas> implementations.  I was surprised by all of the browser quirks that I found!But compiling to WASM and running side-by-side on that page is definitely something that I've thought about to make the comparison easier.  (For now, I just have my test suite write out PNGs and compare them in an image viewer split-screen with the browser.)reply",
      "Thank you for sharing. The only thing I don't understand why this is a header only implementation with a macro that goes in a C++ file.    #define CANVAS_ITY_IMPLEMENTATIONreply",
      "It is common for header-only  libraries: you need to include this header in one c++ using the macro for linking (don't use that macro in other c++ files to avoid duplicate symbols). In C++, you can declare a function as many times as you want, but you can only define it (write the actual body) once in the entire project.reply",
      "I understand that part, but I don't see why do this instead of basic Makefile or CMake setup. It seems like more work than a regular linker at that point. For what purpose?reply",
      "Because not everyone is using Makefiles or CMake.A true header-only library should be build-system agnostic and this is one way to do that.We can argue about build systems for C++ all day long and never come to an agreement. With this approach this piece of code can be used anywhere.reply"
    ],
    "link": "https://github.com/a-e-k/canvas_ity",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A tiny, single-header <canvas>-like 2D rasterizer for C++\n      This is a tiny, single-header C++ library\nfor rasterizing immediate-mode 2D vector graphics, closely\nmodeled on the basic W3C (not WHATWG) HTML5 2D canvas\nspecification.The priorities for this library are high-quality rendering, ease of use, and\ncompact size.  Speed is important too, but secondary to the other priorities.\nNotably, this library takes an opinionated approach and does not provide\noptions for trading off quality for speed.Despite its small size, it supports nearly everything listed in the W3C\nHTML5 2D canvas specification, except for hit regions and getting certain\nproperties.  The main differences lie in the surface-level API to make this\neasier for C++ use, while the underlying implementation is carefully based\non the specification.  In particular, stroke"
  },
  {
    "title": "Personal Statement of a CIA Analyst (antipolygraph.org)",
    "points": 169,
    "submitter": "grubbs",
    "submit_time": "2026-02-21T17:49:02 1771696142",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=47102975",
    "comments": [
      "I'm always surprised to hear that a government agency administers polygraph tests in something as serious as hiring but then I remember the CIA also spent millions of dollars trying to develop telekinetic assassins and train clairvoyants to spy on the Kremlin.reply",
      "The polygraph doesn't have to emit any useful data at all to be very useful in interrogations. Like a bomb doesn't have to have any explosive in it to clear a building. Interrogation is a head game and a complicated box with knobs and buttons and maybe even blinking lights makes a fine prop.And there's enough ambiguity in it that it's easy for the operator to believe it helps. Like a dowser with their rods, a clergyman with a holy book or an astrologist with a horoscope. That gives them the power boost of sincerity.reply",
      "That research was oriented towards making sure it wasn't possible though.You're saying \"of course it isn't\" - but how do you know that?At the time the Soviets had the same sort of projects. So until you're sure it's not possible, the potential capability is an enormous threat if it is.How they went about that research is where the waste creeps in.reply",
      "> General Brown: So they started doing psy-research because they thought we were doing psy-research, when in fact we weren't doing psy-research?> Brigadier General Dean Hopgood: Yes sir. But now that they are doing psy-research, we're gonna have to do psy-research, sir. We can't afford to have the Russian's leading the field in the paranormal.Source: The Men Who Stare at Goatsreply",
      "Plenty of things we could be wasting money on if the only criteria is \"how do you know it's not real?\",  why stop at killing goats with mind bullets?  We could be looking for yetis or Atlantis or lunar nazi spaceships.It was a giant waste of time and money and, this being the CIA, it likely harmed many people.reply",
      "I always wonder when I see one of those hypnosis shows, where someone from the audience makes themselves a docile fool in front of a large crowd, whether they are stooges or it is the real deal. But I wouldn't volunteer to get hypnotised to figure that out, in fear of being the next person who stands imitating a dog in heat on such a stage.reply",
      "There\u2019s a good book about this called Reality is Plastic. It may give you a new perspective.reply",
      "Was drugging random Americans with LSD also a valid experiment? Parts of the CIA was just insane back then, maybe still is.reply",
      "Yeah absolutely. Figuring out which, if any, drugs can be used to control people is extremely valuable for defence, not to mention offence. Same with the fascist Japanese frostbite experiments.Let me be clear: these were all wrong and unethical, and I would not have approved or conducted them. But if you're a government agency tasked with doing wrong and unethical things in the name of national security, they were all good ideas to at least try.reply",
      "> but I wondered why a petty thief thought she could get into the Agency.It\u2019s reassuring to know no one at the CIA has ever done anything wrong, like stealing fifty dollars.reply"
    ],
    "link": "https://antipolygraph.org/statements/statement-038.shtml",
    "first_paragraph": "4 October 2018I first took a polygraph when I applied to the CIA and went through the applicant screening process.To prepare for the test, I read A Tremor in the Blood by David T. Lykken. The book described the use of control versus relevant questions as well as countermeasures such as butt-clenching. I had no desire to use countermeasures. I wasn't out to \"beat\" the test: I wanted to understand how it worked. A future colleague at the Agency advised me, \"Spill your guts.\" I thought it was good advice, and I planned to follow it.I knew I was taking a risk in applying to the Agency. I worked as a defense contractor on a project for the CIA, so I already held CIA TS/SCI clearances. If I failed the polygraph, I could lose my clearances, and I might lose my job as well.I flew to Northern Virginia for two days of pre-employment screening. A bus took us from the hotel to a nondescript building in Vienna. The examiner was a young woman. She asked me to sign a consent form and told me not to t"
  },
  {
    "title": "Toyota Mirai hydrogen car depreciation: 65% value loss in a year (carbuzz.com)",
    "points": 95,
    "submitter": "iancmceachern",
    "submit_time": "2026-02-21T18:09:24 1771697364",
    "num_comments": 232,
    "comments_url": "https://news.ycombinator.com/item?id=47103136",
    "comments": [
      "Theres something clickbaity and missing from this article, I encourage watching youtubers like 'mirai club' for better info. What i recall from his videos is:- The Mirai made financial sense AS A LEASE for folks in Southern California back in 2022 (possibly 2023) because:  - Car prices in general (including EVs) were fairly highly priced at the time due to demand, the chip shortage, etc.\n\n  - There were clean vehicle incentives to get a Toyota Mirai, including things like a hydrogen fuel fill up card to cover expenses.\n\n  - At the time there was some assumptions that hydrogen fuel costs would go down over time, but they actually went up.\n\n\nAgain, I suspect most folks LEASED the Mirai due to it being a very niche car with limited usage outside of california due to the lack of hydrogen fuel stations. Youre now seeing some viral videos on the ultra low cost  used Mirai's showing up in states that dont have hydrogen infrastructure due to some odd car dealer auction buys (Transport Evolved has a youtube video on this.)The article does talk about the lack of investment in hydrogen infrastructure, this is true and theres been a huge split between announced infrastructure investments and what has actually happened (see https://bsky.app/profile/janrosenow.bsky.social/post/3labfzi... for a chart going through 2021-2024). The current US political situation and its impact on clean energy probably doesn't help either.reply",
      "Kinda glad this is the case. When people go out of their way to avoid common sense they should be punished.Hydrogen is such a terrible idea it was never getting off the ground. There seems to be some kind of psychosis around it being the next oil and therefore greedy people want to get in early on. But this blinds them to the basic chemistry and physics.reply",
      "People looked at how the cost of wind and solar went down and made a assumption that green hydrogen would follow. The reasoning was that the cost of green hydrogen was energy, and thus at some point green hydrogen would be too cheap to meter.The whole energy plan of central/northen Europe, especially Germany, was built for the last several decades on the idea that they would combine wind, solar and cheap natural gas and then replace the natural gas part with green hydrogen. In Sweden there were even several municipalities that spear headed this by switching mass transportation and heating towards hydrogen, initially with hydrogen produced through natural gas, as a way to get ahead on this plan.The more sensible project were the green steel project. As experts in green hydrogen said consistently said through those decades, is that green steel would be the real test to make green hydrogen economical. The economics of burning it for energy or transportation would come several decades later, if ever. The green steel project however has not ended up as planned and gotten severely delayed and has seen a cost increase by an estimated 10x. municipalities are now giving up the hydrogen infrastructure and giving it an early retirement, as maintenance costs was significantly underestimated. There is very little talk now about replacing natural gas with green hydrogen, and the new plan is instead to replace the natural gas with bio fuels, hinted at carbon capture, at some unspecified time.reply",
      "The idea was to transition from coal to natural gas while using solar and wind to reduce fuel consumption, thereby significantly reducing CO2 emissions. Any claims of hydrogen being burned were either lies to the public to get the gas plants built despite the non-green optics or lies to investors as part of a fraud scheme.reply",
      "Sweden has very little natural gas in its energy mix:https://ourworldindata.org/grapher/energy-consumption-by-sou...I highly doubt that hydrogen heating was ever considered. It's usually pushed by the gas lobby (since most hydrogen comes from gas), and Sweden doesn't have a strong gas lobby.reply",
      "That was extremely stupid of them then. Hydrogen has been very good at one thing: subsidy extraction. But I don't think it was or ever will be a viable fuel for planetary transportation.reply",
      "Good context. It's a shame none of these people did high school chemistry.I do remember there being some news about the steel manf.I wonder if further advancements in rocketry are adding H2 tech that could help us manage the difficulties of dealing with the stuff. It still only makes sense in very specific circumstances. Like when you need energy in tank form.But I think battery / biofuel is the future.reply",
      "There is a great way to store, transport, and use hydrogen:Bind it to various length carbon chains.When burned as an energy source the two main byproducts are carbon dioxide which is an essential plant growth nutrient, and water which is also essential to plant growth.Environmentalists will love it!And they can prise my turbo diesel engines from my cold dead hands.reply",
      "Carbon Dioxide is a greenhouse gas, which makes the world warmer on average. It also lowers the PH levels of the oceans.If the oceans die, its very likely that many or even most humans will also. As a human I am pretty strongly opposed to dying, but thats just, like, my opinion man.reply",
      "The major problem with hydrocarbons today is that we are releasing carbon dioxide stored hundreds of millions of years ago.If, theoretically, you could produce hydrocarbons from the carbon dioxide that is currently in our atmosphere, then it could be a substantial reduction in net carbon dioxide being added; and it would be compatible with the fuel infrastructure of today.reply"
    ],
    "link": "https://carbuzz.com/toyota-mirai-massive-depreciation-one-year/",
    "first_paragraph": "The story of hydrogen propulsion in the automotive world has always been a convoluted one. Infrastructure difficulties and the rise of competitors \u2013 most notably battery-electric power \u2013 have made it extremely difficult for hydrogen to ever properly take hold. Despite these struggles, some hydrogen-powered models are still circulating on both the new and used car market.One such model is the Toyota Mirai, the first and perhaps the most famous hydrogen-powered production car in existence. The Mirai is still being built today, but has faced several struggles which meant that even very recent used examples are catastrophically depreciating. Let\u2019s take a closer look at this phenomenon, the reasons behind it, and the Mirai model as a whole.UpdateThis feature was updated with information about synthetic fuel, which is seen as yet another alternative to hydrogen fuel cells and hydrogen combustion.2025 Toyota MiraiPowertrainOne electric motor (hydrogen fuel cell)Power182 hpTorque220 lb-ft0-60 "
  },
  {
    "title": "What not to write on your security clearance form (1988) (milk.com)",
    "points": 382,
    "submitter": "wizardforhire",
    "submit_time": "2026-02-21T17:08:12 1771693692",
    "num_comments": 167,
    "comments_url": "https://news.ycombinator.com/item?id=47102576",
    "comments": [
      "> When I handed the form in to the security officer, he scanned it quickly, looked me over slowly, then said, ``Explain this''--pointing at the FBI question. I described what had happened. He got very agitated, picked up my form, tore it in pieces, and threw it in the waste basket.> He then got out a blank form and handed it to me, saying ``Here, fill it out again and don't mention that. If you do, I'll make sure that you never get a security clearance.''It's important to \"see like the government\" when dealing with the government (pun on \"seeing like a bank\" by https://www.bitsaboutmoney.com/archive/seeing-like-a-bank/ if anyone didn't catch the reference).Everything fits into bins and categories with checkmarks and such. As an entity it has no \"bin\" for \"investigated as Japanese spy as a joke when was a child\". So you have to pick the closest bin that matches. However, that doesn't mean the same government later won't turn around also punish you for not picking the right \"bin\". Not \"realizing\" that it's its own fault for not having enough categories i.e. bins for you to pick. And, some may argue, that's a feature not a bug...reply",
      "Not sure if you were maybe joking, but Seeing like a Bank is itself a pun on the famous book \"Seeing like a state\"! https://en.wikipedia.org/wiki/Seeing_Like_a_StateSo you've come almost full circle!reply",
      "It is the full circle! patio11 refers to that explicitly in the blog. But most people here probably saw and remember Pat's blog more than the book.reply",
      "The book is very famous! I would guess more people have heard of it than read that specific BAM post.reply",
      "And then, over with AGSVA, they just do interviews. Every candidate gets one, and they absolutely do bring up all the random crap that happens to various people as kids. And ask why it wasn't on your form.reply",
      "the challenge is always determining what the \"bins\" are.maybe the government has no bin for \"investegated by the FBI for a silly and innocuous reason\". but maybe they do, and lying about it slots you into the bin for \"lied on their security clearance form\".reply",
      "In the security space you\u2019re encouraged to be as transparent as possible. Most modern forms have ample space to write in detailed explanations.I have some silly not nearly as interesting infractions and I wrote them out in detail explaining, without any issue in processing background checks. It usually is something that\u2019s asked about in an in person interview at that point.reply",
      "The danger isn't just being risky, it's being anomalousreply",
      "The fact is that even for (NATO) top secret security clearances, there are lots of people that lie through their teeth, and receive the clearance. Obviously on things that aren't in any records. The big ones being alcohol use, drug use, personal finances, foreign partners. Some are more forgiving than others, though.The military is unfortunately chock full of functional alcoholics. As long as they don't get caught drunk on the job, seen partying too much, DIU, or admit anything to their doctor, they keep getting renewed their clearance.Interestingly enough, if there's even the smallest suspicious that you smoke weed, they'll put you through the wringer. I've seen more people lose their clearance for pissing hot, than those with six figure debts or drinking 5 days a week.reply",
      "I was chatting with an old classmate at a homecoming a few months ago, and he mentioned that, during the polygraph top get Canadian Top Secret clearance for a co-op job, he had to say how many drinks he had each week. Being a university student, it got brushed aside, but the answer was considered to be alcoholism-level.reply"
    ],
    "link": "https://milk.com/wall-o-shame/security_clearance.html",
    "first_paragraph": "\nDate: 01 Apr 88  1620 PST\nFrom: Les Earnest <LES...@S...>\nSubject: The \"previous account\" referred to in RISKS-6.51\ne-t-a-o-n-r-i Spy and the FBI\n\nReading a book got me into early trouble--I had an FBI record\nby age twelve. This bizarre incident caused a problem much later\nwhen I needed a security clearance. I learned that I could obtain\none only by concealing my sordid past.\n\n\nA friend named Bob and I read the book ``Secret and Urgent,'' by\nFletcher Pratt [Blue Ribbon Books; Garden City, NY; 1942] which was\nan early popular account of codes and ciphers. Pratt showed how to\nuse letter frequencies to break ciphers and reported that the most\nfrequently occurring letters in typical English text are\ne-t-a-o-n-r-i, in that order. (The letter frequency order of the\nstory you are now reading is e-t-a-i-o-n-r. The higher frequency\nof ``i'' probably reflects the fact that _I_ use the first person\nsingular a lot.) Pratt's book also treated more advanced\ncryptographic schemes.\n\n\nBob and I decide"
  },
  {
    "title": "Inputlag.science \u2013 Repository of knowledge about input lag in gaming (inputlag.science)",
    "points": 63,
    "submitter": "akyuu",
    "submit_time": "2026-02-21T19:41:52 1771702912",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=47103945",
    "comments": [
      "I wish they included the window compositor as something that can introduce latency because I'd like to learn more about it.When I switched from Windows to Linux on the same hardware I noticed a lot of keyboard input latency when playing games, at least 150ms. This only happens to me with niri, KDE Plasma (Wayland) feels identical to Windows. So did Hyprland. I'm able to reproduce it on multiple systems when I have a 4k display running at 1:1 native scaling. On AMD cards, turning off v-sync helped reduce it but it didn't remove it. With an NVIDIA card, turning off v-sync made no difference. I believe it's semi-related to that 4k display because when I unplug that display and use my 2560x1440 monitor, it's much less noticeable despite getting a solid 60 FPS with both monitors. All that to say, there's certainly a lot more than your input device, GPU and display playing a role.If anyone played Quake on a dial-up connection with client side prediction turned off, that is the exact same feeling. It's pressing a key and then seeing the screen update X ms afterwards.reply",
      "Windows solution to this is exclusive fullscreen, which bypasses the compositor.You can try Gamescope [1] from Valve, that's what Steam Deck uses - i think its a compositor designed to minimize latency but support the few things games need. Some compositors like KDE Plasma KWin support a direct scanout mode which is the same idea as windows' exclusive fullscreen. You might need to look for support for something similar in niri.[1] https://wiki.archlinux.org/title/Gamescopereply",
      "Thanks, I have tried gamescope but it kills the performance of games for me. All games have a lot of stuttering when I use it. It also didn't reduce the input latency. Same hardware is liquid smooth on Windows.As far as I know niri enables direct scanout by default. It's an option you can disable if you want https://niri-wm.github.io/niri/Configuration%3A-Debug-Option.... I do not have this set which indicates direct scanout is enabled.It's interesting because the latency is only when pressing keys on the keyboard. Mouse movement and button press latency feels as good as Windows, I can't perceive any delay. I tried 3 keyboards, it's all the same. I'm also not running anything like keyd or anything that intercepts keys. It's a vanilla Arch Linux system on both of the systems I tested.reply",
      "Input lag is one of those things you feel before you can explain it. Good to finally have a resource that breaks down the full chain \u2014 controller, engine, display \u2014 instead of just blaming the monitor like everyone doesThe engine section is the part most developers seem to ignore. A locked 60fps doesn't mean 16ms latency, and that gap make me surprisereply",
      "I used to get into arguments all the time about how triple-buffering reduces latency, and I think it's because we lacked resources like this; people assume it adds the additional back buffer to a queue, when the traditional implementation \"renders ahead\" and swaps the most recently-completed back buffer. It's a subtle difference but significantly reduces the worst-case latency vs. a simple queue.I think most people get their information from help blurbs in settings menus for PC games, which are often hilariously vague or incorrect.reply",
      "Vulkan's presentation API makes this distinction explicit: VK_PRESENT_MODE_MAILBOX_KHR is the \"replace if already queued\" mode that actually reduces latency, while VK_PRESENT_MODE_FIFO_KHR is the pipeline-queue variant that adds frames ahead of time. OpenGL never standardized the difference,\n  so \"triple buffering\" meant whatever the driver implemented -- usually vendor-specific extension behavior that varied between hardware. The naming confusion outlived OpenGL's dominance because the concepts got established before any cross-platform API gave them precise semantics.reply",
      "1. It doesn\u2019t help that on Windows\u2019 \u201cTriple buffering\u201d options actually means FIFO forced three-frame buffering. So people had prestablished PTSD from those dreadfully laggy smoothing.2. Triple buffering does not reduce latency compared to unsynced tearing. It\u2019s a spatial vs temporal tradeoff between whether to let frequency mismatches manifest as tearing or jitter. For passive consumption of motion, losing temporal consistency in exchange for spatial cohesion is the better tradeoff and so triple buffering is appropriate. For active controls of motion and its feedback, temporal consistency is absolutely critical whereas spatial cohesion while in motion is far, far less important, so triple buffering is unacceptable in this use case.reply",
      "'Input lag' should really be called 'Output lag', as most of it usually comes from the display device and/or graphics pipeline, not input devicesreply",
      "One area of focus missing here is game streaming / remote play (Steam Link, Moonlight, etc. over a local network).I've come to accept input lag, but mostly play games where it doesn't matter (simple platformers, turn-based games, etc). I know steam link from my home desktop to my ~5 year smart TV is adding latency to my inputs \u2013 though I can't tell if it's from my router, desktop, or TV \u2013 but I've come to accept it for the convenience of playing on the couch (usually with someone watching next to me).I know some blame is on the TV, as often if I just hard-reset the worst of the lag spikes go away (clearly some background task is hogging CPU). And sometimes the sound system glitches and repeats the same tone until I reset that. Still worth putting up with for the couch.reply",
      "Build an sffpc, have it by the tv :)reply"
    ],
    "link": "https://inputlag.science",
    "first_paragraph": "Hello traveler, welcome to the repository of knowledge about input lag in gaming.The input lag in a gaming system, or any interactive system, is the latency between the user input and a reaction on the\nscreen. Input lag is an issue that has crept in the industry, little by little, without being noticed over the years.\nNowadays, finding a gaming system with a latency similar to early 2000 without image degradation is a definitive challenge.\nIt has come to a point where some games have major issues and it can cause\nuproars\nin\nthe\npress.The reasons behind this rise of the latency is mainly that systems have become more and more complex and developers\noften don't know or don't understand each part that can impact the latency.This website has been made to help developers and consumers better understand the latency issues and how to tackle them.There are three majors components in the lag chain:There are obviously plenty of subtleties around those three points. This website tries to referenc"
  },
  {
    "title": "I verified my LinkedIn identity. Here's what I handed over (thelocalstack.eu)",
    "points": 1178,
    "submitter": "ColinWright",
    "submit_time": "2026-02-21T07:06:18 1771657578",
    "num_comments": 409,
    "comments_url": "https://news.ycombinator.com/item?id=47098245",
    "comments": [
      "I'll note that Persona's CEO responded on LinkedIn [1] pointing out that:  - No personal data processed is used for AI/model training. Data is exclusively used to confirm your identity.\n  - All biometric personal data is deleted immediately after processing.\n  - All other personal data processed is automatically deleted within 30 days. Data is retained during this period to help users troubleshoot.\n  - The only subprocessors (8) used to verify your identity are: AWS, Confluent, DBT, ElasticSearch, Google Cloud Platform, MongoDB, Sigma Computing, Snowflake\n\nThe full list of sub-processors seems to be a catch-all for all the services they provide, which includes background checks, document processing, etc. identity verification being just one of them.I have I've worked on projects that require legal to get involved and you do end up with documents that sound excessively broad. I can see how one can paint a much grimmer picture from documents than what's happening in reality. It's good to point it out and force clarity out of these types of services.[1]: https://www.linkedin.com/feed/update/urn:li:activity:7430615...reply",
      "All of which is meaningless if it's not reflected properly in their legal documents/terms. I've had interactions with the Flock CEO here on Hacker News and he also tried to reassure us that nothing fishy is/was going on. Take it with a grain of salt.reply",
      "Why anyone would trust the executives at any company when they are only incentivized to lie, cheat, and steal is beyond me. It's a lesson every generation is hellbent on learning again and against and again.It use to be the default belief, throughout all of humanity, on how greed is bad and dangerous; yet for the last 100 years you'd think the complete opposite was the norm.reply",
      "> when they are only incentivized to lie, cheat, and steal\n\nThe fact that they are allowed to do this is beyond me.The fact that they do this is destructive to innovation and I'm not sure why we pretend it enables innovation. There's a thousands multi million dollar companies that I'm confident most users here could implement, but the major reason many don't is because to actually do it is far harder than what those companies build. People who understand that an unlisted link is not an actual security measure, that things need to actually be under lock and key.I'm not saying we should go so far as make mistakes so punishable that no one can do anything but there needs to be some bar. There's so much gross incompetence that we're not even talking about incompetence; a far ways away from mistakes by competent people.We are filtering out those with basic ethics. That's not a system we should be encouragingreply",
      "Because the liars who have already profited from lying will defend the current system.The best fix that we can work on now in America is repealing the 17th amendment to restrengthen the federal system as a check on populist impulses, which can easily be manipulated by liars.reply",
      "So your senators were appointed before that? No election needed?reply",
      "Yes, by state legislatures. The concept was the Senate would reflect the states' interests, whereas the House would reflect the people's interests, in matters of federal legislation.reply",
      "Yup exactly, if this is the truth then put it on the terms/privacy policy etc... exec's say anything these days with zero consequences for lieing in a public forum.reply",
      "Can a ceo's word on linkedin and X be used to make claims against them?reply",
      "This is not the concern for me. I thought the risk was obvious to everyone. Tho I've been tempted because it means I'll \"have more interactions\" or whatever LinkedIn pitches with, I didn't want to put a public signal out there with yes: \"This is my real name, real job, real city\" - to me it's like a pre-vetted database of marks for identity theft criminals or whatnot. You know?I thought everyone, at least in security would be somewhat concerned about this, but they're not. I get the benefits, and I want to enjoy those benefits too. I'd much prefer if I could privately confirm my name using IDs (zero problem with that) but then not have to show it or an exact profile photo. I'm sure there's a cryptographic way for my identity to be proven to any who I chose to prove it to who required such bona fides. I dislike the surface of \"proven identity for everyone\". You know?This to me is the far more important thing than: \"security focused biometric company processed my data, therefore being rational and modern I will now have a meltdown.\" Everytime you drive, use a payment method linked to your name, use your plan phone, your laptop, go to a venue that ID scans, make a rental, catch a flight, cross a border, etc, your ID (or telemetric equivalents sufficient to ID you) is processed by some digital entity. If you will revolt against the principle of \"my government issued and not-truly-mine-anyway ID documents, or other provided bona fides are being read by digital entities contracted to do that\", it seems nonsensical.I think the bigger risk is always taking a photo of your passport and putting it on the internet, which is basically what the current LI verification means. Casual OSINT on a verified profile likely reveals the exact birthday (or cross-referenced on other platforms), via \"happy birthday\" type posts. How old am I type image AI can give you rough years.reply"
    ],
    "link": "https://thelocalstack.eu/posts/linkedin-identity-verification-privacy/",
    "first_paragraph": "I wanted the blue checkmark on LinkedIn. The one that says \u201cthis person is real.\u201d In a sea of fake recruiters, bot accounts, and AI-generated headshots, it seemed like a smart thing to do.So I tapped \u201cverify.\u201d I scanned my passport. I took a selfie. Three minutes later \u2014 done. Badge acquired. I felt a tiny dopamine hit of legitimacy.Then I did what apparently nobody does. I went and read the privacy policy and terms of service.Not LinkedIn\u2019s. The other company\u2019s.When you click \u201cverify\u201d on LinkedIn, you\u2019re not giving your passport to LinkedIn. You get redirected to a company called Persona. Full name: Persona Identities, Inc. Based in San Francisco, California.LinkedIn is their client. You are the face being scanned.I had never heard of Persona before this. Most people haven\u2019t. That\u2019s kind of the point \u2014 they sit invisibly between you and the platforms you trust.So I downloaded their privacy policy (18 pages) and their terms of service (16 pages). Here\u2019s what I found.For a three-minute "
  },
  {
    "title": "Acme Weather (acmeweather.com)",
    "points": 194,
    "submitter": "cryptoz",
    "submit_time": "2026-02-21T07:13:38 1771658018",
    "num_comments": 122,
    "comments_url": "https://news.ycombinator.com/item?id=47098296",
    "comments": [
      "Check out zoom.earth, found it recently. They have an app too.https://zoom.earth/Apparently it's by https://neave.com/ who looks like an indy developer out of london (according to this: https://neave.com/legal/privacy/)Also check https://earth.nullschool.net/ by https://github.com/cambeccreply",
      "Good one thanks.reply",
      "Neave has been around forever they\u2019re greatreply",
      "The site doesn\u2019t make it clear, but it\u2019s not available worldwide. The App Store doesn\u2019t tell you where exactly it is available, but it\u2019s not in the UK.This surprised me seeing as one of the example images shows Europe, including the south coast of Britain.reply",
      "Amazing. Just downloaded and will happily pay the $25 if this operates anywhere close to what dark sky was before the buyout. We\u2019ve got some interesting weather on the horizon so this should be a good test.Now they just need to offer an Apple Watch complication like this: https://imgur.com/oTG7MH6Still bothers me that apple weather doesn\u2019t offer this.reply",
      "The app looks beautiful and the multi forecast model makes a lot of sense.I don't think I am ready to pay an annual subscription for it.  Feels like a big ask for the weather when there are so many other free sources to get a forecast.  But I appreciate that the app was made with real intention and wish I you success with it.reply",
      "Looks lovely. I was keen to try this but US and Canada only unfortunately.Also: subscription fatigue is real. Of course I understand that fetching weather data isn\u2019t free etc. (even though I\u2019m intrigued by their homegrown forecast model) but I\u2019ve already got 10+ subscriptions on iOS and I\u2019m not sure if I\u2019ve got the stomach for another. Apple\u2019s weather app is finally good though since the Dark Sky acquisition.reply",
      "> Also: subscription fatigue is real.This. I just went and cancelled a bunch of vampire subscriptions that had accrued in my life (both in and out of the Apple ecosystem) and ended up saving somewhere in the range of $60 a month.I get that people have bills to pay and building and maintaining software costs money, but when everyone wants money from me for every little thing, eventually I have to decide who gets what cut from an increasingly limited sized pie.Apps like this that, while beautiful, replicate functionality that is \"good enough\" that I can get for free are the first thing to be cut.reply",
      "How about reporting on yesterday's weather? Its hard to plan a walk in the forest today if I dont know how much it rained yesterday.reply",
      "I'm having this problem right now, trying to plan some nice long walks out of the city but it's been raining a lot lately. I'd love some kind of map of flooding/muddy conditions, but I don't think it would be feasible without a massive effort (as whether an area is prone to flooding or turning into a mudbath after rain depends on a lot of factors).reply"
    ],
    "link": "https://acmeweather.com/blog/introducing-acme-weather",
    "first_paragraph": "\nAdam Grossman February 16, 2026\n        Fifteen years ago, we started work on the Dark Sky weather app.Over the years it went through numerous iterations \u2014 including more than one major redesign \u2014 as we worked our way through the process of learning what makes a great weather app. Eventually, in time, it was acquired by Apple, where the forecast and some core features were incorporated into Apple Weather.We enjoyed our time at Apple. So why did we leave to start another weather company?It\u2019s simple: when looking at the landscape of the countless weather apps out there, many of them lovely, we found ourselves feeling unsatisfied. The more we spoke to friends and family, the more we heard that many of them did too. And, of course, we missed those days as a small scrappy shop.So let\u2019s try this again\u2026\n\n\n\nOur biggest pet peeve with most weather apps is how they deal (or rather, don\u2019t deal) with forecast uncertainty. It is a simple fact that no weather forecast will ever be 100% reliable: th"
  },
  {
    "title": "Who's liable when your AI agent burns down production? (reading.sh)",
    "points": 25,
    "submitter": "zenoware",
    "submit_time": "2026-02-21T23:52:00 1771717920",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=47106406",
    "comments": [
      "A better analogy would be: you hire a robot contractor to do work. Before it arrives, you are asked if the robot should request permission before going into rooms. You say no, and the robot enters the server room.It does change something for me, despite the meat of your argument still being valid. It clearly is responsible, but so are you.reply",
      "FYI, the HN guidelines state, \"Please don't use HN primarily for promotion. It's ok to post your own stuff part of the time, but the primary use of the site should be for curiosity.\" I would encourage you to submit content from others rather than just your own.reply",
      "Who is liable when your AI sneaks patented or copyrighted code into your codebase?reply",
      "No one, if you don't get caught.reply",
      "I won't read the article but I think this is the role that will remain for humans, to be the 'fall guy' when the vibes go wrong. You will have to live in chronic stress, on call so to speak, so when prod goes down, you will take the blame. Maybe that vibe coded PR you didn't and couldn't read contained a serious bug or security lapse, maybe the system design you didn't do but approved contained a RCE you never knew about. Fun times ahead.reply"
    ],
    "link": "https://reading.sh/whos-liable-when-your-ai-agent-burns-down-production-039193d82746?sk=4921ed2dbc46f0c618835ac458cf5051",
    "first_paragraph": ""
  }
]