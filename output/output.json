[
  {
    "title": "Our Audit of Homebrew (trailofbits.com)",
    "points": 182,
    "submitter": "zdw",
    "submit_time": "2024-07-30T22:39:21",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=41114839",
    "comments": [
      "I'm the author of this post and one of the people behind the audit; happy to answer questions about it.If you're having trouble finding the audit itself (it's linked indirectly), I'm linking a copy here as well[1].[1]: https://github.com/trailofbits/publications/blob/eb9344f2261...\n \nreply",
      "Excellent work - a methodical review like this is exactly what I\u2019ve been looking for in these sorts of open source solutions.I know it\u2019s not the focus of a code review like this, but I\u2019m interested to hear your views on the general supply chain lifecycle problems inherent to open-source package management platforms. Principally, are vetting processes appropriate to ensure that new formulas refer to the correct source? How does the user gain confidence that their brew update is still referencing a trusted source? What happens when a domain is taken over? How quickly can the team respond to untrusted sources from formulas?I know these aren\u2019t all Homebrew problems to solve, but they\u2019re important ecosystem considerations.(These problems also exist in the winget and choco platforms, but less so in commercially supported repos like apt and yum. For me, and many other admins, they are a major concern when it comes to the Windows Store.)Edit: lastly, in case the homebrew team are watching: an npm-style vulnerability notice would be awesome\n \nreply",
      "There's a bunch of TOB-BREW-n listed - are those like CVE numbers just for this project?Edit: Oh, it's \"Trail Of Bits - homeBREW\". But probably still yes.\n \nreply",
      "Yep. We use the TOB-$PRODUCT-$XXXX convention for our audit findings, where $PRODUCT is the target under audit and $XXXX is a unique incrementing counter for each finding.(As far as I know, a lot of audit firms do similar things.)\n \nreply",
      "First time seeing TOB being used honestly, it would\u2019ve helped saying something along the lines of \u201dTrait of Bits (TOB from now on)\u201d\n \nreply",
      "Well done! And thank you to the Open Tech Fund for sponsoring work to protect everyone who uses Homebrew.\n \nreply",
      "A while back I was trying to understand why Homebrew requires pre-built executables to be installed into /home/linuxbrew. I asked about it here[0]. This requirement basically makes it impossible to use homebrew to quickly install programs on systems where you don't have root, or at least have homebrew already configured (not sure if that would solve it but I assume so).They pointed me to an example program that would break if not run this way: Facebook's Watchman[1].It bizarrely (to me) has hard coded paths compiled into it, which force you to run it from specific directories.Would love to understand what's going on here and why you would ever make software work this way. I feel I'm missing a fairly obvious Chesterton's Fence.[0]: https://github.com/orgs/Homebrew/discussions/5371[1]: https://facebook.github.io/watchman/docs/install#prebuilt-bi...\n \nreply",
      "The short (but possibly not satisfying) answer is that Homebrew's relocation of packages (including binary relocation) is best effort, in part because of the myriad ways in which packages can embed absolute (or incorrect relative) paths and other state in their build products. macOS bottles are generally more relocatable (in part because of a lot of scar tissue around binary relocation), but it's a general problem with build system quality, build complexities, and - reasonably - disinterested upstreams.\n \nreply",
      "In the case of Watchman, I have to assume that internal use is the most supported use case, and uniformity of deployment is desirable across the fleet there, and so, configurability wasn't as big of a concern?\n \nreply",
      "That makes sense. The weird part to me is that Homebrew would limit their approach and eliminate an entire class of use cases to accommodate programs that work this way. There has to be more to it.\n \nreply"
    ],
    "link": "https://blog.trailofbits.com/2024/07/30/our-audit-of-homebrew/",
    "first_paragraph": "By William WoodruffThis is a joint post with the Homebrew maintainers; read their announcement here!Last summer, we performed an audit of Homebrew. Our audit\u2019s scope included Homebrew/brew itself (home of the brew CLI), and three adjacent repositories responsible for various security-relevant aspects of Homebrew\u2019s operation:We found issues within Homebrew that, while not critical, could allow an attacker to load executable code at unexpected points and undermine the integrity guarantees intended by Homebrew\u2019s use of sandboxing. Similarly, we found issues in Homebrew\u2019s CI/CD that could allow an attacker to surreptitiously modify binary (\u201cbottle\u201d) builds of formulae and potentially pivot from triggering CI/CD workflows to controlling the execution of CI/CD workflows and exfiltrating their secrets.This audit was sponsored by the Open Tech Fund as part of their larger mission to secure critical pieces of internet infrastructure. You can read the full report in our publications repository.H"
  },
  {
    "title": "Porffor: A from-scratch experimental ahead-of-time JS engine (porffor.dev)",
    "points": 267,
    "submitter": "bpierre",
    "submit_time": "2024-07-30T18:55:10",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=41112854",
    "comments": [
      "Oliver (the main developer) just announced that they\u2019re going to work full time on Porffor: https://x.com/canadahonk/status/1818347311417938237\n \nreply",
      "Financed by defunkt[1], GitHub cofounder and ex CEO, for an undisclosed future project.[1] https://news.ycombinator.com/user?id=defunkt\n \nreply",
      "Also funded the Ladybird browser recently. Seems to like his web.\n \nreply",
      "I have thought about doing this and I just can't get around the fact that you can't get much better performance in JS. The best you could probably do is transpile the JS into V8 C++ calls.The really cool optimizations come from compiling TypeScript, or something close to it. You could use types to get enormous gains. Anything without typing gets the default slow JS calls. Interfaces can get reduced to vtables or maybe even straight calls, possibly on structs instead of maps. You could have an Int and Float type that degrade into Number that just sit inside registers.The main problem is that both TS and V8 are fast-moving, non-standard targets. You could only really do such a project with a big team. Maintaining compatibility would be a job by itself.\n \nreply",
      "At least without additional extensions, TypeScript would help less than you think. It just wasn\u2019t designed for the job.As a simple example - TypeScript doesn\u2019t distinguish between integers and floats; they\u2019re all just numbers. So all array accesses need casting. A TypeScript designed to aid static compilation likely would have that distinction.But the big elephant in the room is TypeScript\u2019s structural subtyping. The nature of this makes it effectively impossible for the compiler to statically determine the physical structure of any non-primitive argument passed into a function. This gives you worse-than-JIT performance on all field access, since JITs can perform dynamic shape analysis.\n \nreply",
      "> A TypeScript designed to aid static compilation likely would have that distinction.AssemblyScript (https://www.assemblyscript.org/) is a TypeScript dialect with that distinction\n \nreply",
      "Such Typescript already exists, Static Typescript,https://makecode.com/languageMicrosoft's AOT compiler for MakeCode, via C++.\n \nreply",
      "Outside of really funky code, especially code originally written in TS, you can assume the interface is the actual underlying object. You could easily flag non-recognized-member accesses to interfaces and then degrade them back to object accesses.\n \nreply",
      "You\u2019re misunderstanding me, I think.Suppose you have some interface with fields a and c. If your function takes in an object with that interface and operates on the c field, what you want is to be able to do is compile that function to access c at \u201cthe address pointed to by the pointer to the object, plus 8\u201d (assuming 64-bit fields). Your CPU supports such addressing directly.Because of structural subtyping, you can\u2019t do that. It\u2019s not unrecognized member. But your caller might pass in an object with fields a, b, and c. This is entirely idiomatic. Now c is at offset 16, not 8. Because the physical layout of the object is different, you no longer have a statically known offset to the known field.\n \nreply",
      "> Because of structural subtyping, you can\u2019t do thatIn practice v8 does exactly what you're saying can't be done, virtually all the time for any hot function. What you mean to say is that typescript type declarations alone don't give you enough information to safely do it during a static compile step. But modern JS engines, that track object maps and dynamically recompile, do what you described.\n \nreply"
    ],
    "link": "https://porffor.dev/",
    "first_paragraph": "Porffor is a unique JS engine/compiler/runtime, compiling JS code to WebAssembly or native ahead-of-time.It is seriously limited for now; intended for research, not serious use!Porffor's WebAssembly output is much faster and smaller compared to existing JS -> Wasm projects. This is because Porffor compiles JS AOT instead of bundling an interpreter.Due to compiling JS ahead-of-time, Porffor can compile to real native binaries without just packaging a runtime like existing solutions. This leads to:Porffor is safe as it compiles to Wasm (and then native). It is also written in a memory safe language (JS).Porffor is written from the ground-up with AOT in mind instead of being based on any existing JS engine. The only dependency is a JS parser.Porffor supports TypeScript input, no clunky transpiler step needed: just feed it a TS file."
  },
  {
    "title": "Troubleshooting: Terminal Lag (cmpxchg8b.com)",
    "points": 67,
    "submitter": "janvdberg",
    "submit_time": "2024-07-30T22:04:39",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41114569",
    "comments": [
      "Upvote just for teaching me about the existence of `hyperfine`.    $ hyperfine 'alacritty -e true'\n    Benchmark 1: alacritty -e true\n      Time (mean \u00b1 \u03c3):      84.1 ms \u00b1   4.9 ms    [User: 40.1 ms, System: 30.8 ms]\n      Range (min \u2026 max):    80.5 ms \u2026 104.4 ms    32 runs\n    \n    $ hyperfine 'xterm -e true'\n    Benchmark 1: xterm -e true\n      Time (mean \u00b1 \u03c3):      81.9 ms \u00b1   2.6 ms    [User: 21.7 ms, System: 7.9 ms]\n      Range (min \u2026 max):    74.9 ms \u2026  87.1 ms    37 runs\n    \n    $ hyperfine 'wezterm -e true'\n    Benchmark 1: wezterm -e true\n      Time (mean \u00b1 \u03c3):     211.7 ms \u00b1  13.4 ms    [User: 41.4 ms, System: 60.0 ms]\n      Range (min \u2026 max):   190.5 ms \u2026 240.5 ms    15 runs\n \nreply",
      "it was a fun read\n \nreply"
    ],
    "link": "https://lock.cmpxchg8b.com/slowterm.html",
    "first_paragraph": "Tavis Ormandy$Id: a07cf90837a3c4373b82d6724b97593810766af7 $I think I should blog more about random troubleshooting sessions, if nothing else it will remind me what steps I took when it inevitably happens again!Okay, here is the first one \u2013 why is my xterm opening so slowly?I have two similarly specced machines at my desk \u2013 my primary workstation running Fedora Linux, and a Windows 11 machine. They share the same monitor and input devices, and I switch between them with an iogear KVM.I do the bulk of my work in either a browser or a terminal. This is true even on Windows, where I rely heavily on  WSL.This works well for me, and I\u2019m happy enough with the setup.I have the shortcut Super+1 bound to xterm on both machines, and I probably use this hundreds of times per day.Here is how that looks on Fedora:It takes about 300ms from key activation to a terminal being ready. This is fine, I\u2019ve never noticed any problem.However, let\u2019s compare that to Windows:That\u2019s about 1600ms before I can typ"
  },
  {
    "title": "Launch HN: SSOReady (YC W24) \u2013 Making SAML SSO painless and open source",
    "points": 201,
    "submitter": "ucarion",
    "submit_time": "2024-07-30T16:19:45",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=41110850",
    "comments": [
      "I can only wish you good luck. I mean it, best of luck to you all.We wrote our own IdP back in the day. It was a cool project, Single Sign On, Single Sign OUT, User provisioning, just all sorts of stuff.And it worked! It's amazing when it works, it's just like magic. You giggle when it works.We did all sorts of integrations. To random Service Providers, integrating with other IdPs, etc. Some were really cool. Great functionality.But I simply float this one caveat.It was never \"painless\". Ever. It was always pulling teeth.The dark truth is you can have the best IdP in the world, but everyone on the other side of the conversation is a black box. You get a lot of payloads simply shipped into the void, never to be seen again, consumed for some unknown reason.Add to that the very often the people you're integrating with have no concept of SAML, its workflows, its payloads, etc., much less the capabilities of their own stack in regards to SAML. So you get to train them (and learn about their system) at the same time.We never had real problems with signing and formatting and such that folks worry about. It was mostly just diagnosing black boxes more than anything, the endless black hole of cert management, etc.So, good luck! I hope it works for you! It's a neat space to play.\n \nreply",
      "> Add to that the very often the people you're integrating with have no concept of SAML, its workflows, its payloads, etc., much less the capabilities of their own stack in regards to SAML. So you get to train them (and learn about their system) at the same time.This is true of a great many protocols, unfortunately. I've seen this with IPSec, HL7v2, \u2026 CSV.IPSec was perhaps the most \u2026 scarring. Always sort of feeling your stomach turn to acid as you wonder to yourself \"will we be able to integrate with the other end?\" when you're trying to work with \"network engineers\" who cannot establish a TCP connection to test if the VPN tunnel is alive. And yeah, it's learn their system as fast as humanly possible to then determine if their setup is correct, and to hunt where the inevitable integration problems lie. (\u2026in the firewall. It was always a firewall, somewhere.) Why other systems feel the need to take the standard terms and reinvent new words for them is beyond me to this day. \"Enterprise\" junk is particularly guilty of it. Most of the learning is just building a mental Rosetta stone of what does the other end's \"appliance\" call this term or that term.\n \nreply",
      "SAML and IPsec both make my eye twitch, and I too have struggled where the person on the other end has no idea.One of my favourites was the time I was trying to figure out a SAML integration with a client, and before the person on the client's \"SSO team\" could figure it out, I installed a demo of their SSO solution, integrated with my own dev AD, and found the checkbox.Yay enterprise!  The Q in enterprise is for quality.\n \nreply",
      "Yeah, I\u2019ve done this too. On the one hand, it\u2019s crazy annoying. On the other hand, at least they _had_ a docker image I could use.\n \nreply",
      "HL7v2, the protocol of \"we just put all the data in this one random field\".\n \nreply",
      "As a base64\u2019d pdf\n \nreply",
      "It's amazing how normalized this is, I was baffled many years ago and I have just accepted it at this point.\n \nreply",
      "Saying> And it worked! It's amazing when it works, it's just like magic. You giggle when it works.And then this> It was never \"painless\". Ever. It was always pulling teeth.Even with a caveat in between, is misleading. I get you\u2019re probably trying to generate revenue, and more power to you. I\u2019d re-work that phrasing next time.\n \nreply",
      "You\u2019re responding to a comment not the op? The commenter isn\u2019t trying to make money just sharing an experience\n \nreply",
      "For info, the documentation for the project https://ssoready.com/docs seems down:502 ERROR\nThe request could not be satisfied.\nCloudFront wasn't able to connect to the origin. We can't connect to the server for this app or website at this time. There might be too much traffic or a configuration error. Try again later, or contact the app or website owner.\nIf you provide content to customers through CloudFront, you can find steps to troubleshoot and help prevent this error by reviewing the CloudFront documentation.\nGenerated by cloudfront (CloudFront)cf.\nhttp://www.site-shot.com/Lu-cUE7TEe-S-wJCrBEAAg\nor\nhttps://archive.is/wip/FhNfDOtherwise the project looks really well done and by nice people.\n \nreply"
    ],
    "link": "item?id=41110850",
    "first_paragraph": ""
  },
  {
    "title": "Metaphysical Experiments Probe Our Hidden Assumptions About Reality (quantamagazine.org)",
    "points": 18,
    "submitter": "Gooblebrai",
    "submit_time": "2024-07-30T22:56:07",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.quantamagazine.org/metaphysical-experiments-test-hidden-assumptions-about-reality-20240730",
    "first_paragraph": "July 30, 2024Nico Roper/Quanta MagazineContributing WriterJuly 30, 2024Metaphysics is the branch of philosophy that deals in the deep scaffolding of the world: the nature of space, time, causation and existence, the foundations of reality itself. It\u2019s generally considered untestable, since metaphysical assumptions underlie all our efforts to conduct tests and interpret results. Those assumptions usually go unspoken.Most of the time, that\u2019s fine. Intuitions we have about the way the world works rarely conflict with our everyday experience. At speeds far slower than the speed of light or at scales far larger than the quantum one, we can, for instance, assume that objects have definite features independent of our measurements, that we all share a universal space and time, that a fact for one of us is a fact for all. As long as our philosophy works, it lurks undetected in the background, leading us to mistakenly believe that science is something separable from metaphysics.But at the unchar"
  },
  {
    "title": "Missing Henry VIII portrait found after random X post (bbc.com)",
    "points": 162,
    "submitter": "Archelaos",
    "submit_time": "2024-07-30T01:10:28",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=41105229",
    "comments": [
      "As an aside, I really appreciated the BBC not loading twitter content by default and asking first.This is pretty amazing that such an offhanded catch led to a recovery of something assumed lost to history though.\n \nreply",
      "Especially when you consider the vast majority of art is hidden away in storage boxes as investments (someone for money laundering) never to be seen again.\n \nreply",
      "When you're an expert at something and you spend all your days looking at these things, you can pick out remarkable details from nothing. Any regular person looking at the original photo (below) would have seen nothing!https://x.com/Warkslieutenant/status/1808884139585610231/pho...(it's the picture with the curved frame on the left)\n \nreply",
      "An article from Smithsonian shows that there's a drawing showing what the gallery had looked like.https://www.smithsonianmag.com/smart-news/art-historian-disc...So somewhere in his brain he must have had \"missing rounded-top portraits of 16th century VIPs, including one of Henry VIII posed kind of like this\" filed away.\n \nreply",
      "my nephew is good at identifying teslas.\"if the door handles are straight - S, together - X, hockey sticks - 3 or Y.  No handles? cybertruck\"  :)\n \nreply",
      "Ultrasonics-2018\u2026\n \nreply",
      "Yeah, I worked with radiologists...  What they can see in x-ray, CT, MR, PET, ultrasound...  it's mind-boggling.\n \nreply",
      "The most mind-blowing thing is just how much those skills rely on tacit knowledge and subconscious pattern recognition.There is really a lesson to be learned here in our age when data-based everything is en vogue. Our decision-making shouldn\u2019t be data-driven, it should be data-informed.\n \nreply",
      "Apparently was part of a set of 22 portraits commissioned in the 1590s by tapestry maker Ralph Sheldon and not the only one missing.According to https://adamfineart.wordpress.com/2024/07/04/ralph-sheldons-...The 1781 sale at Christie\u2019s of the Weston portraits show that the group included portraits of Henry IV, Henry V, Edward IV, Richard III, Henry VII, Queen Elizabeth, Charles V, Prince Arthur, Henry VIII, Francis King of France, Edward VI, Queen Mother of France, Henry of Bourbon, King of France, Cardinal Wolsey, L. Cromwell Earl of Essex, Sir Thomas Moore, Duke of Alva, Comte Eglemont, Duke of Guise, Duke of Parma and the Earl of Essex.\n \nreply",
      "> Apparently was part of a set of 22 portraits commissioned in the 1590s by tapestry maker Ralph Sheldon and not the only one missing.Yes, that's what the article says.\n \nreply"
    ],
    "link": "https://www.bbc.com/news/articles/ckdgp7r5y11o",
    "first_paragraph": "A post on X spotted randomly by an art historian has led to a portrait of King Henry VIII - hanging in a West Midland council hall - to be identified as a famous missing artwork.Adam Busiakiewicz, who works as a consultant for famous auction house Sotheby's, said that when he saw a photo of the work hanging in the Shire Hall, Warwick, it \"just stood out to me\". After inspecting it personally to test his theory, he confirmed the artwork was created for tapestry maker Ralph Sheldon and dated back to the 1590s.It was one of a collection of 22 portraits made for Sheldon, but the whereabouts of only a handful were known.\"The fact I was lucky to piece together [what it was] in an hour is very exciting,\" said Mr Busiakiewicz.The post on X that caught his eye was from the Warwickshire Lieutenancy. The account had shared an image of a reception at the Shire Hall, with the portrait visible - just about - in the background. \"I spend a lot of time thinking about paintings and looking at people's w"
  },
  {
    "title": "The Truth About Linear Regression (2015) (cmu.edu)",
    "points": 192,
    "submitter": "sebg",
    "submit_time": "2024-07-30T16:38:40",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=41111115",
    "comments": [
      "Most people don't appreciate linear regression. \n1) All common statistical tests are linear models: https://lindeloev.github.io/tests-as-linear/\n2) Linear models are linear in the parameters, not the response! E.g. y = a*sin(x)+bx^2 is a linear model.\n3) By choosing an appropriate spline basis, many non-linear relationships between the predictors and the response can be modelled by linear models.\n4) And if that flexibility isn't enough, by virtue of the Taylor Theorem, linear relations are often a good approximation of non-linear ones.\n \nreply",
      "These are all fantastic points, and I strongly agree that most people don't appreciate linear models nearly enough.Another one I would add that is very important: Human beings, especially in groups, can only reasonably make linear decisions.That is, when we are in a meeting making decisions for the direction of the company we can only say things like \"we need to increase ad spend, while reducing the other costs of acquisition such as discount vouchers\". If you want to find the balance between \"increasing ad spend\" while \"decreasing other costs\" that's a simple linear model.Even if you have a great non-linear model, it's not even a matter of \"interpretability\" so much as \"actionability\". You can bring the results of a regression analysis to a meeting and very quickly model different strategies with reasonable directional confidence.I struggled communicating actionable insights upward until I started to really understand regression analysis. After that it became amazingly simple to quickly crack open and understand fairly complex business processes.\n \nreply",
      "I don't follow - could you explain this with a couple of examples? What would a business proposal look like that is analogous to a nonlinear model vs. one that is analogous to a linear model?\n \nreply",
      "If you add a multilevel structure to shrink your (generalized) linear predictors, this framework becomes incredibly powerful.There are entire statistics textbooks devoted to multilevel linear models, you can get really far with these.Shrinking through information sharing is really important to avoid overly optimistic predictions in the case of little data.\n \nreply",
      "Especially when you use the mixed model (aka MLM) framework to automatically select the smoothing penalty for your splines. So in one simple and very intuitive framework, you can estimate linear and nonlinear effects, account for repeated measurements and nested data, and model binary, count, or continuous outcomes (and more), all fitting the model in one shot, yielding statistically valid confidence intervals and p-values.R's mgcv package (which does all of the above) is probably the single reason I'm still using R as my primary stats language.\n \nreply",
      "If you like shrinkage (I do), I highly recommend the work of Matthew Stephens, e.g. ashr [1] and vash [2] for shrinkage based on an empirically derived prior.[1] https://cran.r-project.org/web/packages/ashr/index.html\n[2] https://github.com/mengyin/vashr\n \nreply",
      "Yes, the article linked to ashr is quite famous.\n \nreply",
      "I have a degree in statistics yet I've never thought about the relationship between linear models and business decisions in this way. You're absolutely right. This is the best comment I've read all month.\n \nreply",
      "I don't follow - could you explain this with a couple of examples? What would a business proposal look like that is analogous to a nonlinear model vs. one that is analogous to a linear model?\n \nreply",
      "> Human beings, especially in groups, can only reasonably make linear decisions.There are absolutely decisions that need to get made, and do get made, that are not linear. Step functions are a great example. \"We need to decide if we are going to accept this acquisition offer\" is an example of a decision with step function utility. You can try to \"linearize\" it and then apply a threshold -- \"let's agree on a model for the value at which we would accept an acquisition offer\" -- but in many ways that obscures that the utility function can be arbitrarily non-linear.\n \nreply"
    ],
    "link": "https://www.stat.cmu.edu/~cshalizi/TALR/",
    "first_paragraph": ""
  },
  {
    "title": "Ngtop \u2013 Request analytics from the Nginx access logs (github.com/facundoolano)",
    "points": 13,
    "submitter": "facundo_olano",
    "submit_time": "2024-07-30T23:31:33",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/facundoolano/ngtop",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Request analytics from the nginx access logs\n      ngtop is a command-line program to query request counts from nginx's access.log files.Download the latest release binary for your platform, for example:Alternatively, install with go:Count requests from the last hour:Count requests from the last second, minute, day, week, or month:Count requests from the day before:Show the top 5 urls in the last hour:Show the top 5 urls in the last minute:Show the top 10 urls in the last hour:Count total requests to a specific url in the last hour:Count total requests to urls matching a pattern:Count total requests to urls excluding a value or pattern:Count total requests to one of mutliple urls (one OR another):Count total requests to a specific urls AND referer:Show the top visited urls matching a pattern:Show the top requesting ips:Show the top "
  },
  {
    "title": "Swift Homomorphic Encryption (swift.org)",
    "points": 195,
    "submitter": "yAak",
    "submit_time": "2024-07-30T16:40:13",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=41111129",
    "comments": [
      "I wrote some basic homomorphic encryption code for a hackathon like 8 years ago.  When I interviewed for a BigTechCo [1] about a year later, the topic came up, and when I tried explaining what homomorphic encryption was to one of the interviewers, he told me that I misunderstood, because it was \"impossible\" to update encrypted data without decrypting it. I politely tried saying \"actually no, that's what makes homomorphic encryption super cool\", and we went back and forth; eventually I kind of gave up because I was trying to make a good impression.I did actually get that job, but I found out that that interviewer actually said \"no\", I believe because he thought I was wrong about that.[1] My usual disclaimer: It's not hard to find my work history, I don't hide it, but I politely ask that you do not post it here directly.\n \nreply",
      "I had the same experience with Python's walrus operator [0] in a BigTechCo interview. After few times of the interviewer insisting I had no idea what I was talking about, I wrote it a different way. I can't imagine trying explaining something actually complicated in that environment.It didn't hold me back from the job either. I like to believe the interviewer looked it up later, but I never poked into my hiring packet.[0] It was useful at the time to have a prefix sum primitive. Ignoring annotations, something like this:    def scan(f, items, x):\n        return [x := f(x, item) for item in items]\n \nreply",
      "This happened to me in a grant application. We had written a web application that did a homomorphic encryption based calculation of molecular weight to demonstrate that HE could be used to build federated learning models for chemical libraries.Our reviewers told us that machine learning on encrypted data was impossible. We had the citations and the working model to refute them. Very frustrating.\n \nreply",
      "This is pretty bad. We learned in school how RSA works, which can be easily extended to show HME multiplication at least. I can't remember it off the top of my head, but I know it's possible.\n \nreply",
      "> he told me that I misunderstood, because it was \"impossible\" to update encrypted data without decrypting it. I politely tried saying \"actually no, that's what makes homomorphic encryption super cool\", and we went back and forth; eventually I kind of gave up because I was trying to make a good impression.The moment you have to explain yourself you've already lost.No argument you make will change their mind.They are just stupid and that will never change.And never forget, these people have power over you.\n \nreply",
      "Digression-- this is a good example where the mumbo jumbo that anarchists buzz on about applies in a very obvious way.You were literate in that domain. The interviewer wasn't. In a conversation among equals you'd just continue talking until the interviewer yielded (or revealed their narcissism). The other interviewers would then stand educated. You see this process happen all the time on (healthy) FOSS mailing lists.Instead, you had to weigh the benefit of sharing your knowledge against the risk of getting in a pissing contest with someone who had some unspecified (but real!) amount of power over your hiring.That's the problem with a power imbalance, and it generally makes humans feel shitty. It's also insidious-- in this case you still don't know if the interviewer said \"no\" because they misunderstood homomorphic encryption.Plus it's a BigTechCo, so we know they understand why freely sharing knowledge is important-- hell, if we didn't do it, nearly none of them would have a business model!\n \nreply",
      "In my experience this comes up a lot less often when people are paid to be empirically right, and the most annoying arguments occur when no one has an interest in being right and instead wants to defend their status. e.g. try telling a guy with his date nearby that he's wrong about something irrelevant like how state alcohol minimum markups work. An even more common scenario is when someone is passionate about a political topic and they publicly say something incorrect, and now would look like a fool if they admitted they were wrong. Sometimes I worry that a post-money future would become entirely dominated by status considerations and there would be no domain where people are actually incentivized to be right. Do you know if there's any anarchist thought related to this topic?\n \nreply",
      "That does kind of make sense though - if you are paid to be right but someone doesn't believe you, you are still getting paid, so what does it matter?\n \nreply",
      "> the mumbo jumbo that anarchists buzz on aboutI enjoy exposing myself to new-to-me opinions. Do you know a decent anarchist blog/vlog to dip my toes into this area?\n \nreply",
      "Not OP, nor do I understand what he's referring to, but https://theanarchistlibrary.org/special/index is a good starting point.\n \nreply"
    ],
    "link": "https://www.swift.org/blog/announcing-swift-homomorphic-encryption/",
    "first_paragraph": ""
  },
  {
    "title": "Creativity fundamentally comes from memorization (shwin.co)",
    "points": 14,
    "submitter": "shw1n",
    "submit_time": "2024-07-30T22:37:24",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41114825",
    "comments": [
      "This seems to resonate with my experience, although I feel myself bristling due to the baggage of the word memorization.Although sometimes \u201cmemorization\u201d doesn\u2019t happen because you sit down to do it but rather that you keep using the same things over and over when solving problems that they become internalized. I find that to be a more fruitful path towards understanding that I don\u2019t want to call memorization but it is.\n \nreply",
      "Thanks for reading!And agreed -- it's this exact realization that led me to both this method and titleImo this negative connotation has made many people refrain from calling internalization what it isBut acknowledging that it's memorization has actually made me more efficient at learning, since I can now consciously look for the heuristic, codify it, and try to commit it to memory\n \nreply",
      "I genuinely thought creativity was something else until LLMs hit escape velocity and humbled me hard.After that I realized that creativity wasn't some magical quality that would be hard to reproduce mechanically.And that also made me a little sad.\n \nreply",
      "I will disagree. Creativity comes from applying acquired knowledge (that's where memorization comes into account) in new contexts.\n \nreply",
      "Appreciate you reading!But how do you know how to apply this acquired knowledge in this new context?It's some form of pattern matching right -- which imo is just a less obvious form of memorizationi.e. you've memorized the match between inherent traits of the context with a specific application of that knowledge\n \nreply",
      "Something I noticed from being raised by Indian parents while going through the US school systemAnd again after learning how to acquire new skills quickly\n \nreply"
    ],
    "link": "https://shwin.co/blog/creativity-fundamentally-comes-from-memorization",
    "first_paragraph": "July 2024I'm often made fun of for bringing a \"system\" into creative outlets.Things like:But I think this critique misunderstands what creativity truly is: a flash of inspiration connecting internalized concepts.The inspirational lightning bolt writers and artists experience can't happen unless they know how to write or draw. A pun can't be created unless the author sees the similarity between one word and another they already know. A DJ can't mashup two songs unless they're familiar with both.By definition, you can't even be certain of novelty without familiarity of existing works.Creativity comes to those who have internalized the patterns of their art -- they can see the connection or novelty because it's all in their head.Therefore autonomy enables creativity, and a system helps achieve autonomy quicker.Some time back I discovered a method for learning anything quickly.It involved two steps:For many that remember school, this won't sound novel. Yet what's often missed is the applic"
  },
  {
    "title": "Show HN: Stempad \u2013 Fast Online Scientific Writing (stempad.io)",
    "points": 75,
    "submitter": "ralph_r",
    "submit_time": "2024-07-28T22:54:16",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41096613",
    "comments": [
      "Very nice work. I'll definitely play around with it. For the use case of a digital lab notebook, it would be nice to be able to annotate (superimpose drawings, highlight, etc.) images and pdfs directly in the editor. This would be even more powerful if such annotations could be grouped or locked, so that their position relative to the annotated image remains fixed. This is something that all other note taking apps either miss entirely or implement poorly (or maybe there's something else out there that does this well?). Is this possible in Stempad, or are you considering adding such functionality?\n \nreply",
      "Glad to hear you're interesting in trying it. For your use case, I'm curious, are you referring to general use annotations (ex. highlighting, text boxes, commenting, drawing), or are you specifically interested in scientific annotations (ex. Adding an annotation with math or code in it)?Regardless, it's definitely a realistic feature to have. I'm thinking of it as a block right now, perhaps a file block, that can be resized and annotated freely. Maybe annotations can be mini, floating, moveable and resizable editors. Would that make a big difference for you?\n \nreply",
      "My biggest frustration as an academic was reproducibility of papers I was reading. The pdf is such a useless medium for information transfer and the academic publishing industry is a complete racket where all the value is generated by the authors and reviewers who work for free and have to pay (in most cases) to have their work accessible freely to the public. I would love to see this turn into a default way to publish papers\n \nreply",
      "If you're lucky, someone releases code+data associated with their published paper. If you're really lucky, that code and data is in the same state as it was in the published paper. If you're really really lucky, someone besides the author can get it to run.If you can consistently locate and run academic publication code without direct help from the authors, you are The Chosen One.[edit]In seriousness, reproducibility is also my biggest concern. Scientific/academic publishing could do a lot better than rendering pretty static documents - we can provide the data, code, version control, and build processes which produced the paper so anyone can reproduce what they see in the paper. AND we could host them together so they're bidirectionally linked, to facilitate other scientists building on top of our work.That could be our future, with the right incentive structures in place.\n \nreply",
      "Isn't that the idea (or perhaps the promise) of languages like R or notebook tools like Jupyter or Collab, which provide a means to ingest, clean, analyse and present your data, then share the code you've used to do that.\n \nreply",
      "I like notebooks, they are a useful tool. But they are just a slight adjustment to the programming model and an alternative type IDE. It does not do much in terms of helping reproducibility. Data, software and dependency versioning is much more important. And verification that the code indeed runs on another machine, and produces the correct results. Setting up CI for the project, and basic end2end tests is the minimum level I set for my research (in applied machine learning).\n \nreply",
      "> I hated that handwriting was the only viable way to do fast or impromptu scientific writing.> Messy class notes: Professors upload pictures of hastily handwritten class notes as supplementary materialOff-topic: What state of the art open source methods of doing handwriting OCR?\n \nreply",
      "I've always been confused by these takes because it seems to me that the effort spent reviewing and organizing the original less-organized version of lecture notes is precisely when I learn the most.But to your question about OCR, I'd like to extend it with another question? Is there a fundamental difference between the OCR that we might use to solve a caption or read a street sign versus the technology to OCR a paragraph of prose?\n \nreply",
      "Is the Python code executing via Pyodide in the browser?\n \nreply",
      "That's correct\n \nreply"
    ],
    "link": "https://www.stempad.io/editor/85b7f0fc-c865-4011-89a8-216d8b88b249?copy=true",
    "first_paragraph": "Welcome to Stempad, a place where all different types of scientific writing can co-exist. Yesterday, I had a dream about the Butterfly Curve. The equation of this curve generates an elegant butterfly-shaped plot.Let's try and graph it.The butterfly curve was discovered by Temple H. Fay in 1989. Not to be confused with the Sextic Plan Curve shown below.We can plot a variation of the butterfly curve using Python.We can even try to draw a molecule that resembles the Butterfly Curve if we wanted to.Why stop there? What about a butterfly shaped short circuit?Remarkable work. But there are likely better tools suited for drawing arbitrary shapes.And that's a wrap."
  },
  {
    "title": "Was the Internet created to survive a nuclear strike? (2022) (siliconfolklore.com)",
    "points": 154,
    "submitter": "edward",
    "submit_time": "2024-07-30T13:20:14",
    "num_comments": 111,
    "comments_url": "https://news.ycombinator.com/item?id=41108884",
    "comments": [
      "I don't think that the Internet was created to survive a nuclear strike, but I think we can say that it was _designed_ to survive a nuclear strike, that was one of the reason that packet switching was invented (compared to the traditional, at the time, circuit switching).The idea of packet switching as a way make a communication network more robust came to Paul Baran a few years earlier, and while it was not the basis of Arpanet, it probably influenced it. Wikipedia [1] is not the best of the sources, but it sums it up nicely:\"After joining the RAND Corporation in 1959, Baran took on the task of designing a \"survivable\" communications system that could maintain communication between end points in the face of damage from nuclear weapons during the Cold War. Then, most American military communications used high-frequency connections, which could be put out of action for many hours by a nuclear attack.[...]After proving survivability, Baran and his team needed to show proof of concept for that design so that it could be built. [...] The result was one of the first store-and-forward data layer switching protocols, a link-state/distance vector routing protocol, and an unproved connection-oriented transport protocol. Explicit detail of the designs can be found in the complete series of reports On Distributed Communications, published by RAND in 1964.[...]The Distributed Network that Baran introduced was intended to route around damage.[...]In 1969, when the US Advanced Research Projects Agency (ARPA) started developing the idea of an internetworked set of terminals to share computing resources, the reference materials that they considered included Baran and the RAND Corporation's \"On Distributed Communications\" volumes. The resiliency of a packet-switched network that uses link-state routing protocols, which are used on the Internet, stems in some part from the research to develop a network that could survive a nuclear attack.\"[1] https://en.wikipedia.org/wiki/Paul_Baran\n \nreply",
      "I've noticed that the closer I am to historical events, the more wrong reporting of those events tends to be. I have a neighbor who worked at RAND in the 60s and will ask him about this next time we meet.Worth noting perhaps that many (all?) technical innovations are the result of some underlying technology maturing to the point that it can be applied to a problem. In this case, I bet that nobody liked the fragility and brittleness of circuit switched networking, but in order to make a packet switched network you need small fast computers that are cheap enough to deploy as network nodes. These appeared : minicomputers. The first ARPANet nodes were minicomputers running routing software. In fact the Internet used regular computers as routers into near modern history (IBM RISC machines iirc were deployed at the DS3 upgrade). So PSN is the result of a) people sitting around wishing they could have a PSN, and b) the technology to actually realize that becoming practical. There's no eureka moment.\n \nreply",
      "It\u2019s impossible to know everything that\u2019s happening at the same time.  So while ARPANET was the 2nd packet switched network behind NPL a journalist wouldn\u2019t have a clue.The original conception of Message Blocks was for routing around a damaged network, but the term for Packet Switching was actually a means for multiple users to share a single connection.  ARPANET included many ideas from NPL but was also its own thing.So who invented Packet Switching depends on what parts of it you consider critical.\n \nreply",
      "> IBM RISC machines iirc were deployed at the DS3 upgradeYou recall correctly. Info/pictures here[0].0 - https://www.rcsri.org/collection/nsfnet-t3/\n \nreply",
      "> I've noticed that the closer I am to historical events, the more wrong reporting of those events tends to be.That is not just for historical events, but for anything you are familiar with.Michael Crichton coined the term Gell-Mann Amnesia Effecthttps://theportal.wiki/wiki/The_Gell-Mann_Amnesia_Effect\n \nreply",
      "For those who are interested in that sort of thing, Baran's full 11 publications outlining the design goals and principles of packet-switched networks in January of 1964 are available from RAND as downloadable PDFs:<https://www.rand.org/about/history/baran.html>The first document introduces the basic problem:Let us consider the synthesis of a communication network which will allow several hundred major communications stations to talk with one another after an enemy attack. As a criterion of survivability we elect to use the percentage of stations both surviving the physical attack and remaining in electrical connection with the largest single group of surviving stations. This criterion is chosen as a conservative measure of the ability of the surviving stations to operate together as a coherent entity after the attack. This means that small groups of stations isolated from the single largest group are considered to be ineffective.<https://www.rand.org/pubs/research_memoranda/RM3420.html>\"Attack\" isn't defined, but it's clear that resilience against broad assaults was a key consideration from the very beginning.\n \nreply",
      "> \"Attack\" isn't defined, but it's clear that resilience against broad assaults was a key consideration from the very beginning.It was a consideration for Baran's work, but not for ARPAnet:From chapter two of Wizards:> Roberts also learned from Scantlebury, for the first time, of the work that had been done by Paul Baran at RAND a few years earlier. When Roberts returned to Washington, he found the RAND reports, which had actually been collecting dust in the Information Processing Techniques Office for months, and studied them. Roberts was designing this experimental network not with survivable communications as his main\u2014or even secondary\u2014concern. Nuclear war scenarios, and command and control issues, weren\u2019t high on Roberts\u2019s agenda. But Baran\u2019s insights into data communications intrigued him nonetheless, and in early 1968 he met with Baran. After that, Baran became something of an informal consultant to the group Roberts assembled to design the network. [\u2026]* https://www.goodreads.com/book/show/281818.Where_Wizards_Sta...* https://en.wikipedia.org/wiki/Larry_Roberts_(computer_scient...\n \nreply",
      "Influences can be manifold and subtle.  What's clear is that in 1964, as packet-switched networks were first being considered, resilience against \"attack\", however defined, was a key consideration for one significant set of innovators.  I suspect some of that logic was incorporated into ARPANET's eventual design, whether the nominal head designer was aware of this or not.One underappreciated aspect of complex projects is how different goals and intentions may exist simultaneously, and how much active participants even at a high level may not be awarer of this.  A friend had a uni professor who'd been part of the Glomar Explorer scientific mission which served as a cover for Project Azoran, the clandestine recovery of a Soviet nuclear submarine.  The professor was unaware of the actual mission until well after the mission had concluded, when he read about it in the newspaper.<https://en.wikipedia.org/wiki/Project_Azorian>I tend to strongly discount personal-experience denials of covert or secondary mission roles by people otherwise connected with an activity (e.g., company employees, contractors, government workers, contractors, etc.).  It's not that these are never, or even mostly incorrect.  It's simply that for a large enough project, some secondary goal might well exist without the conscious awareness of many of those involved.And again, in the case of Baran, we have the receipts.\n \nreply",
      "> but I think we can say that it was _designed_ to survive a nuclear strikeOn what basis? What is the distinction between being \"created\" to survive a nuclear strike, and being \"designed\" to do so?>  that was one of the reason that packet switching was invented (compared to the traditional, at the time, circuit switching).Yes, but I don't think it's a relevant one. Baran's papers kinda-sorta-maybe had some influence on ARPANET, but ARPANET mostly got packet-switching (and certainly the term \"packet\")  from Donald Davies. If you look at the actual layout of ARPANET it wasn't very survivable (not much redundancy in the links) [0], compared to Baran's proposal [1].  Internetworking and \"the Internet\" as we know it came much later and was way beyond the point where Baran had any influence.[0]: https://commons.wikimedia.org/wiki/Category:ARPANET_maps\n[1]: https://personalpages.manchester.ac.uk/staff/m.dodge/cyberge...\n \nreply",
      "> but I think we can say that it was _designed_ to survive a nuclear strikeThere is a lot more to the design of 'the internet' than the selection of a packet switched protocol. The early boxes were not hardened in any way and were intended to support computer timesharing amongst academic researchers. The DoD's C2 providers themselves rejected the concept of a decentralised packet-switched network because 'it would never work'. Which is why the relevant theoretical papers were sitting on the shelf and available when ARPANET was designed\n \nreply"
    ],
    "link": "https://siliconfolklore.com/internet-history/",
    "first_paragraph": "You've probably heard the story of how the Internet was designed to withstand a nuclear attack. It usually goes \"DARPA was doing cold war planning and was eager for a distributed resilient command-and-contro\"\u2026 actually let's hear Professor Rob Larson, author of Bit Tyrants: The Political Economy of Silicon Valley, give it on Oct 18, 2022:\n\n\nIs any of this true?\n\nApparently, the Internet is from \"back in the 1980s\", was called DARPAnet and was built to defend against a nuclear attack.\nIs that true? Let's find out.\nThis is the story of the various histories of the Internet, where they came from and which ones are real.\nThe early Internet didn't look like the mountainside NORAD bunkers you see in movies. Instead it was computers sitting in normal offices in university buildings without any backup power, fortification, security (network or physical) and no connections to military communication. Designed for World War 3? The children in \"Duck and Cover\" from 1951 had a better chance of surv"
  },
  {
    "title": "The Hitchhiker's Guide to Logical Verification [pdf] (2023) (browncs1951x.github.io)",
    "points": 56,
    "submitter": "nextos",
    "submit_time": "2024-07-30T17:51:08",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41112103",
    "comments": [
      "(2023)Official link: https://lean-forward.github.io/hitchhikers-guide/2023/\n \nreply",
      "Lean is quite nice, but last time I checked this was the only software verification-oriented book. Anything else worth knowing?Most work in Lean is geared towards mathematics, in contrast to Isabelle and Coq. Actually, The Hitchhiker's Guide is a rewrite of Concrete Semantics (Isabelle).Concrete Semantics is really worth checking out too: http://concrete-semantics.org/concrete-semantics.pdfI'm optimistic about LLMs in this context, as I think they will reduce the cost of formal methods making them much more practical.\n \nreply",
      "> Anything else worth knowing?K frameworkhttps://runtimeverification.com/blog/k-framework-an-overview\n \nreply",
      "K Framework is very interesting.On its surface, you could say it\u2019s a language and toolset for language designers to implement languages and construct formal proofs of your language\u2019s semantics.The underlying semantics for this language is based on something called matching logic, which defines a system of logic, not for true and false statements, but pattern matching expressions where true corresponds to \u201cmatches everything\u201d and false corresponds to \u201cmatch nothing\u201d.There\u2019s even a Lean proof for matching logic.https://gitlab.com/ilds/aml-lean/MatchingLogicAlso, it seems like matching logic is able to subsume so many other systems of logic:https://en.m.wikipedia.org/wiki/Matching_logicThe craziest part of K Framework is how so many of the fundamentals ideas go back to the 70s in a language called OBJ.What you have to understand is OBJ was this language was inspired by the collaboration and lifetime friendship between Rod Burstall and Joe Goguen, who was a category theorist working on defining algebraic data types.They laid the foundation to create a formal specification language, that let you construct abstract data types, mathematical objects, systems of logic and domain specific languages in a REPL environment. They also formalized what a system of logic was in category theory and then used THAT as the semantic foundation for their new language.OBJ influenced all sorts of languages like ML, C++, Haskell, Coq, and even the Lean language itself.So when I say K Framework has roots, those roots really go back to a family of logic based languages that have been living in relative obscurity for decades, but is now finding itself well positioned to solve many new interesting applications.IMO, OBJ and Maude (based on OBJ) are like the Lisps of category theory based languages, especially after looking at how Maude is defined in Maude.Sorry for the long post. I\u2019ve been exploring this history for months and I\u2019m still floored by OBJ.\n \nreply",
      "> Sorry for the long post.No need at all. I'm very interested in this area too, but didn't know all the OBJ lore. I got into this kind of thing after learning about Hilbert's program, which goes all the way back to the late 1800s. The history of formal logic in general is incredibly deep/very very interesting.\n \nreply",
      "Logicomix: An Epic Search for Truth (2009) hints that pushing formal logic forward may be as attractive, yet dangerous, for mental health as ascending Everest without supplemental O\u2082 is for physical:  Weave a circle round them thrice,\n  And close your eyes with holy dread\n  For they on honey-dew hath fed,\n  And drunk the milk of Paradise.\n \nreply",
      "It really is and I\u2019m glad you appreciated it.I love these stories and how you can show the lasting impact that these very direct and personal influences had on the tools we use today.I recently learned that Burstall met Peter Landin and Christopher Strachey through this underground study group forms by a gentleman named Mervyn Pragnell, who skulked around in book shops so he could invite people who were reading books on logic to his group where they would study lambda calculus together.To think a meetup organizer in the early 60s had this lasting impact that carries to today.\n \nreply",
      "Thanks, I meant other Lean resources. This is also interesting, though.\n \nreply",
      "Building High Integrity Applications with SPARKhttps://www.amazon.com/Building-High-Integrity-Applications-...That will be more practical than most works on formal verification.\n \nreply",
      "Actual source, which also provides an alternative PDF suitable for smaller screens: https://github.com/blanchette/logical_verification_2023\n \nreply"
    ],
    "link": "https://browncs1951x.github.io/static/files/hitchhikersguide.pdf",
    "first_paragraph": ""
  },
  {
    "title": "Functional programming languages should be better at mutation than they are (cohost.org)",
    "points": 66,
    "submitter": "thunderbong",
    "submit_time": "2024-07-30T19:18:16",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=41113101",
    "comments": [
      "The article utterly falls apart in its first paragraph where it itself acknowledges that the whole ML family including Ocaml has perfect support for mutation, rightfully assume most Ocaml programmers would choose to not use it most of the time but then assume incorrectly that it\u2019s because the language makes it somehow uneasy. It\u2019s not. It\u2019s just that mutation is very rarely optimal. Even the exemple given fails:> For example, let's say you're iterating over some structure and collecting your results in a sequence. The most efficient data structure to use here would be a mutable dynamic array and in an imperative language that's what pretty much everyone would use.Well, no, this is straight confusion between what\u2019s expressed by the program and what\u2019s compiled. The idiomatic code in Ocaml will end up generating machine code which is as performant than using mutable array.The fact that most programming languages don\u2019t give enough semantic information for their compiler to do a good job doesn\u2019t mean it necessary has to be so.  Functional programmers just trust that their compiler will properly optimize their code.It gets fairly obvious when you realise that most Ocaml developers switch to using array when they want to benefit from unboxed floats.The whole article is secretly about Haskell and fails to come to the obvious conclusion: Haskell choice of segregating mutations in special types and  use monads was an interesting and fruitful research topic but ultimately proved to be a terrible choice when it comes to language design (my opinion obviously not some absolute truth but I think the many fairly convoluted tricks haskellers pull to somehow reintroduce mutations support it). The solution is simple: stop using Haskell.\n \nreply",
      "> Well, no, this is straight confusion between what\u2019s expressed by the program and what\u2019s compiled. The idiomatic code in Ocaml will end up generating machine code which is as performant than using mutable array.This cannot be true in general. There are machine code patterns for which arrays are faster than linked lists. The OCaml compiler, great as it is, won't turn linked list source code into array machine code. Therefore, there is idiomatic code in OCaml that will not be as performant as arrays.> It gets fairly obvious when you realise that most Ocaml developers switch to using array when they want to benefit from unboxed floats.This is one example why your statement above is not true.\n \nreply",
      "> This is one example why your statement above is not true.You are misreading my comment. I\u2019m not intentionally contradicting myself in two paragraphs next to each other (I\u2019m not always the brightest but still).The point is that contrary to what the article states ML developers are not avoiding mutations because they are uneasy to use but because they trust their compiler when they know it will do good. Proof is that in other case they will use mutations when it makes sense to do so because the compiler does not do a good job.The first paragraph refers to the specific case my comment quotes just before: data structure traversal and storage of elements in a set.\n \nreply",
      "> The point is that contrary to what the article states ML developers are not avoiding mutations because they are uneasy to use but because they trust their compiler when they know it will do good. Proof is that in other case they will use mutations when it makes sense to do so because the compiler does not do a good job.It will do a good job, yes. Will it do the best possible job compared to some other algorithm or data structure? It can't. Not in general.And maybe not in the specific case either:> The first paragraph refers to the specific case my comment quotes just before: data structure traversal and storage of elements in a set.So, this: https://ocaml.org/play#code=bW9kdWxlIE15X2R5bmFycmF5ID0gc3Ry...    $ for len in 5_000_000 10_000_000 25_000_000; do echo \"-- ${len} elements --\"; ./a.out list $len; ./a.out dynarray $len; ./a.out my_dynarray $len; echo; done\n    -- 5_000_000 elements --\n    list:        0.191551 sec\n    list:        0.196947 sec\n    list:        0.192806 sec\n    dynarray:    0.301362 sec\n    dynarray:    0.268592 sec\n    dynarray:    0.266118 sec\n    my dynarray: 0.163004 sec\n    my dynarray: 0.142986 sec\n    my dynarray: 0.143634 sec\n    \n    -- 10_000_000 elements --\n    list:        0.377447 sec\n    list:        0.367951 sec\n    list:        0.312575 sec\n    dynarray:    0.607158 sec\n    dynarray:    0.582378 sec\n    dynarray:    0.538621 sec\n    my dynarray: 0.319705 sec\n    my dynarray: 0.296607 sec\n    my dynarray: 0.286634 sec\n    \n    -- 25_000_000 elements --\n    list:        0.971244 sec\n    list:        0.953493 sec\n    list:        0.922049 sec\n    dynarray:    1.515892 sec\n    dynarray:    1.319543 sec\n    dynarray:    1.328461 sec\n    my dynarray: 1.119322 sec\n    my dynarray: 0.971288 sec\n    my dynarray: 0.973556 sec\n    \n    -- 50_000_000 elements --\n    list:        1.852812 sec\n    list:        1.848514 sec\n    list:        1.505391 sec\n    dynarray:    3.065143 sec\n    dynarray:    2.941400 sec\n    dynarray:    2.672760 sec\n    my dynarray: 2.115499 sec\n    my dynarray: 1.963535 sec\n    my dynarray: 1.995470 sec\n    \n    -- 75_000_000 elements --\n    list:        2.942536 sec\n    list:        2.910063 sec\n    list:        2.354291 sec\n    dynarray:    4.567284 sec\n    dynarray:    4.342670 sec\n    dynarray:    3.979809 sec\n    my dynarray: 2.528073 sec\n    my dynarray: 2.225738 sec\n    my dynarray: 2.226844 sec\n\nA simple dynamic array implementation (my_dynarray) beats a list over a wide range of lengths. But not at all lengths! OCaml's built-in Dynarray is not competitive, but that's because it wants to make certain strong guarantees.To be clear, I agree with your general point that we can do just fine writing nice clean pure functional OCaml code for most of our code and can hand-optimize where needed. But your very specific claims rub me the wrong way.\n \nreply",
      "> The OCaml compiler, great as it is, won't turn linked list source code into array machine code.Why not? If the compiler can see that you have a short-lived local linked list and are using it in a way for which an array would be faster, why would it not do the same thing that an array would do?\n \nreply",
      "> Why not?Because it doesn't. Doesn't mean it couldn't, if it tried hard enough. But it doesn't, as a statement of current fact.\n \nreply",
      "Well, it doesn't make the substance wrong. This paragraph rightly summarizes it:\"The fact that most programming languages don\u2019t give enough semantic information for their compiler to do a good job doesn\u2019t mean it necessary has to be so. Functional programmers just trust that their compiler will properly optimize their code.\"\n \nreply",
      "I mostly agree with your sentiment but this:> Well, no, this is straight confusion between what\u2019s expressed by the program and what\u2019s compiled. The idiomatic code in Ocaml will end up generating machine code which is as performant than using mutable array.I disagree with. There are different ways to get close to the performance of `Array.map` with lists (best case scenario you don't care about order and can use `List.rev_map`), but you will always have memory (and GC) overhead and so lists are strictly inferior to arrays for the presented use case.\n \nreply",
      "That\u2019s not what the article is talking about. The proposed exemple is a traversal of a different data structure to collect results in an array. That\u2019s a fold and will properly be tco-ed to something equivalent to adding to an array if you use list cons in the aggregation, might actually be better depending on how much resizing of the array you have to do while traversing.\n \nreply",
      "Works if you are building one list, but what if you are building multiple? What's suggested on the OCaml site and what's taught in most of academia is to use a recursive function with accumulator arguments that are reversed before returning to make the function tco:able. I doubt OCaml can optimize that pattern well, but idk.\n \nreply"
    ],
    "link": "https://cohost.org/prophet/post/7083950-functional-programming",
    "first_paragraph": "Highly, terribly dangerous posts. Misuse of this profile can invite eggbug to trounce upon your data and then laugh in your face. You don't want this profile. Really.A lot of people think that functional programming is mostly about avoiding mutation at all costs.\nEven though persistent data structures are great and there is definitely some truth to it, this view just doesn't really hold up in reality.Many data structures fundamentally require some form of mutation (e.g. union find) and even something simple like a persistent sequential data structure that allows both fast appends and fast random access is orders of magnitude more complicated and still slower than a naive dynamic mutable array.So really, functional languages need to allow mutation in some way if they don't want nearly every program to suffer from completely unnecessary overhead in terms of both time and implementation complexity.Existing languages have a few options here but I don't think any of these are good enough.Th"
  },
  {
    "title": "Building static binaries with Go on Linux (thegreenplace.net)",
    "points": 11,
    "submitter": "ingve",
    "submit_time": "2024-07-30T22:07:53",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://eli.thegreenplace.net/2024/building-static-binaries-with-go-on-linux/",
    "first_paragraph": "One of Go's advantages is being able to produce statically-linked\nbinaries [1]. This doesn't mean that Go always produces such binaries by default,\nhowever; in some scenarios it requires extra work to make this happen.\nSpecifics here are OS-dependent; here we focus on Unix systems.This post goes over a series of experiments: we take simple programs and use\ngo build to produce binaries on a Linux machine. We then examine whether\nthe produced binary is statically or dynamically linked. The first example is\na simple \"hello, world\":After building it with go build, we get a binary. There are a few ways on\nLinux to determine whether a binary is statically or dynamically linked. One\nis the file tool:You can see it says \"statically linked\". Another way is to use ldd, which\nprints the shared object dependencies of a given binary:Alternatively, we can also use the ubiquitous nm tool, asking it to list the\nundefined symbols in a binary (these are symbols the binary expects the dynamic\nlinker to p"
  },
  {
    "title": "Zuo: A Tiny Racket for Scripting (github.com/racket)",
    "points": 14,
    "submitter": "pretext",
    "submit_time": "2024-07-25T12:45:36",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41068016",
    "comments": [
      "https://docs.racket-lang.org/zuo/Note that this isn't an introduction to the language.  It looks like a Scheme variant for specialized purposes of the developers of Racket, with some opinionated conveniences for that.\n \nreply",
      "It's a replacement for make, but definitively not a drop in replacement. To understand why, it's better to read the initial announcement/pull-request by Matthew https://github.com/racket/racket/pull/4179\n \nreply",
      "See also Rash: The Reckless Racket Shellhttp://rash-lang.org/https://github.com/willghatch/racket-rash\n \nreply",
      "Neat.  Rash makes some different syntax decisions than Scsh.  https://scsh.net/\n \nreply"
    ],
    "link": "https://github.com/racket/zuo",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A tiny Racket for scripting\n      This is a mirror of the Zuo sources in the racket/src/zuo directory of\nthe Racket repository.You should use Racket to write scripts. But what if you need something\nmuch smaller than Racket for some reason \u2014 or what if you're trying\nto script a build of Racket itself? Zuo is a tiny Racket with\nprimitives for dealing with files and running processes, and it comes\nwith a make-like embedded DSL.Zuo is a Racket variant in the sense that program files start with\n#lang, and the module path after #lang determines the parsing and\nexpansion of the file content. That's how the make-like DSL is\ndefined, and even the base Zuo language is defined by layers of\n#langs. One of the early layers implements macros.See local/hello.zuo,\nlocal/tree.zuo,\nlocal/image.zuo, and\nbuild.zuo.Compile zuo.c with a C compiler. No ad"
  },
  {
    "title": "Implementing and Improving Skiplists (mattjhall.co.uk)",
    "points": 15,
    "submitter": "tosh",
    "submit_time": "2024-07-26T17:06:36",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41080273",
    "comments": [
      "> Ever implemented a balanced binary tree? Me neither, seems a pain! Red-Black, AVL, rotation, uncles/aunts, balance factors, sub-trees - that's a lot to keep track of.Yeah, because they give you pretty darn strong deterministic guarantees.> Luckily the world of computer science has a data structure that is far easier to implement and has similar properties: the skiplist.It's far easier to implement... and it doesn't provide the same guarantees.\n \nreply",
      "I somehow misread that as \"ski pistes\" rather than skiplists! :-)Is it winter yet?\n \nreply"
    ],
    "link": "https://mattjhall.co.uk/posts/skiplists.html",
    "first_paragraph": ""
  },
  {
    "title": "Translating All C to Rust (TRACTOR) (darpa.mil)",
    "points": 229,
    "submitter": "steveklabnik",
    "submit_time": "2024-07-30T15:42:06",
    "num_comments": 197,
    "comments_url": "https://news.ycombinator.com/item?id=41110269",
    "comments": [
      "See also https://sam.gov/opp/1e45d648886b4e9ca91890285af77eb7/view",
      "Direct link to Proposer's Day info [PDF]:  https://sam.gov/api/prod/opps/v3/opportunities/resources/fil...\"The purpose of this event is to provide information on the TRACTOR technical goals and challenges, address questions from potential proposers, and provide an opportunity for potential proposers to consider how their research may align with the TRACTOR program objectives.\"",
      "That sounds ... hard. Especially as idiomatic Rust as written by skilled programmers looks nothing like C, and most interesting code is written in C++ anyway.Isn't it equivalent to statically determining the lifetimes of all allocations in the C program, including those that are implemented using custom allocators or which cross into proprietary libraries? There's been a lot of research into this sort of thing over the years without much success. C/C++ programs can do things like tie allocation lifetimes to what buttons a user clicks, without ref counting or other mechanisms to ensure safety. It's not a good idea, but, they can do it.The other obvious problem with trying to write such a static analysis is that the programs you're analyzing are by definition buggy and the lifetimes might not make sense (if they did, they wouldn't have memory safety holes and wouldn't need to be replaced). The only research I've seen on this problem of statically detecting what lifetimes should be does assume the code being analyzed is actually correct to begin with. I guess you could try and aim for a program that detects where lifetimes can't be worked out and asks the developer for help though.\n \nreply",
      "It's very hard; DARPA likes to fund hard things[1] :-).This isn't, however, DARPA's first foray into automatic program translation, or even automatic translation into Rust[2].[1]: https://www.urbandictionary.com/define.php?term=DARPA%20hard[2]: https://c2rust.com/\n \nreply",
      "DARPA is basically a state-sponsored VC that optimizes for completely different things. Instead of looking for 100x financial returns, they want technical advantages for the United States. The \"moat\" is the hardness of developing and operationalizing those technologies first.\n \nreply",
      "To be pedantic, In-q-tel is the literal state-sponsored VC.DARPA is a step closer to traditional research labs but there is obviously some overlap.https://en.wikipedia.org/wiki/In-Q-Tel\n \nreply",
      "> DARPA is a step closer to traditional research labs but there is obviously some overlap.It's more like the NSF but focused on commercial grantees with project management thrown on top to orchestrate everything.The really unique part is how much independence each program manager has and the term limits that prevent empire building.\n \nreply",
      "DARPA's commercialization track record is decidedly mixed, so the VC comparison is unexpectedly apt :-)(But yes: DARPA's mandate is explicitly to discover and develop the next generation of emerging technologies for military use.)\n \nreply",
      "Decades ago, as my father explained to me, ARPA (no \"D\" at that time) was happy if 1% of their projects went all the way through to successful deployment.  If they had a higher success rate it would mean they weren't aiming high enough.\n \nreply",
      "> DARPA's commercialization track record is decidedly mixed...If you count my number of attempts, sure.If you count by impact, it's hard to come up with many things more impactful than the Internet...?\n \nreply"
    ],
    "link": "https://www.darpa.mil/program/translating-all-c-to-rust",
    "first_paragraph": "\u00a0\u00a0\u00a0"
  },
  {
    "title": "Butterflies accumulate static electricity to attract pollen without contact (bristol.ac.uk)",
    "points": 174,
    "submitter": "thunderbong",
    "submit_time": "2024-07-30T09:32:56",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=41107480",
    "comments": [
      "Similarly, female trees are charged to attract pollen to them.  In most cities (in the US) trees planted on streets aren't allowed to be fruiting (because of rats?).  Most fruitless/litterless trees are just the Dioecious male variety.  This causes air quality issues as there is an over abundance of pollen in the air, which causes allergies and increased respiratory problems.\n \nreply",
      "That's a myth started by one guy. https://slate.com/technology/2021/10/botanical-sexism-viral-...\n \nreply",
      "This suggests installation of electrostatic air cleaners, perhaps solar powered.\n \nreply",
      "Infinitely complex solutions to self inflicted and completely trivial issues, humans are amazing!\n \nreply",
      "I think you may have missed the subtle joke in:> electrostatic air cleaners, perhaps solar poweredAKA trees.\n \nreply",
      ">.>\n \nreply",
      "If only it wasn't impossible to design cities for streets to maintain tree cover to help air stay cooler near ground level, etc..\n \nreply",
      "Or fake plastic trees.\n \nreply",
      "Or planting more female trees and hiring cats to hunt rats?\n \nreply",
      "To much cats already, they hunt the birds that used to hunt mosquitos. Suburb tragedy.\n \nreply"
    ],
    "link": "https://www.bristol.ac.uk/news/2024/july/butterflies-static-electricity.html",
    "first_paragraph": "HawkmothDr Sam EnglandPeacock butterflyDr Sam EnglandPress release issued: 24 July 2024Butterflies and moths collect so much static electricity whilst in flight, that pollen grains from flowers can be pulled by static electricity across air gaps of several millimetres or centimetres.The finding, published today in the Journal of the Royal Society Interface, suggests that this likely increases their efficiency and effectiveness as pollinators.The University of Bristol team also observed that the amount of static electricity carried by butterflies and moths varies between different species, and that these variations correlate with differences in their ecology, such as whether they visit flowers, are from a tropical environment, or fly during the day or night. This is the first evidence to suggest that the amount of static electricity an animal accumulates is a trait that can be adaptive, and thus evolution can act upon it by natural selection.Lead author Dr Sam England from Bristol\u2019s Sch"
  },
  {
    "title": "Taking command of the Context Menu in macOS (gingerbeardman.com)",
    "points": 79,
    "submitter": "msephton",
    "submit_time": "2024-07-30T16:20:45",
    "num_comments": 49,
    "comments_url": "https://news.ycombinator.com/item?id=41110866",
    "comments": [
      "BetterTouchTool also includes functionality to customise Finder's context-menu. I was hoping Hammerspoon would have something similar in their arsenal of APIs, but it doesn't look like they do.\n \nreply",
      "Nice tip, thanks!\n \nreply",
      "I have recently been looking at how I can use some Mac native tools to improve my workflow. Finally exploring tags and smart folders for example.I even built my own Mac app which sits in my menubar syncing tags on project folders with the status of projects in a web app I built. Even better that is synced with iCloud so I can access all of those files and tags from my other devices.I had not considered how else I could improve this situation with context menu options, but now I want to explore how I could use Automator to improve this setup even more.\n \nreply",
      "Good work! Keep going with it. Power user status.\n \nreply",
      "Its kinda funny, I have shown 2 people this tool so far.One is another engineer, who we kinda laughed over a tendency to spend an insane amount of time automating things that are quick. Like adding a tag to a folder is quick, I just didnt want to do it and things get out of sync.But he also thought it was really cool and also did not really know about tags until we started talking about this. Kinda like me, he knew they were there but just never did anything with it.I also showed it to my partner who kinda just keeps laughing every time he sees me glued to my laptop debugging this thing and tweaking it.It has honestly been a really fun project though. Trying to keep things as native functionality as possible since the entire point of this is to make it so I can more easily work on a script or some other supporting document for a project on my iPad or iPhone and once I get on my Mac everything will just fall into place. I even made it so if I make a folder, it will send the name of that folder to my web app as a new project. (well the web app handles if its new or not, but still).My only real annoyances so far:1: Mac doesn't have the ability natively to automatically tag all files in a folder that the tags of the parent folder. I could script this, and I might. But I kinda wish I didn't have too.2: Why is there no ability in Swift or a Mac app in general to read the contents of a Pages file. Honestly this kinda drives me insane. (I found a workaround by using a github project to turn it into HTML, but that really should not be necessary).3: I really wish I could make a smart folder that would show me files based on the tags of the parent folder. This way I could have 2 folders in different locations (one that syncs with iCloud and one that syncs with my NAS with very different data, but used in combination) and I could create a smart folder with a single view of all of those files. But this unfortunately seems impossible. (Edit: I guess I could just automatically create symlinks, but a smart folder would be so much cleaner)\n \nreply",
      "Could this have been done with Folder Actions and AppleScript?\n \nreply",
      "Sounds like we do similar organization, though I avoid Tags and use Keywords, which I prefer primarily because they can be embedded in many files types and can be more numerously assigned. Regardless, for much of your work assigning Tags and organizing files, have a look at Hazel for automating these tasks. And yes, it can read the contents of Pages files.https://www.noodlesoft.com\n \nreply",
      "I did briefly look at Hazel, I think at this point it would likely be unnecessary.I already have my app which has the custom code to work with the API of my web app and set tags accordingly, at that point it would honestly be trivial to have it go through all of the subdirectories of a folder. I would be quite surprised if I could have Hazel query an API, parse the json, loop through it, and then tag based on the various values.Unless it solves my last issue? I did not think it did when I looked at it, since that is more a limitation of the smart folder filter than anything else.That being said, I am confused by the difference between keywords and tags? Since on Mac a tag can be put on any file, folder, etc and doesn't care about the filetype. A single tag can be on any number of files/folders and a file/folder can have as many tags as I want (at least seemingly, I have not seen a limit yet).When you say keywords, are you just referring to using certain... well keywords... in the filenames?\n \nreply",
      ">Unless it solves my last issue? I did not think it did when I looked at it, since that is more a limitation of the smart folder filter than anything else.I don't believe Hazel will solve that last issue, but I may be off about that and you're certainly more skilled than I am.>When you say keywords, are you just referring to using certain... well keywords... in the filenames?Keywords are not in the file name, they're embedded in a file's metadata. For example, do a Print to PDF of a web page, but in the last dialog where you can save, note there's a Keyword field. Add something there that's unique, then search on it with Spotlight. Have lots of Keywords in use? Then Spotlight can use a keyword:[search term] search, similar to how there are many other built-in search queries.My primary setup is to have Hazel monitor specific files in specific folders looking for specific content within those files. Based on the results, Hazel runs my Automator Actions to embed the Keywords I want (especially helpful for organizing my business files, but I have plenty of other uses, like Hazel filing documents based on the Keywords I added when using Print to PDF).Ultimately, it sounds like you have a solid system accomplishing most of what you want done. Certainly no need to redo a viable setup. I was just hoping Hazel could help with the Pages files, and maybe secondarily some other parts of your process.\n \nreply",
      "> Keywords are not in the file name, they're embedded in a file's metadata. For example, do a Print to PDF of a web page, but in the last dialog where you can save, note there's a Keyword field. Add something there that's unique, then search on it with Spotlight. Have lots of Keywords in use? Then Spotlight can use a keyword:[search term] search, similar to how there are many other built-in search queries.Ah I see now. If I understand properly, I can see the value in this if you are setting a ton of keywords across multiple systems, and you don't really want those to muck up your finder window or list of tags.Out of curiosity I went to the save as pdf, and tag is also there. Honestly never noticed either being there.I wonder why tags on Mac isn't just a layer built on top of keywords if that is already there. Tags are even available in smart folder filters.Thanks for mentioning that, I think I might honestly use both. Since I use some tags for project status. So things like \"research\" \"production\" , etc etc. Things that are actually valuable as tags on my sidebar to quickly filter with. I even have colors associated with those to a quick glance at my folder.But being able to add a keyword for each project would also be beneficial. It doesn't muck up my sidebar.\n \nreply"
    ],
    "link": "https://blog.gingerbeardman.com/2024/07/30/taking-command-of-the-context-menu-in-macos/",
    "first_paragraph": "Yesterday on Twitter the inimitable Morten Just posted a preview of a tool he\u2019s created that wrap ffmpeg to allow movies, such screen recordings but pretty much anything, to be re-encoded to a smaller filesize.I responded with a trick I use to do the same on \u201cright-click\u201d context menu using a macOS app called ContextMenu, and others said it was possible to do it using Automator (with some caveats). In this blog post I\u2019ll compare the two.But first\u2026 let\u2019s talk about how we will make this work.We\u2019ll be making use of command line tools to do the heavy lifting, but don\u2019t worry I\u2019ll show that this can be as simple as a single-line command to process a single file, or a shell scripts of a few lines to process multiple files.The one-line command to convert a video or audio file can be as simple as the following:There are some assumptions here:You can take the exact same approach with any destination format as it is decided by the file extension. Cool. You could convert any of ffmpeg\u2019s supporte"
  }
]