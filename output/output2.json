[
  {
    "title": "Webb captures iconic Horsehead Nebula in unprecedented detail (esa.int)",
    "points": 129,
    "submitter": "rbanffy",
    "submit_time": "2024-04-29T15:31:49",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=40199624",
    "comments": [
      "Wow. The NIRCam image is probably going to be the most exciting new photo, but I can't get over how well MIRI reveals the internal structure of the nebula.NIRCam: https://www.esa.int/ESA_Multimedia/Images/2024/04/Horsehead_...MIRI: https://www.esa.int/ESA_Multimedia/Images/2024/04/Horsehead_...Comparison: https://www.esa.int/ESA_Multimedia/Images/2024/04/Slider_Too...",
      "I love that there are multiple sensors that can be compared to like this, but also love when the optical images from Hubble are compared as well.The images that combine all of the frequencies from Chandra X-rays, Hubble's optical, and now Webb's IR make for some truly fascinating images.",
      "The youtube link to a 'zoom' in video to the image:https://www.youtube.com/watch?v=TkVprNB5XbIWhat is really, really neat to notice isn't just the detail in that final image.... look behind it, and there are whole edge-on spiral galaxies in the distance.  Not stars.  Galaxies.The nebula is about 1375 light years away.  Those galaxies in the distance.... are billions of light years away.  It's hard to comprehend.",
      "> look behind it, and there are whole edge-on spiral galaxies in the distance. Not stars. Galaxies.just to add to the awe of that, pretty much every \"dot\" in one of these images is going to be another galaxy. individual stars from within the Milky Way will have diffraction spikes and very obvious as a single item.",
      "There really is a lot of stuff left to see for the first time",
      "\"A lot\" is the number of fish in a swarm maybe.This is so far away from our concept of counting things that the mind just gives up. There's no comparison, no dumbing down to X amount of football fields, just nothing.I find it depressing, confusing but also inspiring and fascinating at the same time.",
      "That's an incredibly detailed image.Every single time I see one of these amazing space pics, it's hard not to get all philisophical and wonder about the size of space & time on cosmic scale, how small our earth is and how insignificant our regular problems are.I don't care if I don't get to see flying cars or AGI in my lifetime but I will be very  disappointed if our knowledge of space remains more or less the same as today without much progress.",
      "The zoom-in video at the end is utterly unbelievable, don't miss it. What an engineering and scientific triumph.",
      "And it's in glorious 432p resolution!Edit: Here is the 2160p version: https://www.youtube.com/watch?v=UdHnF9Go_DQ",
      "I wonder how fast an observer would need to be traveling for it to look like that!"
    ],
    "link": "https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_captures_iconic_Horsehead_Nebula_in_unprecedented_detail",
    "first_paragraph": "The NASA/ESA/CSA James Webb Space Telescope has captured the sharpest infrared images to date of one of the most distinctive objects in our skies, the Horsehead Nebula. These observations show a part of the iconic nebula in a whole new light, capturing its complexity with unprecedented spatial resolution.",
    "summary": "In a stunning display of space voyeurism, the James Webb Space Telescope peeps at the Horsehead Nebula with its infrared eyes, delivering images so crisp that every astronomy enthusiast's knees go weak. The comment section becomes a battleground of awe and pseudo-philosophical musings with links thrown around as if they were confetti at a parade of self-congratulation. One commenter marvels at the *cosmic insignificance* of humanity in a tone that suggests they\u2019ve just discovered nihilism, while another offers a URL to a \"zoom-in\" video that promises to change your life but is originally about as high-definition as a potato. Thankfully, they edit to include a higher resolution link, saving us from squinting into the void. As we stand on the precipice of cosmic understanding, someone inevitably brings up the real question: how fast must one travel through space to witness this majesty in real-time, because apparently, the existential dread of our pale blue dot\u2019s insignificance isn\u2019t enough\u2014we need the sensation of warp speed to truly feel alive."
  },
  {
    "title": "Project Habbakuk: Britain's Ice \"Bergship\" Aircraft Carrier Project (99percentinvisible.org)",
    "points": 30,
    "submitter": "not_a_boat",
    "submit_time": "2024-04-29T15:35:31",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=40199679",
    "comments": [
      "This is one of those \u201cso cool yet so silly\u201d brainstorms that I\u2019m grateful someone was audacious enough to entertain. I\u2019m both relieved and saddened that it never came to fruition.There\u2019s a word, chindogu, to describe things that are less than useless. In some sense this project engendered more problems than it solved. Like so many other attractive brainstorms.",
      "The outcome of this project is an illustration of a rule of thumb from materials science:  many solid materials begin to lose their mechanical properties at about half their absolute melting point.  This is why (for example) ordinary steel should not be used above about 550 C; there's too much creep.",
      "It's probably worth the trouble to try making some DIY pykrete.Take a chain saw to it. Take a sledgehammer to it.",
      "I believe they didn't actually use pykrete in the Canadian test model. It was just ice."
    ],
    "link": "https://99percentinvisible.org/article/project-habbakuk-britains-secret-ice-bergship-aircraft-carrier-project/",
    "first_paragraph": "",
    "summary": "In an impressive display of gullible enthusiasm, readers of a 99% Invisible post dive headfirst into the icy waters of history to marvel at Project Habbakuk, Britain's fantastical attempt at creating an indestructible aircraft carrier out of *ice*. Commenters, exercising the maximum extent of their armchair engineering degrees, wax poetic about the lost potential of a battleship better suited for a *Frozen* sequel than actual combat. The project, described as a \"chindogu\"\u2014which is Japanese for \"hey, watch me pour energy into a gloriously inefficient idea\"\u2014serves as a cold reminder that not all that glitters is gold, and not all that is ice is pykrete. Amid reveries of what could have been and misguided DIY pykrete tutorials, we're gently nudged toward the realization that sometimes, the line between revolutionary and ridiculously redundant is thinner than ice."
  },
  {
    "title": "Memary: Open-Source Longterm Memory for Autonomous Agents (github.com/kingjulio8238)",
    "points": 114,
    "submitter": "james_chu",
    "submit_time": "2024-04-29T11:12:51",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=40196879",
    "comments": [
      "This seems like its overloading the term knowledge graph from its origins. Rather than having information and facts encoded into the graph, this appears to be a sort of similarity search over complete responses. It's blog style \"related content\" links to documents rather than encoded facts.Searching through their sources, it looks like the problem came from Neo4j's blog post misclassifying \"knowledge augmentation\" from a Microsoft research paper with \"knowledge graph\" (because of course they had to add \"graph\" to the title).This approach is fine, and probably useful but its not a knowledge graph in the sense that its structure isn't encoding anything about why or how different entities are actually related. A concrete example in a knowledge graph you might have an entity \"Joe\" and a separate entity \"Paris\". Joe is currently located in Paris so would have a typed edge between the two entities of something like \"LocatedAt\".I didn't dive into the code but what I inferred from the description and referenced literature, it is instead storing complete responses as \"entities\" and simply doing RAG style similarity searches to other nodes. It's a graph structured search index for sure but not a knowledge graph by the standard definitions.",
      "Exactly. Glad to see this. I do think knowledge graphs are important to AI assistants and agents though and someone needs to build a knowledge graph solution for that space.The idea of actual entities and relationships defined like triples with some schema and appropriately resolved and linked can be useful for querying and building up the right context. It may even be time to start bringing back some ideas from the schema.org back the day to standardize across agents/assistants what entities and actions are represented in data fed to them.",
      "Yeah precisely. Knowledge graphs are simple to think about but as soon as you look into them you realize all the complexity is in the creation of a meaningful ontology and loading data into that ontology. I actually think LLMs can be massively useful for building up the ontology but probably not in the creation of the ontology itself (far too ambiguous and large/conceptual task for them right now).",
      "How do we build ontology using LLMs? Will the building blocks be like the different parts of a brain?\nP.S I am assuming that by \"creation of ontology itself\" means creation of AGI.",
      "Ontologies are just defining what certain category, words, and entity types mean. Commonly used in NLP for data representation (\u201cfacts\u201d/triples/etc.) in knowledge graphs and other places where the definition of an ontology helps provide structure.This doesn\u2019t have anything to do with AGI or brains. They are typically created or tuned by humans and then models fit/match/resolve entities to match the ontology.",
      "Yeah, one of the specific things I'd love to do is collaboratively bulking up WikiData more. It's missing a ton of low hanging fruit that people using an ML augmented tool could really make some good progress on, similar to ML assisted OpenStreetMapping work",
      "While I'm 100% on board with RAG using associative memory, I'm not sure you need Neo4J.  Associative recall is generally going to be one level deep, and you're doing a top K cut so even if it wasn't the second order associations are probably not going to make the relevance cut.  This could be done relationally, and then if you're using pg_vector you could retrieve all your rag contents in one query.",
      "I think there's a lot of cases where you don't want to just RAG it. If you're going for tool assisted, it's pretty neat to have agent write out queries for what it needs against the knowledge graph. There was an article recently about how LLMs are bad at inferring B is A from A is B. You can also do more precise math against it, which is useful for questions even people need to reason out.I need to dig into what they're doing here more with their approach, but I think using an LLM for both producing and consuming a knowledge graph is pretty nifty, which I wrote up about a year ago here, https://friend.computer/jekyll/update/2023/04/30/wikidata-ll... .I will say figuring out how to actually add that conversation properly into a large knowledge graph is a bit tricky. ML does seem slightly better at producing an ontology than humans though (look how many times we've had to revise scientific names for creatures or book ordering)",
      "Yes, but this doesn\u2019t seem to be an actual knowledge graph which is part of the issue imho. If you look at the Microsoft knowledge graph paper linked in the repo it looks like they build out a real entity-relationship based knowledge graph rather then storing responses and surface form text directly.",
      "My initial thought was \"building the knowledge graph is what LLMs and the embedding process does implicitly\", why the need for a graphdb like Neo4j?"
    ],
    "link": "https://github.com/kingjulio8238/memary",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In the latest attempt to reinvent the wheel, <i>Memary: Open-Source Longterm Memory for Autonomous Agents</i> manages to confuse everyone by overloading the term \"knowledge graph\" with what essentially amounts to a glorified search engine for AI chatbots. Commenters, armed with their Ph.D. in Blog Reading, quickly assemble to dissect the glaring misuse of terminology, reminiscing about the good old days when words actually meant something. One enlightened soul tries to guide the conversation towards the practical implications for AI assistants, only for the debate to spiral into a semantics rabbit hole about whether storing complete responses qualifies as a meaningful relationship between entities. Meanwhile, someone else dreams of a utopian future where AI not only builds our ontologies but also decides what we have for breakfast, all without understanding the basics of relational databases."
  },
  {
    "title": "Husband and wife outed as GRU spies aiding bombings and poisonings across Europe (theins.ru)",
    "points": 149,
    "submitter": "dralley",
    "submit_time": "2024-04-29T15:00:15",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=40199193",
    "comments": [
      "This is my semi annual plug for all of you to watch the fantastic and somehow forgotten FX network TV show, The Americans, A spy drama set in Washington DC in the 1980s about KGB \u201cillegals\u201d posing as travel agents.It\u2019s way better than any basic cable TV show had any right to be. Plus, all seasons are streaming on Hulu, so you don\u2019t have to worry about whether the story will be completed.https://www.hulu.com/series/the-americans-6deba130-65fb-4816...",
      "I want to give some advice: Don't judge it by its first few episodes. When I first checked it out, the basic setup seemed rather inane and I stopped watching.Then, a couple years later, I needed something to watch during long exercise sessions and I checked it out again. It was getting much more interesting by the end of the first season.And every season got better and richer. By the very end, I experienced it as actually deep. It was a real pleasure and I'm very glad I had the chance to enjoy it. Recommended!",
      "I dunno. I watched the first few episodes over COVID and I thought it just the same as any US cable show: sex, crash-bangs and manufactured plot twists. IIRC there's a sex scene in the first five minutes.",
      "The most realistic Russian sleeper scenario imo was in Slow Horses. Brits don't insult your intelligence too greatly in their spook shows; the American variety always involves some sort of super-human characters. It's good as entertainment, the Americans, but just over the top.",
      "The Americans is pretty banal and follows the basic script you would expect. It's good background noise if you want 80s nostalgia though.",
      "I'm with you. I don't really agree with all the praise it gets. I liked it at first but it really seemed to run into the \"manufactured drama\" trap that a lot of TV shows run into when they try and keep it going. It really ruined my suspense of disbelief.",
      "I'd recommend Tinker Tailor Soldier Spy instead.",
      "Loosely based on an actual case.",
      "One of the best TV shows of all time, I second your recommendation. Rare show that gets better with each season.",
      "i don't know about that, the last 1 or to some extent the last 2 seasons were a bit lazy"
    ],
    "link": "https://theins.ru/en/politics/271205",
    "first_paragraph": "",
    "summary": "In a shocking turn of events that no one could have predicted, a couple was outed as GRU spies, apparently because watching *The Americans* wasn't enough of a training manual for them. Meanwhile, the comment section quickly devolves into a debate club for middle-aged Hulu subscribers, where suggestions for superior espionage entertainment are thrown around like free candy at a parade. One commenter's desperate attempt to validate their time investment into *The Americans* is met with the usual internet cocktail of apathy and dissent, cementing the fact that even in discussions about real-life espionage, the true battleground remains which TV show better fills the endless void of evening boredom. \ud83d\udcfa\ud83d\udd75\ufe0f\u200d\u2642\ufe0f"
  },
  {
    "title": "Answering Legal Questions with LLMs (hugodutka.com)",
    "points": 88,
    "submitter": "hugodutka",
    "submit_time": "2024-04-29T14:01:47",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=40198458",
    "comments": [
      "I saw a RAG demo from a startup that allows you to upload patient's medical docs, then the doctor can ask it questions like:> what's the patient's bp?even questions about drugs, histories, interactions, etc. The AI keeps in mind the patient's age and condition in its responses, when recommending things, etc. It reminded me of a time I was at the ER for a rib injury and could see my doctor Wikipedia'ing stuff - couldn't believe they used so much Wikipedia to get their answers. This at least seems like an upgrade from that.I can imagine the same thing with laws. Preload a city's, county's etc. entire set of laws and for a sentencing, upload a defendant's criminal history report, plea, and other info then the DA/judge/whoever can ask questions to the AI legal advisor just like the doctor does with patient docs.I mention this because RAG is perfect for these kinds of use cases, where you really can't afford the hallucination - where you need its information to be based on specific cases - specific information.I used to think AI would replace doctors before nurses, and lawyers before court clerks - now I think it's the other way around. The doctor, the lawyer - like the software engineer - will simply be more powerful than ever and have lower overhead. The lower-down jobs will get eaten, never the knowledge work.",
      "> It reminded me of a time I was at the ER for a rib injury and could see my doctor Wikipedia'ing stuffTo be honest, I'm much more comfortable with a doctor looking things up on wikipedia than using LLMs. Same with lawyers, although the stakes are lower with lawyers.If I knew my doctor was relying on LLMs for anything beyond the trivial (RAGS or not), I'd lose a lot of trust in that doctor.",
      "> I mention this because RAG is perfect for these kinds of use cases, where you really can't afford the hallucination - where you need its information to be based on specific cases - specific information.I think it's worth cautioning here that even with attempted grounding via RAG, this does not completely prevent the model from hallucinating. RAG can and does help improve performance somewhat there, but fundamentally the model is still autoregressively predicting tokens and sampling from a distribution. And thus, it's going to predict incorrectly some of the time even if its less likely to do so.I think its certainly a worthwhile engineering effort to address the myriad of issues involved, and I'd never say this is an impossible task, but currently I continue to push caution when I see the happy path socialized to the degree it is.",
      "I've 100% found AI to be super helpful in learning a new programming language or refreshing on one I haven't used in a while. Hey how do I this thing in Gleam? What's Gleams equivalent of y? I turn it first instead of forums/stackoverflow/google now and would say I only need to turn to other sources less than maybe 5% of the time.",
      "I think that is right. The sweat spot is twofold: 1) A replacement for general search on a topic where you have limited familiarity that can give you an answer for a concise question, or a starting point for more investigation or 2) For power-user use cases, where there already exists subject matter expertise, elaboration or extrapolation from a clear starting point to a clear end state, such as translation or contextualized exposition.The problem comes with thinking you can bridge both of those use cases - vague task descriptions to final output. The work described in the article of getting an LLM itself to break down a task seems to work sometime but struggles in many scenarios. Products that can define their domain narrowly enough, and embed enough domain knowledge into the system, and can ask the feedback at the right points, and going to be successful and more generalized systems will either need to act more like tools rather than complete solutions.",
      "Absolutely, I can't imagine doing Angular without an LLM sidekick.Curiosity + LLM = instant knowledge",
      "Yup. Entirely replaced the \"soft\" answers online like stack overflow for me. Now its LLM and if that isnt good enough then right to docs. I actually read documentation more often now because its pretty clear when I'm trying to do something common (LLM handle this well) vs uncommon (LLM often do not handle this well).",
      "I found this to be the case recently when I built something new in a framework I hadn't used before. The AI replaced Google most of the time and I learned the syntax very fast.",
      "> I used to think AI would replace doctors before nurses, and lawyers before court clerks - now I think it's the other way around.I've come to this conclusion as well.  AI is a power tool for those that know what questions to ask and will become a crunch for those that don't.  My concern is with the latter, as I think they will lose the ability develop critical thinking skills.",
      "I wonder if this \"AI will replace your job\" is like \"AI will drive your car\" in that where once something can solve 95% of the problem the general public assumes the last 5% will come very quickly.Rodney Brooks used to point out that self-driving was perceived by the public as happening very quickly, when he could show early examples in Germany from the 1950s.  We all know this kind of AI has been in development a long time and it keeps improving.    But people may be overestimating what it can do in the next five years -- like they did with cars."
    ],
    "link": "https://hugodutka.com/posts/answering-legal-questions-with-llms/",
    "first_paragraph": "",
    "summary": "In a world where clicking on Wikipedia articles is *too mainstream* for our medical and legal professionals, the bright minds at hugodutka.com proudly present an article championing the cause of shoving the entirety of a city\u2019s laws into a chatbot, because who needs years of law school when you have a glorified Clippy at your service? Commenters, in an impressive display of missing the point, joyfully recount tales of AI replacing Google for coding questions, apparently convinced that \"Curiosity + LLM = instant knowledge\" is the formula that will save humanity. Meanwhile, someone rightly points out the terrifying prospect of being diagnosed or defended in court by an algorithm with a penchant for \u201challucinating\u201d answers, but hey, as long as it keeps the overhead low, who cares about a few legal or medical mishaps? Welcome to the future, where your doctor Googles your symptoms and your lawyer might just be a chatbot with a good memory for laws but a bad grasp on reality."
  },
  {
    "title": "Claude \u00c9mile Jean-Baptiste Litre (wikipedia.org)",
    "points": 77,
    "submitter": "apollinaire",
    "submit_time": "2024-04-28T22:04:20",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=40192403",
    "comments": [
      "Ah, like 'Arkhan Land', rediscoverer of the Land Raider.(And of course, Jimmy Space).",
      "Adding this to my list of favorite April Fool's pranks -- my favorite being the recently featured islands of San Serriffe.",
      "lol, I love this. These gems are one of the reason I hang out on HN ;-)"
    ],
    "link": "https://en.wikipedia.org/wiki/Claude_%C3%89mile_Jean-Baptiste_Litre",
    "first_paragraph": "Claude \u00c9mile Jean-Baptiste Litre (1716-1778) is a fictional character created in 1978 by Kenneth Woolner of the University of Waterloo to justify the use of a capital L to denote litres.",
    "summary": "In yet another astonishing revelation, the internet has uncovered the truth behind Claude \u00c9mile Jean-Baptiste Litre, a made-up figure who apparently roamed the earth inventing measurements to save lazy engineers from using a lowercase \"l.\" Meanwhile, the comment section, a circus of self-appointed historians, crowns themselves connoisseurs of fictional characters, proudly parading their obscure references as if they've just cracked the Da Vinci code. \ud83d\ude02 With every chuckle, they slap each other on the back, celebrating their 'cultured' humor while blissfully ignoring the depths of their historical inaccuracy. In the land of the blind, the one-eyed man is king, and in the world of internet commenters, the slightly less misinformed reign supreme."
  },
  {
    "title": "Atomic nucleus excited with laser: A breakthrough after decades (tuwien.at)",
    "points": 337,
    "submitter": "geox",
    "submit_time": "2024-04-29T05:01:33",
    "num_comments": 152,
    "comments_url": "https://news.ycombinator.com/item?id=40194636",
    "comments": [
      "When you stop and look at QCD in the big picture, it's sort of shocking how little we know - like, really, really know - about the internal structure of the proton, or even the nucleon!It's the curse of \"probing\" with massive energies. No one's a hundred percent certain of whether they're detecting something that's actually there - like there there - or whether they're looking at by-product of enormous collision energies.Physicists are smart people! I could never do what they do. But there's a limit to certainty, and inside the proton especially there's unknown first principles at work. Bringing the precision of photons and lasers into this nucleon party is going to be huge. I can't wait!",
      "The measurement was already confirmed by a different group: https://arxiv.org/abs/2404.12311This is important since impurities in the crystals used lead to all kinds of fluorescence that could be mistaken for a signal from the Thorium ions. Now two groups have seen exactly the same signal in different Thorium-doped crystals which is very covincing that they have found the actual nuclear transition.",
      "I find it satisfying to see a researcher called THORsten SchUMm devoting his research to THORiUM.",
      "Nominative determinism :-)https://en.m.wikipedia.org/wiki/Nominative_determinism",
      "Frequency illusion :-Phttps://en.wikipedia.org/wiki/Frequency_illusion",
      "Lol that's a comic book name if I ever heard one. He's only one lab accident away from becoming a super hero/villain.",
      "And he's working with a dangerous radioactive isotope!",
      "We\u2019re so close! Send some spiders into the reaction chamber!",
      "But then I'm only a name change and a lab accident away.  And a name change isn't that hard...",
      "It doesn't work with name changes, unfortunately."
    ],
    "link": "https://www.tuwien.at/en/tu-wien/news/news-articles/news/lange-erhoffter-durchbruch-erstmals-atomkern-mit-laser-angeregt",
    "first_paragraph": "The \"thorium transition\", which physicists have been looking for for decades, has now been excited for the first time with lasers. This paves the way for revolutionary high precision technologies, including nuclear clocks.",
    "summary": "In an exciting world where lasers and thorium nuclei come together for the party of the century, some physicists at TU Wien finally got the lights to blink in a way they've been dreaming about for decades. The so-called \"thorium transition\" is now more than just physicist fan-fiction, promising a future where we might actually know what time it is, down to the nuclear tick. Meanwhile, the comment section becomes a playground for quantum enthusiasts and nominative determinism fans alike, marveling at how little we understand about anything smaller than a breadbox and poking fun at a researcher's name as if we're all back in high school. It's a thrilling day for science, with confirmations flying faster than back-patting congratulations, and everyone's just one accidental superhero origin story away from changing their names to something more thematically appropriate for nuclear research."
  },
  {
    "title": "How do satellites communicate with a GPS system? (2018) (allaboutcircuits.com)",
    "points": 5,
    "submitter": "reqo",
    "submit_time": "2024-04-28T15:34:59",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://www.allaboutcircuits.com/technical-articles/how-do-satellites-communicate-with-a-gps-system-a-look-at-the-gps-antenna/",
    "first_paragraph": "",
    "summary": "In an earth-shattering revelation that <em>completely</em> blindsides anyone who thought that satellites were just fancy space ornaments, allaboutcircuits.com dives headfirst into the complex and utterly bewildering world of GPS communication. Revel in the enlightenment as the article meticulously breaks down concepts that your smartphone comprehends better than you do. The comment section, a veritable cornucopia of self-proclaimed tech gurus, decides to engage in the digital equivalent of a space race, outdoing each other with anecdotal evidence and a generous sprinkling of misunderstanding. Marvel as they navigate the treacherous terrain of satellite communication with the finesse of a GPS system caught in a solar storm."
  },
  {
    "title": "Cheyenne Super Computer Auction (gsaauctions.gov)",
    "points": 81,
    "submitter": "zrules",
    "submit_time": "2024-04-29T12:10:47",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=40197277",
    "comments": [
      ">Components of the Cheyenne SupercomputerInstalled Configuration: SGI ICE\u2122 XA.E-Cells: 14 units weighing 1500 lbs. each.E-Racks: 28 units, all water-cooledNodes: 4,032 dual socket units configured as quad-node bladesProcessors: 8,064 units of E5-2697v4 (18-core, 2.3 GHz base frequency, Turbo up to 3.6GHz, 145W TDP)Total Cores: 145,152Memory: DDR4-2400 ECC single-rank, 64 GB per node, with 3 High Memory E-Cells having 128GB per node, totaling 313,344 GBTopology: EDR Enhanced HypercubeIB Switches: 224 unitsMoving this system necessitates the engagement of a professional moving company. Please note the four (4) attached documents detailing the facility requirements and specifications will be provided. Due to their considerable weight, the racks require experienced movers equipped with proper Professional Protection Equipment (PPE) to ensure safe handling. The purchaser assumes responsibility for transferring the racks from the facility onto trucks using their equipment.",
      "Given that the individual nodes are just x86_64 Xeons and run linux... it would be interesting to part it out for sale as individual, but functional, nodes to people.  There are a lot of people would like to have a ~2016 era watercooled 1U server from a supercomputer that was once near the top of the Top500 just to show to people.Get little commemorative plaques for each one and sell for $200 each or so.edit:\nit seems each motehrboard is a dual CPU board and so there are 4032 nodes, but the nodes are in blades that likely need their rack for power.  But I think individual cabinets would be cool to own.There are 144 nodes per cabinet... so 28 cabinets.  \nI'd pay a fair amount just to own a cabinet to stick in my garage if I was near there.",
      "The individual servers are not watercooled. The compute racks are air-cooled; the adjacent cooling racks then exchange that heat using the building's chilled water. It's the rack as a whole that is watercooled. If you extract a single node, you won't get any of that. As the other commenters also point out, these are blades; you can't run an individual node by itself.",
      "These are blades, so there is probably some kind of container chassis required to run them.Using them as desktop PCs would likely be a challenge.",
      "I can find a bunch of the E5-2697v4 CPUs on eBay in the $30-40 range.I wonder if there is a market for the SGI hardware.",
      "So getting 8,064 of them for $3,085 - 38 cents per CPU - is great value for money!",
      "this is basically \"free grand piano\" - not so free once you hire the movers and tuners",
      "Dump 8,064 old processors on eBay and you'll probably introduce some downwards price pressure.",
      "Does it come with a portable nuclear reactor to power it?",
      ">...totaling 313,344 GBCan you imagine the RAMDisk? Yes, you can. Especially in 20 years when it will be the norm. And also the Windows version that will require half of it in order to run /s"
    ],
    "link": "https://gsaauctions.gov/auctions/preview/282996",
    "first_paragraph": "",
    "summary": "In an event that sounds more like an IT fire sale than a historic moment, the government has decided to auction off the Cheyenne Supercomputer, a machine that once crunched numbers faster than your mom at a Black Friday sale. The listing detailed enough silicon to shame a Silicon Valley startup, suggesting movers might need a degree in physics and a forklift license to relocate this mechanical behemoth. Commenters, in a display of nerd bravado reminiscent of a Trekkie convention, fantasized about turning these relics into the world\u2019s most overkill Minecraft servers. One visionary even imagined sticking a water-cooled rack in their garage, because nothing says \"I'm a tech savant\" like a decommissioned supercomputer next to your lawnmower. Meanwhile, someone else speculated on the DIY market for the CPUs, because evidently, the world desperately needs more coasters fashioned from outdated hardware."
  },
  {
    "title": "Pdf.tocgen (krasjet.com)",
    "points": 124,
    "submitter": "nbernard",
    "submit_time": "2024-04-28T08:51:18",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=40187072",
    "comments": [
      "Recently I found the getToc function in PyMuPdf was too slow. I told them about it in their discord, and a day later they had fixed it. Now it only takes a couple of milliseconds. I'm using it for my project pdftomp3. Pdf.tocgen looks useful too, but I'm not sure if I can use it because of the licencse?",
      "Of course you can use it.What you can't do is deny others the same freedoms the license grants to you.",
      "There does appear to be some licensing awkwardness here. The license is nominally GPLv3, but it says it is based on AGPLv3 projects. It also appears to misidentify (it may have been correct at the time) PyMuPDF as GPLv3 when that appears to actually be AGPLv3. My assumption is that using this would require complying with AGPLv3?There's the additional oddity that a portion of the repository (the recipes directory) is licensed under CC-BY-NC-SA, and so the repository is not fully open source. This is particularly confusing, however, as the functional content of the recipes directory appears to be mostly records of direct observations of parameter choices in external documents and tools, and so doesn't seem like it would be copyrightable at all, at least in the US.",
      "Interested to know what is pdftomp3?",
      "You can upload a PDF and convert the chapters into MP3s (either original text or simplified text). But for PDFs without a table of contents, you can only convert single pages.",
      "- \"That is, you shouldn\u2019t expect it to work with scanned PDFs\"It's surprisingly easy to extend this type of workflow to scanned pdfs (as opposed to software-generated, text-containing ones). tesseract(1) makes short work of ToC pages with --psm set to 6 (an OCR setting that tends to collapse convoluted text layouts into a regular, software-parseable output).It should also be straightforward, but I don't know of an out-of-the-box solution, to automate that example of extracting \"text that looks like a header\"\u2013based on page layout/relative positioning, or font weight.  (I'm working on an adjacent problem, an automatic re-layout of raster documents to squeeze out whitespace and make them slightly nicer on small e-ink devices. Text islands are trivial to identify. I don't know how to quantify font weight, or things like that. I'm \"wasting\" a lot of time diving into lots of mathematics rabbit holes, but I don't know in advance which ones will be productive or not).",
      "I've found EasyOCR to work much better at pulling text out of irregular or unknown images. Requires more resources than tesseract but gets much better results in my projects.",
      "tesseract is fine for basic use cases, but it fails when the image is tilted (and thus the text isn't laid out horizontally), which can happen several times with scanned books. Compared to how well the Google OCR engine works, tesseract should be much better than it is.I wonder how difficult it is to develop a better OCR engine than tesseract.",
      "Tesseract is last gen. Multimodal is SOTA, and can handle even heavily distorted or destroyed text.",
      "Am I overlooking something, or is automating page rotation no more work than just a 2d FFT?"
    ],
    "link": "https://krasjet.com/voice/pdf.tocgen/",
    "first_paragraph": "",
    "summary": "In a groundbreaking display of efficiency that would make a sloth on tranquilizers look industrious, the PyMuPDF team reacts with lightning speed (merely a day) to a user complaint about their getToc function's glacial pace, catapulting it into the realm of milliseconds. This monumental achievement allows for the birth of pdftomp3, a project surely pivotal to humanity's progress, transforming PDF chapters into MP3s with all the grace of turning wine back into water. Meanwhile, the licensing kerfuffle around Pdf.tocgen provides an intellectual maze for the commentariat, whose members trip over themselves in a race to unravel the Gordian knot of GPLv3 vs. AGPLv3, all while debating the merits of OCR tools as if the fate of the written word depended on their conclusions. In this thrilling saga of copyright confusion and optical character recognition one-upmanship, the real question remains unasked: who among us was desperately seeking to convert their unread PDFs into even less listened-to MP3 files?"
  }
]