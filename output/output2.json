[
  {
    "title": "Atuin Desktop: Runbooks That Run (atuin.sh)",
    "points": 213,
    "submitter": "freetonik",
    "submit_time": "2025-04-22T20:54:52 1745355292",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=43766200",
    "comments": [
      "My dream tooling is for every tool to have an terminal interface so that I can create comprehensive megabooks to get all the context that lives in my head. i.e. jira, datadog, github etc, all in one pane.\n \nreply",
      "You might like MCP then.\n \nreply",
      "> You might like MCP then.That's entirely different to what's being desired by GP.> > My dream tooling is for every tool to have an terminal interface so that I can create comprehensive megabooks to get all the context that lives in my head. i.e. jira, datadog, github etc, all in one pane.My perspective on this is essentially having jira/datadog/github/etc be pluggable into the CLI, and where standard bash commands & pipes can be used without major restrictions. (Something akin to Yahoo Pipes)MCP is highly centered around LLMs analyzing user requests & creating queries to be run on MCP servers. What's being desired here doesn't centralize around LLMs in any sense at all.\n \nreply",
      "All the problems of reproducibility in Python notebooks (https://arxiv.org/abs/2308.07333, https://leomurta.github.io/papers/pimentel2019a.pdf) with the power of a terminal.\n \nreply",
      "what are the problems you're talking about? your references seem to refer to reproducing scientific publications, dependency issues, and cell execution ordering.this project appears to be intended for operational documentation / living runbooks. it doesn't really seem like the same use case.\n \nreply",
      "I mean it feels pretty obvious to me that cell execution order is a pretty real issue for a runbook with a bunch of steps if you're not careful.I do think that given the fragile nature of shell scripts people tend to write their operation workflows in a pretty idempotent way, though...\n \nreply",
      "agreed - we actually have a dependency system in the works too!you can define + declare ordering with dependency specification on the edges of the graph (ie A must run before B, but B can run as often as you'd like within 10 mins of A)\n \nreply",
      "There of course should be a way to override the dependency, by explicitly pressing a big scary \"[I know what I'm doing]\" button.Another thing is that you'll need branches. As in:  - Run `foo bar baz`\n  - If it succeeds, run `foo quux`,\n    Else run `rm -rf ./foo/bar` and rerun the previous command with `--force` option.\n  - `ls ./foo/bar/buur` and make certain it exists.\n\nDifferent branches can be separated visually; one can be collapsed if another is taken.Writing robust runbooks is not that easy. But I love the idea of mixing the explanatory text and various types of commands together.\n \nreply",
      "Agreed. The problem with reproducing Jupyter runbooks in academia is that someone thought a Jupyter runbook is a way to convey information from one person to another. Those are an awful model for that.As an on-the-fly debugging tool, they're great: you get a REPL that isn't actively painful to use, a history (roughly, since the state is live and every cell is not run every time) of commands run, and visualization at key points in the program to check as you go your assumptions are sound.\n \nreply",
      "Give marimo a try, it's much better for reproducibility.\n \nreply"
    ],
    "link": "https://blog.atuin.sh/atuin-desktop-runbooks-that-run/",
    "first_paragraph": "Atuin Desktop looks like a doc, but runs like your terminal. Script blocks, embedded terminals, database clients and prometheus charts - all in one place.Most infrastructure is held together by five commands someone remembers when shit breaks. Docs are out of date, if they exist. The real answers? Buried in Slack threads, rotting in Notion, or trapped in someone's shell history.Atuin CLI fixed part of this, with synced, searchable shell history. But teams need more than history. They need workflows that don't live in someone's head (or their shell).Set up. SSH in. Export some variables. Run some commands. Hope nothing breaks. Stuff we do every day, but still have to piece together from fragments of the past, or copy paste from some document somewhere.That's why we built the next step.Built to make workflows repeatable, shareable, and reliable.Runbooks should run. Workflows shouldn't live in someone's head. Docs shouldn't rot the moment you write them.Atuin Desktop looks like a doc, but",
    "summary": "**Atuin Desktop: Digital Documentation Dinosaurs Wanted**\n\nIn a digital world where documentation often dies faster than my interest in corporate meetings, Atuin Desktop emerges as a valiant yet misguided attempt to revive the ancient runbooks with a flashy, feature-stuffed frontend. This bulky beast pretends your terminal's messy shell history and fragmented scripts are actually organized\u2014imagine a notepad that executes commands and hopes for the best. Cue to the commenters, who drool over the possibility of jamming Jira, Datadog, and GitHub together into a Frankenstein's terminal monster, displaying a touching faith in software that promises to sort out the mess in their heads better than a therapist. Sit tight as we watch this ensemble inevitably buckle under the weight of its own ambition, while users keep pressing the \"I know what I'm doing\" button\u2014a comedy at its finest, except it\u2019s real. \ud83e\udd21\ud83d\udcdc"
  },
  {
    "title": "Sapphire: Rust based package manager for macOS (Homebrew replacement) (github.com/alexykn)",
    "points": 256,
    "submitter": "adamnemecek",
    "submit_time": "2025-04-22T18:39:20 1745347160",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=43765011",
    "comments": [
      "Hey, so I built this thing, most of it at so far at least. And yeah, right now it isn't doing many things better than Homebrew.Setting of relative paths for bottle installs is still not perfect, well it works for every bottle I have tested except rust. Getting bottles working 100% is very doable though imo.Build from source formulae is still pretty f*ed + I do not know if it is really feasible given that the json API lacks information there and a full on Ruby -> Rust transpiler is way out of scope. Will probably settle for automatic build system detection based on archive structure there. + Maybe do my own version of the .rb scripts but in a more general machine readable format, not .rs lolCasks seem to work but I have only tested some .dmg -> .app ones and .pkg installers so far though. As with bottles 100% doable.Given that almost all formulae are available as bottles for modern ARM mac this could become a fully featured package manager. Actually didn't think so many people would look at it, started building it for myself because Homebrew just isn't cutting it for what I want.Started working on a declarative package + system manager for mac because I feel ansible is overkill for one machine and not really made for that and nix-darwin worms itself into the system so deep. Wrapping Brew commands was abysmally slow though so I started working on this and by now I am deep enough in I won't stop xDAnyway I am grateful for every bug report, Issue and well meaning pull request.\n \nreply",
      "You mentioned a declarative package manager for Mac. I've really liked using Homebrew Bundle [1] over the last couple years. It's about the level of declarative that I've wanted and has made it really easy to bootstrap new laptop or VM (since it also works on Linux). The format for a Brewfile was pretty easy to figure out.The way I ended up using it was that `brew install` would temporarily install something, without adding it to my Brewfile. And a little `brew add` wrapper would add the package to my Brewfile to keep it on the system permanently. That part with the wrapper could have used some love and would be a nice fit for a new brew-compatible frontend IMO. Maybe you could expand on that for Sapphire, if that also scratches your declarative itch?[1] https://docs.brew.sh/Brew-Bundle-and-Brewfile\n \nreply",
      "Cool project, good luck with it!If I may surface one use case: Several years ago I had to manage a bunch of Macs for CI jobs. The build process (Unreal's UAT) didn't support running more than one build process at a time, and Docker was really slow, so I'd hoped to use different user accounts to bypass that and get some parallelization gains. Homebrew made that very difficult with its penchant for system-wide installs. So a feature request: I'd love to see a competitive package manager that limits itself to operating somewhere (overridable) in the user's home directory.\n \nreply",
      "Initial idea for this really came from my dayjob too, we have macs but no way to centrally manage them. The client / server part for the declarative system manager I want to build on top of this is quite far out yet though. At least several months\n \nreply",
      "IIRC the main reason here is that brew path is hardcoded during the build process of packages, which means that you wouldn't be able to use bottles.I didn't check, but there is a chance that path is also hardcoded in (some) formulae, so even building from the source might not help here.\n \nreply",
      "You could run the build process with chroot or inside Docker, so that the hardcoded paths actually resolve to a designated subdirectory.\n \nreply",
      "Incidentally, that\u2019s what is usually done in Nixpkgs in similar situations when there\u2019s no better alternative, see buildFHSEnv et al.\n \nreply",
      "Nix effectively has per-user packages, but it\u2019s hard to read into your full use case from your comment.\n \nreply",
      "oh, I guess this is why the nix installer creates 32 macOS users called _nixbld$N\n \nreply",
      "It's explained in the documentation.https://nix.dev/manual/nix/2.25/installation/multi-user\n \nreply"
    ],
    "link": "https://github.com/alexykn/sapphire",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Rust based package manager for macOS\n      WARNING: ALPHA SOFTWARE > Sapphire is experimental, under heavy development, and may be unstable. Use at your own risk!Uninstalling a cask with brew then reinstalling it with Sapphire will have it installed with slightly different paths, your user settings etc. will not be migrated automatically.Sapphire is a next\u2011generation, Rust\u2011powered package manager inspired by Homebrew. It installs and manages:ARM only for now, might add x86 support eventuallysapphire\u2011core Core library: fetching, dependency resolution, archive extraction, artifact handling (apps, binaries, pkg installers, fonts, plugins, zap/preflight/uninstall stanzas, etc.)sapphire\u2011cli Command\u2011line interface: sapphire executable wrapping the core library.Prerequisites: Rust toolchain (stable).The sapphire binary will be at target/re",
    "summary": "In a dazzling display of <em>not reinventing the wheel</em>, an optimistic developer introduces Sapphire, a Rusty sidekick aiming to replace the all-too-mainstream Homebrew. \ud83c\udf7b It's still in the \"might explode your system\" phase, with special features like <i>\"installing stuff... but differently,\"</i> and the promise of turning macOS into an ARM-only party, because who needs universal support anyway? The commenters, blooming with inevitability, oscillate between offering sympathy and subtly suggesting to take this beast and morph it into yet another missing Brew feature. Because clearly, what the world lacks is a 17th way to mismanage package installations on a Mac."
  },
  {
    "title": "Solidjs: Simple and performant reactivity for building user interfaces (solidjs.com)",
    "points": 39,
    "submitter": "lastdong",
    "submit_time": "2025-04-19T07:50:38 1745049038",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43734911",
    "comments": [
      "Having used Solid on a largish web product for over a year, I am thoroughly convinced and will not be returning to React in the future.This is somewhat of an aside: I am aware that the creator of Solid has long been experimenting with adding laziness to the reactive system. I think it would be a mistake. That everything is immediate keeps state changes intuitive, fairly easy to debug, and is one of the strong points of Solid's design. I've never run into a real world scenario where delaying computations seemed like an optimal way of solving a given problem.\n \nreply",
      "Nice, but Mikado is faster\n \nreply",
      "Portions of the react community are excited about how it's starting to feel more like PHP with the movement towards server actions in \"client\" code etcI personally don't like that direction so looking forward to exploring new frameworks.What I've generally liked about React/Next setups is that the code is generally explicit and less magic (I also have gripes with hooks feeling like magic). Things like Vue/Svelte where they mash together css and js in the same file if you want kind of turns me off.Does anyone know if SolidJs projects are fully js?\n \nreply",
      "> I personally don't like that direction so looking forward to exploring new frameworksI personally am becoming dissolutioned with React because of this, as a former strong advocate.Next.js and Remix - or whatever the hell it's called this week - are both over-engineered messes.Yes, I'm aware that I don't have to use them, but they are where the momentum and energy of the community is being spent in recent years. And I do want a monolithic framework, I just want one that focuses on simplicity and ease of development rather than one designed to funnel me into using Vercel.\n \nreply",
      "Don't worry, we're only four or five years out from the react community discovering/inventing a totally fresh UI paradigm where the entire app is run on the client, no server rendering at all.It will be totally fresh and solve all the annoying problems of legacy react server pages. Of course then 6 months later they'll discover a need to make it work with SEO...\n \nreply",
      "I've been interested on dipping my toes in Solid for a while, but I've heard there's a 2.0 coming soon and I don't want to bother learning Solid in detail if there's going to be a lot of breaking changes soon.Does anyone know the status of 2.0?\n \nreply",
      "Looks like its still in the discussion phase, did not see a timeline but also did not read super close.  Did learn about the tanstack-router which I had never heard of, so that was worth looking this up right there ;)https://github.com/solidjs/solid/discussions/2425\n \nreply"
    ],
    "link": "https://www.solidjs.com/",
    "first_paragraph": "",
    "summary": "Another day, another JavaScript framework to save us from the tyranny of previous JavaScript frameworks. Today, <em>SolidJS</em> enthusiasts are frothing in the comments about \"simple and performant reactivity,\" as if discovering fire for the first time. One user is convinced they'll never crawl back to React\u2014sure, until Next.js releases another shiny object. Meanwhile, a skeptical soul dreads the rumored \"Solid 2.0\" and its potential apocalypse of breaking changes\u2014because what's more thrilling than relearning your entire stack over a minor version update? \ud83d\ude31\ud83d\udd04"
  },
  {
    "title": "Can Citizen Science Be Trusted? New Study of Birds Shows It Can (ucdavis.edu)",
    "points": 20,
    "submitter": "gnabgib",
    "submit_time": "2025-04-23T00:07:45 1745366865",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43767476",
    "comments": [
      "Didn't the majority of science used to be done by \"citizen\" scientists?\n \nreply",
      "Well, define \"trust\".This example is using what sounds like crowdsource apps as a measurement instrument. Which is going to have different limitations and different sources of interference than other measurement instruments, but of course it can work if what you're using it for is able to handle this.\n \nreply",
      "Yeah - it seems a stretch to go from \"These two systems accurately showed a known behaviour\" to \"All citizen science systems can predict everything all the time\"\n \nreply"
    ],
    "link": "https://www.ucdavis.edu/news/can-citizen-science-be-trusted-new-study-birds-shows-it-can",
    "first_paragraph": "Platforms such as iNaturalist and eBird encourage people to observe and document nature, but how accurate is the ecological data that they collect?In a new study published in Citizen Science: Theory and Practice March 28, researchers from the University of California, Davis, show that citizen science data from iNaturalist and eBird can reliably capture known seasonal patterns of bird migration in Northern California and Nevada \u2014 from year-round residents such as California Scrub-Jays, to transient migrants such as the Western Tanager and the Pectoral Sandpiper.\u201cThis project shows that data from participatory science projects with different goals, observers and structure can be combined into reliable and robust datasets to address broad scientific questions,\u201d said senior author Laci Gerhart, associate professor of teaching in the UC Davis Department of Evolution and Ecology. \u201cContributors to multiple, smaller projects can help make real discoveries about bigger issues.\u201dThe study began a",
    "summary": "Title: Hobbyist Birdwatchers Somehow Not a Complete Disaster\n\nIn an electrifying turn of events, the University of California, Davis discovers that avid bird nerds using apps like iNaturalist and eBird aren't just making up numbers to sound smart at their nerdy bird clubs. According to a *groundbreaking* study, these weekend warriors can indeed track bird migration seasons without wildly veering into fantasy\u2014with a professor as cheerful as a jay to stamp approval on this as (<em>surprise, surprise</em>) \"robust datasets.\" Comment sections explode with shock as amateurs aren\u2019t just throwing darts at a board in their basements. One user ponders the infancy of science led by \"citizen\" scientists\u2014<span style=\"color: darkcyan;\"><em>oh how we've come full circle</em></span>. Another, gripping their Darwin beard, skeptically squints at the screen wondering if successful bird tracking on an app should already set the bar this low."
  },
  {
    "title": "Making a smart bike dumb so it works again (francisco.io)",
    "points": 121,
    "submitter": "franciscop",
    "submit_time": "2025-04-19T09:57:10 1745056630",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=43735378",
    "comments": [
      "Maybe I've been listening to too much YKS but I can't imagine buying a bike that required me to use an app and company's cloud to control basic functionality like that\n \nreply",
      "I couldn't imagine buying a dishwasher that did the same... but somehow that happened.There's no indication that functionality is hidden behind an app. You find that out after you get the thing home, set it up, and start using it (a lot of the time).\n \nreply",
      "Bosch I presume?This is one of few places where I\u2019d like to sprinkle a little more government overreach in just the right way - to prevent manufacturers from walling clearly essential behaviour behind an app. That\u2019s far too gray of a line for governments to handle, but I can dream.\n \nreply",
      "Is it possible the app wasn't advertised as required, but shown as a feature?\n \nreply",
      "My clothes dryer has this stupid touch screen interface and keeps reminding me that it can be controlled by an app.I'm all: \"So? I Don't want my phone colluding with the dryer and the garage door opener to overthrow me.\"\n \nreply",
      "I was hoping this was about the Copenhagen Wheel. I'd love if my expensive brick could get going again. It was my first lesson in \"Don't trust a startup\".\n \nreply",
      "I thought I'd read about unbricking it. https://www.reddit.com/r/ebikes/comments/1bis770/diehard_cop... links to a private FB group that I haven't joined, but implies maybe it hasn't been solved yet. https://www.instructables.com/Build-a-Key-for-the-Copenhagen... makes me think at least one piece of the puzzle has been.\n \nreply",
      "I'm sure there are startups that won't screw over their users even if they don't make it. Don't trust/buy any hardware that requires a cell phone app to use though.\n \nreply",
      "If the CEO is an MBA then just don't even give them a chance.\n \nreply",
      "Yeah, if the first thing they show in a promo video is using an app, I'm already rolling my eyes. If they instead just show the product working, and then \"one more thing\" the demo with app control, maybe I'll get my eyes to face forward the entire demo. It seems like the \"but on a computer\" just went to \"but it has an app\" mental picture.\n \nreply"
    ],
    "link": "https://francisco.io/blog/making-a-smart-bike-dumb-work-again/",
    "first_paragraph": "The company that made my bike went bankrupt a few years ago. It isn't truly even my bike, it's my friend's, who bought it God-knows-where, in a country where this company never operated. This friend is no longer here \u2014in this country\u2014[1] so I'm holding it for him and can use it freely.And it's a fine bike, mind you. But there's a teeny tiny problem:The lights don't work without the App.Did I mention the company went bankrupt? And the spotty history of the bike? And that Customer Support is in startup heaven? And how ridiculous it is anyway to need to login into an app to turn your light on?You can just get a bike light in Daiso in Japan. It feels funny riding such a solid bike, with a nice integrated non-working light, and using a cheap light that you have to press just right for it to work. But I got my law-abiding light, so kudos.Not bad until you learn that there are also thieves in Japan. Shocker, I know, I was promised safety here but one nice day the light was gone. Someone riske",
    "summary": "**Internet Dwellers Rediscover Hardware Dependence: News At Eleven**\n\nIn an eerily not-surprising twist, a blogger tells the harrowing tale of a \"smart\" bike turned dumb after its corporate creators toppled into tech oblivion. \ud83d\udeb2\ud83d\udca1 The bicycle's lights can't shine without an accompanying app, a concept consistently ridiculed by all 14 commenters, each aghast at the insane reliance on technology for flipping a switch. Irony peeks through as they recall every appliance in their home, from dishwashers to dryers, also falling prey to the \"smart\" gimmick. Armed with wit dry enough to spark a forest fire, these digital Sherlocks bemoan government oversight while half-heartedly hoping for intervention between fits of helplessly laughing at their own tech-ensnared lives."
  },
  {
    "title": "How long does it take to create a new habit? (2015) (thelogicaloptimist.com)",
    "points": 115,
    "submitter": "rzk",
    "submit_time": "2025-04-22T18:47:05 1745347625",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=43765084",
    "comments": [
      ">\"If you\u2019re trying a new diet, attempting to quit smoking or changing any daily routine, don\u2019t expect new habits to be created in a week, or two or even three.  Research suggests that the process requires 66 days (on average) and up to 8 months.\"Something about the title suggested it was going to be much less than 21 days; maybe it's the domain name biasing my reading of the title.\n \nreply",
      "I agree that the title implies (or even declares) that it takes fewer than 21 days. I'd argue that the articles says that it does take those 21 days, plus many more, making the title deceptive.\n \nreply",
      "Exactly, English semantics dictate that if it takes more than 21 days it must first take 21 days.\n \nreply",
      "There is a very clearly implied only in the title. I agree that it should be included but it is unnecessary to convey the meaning imo.\n \nreply",
      "There is not. I too assumed it must mean it takes less, otherwise there would be an \"only\", or \"It takes more than\".\n \nreply",
      "I wonder what bias I have that made me read the title and assume \"Huh, must be way longer\" then?\n \nreply",
      "I assumed it would take longer than 21 Days based on the title alone.\n \nreply",
      "How I understood it, \u201cform\u201d here means that a habit has been formed.As in: \u201cHabits aren\u2019t formed in 21 days\u201c, just like \u201cRome wasn\u2019t built in one day\u201d.\n \nreply",
      "Effort is usually quoted as a minimum (\"It takes 20 minutes to drive there\" \"It costs $100 to buy this\") which leads to a strange quirk that the negative is then interpreted as being a lower minimum, rather than a higher one (\"It does not take 20min to drive there, \"it does not cost $100 to buy this\")\n \nreply",
      "Your examples make me wonder if the language of marketing has changed our assumptions about implied meanings where otherwise we'd recognize ambiguity.Or perhaps, marketing aside, it is just human nature to conjure implicit meaning rather than be confronted with ambiguity.\n \nreply"
    ],
    "link": "https://thelogicaloptimist.com/index.php/2015/10/25/the-21-day-myth-create-new-habit/",
    "first_paragraph": "However, Dr. Maltz was simply making an observation as a plastic surgeon. He was not declaring a statement of fact that is based on research. Also, the phrase \u201cit usually requires a minimum of about 21 days\u201d was propagated without the words \u201cusually\u201d, \u201cminimum\u201d and \u201cabout\u201d.So how long does it really take to create a new habit?Published in the October 2010 issue of the European Journal of Social Psychology, the research article \u201cHow habits are formed: Modelling habit formation in the real world\u201d (Phillippa Lally, et al.) attempted to answer that question. The study examined the habits of 96 people over a period of 12 week, and the data was then analyzed to determine how long it took each person to go from starting a new behavior to automatically doing it. The answer? Not 21 days.According to Lally\u2019s study, implementing meaningful change in our lives requires 2 to 8 months. The variation is due to the type of habit in question, the person developing it and his/her circumstances. On avera",
    "summary": "\ud83c\udfad *How long does it take to create a new habit? More than your attention span.* A groundbreaking exposition decodes Dr. Maltz's wild guesswork about habit formation, hinting that 21 days might just be the warm-up session. Commenters, stunned by this revelation and struggling with the arduous journey beyond three weeks, delve into the linguistic jungle of English semantics, arguing fervently if the notion of \"minimum\" really implies \"only just to start.\" Meanwhile, another reader is likely crafting a DIY habit-forming kit, promising results in *exactly* 21 days or your existential despair back. \ud83d\udd04"
  },
  {
    "title": "Ping, You've Got Whale: AI detection system alerts ships of whales in their path (biographic.com)",
    "points": 104,
    "submitter": "Geekette",
    "submit_time": "2025-04-22T18:28:20 1745346500",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=43764915",
    "comments": [
      "Short summary is that this isn't going to work.\nI was involved with a company doing some development on whale detection AI. The vast majority of the time you just don't see enough of the animal to make reliable detections. And that is in a best case scenario, add in movement of the boat/camera, waves, sunlight reflections, and having to ignore a vast majority of potential things on/in the water that can look kind of like whales, but are not whales. Dolphins in particular.This isn't a problem for AI/machine vision, IMO.\n \nreply",
      "That\u2019s your opinion not a summary of the article though\n \nreply",
      "Correct, I didn\u2019t mean to imply it was a summary of the article but my comment could be confusing I guess. It\u2019s my opinion based on 18 years in various AI machine vision startups, and having worked on this specific problem.\n \nreply",
      "Acoustic detection methods using hydrophone arrays can supplement visual AI by identifying whale vocalizations at distances of several kilometers even in poor visibility conditions.\n \nreply",
      "Metal whale vocalizations especially. One might say it's the primary application.\n \nreply",
      "Ok, but whales move, sometimes very quickly.  If we assume that the whales are moving, that they are not sleeping, the ship might want to initially aim directly at them.  By the time the ship gets there, they will have moved.\n \nreply",
      "My first thought about reading this was how the Chinese whaling fleets would use this to their advantage. Not something I would be cool with and with any \"good\" technology, you have nefarious parties always looking to exploit it for their own ends.\n \nreply",
      "Iceland, Japan and Norway are the culprits still supporting whaling, not China - just fyi :)\n \nreply",
      "Did China stop whaling or did they just add it to the list of things they have to be quiet about? like how drugs are heavily controlled on the mainland and yet fentanyl precursors almost all come from China before being smuggled into US?\n \nreply",
      "Perhaps use a better word than culprit, since fishing whale is legal in all three of those places!\n \nreply"
    ],
    "link": "https://www.biographic.com/ping-youve-got-whale/",
    "first_paragraph": "03.12.2025 | News, Solutions, Wild LifeFrom inside a package about the size of a shoebox mounted to a ship\u2019s deck, a set of highly stabilized heat-sensing cameras scans the ocean\u2019s surface. Suddenly, against the misty waves far in the distance, they spot a small puff of white. And another. Now the algorithm catches on. A machine learning system snags the footage and runs it through a neural network trained on millions of similar snippets.\u00a0\u00a0Comparing what it\u2019s detecting against its training data, the artificial intelligence model makes a call: that small burst of heat in the distance is a spout of whale breath. The computer system pings a remote expert on standby who double-checks the machine\u2019s work. Within a minute, the expert forwards the alert back to the ship, catching the captain\u2019s attention with enough time for the crew to change course and, hopefully, avoid the whale becoming maritime roadkill.This is WhaleSpotter, an artificial intelligence-powered whale detection system that ai",
    "summary": "**Ping, You've Got Whale: Seriously, though, who needs radar?**\n\nIn an age where smartphones think they know you better than your mother does, technology now presumes it can save whales with the equivalent of a soccer mom's minivan backup camera glued to a ship. Dramatically scanning the waves with \"highly stabilized heat-sensing cameras,\" this digital lookout\u2014dubbed WhaleSpotter\u2014believes it can tell a whale's breath from sea spray. The internet commentariat chime in with their usual expertise, ranging from dismissing AI capabilities to debating international whaling politics, because why solve a problem when you can argue about it instead? Enter the armchair engineers and amateur ethicists, because if there's one thing better than an unfounded opinion, it's three paragraphs of it on a website nobody reads. \ud83d\udc33\ud83d\udcbb"
  },
  {
    "title": "The Illuminated Gospel of St John (cambridge.org)",
    "points": 8,
    "submitter": "ycombinete",
    "submit_time": "2025-04-19T02:32:42 1745029962",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.cambridge.org/universitypress/bibles/illuminated-gospel-of-st-john",
    "first_paragraph": "",
    "summary": "The esteemed intellectuals at Cambridge have once again unearthed a relic, this time it's the *Illuminated Gospel of St John*. Apparently, adding some shiny gold leaf to ancient texts now qualifies for a breakthrough in historical scholarship. Online commenters, squinting through their bifocals, fiercely debate whether the gold is 24k or merely 18k, because this is clearly crucial to understanding the nuances of early Christian theology. It wouldn't be a real academic squabble without narrowly avoiding the topic entirely, but don't worry, they\u2019ve got that covered too. \ud83d\udcdc\u2728"
  },
  {
    "title": "Algebraic Semantics for Machine Knitting (uwplse.org)",
    "points": 180,
    "submitter": "PaulHoule",
    "submit_time": "2025-04-22T15:55:12 1745337312",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=43763614",
    "comments": [
      "Speaking of computational knitting, I recently learned about \"solid knitting\" [1] which is awesome.https://www.cs.cmu.edu/news/2024/solid-knitting\n \nreply",
      "That also links to an older piece about a software project for knitting machines that's really neat. I've been wondering for a while how difficult it would be to build a primitive one at home.https://www.cs.cmu.edu/news/2018/software-automatically-gene...\n \nreply",
      "Braid groups are really interesting, and they also come up in fluid mixing: https://people.math.wisc.edu/~thiffeault/talks/gordon2022.pd...\n \nreply",
      "Back in the 1980s I was taking a foundational computer science course in which we derived Goedel's result using Cantor diagonalization. Excellent course. We were watching the TV version of Hitchhiker's Guide to the Galaxy at the time, too. One day I had the realization that since any recursively enumerable function could be interpreted as a computer program (given the right interpreter), that the sweater I was wearing was in fact possibly a computer program, and that all knitting (and some crocheting) was in fact just a manifestation of code in another language.I then went on to realize any enumerable set could be similarly interpreted, including the entire countable population of Earth. And we already had the answer (42), but what was the question?\n \nreply",
      "I suppose if nothing else, you could encode Wang tiles (https://en.wikipedia.org/wiki/Wang_tile) into knitting and then that's Turing-complete? Or would there be some better CA to encode?\n \nreply",
      "sorry, 42 is not gonna take us much farther42 is a stand in for 41 and 43 which are some twin primefor me to further elaborate on this crazy idea that haunts me (I must admit I also haunt these ideas) requires a twin prime theorem which we are still waiting for in 2025....\n \nreply",
      "Does anyone know a good emulator for knitting machines? I'd love to play with these programs, but I'd like to get some practice before I start messing with real wool.\n \nreply",
      "Someone else mentioned the acrylic, I'm going to mention Scarlett Sparks' Open Source Knitting Machine if part of the fear is actually investing in the machine\nhttps://github.com/ScarlettSparks/KnittingMachine\n \nreply",
      "Possibly off-topic, but if you're looking to reduce your costs you should look at acrylic yarn.  There's also cotton yarn if you're looking for something less scratchy :)\n \nreply",
      "Still kicking myself for not buying a \"3D Knitted Chisel Roll\" back when Lee Valley had them --- last I checked it might have been possible to import one from Europe, but having a hard time justifying that.....\n \nreply"
    ],
    "link": "https://uwplse.org/2025/03/31/Algebraic-Knitting.html",
    "first_paragraph": "As programming languages researchers, we\u2019re entitled to a certain level of mathematical\nrigor behind the languages we write and analyze. Programming languages have semantics, which are\ndefinitions of what statements in the language mean. We can use those semantics to do all\nsorts of useful things, like error checking, compiling for efficiency, code transformation,\nand so on.This blog post is about a programming domain that doesn\u2019t yet enjoy the same level of rigor\nin its semantics: machine knitting. People write programs to control massive arrays of needles\nthat manipulate yarn into useful 3D objects. In this blog post, I\u2019ll run through the process\nof finding \u201cthe right\u201d semantics for machine knitting, touching on why we want semantics, connections\nto traditional programming languages, and what we might use these semantics for in the future.\nIn our search, there are a surprising number of guest appearances by fields of study outside of programming languages:\nalgebraic topology, group t",
    "summary": "<b>Welcome to the glamorous world of Algebraic Semantics for Machine Knitting!</b> Today, we bring you a thrilling exploration of how to infuse \ud83d\udca2 earth-shattering mathematical rigor \ud83d\udca2 into the untamed wilderness of knitting machines. Marvel as programming whizzes ramble through vague connections between yawn-inducing \"traditional programming languages\" and avant-garde yarn manipulation. Never have needle arrays seemed so existential! Meanwhile, in the comments, keyboard warriors enthusiastically confuse knitting patterns with advanced computational theories. Watch as they braid together concepts, fabricating a rich tapestry of pseudo-academic bluster, all while frantically googling terms to make sure they sound smart. Knitting as Turing-complete computational models? Sure, and your grandma's embroidery is probably solving NP-complete problems too! \ud83e\udd13\ud83e\uddf6"
  },
  {
    "title": "ClickHouse gets lazier and faster: Introducing lazy materialization (clickhouse.com)",
    "points": 221,
    "submitter": "tbragin",
    "submit_time": "2025-04-22T16:03:32 1745337812",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=43763688",
    "comments": [
      "God clickhouse is such great software, if it only it was as ergonomic as duckdb, and management wasn't doing some questionable things (deleting references to competitors in GH issues, weird legal letters, etc.)The CH contributors are really stellar, from multiple companies (Altinity, Tinybird, Cloudflare, ClickHouse)\n \nreply",
      "They have an interesting version that's packaged a bit like DuckDB - you can even \"pip install\" it: https://github.com/chdb-io/chdb\n \nreply",
      "They don't do static builds AFAICT, which would make it a real competitor to DuckDB.\n \nreply",
      "This optimization should provide dramatic speed-ups when taking random samples from massive data sets, especially when the wanted columns can contain large values. That's because the basic SQL recipe relies on a LIMIT clause to determine which rows are in the sample (see query below), and this new optimization promises to defer reading the big columns until the LIMIT clause has filtered the data set down to a tiny number of lucky rows.    SELECT *\n    FROM Population\n    WHERE weight > 0\n    ORDER BY -LN(1.0 - RANDOM()) / weight\n    LIMIT 100  -- Sample size.\n\nCan anyone from ClickHouse verify that the lazy-materialization optimization speeds up queries like this one? (I want to make sure the randomization in the ORDER BY clause doesn't prevent the optimization.)\n \nreply",
      "I checked, and yes - it works: https://pastila.nl/?002a2e01/31807bae7e114ca343577d263be7845...\n \nreply",
      "Thanks! That's a nice 5x improvement.  Pretty good for a query that offers only modest opportunity, given that the few columns it asks for are fairly small (`title` being the largest, which isn't that large).\n \nreply",
      "The optimization should work well for your sampling query since the ORDER BY and LIMIT operations would happen before materializing the large columns, but the randomization function might force early evaluation - worth benchmarking both approaches.\n \nreply",
      "Verified:  EXPLAIN plan actions = 1\n  SELECT *\n  FROM amazon.amazon_reviews\n  WHERE helpful_votes > 0\n  ORDER BY -log(1 - (rand() / 4294967296.0)) / helpful_votes\n  LIMIT 3\n\nLazily read columns: review_body, review_headline, verified_purchase, vine, total_votes, marketplace, star_rating, product_category, customer_id, product_title, product_id, product_parent, review_date, review_idNote that there is a setting query_plan_max_limit_for_lazy_materialization (default value 10) that controls the max n for which lm kicks in for LIMIT n.\n \nreply",
      "Awesome! Thanks for checking :-)\n \nreply",
      "I really like Clickhouse. Discovered it recently, and man, it's such a breath of fresh air compared to suboptimal solutions I used for analytics. It's so fast and the CLI is also a joy to work with.\n \nreply"
    ],
    "link": "https://clickhouse.com/blog/clickhouse-gets-lazier-and-faster-introducing-lazy-materialization",
    "first_paragraph": "",
    "summary": "**Lazy Materialization or Lazier Innovation? ClickHouse Plays Catch-Up**\n\nIn a world thirsting for *real* innovation, ClickHouse introduces \"lazy materialization,\" a concept so riveting it almost makes you forget their UX design is as inviting as a cactus hug. Meanwhile, enthusiasts rave about the feature's groundbreaking impact, which promises a speed boost akin to swapping your lead shoes for wooden ones during a sprint. Users are practically giddy, flooding forums with responses ranging from naive adoration to mild skepticism, usually interspersed with soft laments over ClickHouse's corporate antics: deleting competitors from discussions and charming their users with 'unique' legal love letters. Whether you're here for the technical marvels or just for the community drama, remember: every \"pip install\" brings you one step closer to software Shangri-La\u2014or at least a less torturous data sampling experience. \ud83d\ude80\ud83d\ude44"
  },
  {
    "title": "\u03c00.5: A VLA with open-world generalization (pi.website)",
    "points": 121,
    "submitter": "lachyg",
    "submit_time": "2025-04-22T17:29:31 1745342971",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=43764439",
    "comments": [
      "Most of it is open source. Their VLAs are based upon Gemma models + vision encoders, plus their own action experts. You can download and play around or fine tune their Pi0 VLAs from their servers directly (JAX format) or from Huggingface LeRobot safetensors port. They also have notebooks and code in their repo to get started with fine-tuning. Inference runs in a single 4090 RTX streamed over WiFi to the robot.\n \nreply",
      "OpenAI is among their investors, which makes me wonder how long their work remains \"open\".\n \nreply",
      "This is amazing! As someone working with industrial robots, normally under strict environmental constraints and control, witnessing such real-world robotics progress truly excites me about the future!By the way, they\u2019ve open-sourced their \u03c00 model (code and model weights). \nMore information can be found here: https://github.com/Physical-Intelligence/openpi\n \nreply",
      "It seems robotics has advanced more in the last 3 years than the previous 20.\n \nreply",
      "the torrent of funding helps here\n \nreply",
      "These variable-length arrays are getting quite advanced\n \nreply",
      "Precisely my thoughts.\n \nreply",
      "Ignore the haters. This is hilarious\n \nreply",
      "Amazing! On a fun note, I believe if a human kid were cleaning up the spill and threw the sponge into the sink like that, the kid would be in trouble. XD\n \nreply",
      "I'm genuinely asking (not trying to be snarky)... Why are these robots so slow?Is it a throughput constraint given too much data from the environment sensors?Is it processing the data?I'm curious about where the bottleneck is.\n \nreply"
    ],
    "link": "https://pi.website/blog/pi05",
    "first_paragraph": "Robots have come a long way over the past few years\u2014they can perform impressive acrobatic feats, dance on stage, follow language commands and, in some of our own results, perform complex tasks like folding laundry or cleaning off a table. But the biggest challenge in robotics is not in performing feats of agility or dexterity, but generalization: the ability to figure out how to correctly perform even a simple task in a new setting or with new objects. Imagine a robot that needs to clean your home: every home is different, with different objects in different places. Generalization must occur at many levels. At the low level, the robot must understand how to pick up a spoon (by the handle) or plate (by the edge), even if it has not seen these specific spoons or plates before, and even if they are placed in a pile of dirty dishes. At a higher level, the robot must understand the semantics of each task\u2014where to put clothes and shoes (ideally in the laundry hamper or closet, not on the bed",
    "summary": "**\u03c00.5: The future of lazy household help unveiled**\n\nIn today\u2019s technological circus, robots are clearly aiming for the highly coveted honor of doing your laundry. These mechanical marvels, after *years* of back-breaking R&D, can now understand the daunting complexities of picking up dirty spoons and not throwing your shoes onto the bed. In the comments, fervent tech aficionados and casual observers alike marvel at how these open-source pile-sorters (thanks <i>Gemma</i> models and <i>hilarious WiFi overloads</i>!) could not only ensure societal laziness but also spark intense debates about robot-induced unemployment and the ethics of self-cleaning homes. Most notably, while some relish the open-source pi\u00f1ata of robotics, there are whispered fears that Big Investor shadows may soon loom over this utopian open-source landscape. Ah, the joys of tech progress!"
  },
  {
    "title": "Show HN: Rowboat \u2013 Open-source IDE for multi-agent systems (github.com/rowboatlabs)",
    "points": 86,
    "submitter": "segmenta",
    "submit_time": "2025-04-22T16:33:21 1745339601",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=43763967",
    "comments": [
      "\"It\u2019s becoming clear that real-world agentic systems work best when multiple agents collaborate, rather than having one agent attempt to do everything.\"I'll be honest: I don't buy that premise (yet). It's clearly a popular idea and I see a lot of excitement about it (see Google's A2A thing) but it feels to me like a pattern that, in many cases, will make LLM code even harder to get reliable results from.I worry it's the AI-equivalent of microservices: useful in a small set of hyper complex systems, the vast majority of applications that adopt it would have been better off without.If there are strong arguments counter to what I've said here I'd love to hear them!\n \nreply",
      "This sounds really really nice. Potentially exactly what I've been hoping would exist. Thank you for putting it out there!Will try it out over the weekend. Exciting stuff.\n \nreply",
      "Thanks, that's great to hear! We'd love to learn what worked and what didn't for you.\n \nreply",
      "I will test it out. I am mildly skeptical about the use case, but that may be today's experience of current project's PM not knowing anything about background of the project they are managing, which immediately makes me realize the limitations of all similar systems.\n \nreply",
      "Thanks for checking it out. Curious what you think after testing.\n \nreply",
      "1. Are you going to support Google's A2A protocol?2. Are you going to support stateless chat?\n \nreply",
      "1. A2A is on our roadmap (still exploring), for agents built on Rowboat to communicate with external agents. I assume that's what you mean as well.2. Rowboat agents are technically stateless in that they do not store any messages and state between turns. Our HTTP API [0] requires previous messages and state to be explicitly passed from the previous turn to the next turn. For now, the state is pretty simple - just the name of the last agent that responded to the user. This allows the system to start off directly from where the previous agent left off. The Python SDK [1] manages the state for you. Does that make sense?[0] API docs - https://docs.rowboatlabs.com/using_the_api/[1] SDK docs - https://docs.rowboatlabs.com/using_the_sdk/\n \nreply",
      "Now this is what I'm talking about \u2014 this checks all the boxes for me.I was looking at \"Agent builders\" for a while now and nothing really stood out. They all seemed to use a \"node\" type structure, while I was looking to tell something what I need using natural language.The only thing that came close was Vessium, but I never heard back after adding myself to the waiting list weeks ago. I also wasn\u2019t so hot about their questions either \u2014 \"Are you cool with paying for a product just to try it,\" or something to that effect. I\u2019ll admit though, I said yes. =)Either way, congrats on the launch and wishing you much success.\n \nreply",
      "Thanks! We wanted to have the copilot planning and building out agents be a core part of Rowboat (like how Cursor works with code).\n \nreply",
      "It would be awesome if this could be wrapped around a native app rather than another webapp. Otherwise, great job!\n \nreply"
    ],
    "link": "https://github.com/rowboatlabs/rowboat",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        AI-powered multi-agent builder\n      Quickstart | Docs | Website |  DiscordPowered by OpenAI's Agents SDK, Rowboat is the fastest way to build multi-agents!Set your OpenAI keyClone the repository and start Rowboat dockerAccess the app at http://localhost:3000.There are 2 ways to integrate with the agents you create in RowboatHTTP APIPython SDK\nYou can use the included Python SDK to interact with the AgentsSee SDK Docs for details. Here is a quick example:Refer to Docs to learn how to start building agents with Rowboat.\n        AI-powered multi-agent builder\n      ",
    "summary": "Title: Show HN: Rowboat \u2013 Another Way to Make Developers Cry\n\nWelcome to the latest bandwagon, Rowboat, where developers can now convolute their projects with \"AI-powered\" multi-agent chaos, because managing monolithic disasters was just too mainstream. The creators assert that they \"take feedback very seriously,\" ushering a new age where simply compiling feedback forms could also require an OpenAI key. Meanwhile, the comment section oscillates between unbridled joy and existential dread, as armchair engineers debate whether this is the next big thing or just a high-tech way to create more bugs. \"I'll test it over the weekend,\" declares one hopeful user, blissfully unaware that their weekend is about to be hijacked by the hellish labyrinth of HTTP APIs and Python SDKs. At least they\u2019ll have Discord to scream into."
  },
  {
    "title": "How to quickly charge your smartphone: fast charging technologies in detail (eb43.github.io)",
    "points": 26,
    "submitter": "uycyp",
    "submit_time": "2025-04-22T22:00:36 1745359236",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43766728",
    "comments": [
      "One thing I was hoping the article would explain is how fast charging actually works: IE, how does the charge controller turn incoming electricity into a full battery?The reason why I switched to wireless charging was because I had a phone go bad due to problems with the charge port. USB-c ports on my Pixels to tend to clog with dust and other debris, but as long as I use a wireless charger, it doesn't matter.\n \nreply",
      "I feel something is missing at the moment with our ability to choose on the device how I want it to charge. Would be nice to plug the phone in and unless I say otherwise it does a nice slow charge even if the cable and charger is capable of more. But if I press a button on the front after I plug it in then I can select fast and it goes as quickly as the combination allows. Its irritating to have to have different chargers for this to preserve the battery.Its pretty bad that only recent phones have started to add the ability to charge to only 80% to keep the battery in the optimal zone to extend the life given how long we have known that 80% was the optimal maximum. There are also a few phones now able to power themselves from USB without using the battery which if you leave them in chargers a lot throughout the day and night seems like a good feature to have to preserve the battery further.Maybe all the complexity around this is too much and people just want to plug it in and quick 100% as quick as possible and will change their phone regularly but its pretty wasteful. We ended up going through lots of special chargers that all do very similar things and now you get a device and it often doesn't even come with a cable let alone a charger and you are digging through the specs of your charger, cable and device to work out if its all going to mesh correctly together or you'll be stuck on slow mode. We have ended up with so many standards for getting quicker than the basic charge its going to take a while for all these devices to age out and in the meantime chargers are going to be doing QC and PD and a host of other things besides for a while.\n \nreply",
      "> Maybe all the complexity around this is too much and people just want to plug it in and quick 100% as quick as possible and will change their phone regularlyIt's this, 100%. The nerds that care about charge temperature and battery degradation and proliferation of incompatible charging standards are a rounding error, most people just want to know it can charge fast in case they forget and need to leave soon.I am happy with my Chargie [1], an interposing dongle which provides a Bluetooth receiver and app that lets you set arbitrary preferences on your phone and fast charge, slow charge, or turn off the charger at configurable state of charge setpoints or times.[1] https://chargie.org/product/chargie-c-basic/\n \nreply",
      ">In such a situation, the best solution is slow charging, for which a low-power charger from your old feature phone is ideal.It seems like most Android phones now have a feature that does this, though you have to enable it. Mine asked me to put in the time I go to bed and the time I wake up, and it slowly charges the battery over that time period to 100%\n \nreply",
      "I never even strictly enabled it, but I'm a big fan. My pixel 5 from 2020 still easily gets a full day of battery life, and without direct evidence, I suspect always slow-charging has helped with that.\n \nreply",
      "My Pixel just bases this off of the morning alarm clock. Since I don't like waking to an alarm, this just means that I have a permanent silent 9am alarm.I do hope that USB PD and (to a lesser extent) Qi takeover the charging space. There is really no reason to have every SoC vendor create their own incompatible fast charging standard.\n \nreply",
      "Many phones/chargers with propietary protocols are also equipped with USB PD compatibility. For example, you can pick up some random Xiaomi phone labelled at 90W and expect it to charge at more than 20W with USB PD, or even ~90W on some vivo models.I don't see why the best charger is the old feature phone's charger - the point in fast charging is that you don't have to wait too long if in a hurry. That charger is the \"best\" if only you have almost unlimited time to charge, like during the night.\n \nreply",
      "Wish they\u2019d kept 12V throughout the PD standards. So useful for hobby stuff\n \nreply",
      "I don't think Apple was the first to use the magnets to align the phone to the charger.  I think I had a nexus 5 that had the magnets in the phone and google Qi charger -- that would perfectly align the phone and charger.\n \nreply",
      "It is annoying that while the connector itself is now finally standardized, the details of implementation diverge. So while you may be able to charge, it'll be slow charge.\n \nreply"
    ],
    "link": "https://eb43.github.io/articles/fast-charging-technologies-in-detail.html",
    "first_paragraph": "Published: 22 April 2025\u2190\ud83c\udfe0 Back to eb43.github.io articles listThe great days are gone when charging a smartphone only required two wires. Today, a charger is a full-fledged computer with more processing power than the Apollo 11 spacecraft that brought humans to the Moon. A processor is necessary in the charger to negotiate charging parameters with the smartphone. Let's dive into fast charging technologies in detail.If you plug in your smartphone before bed, unplug it in the morning, and the charge lasts the whole day \u2013 you don't need to worry about specific charging standards or chargers.In such a situation, the best solution is slow charging, for which a low-power charger from your old feature phone is ideal. These devices offer a small power output of 2\u20135 W, stretching the charging process to 5\u20137 hours depending on the adapter's power and the battery capacity. This way, your phone's battery charges under the best conditions, minimizing degradation. Fast charging leads to up to 1.6% ",
    "summary": "**Hackernews Tries to Think About Phones**\n\nIn a shocking display of missed-the-point, a spirited troupe of would-be engineers debate the sinful pleasures of fast charging versus the puritanical ecstasy of snail-paced power dribbles. In a thrilling article that boasts more complexity than the Apollo 11\u2019s lunar escapade, enthusiasts wax poetic about the moral hazards of pushing electrons too quickly. The commenters, armed with an arsenal of personal anecdotes and speculative tech utopias, manage to both deeply understand and completely misunderstand the basic function of charging a phone. As the argument about optimal battery life heats up, one can't help but marvel at the <em>critical</em> tech issue of our time: should my phone charge in 30 minutes or overnight? \ud83d\ude80\ud83d\udcf1\ud83d\udca4"
  },
  {
    "title": "Classic Computer Replicas (obsolescence.dev)",
    "points": 41,
    "submitter": "dbelson",
    "submit_time": "2025-04-22T20:16:04 1745352964",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43765832",
    "comments": [
      "Old mini-computers, one of my favourite topics! Obligatory plug for doing achieving something similar today on a budget https://rodyne.com/?p=1751 discussed here https://news.ycombinator.com/item?id=42672366\n \nreply",
      "The \"Extinct\" arrow next to \"Workstations - Sun, SGI\" is sad - I miss the days of workstation-class machines\n \nreply",
      "I have a few of these and enjoy them \u2014 the PDP-8 (emulated: PiDP-8), Altair 8800 (emulated: Altair-Duino), KIM-1 (more or less the real deal modulo the 6530 RIOT chips that are no longer available: PAL-1 and PAL-2).These, as kits, are fun to assemble, fun then to play around with.\n \nreply",
      "Obsolescence Guaranteed Is a good name for a replica retro computer store.If I ever retire and run a bar on the beach^w^w^w^w retro computer store, that\u2019s what it\u2019ll be called.\n \nreply",
      "I may be alone in craving an AN/UYK-7.https://en.m.wikipedia.org/wiki/AN/UYK-7\n \nreply",
      "by \"replica\" you mean emulator or something else?\n \nreply",
      "They do appear to mean emulator.The PDP-8 was hardware replicated many times.  In the '80s it was a common final year project.  There's a classic textbook that works through designing and implementing a clone of the PDP-8/I [1].  I've run into a number of threads over the years where hobbyists have done it with TTL to varying degrees of completeness.The Apollo Guidance Computer was recreated by a hobbyist from the original designs using a modern logic family but gate-equivalent -- and I can't find it online anymore!  Anyone know?You can still build an original Apple II.  [2]  Being from the late 1970s there was no custom logic; it's straight TTL plus a 6502, and all the chips are still in production except for the ROMs and DRAM, which are easy enough to work around or find used.[1] https://www.amazon.ca/Art-Digital-Design-Introduction-Top-Do...[2] https://www.reactivemicro.com/product/apple-ii-plus-rev-7-rf...\n \nreply"
    ],
    "link": "https://obsolescence.dev/index.html",
    "first_paragraph": "",
    "summary": "Welcome to the niche-enthusiast swamp of _obsolescence.dev_, a throbbing echo chamber where grey-bearded geeks rally to out-nerd each other with their scrapyard-treasures-turned-desk-decor. Here, the phrases \"emulated nostalgia\" and \"replica\" are wielded like weapons in a war reenactment, except with less movement and more carpal tunnel syndrome. Users wax poetic over their soldering conquests\u2014the PDP-8 made from Pixy Stix and dreams, while pining for the neon-lit days of Sun Workstations with a melancholy typically reserved for lost pets. Dive into the comments and experience the thrill of watching adults clamor over digital antiques with the same fervor your grandma reserves for limited edition Beanie Babies. \ud83e\udd13\ud83d\udee0\ufe0f"
  },
  {
    "title": "Onyx (YC W24) Is Hiring for ML Engineer (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-04-22T21:00:48 1745355648",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/onyx/jobs/3Se5ptG-machine-learning-engineer",
    "first_paragraph": "Open Source AI Assistant and Enterprise SearchOnyx is a popular open source project with hundreds of thousands of users. The project has over 10K stars and over 3K community members across Slack and Discord (these stats may already be out of date when you read this). You\u2019ll have the opportunity to build in the open and your work may be used by millions of people in the future.Onyx is the knowledge layer on top of LLMs. Help us improve our agent and knowledge retrieval capabilities to push the frontier on unsolved problems like multi-hop QA, needle in haystack, aggregation type RAG, etc. This is an in-person role based in San Francisco, CA.You\u2019ll be:Onyx is the open source GenAI platform connected to your company's docs, apps, and people. We ingest and sync from all sources of information (Google Drive, Slack, GitHub, Confluence, Salesforce, etc.) to provide a centralized place for users to ask about anything. Imagine your most knowledgeable co-workers, all-rolled into one, and availabl",
    "summary": "**Onyx (YC W24) Engineers Paradise or Prison?**\n\nOnyx, yet another Y-Combinator progeny, pretends to revolutionize the mundane task of finding that email about the offsite three months ago. Boasting a bloated star count on GitHub like it's their Tinder bio, Onyx promises you'll work on \"unsolved problems\" \u2013 the corporate jargon equivalent of \"please fix this mess.\" The enthusiasts in Slack and Discord are already winding up, ready to mansplain the prowess of the \"knowledge layer\" to any wrongdoer who dares question the tech. San Francisco, hyper-caffeinated engineers, and existential dread\u2014what could go wrong? \ud83d\ude80\ud83e\udd13"
  },
  {
    "title": "Show HN: Morphik \u2013 Open-source RAG that understands PDF images, runs locally (github.com/morphik-org)",
    "points": 117,
    "submitter": "Adityav369",
    "submit_time": "2025-04-22T16:18:41 1745338721",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=43763814",
    "comments": [
      "We\u2019re open\u2011source under the MIT Expat license\"Not quite. You should clarify a bit more. The README has this about their license.\"Certain features - such as Morphik Console - are not available in the open-source version. Any feature in the ee namespace is not available in the open-source version and carries a different license. Any feature outside that is open source under the MIT expat license.\"\n \nreply",
      "Thanks we should have been more clear. The part in ee is our UI, which can be used to test or in dev environments. The main code, including API, SDK, and the entire backend logic is MIT expat.\n \nreply",
      "I'd love to have something like this but calling a cloud is a no-go for me. I have a half baked tool that a friend of mine and I applied to the Mozilla Builders Grant with (didn't get in), it's janky and I don't have time to work on it right now but it does the thing. I also find myself using OpenWebUI's context RAG stuff sometimes but I'd really like to have a way to dump all of my private documents into a DB and have search/RAG work against them locally, preferably in a way that's agnostic of the LLM backend.Does such a project exist?\n \nreply",
      "Just curious, are you fine with running things in your own AWS / Azure / GCP account or do you really mean that the solution has to be fully on-premise?\n \nreply",
      "You can run this fully locally using Ollama for inference, although you'll need larger models and a beefy machine for great results. On my end llama 3.2 8B does a good job on technical docs, but bigger the better lol.\n \nreply",
      "Ahh, I didn't see that, I just saw them talking about a free tier or whatever and my eyes glazed over. I'll try it out with Mistral-small 3.1 at some point tonight, I've been having really great results with it's multimodal understanding.\n \nreply",
      "The architecture sounds very, very promising.  Normalizing entities and relations to put in a graph for RAG sounds great.  (I'm still a bit unclear on ingesting or updating existing graphs.)Curious about suitability of this for PDF's as conference presentation slides vs academic papers.  Is this sensitive or tunable to such distinctions?Looking for tests/validation; are they all in the evaluation folder?  A Pharma example would be great.Thank you for documenting the telemetry.  I appreciate the ee commercialization dance :)\n \nreply",
      "For ingesting graphs, you can define a filter, or certain document ids. When updating, we look at if any other docs are added with that filer (or you can specify new doc ids). We then do entity and relationship extraction again, and do entity resolution with the existing graph to merge the two.Creating graphs and entity resolution are both tunable with overrides, you can specify domain specific prompts and overrides (will add a pharma example!) (https://docs.morphik.ai/python-sdk/create_graph#parameters). I tried to add code, but was formatting badly, sorry for the redirect.\n \nreply",
      "ColQwen is basically a strict upgrade \u2014 would give it a go!\n \nreply",
      "We do use ColQwen! Currently 2, but upgrading to 2.5 soon :)\n \nreply"
    ],
    "link": "https://github.com/morphik-org/morphik-core",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Open source multi-modal RAG for building AI apps over private knowledge. \n      \n\n\n\n\n\n\n\n\nDocs - Community - Why Morphik? - Bug reports\nMorphik provides developers the tools to ingest, search (deep and shallow), transform, and manage unstructured and multimodal documents. Some of our features include:The best part? Morphik has a free tier and is open source! Get started by signing up at Morphik.The fastest and easiest way to get started with Morphik is by signing up for free at Morphik. Your first 200 pages and 100 queries are on us! After this, you can pay based on usage with discounted rates for heavier use.If you'd like to self-host Morphik, you can find the dedicated instruction here. We offer options for direct isntallation and installation via docker.Important: Due to limited resources, we cannot provide full support for open-s",
    "summary": "**Title: Tech Bros Unleash AI to Skim Your Pdfs, Boasting Open Source (But Read the Fine Print)**\n\nToday, in a thrilling display of selective transparency, alpha geeks at Morphik rolled out an \"open-source* (*terms and conditions may apply)\" tool designed to help you not read your PDFs by having an AI do it for you\u2014because who needs comprehension when you can have automation? The enthusiastic early adopters on HN quickly flooded the comment section with the usual humble-bragging about their semi-functional pet projects that barely scrape by on the latest machine learning buzzwords. Others poked around the licensing details only to discover that \"open-source\" actually means \"open to a very detailed EULA.\" Someone tried to pin down feature specifics but got distracted by the free tier's spa-like onboarding, eventually succumbing to cloud storage Stockholm syndrome despite initial objections. \ud83d\udcc4\u2728\ud83d\udc40"
  },
  {
    "title": "Are polynomial features the root of all evil? (2024) (alexshtf.github.io)",
    "points": 133,
    "submitter": "Areibman",
    "submit_time": "2025-04-22T16:49:55 1745340595",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=43764101",
    "comments": [
      "In another post (https://alexshtf.github.io/2025/03/27/Free-Poly.html) the author fits a degree-10000 (ten thousand!) polynomial using the Legendre basis. The polynomial _doesn't overfit_, demonstrating double descent. \"What happened to our overfitting from ML 101 textbooks? There is no regularization. No control of the degree. But \u201cmagically\u201d our high degree polynomial is not that bad!\"So... are _all_ introductions to machine learning just extremely wrong here? I feel like I've seen tens of reputable books and courses that introduce overfitting and generalization using severe overfitting and terrible generalization of high-degree polynomials in the usual basis (1,x,x^2,...). Seemingly everyone warns of the dangers of high-degree polynomials, yet here the author just says \"use another basis\" and proves everyone wrong? Mind blown, or is there a catch?\n \nreply",
      "The catch is that orthogonal polynomial bases (like Legendre) implicitly regularize by controlling the Riesz representer norm, effectively implementing a form of spectral filtering that penalizes high-frequency components.\n \nreply",
      "No. All the textbooks know that polynomials of high degree are numerically dangerous and you need to be careful when handling them.The articles examples only work because the interval 0 to 1 (or -1 to 1) were chosen. For whatever reason the author does not point that out or even acknowledges the fact that had he chosen a larger interval the limitations of floating point arithmetic would have ruined the argument he was trying to make.10^100 is a very large number and numerically difficult to treat. For whatever reason the author pretends this is not a valid reason to be cautious about high degree polynomials.\n \nreply",
      "He seems reasonably explicit about this:\"\"This means that when using polynomial features, the data must be normalized to lie in an interval. It can be done using min-max scaling, computing empirical quantiles, or passing the feature through a sigmoid. But we should avoid the use of polynomials on raw un-normalized features.\"\"\n \nreply",
      "No.This paragraph has nothing to do with numerics. It is about the fact that continuous functions can not be approximated globally by polynomials. So you need to restrict to intervals for reasons of mathematical theory. This is totally unrelated to the numerical issues, which are nowhere even acknowledged.\n \nreply",
      "But what's the point in acknowledging numerical issues outside of [-1,1] if polynomials do not even work there, as author explicitly notes?\n \nreply",
      "Neural network training is harder when the input range is allowed to deviate from [-1, 1]. The only reason why it sometimes works for neural networks is because the first layer has a chance to normalize it.\n \nreply",
      "The degree-10000 polynomial is definitely overfitting - every data point has its own little spike.  The truncated curves look kind of nice, but the data points aren't shown on those plots, and the curves aren't very close to them.There are also some enormous numbers hidden away here too, with associated floating point precision problems; the articles show the coefficients are small, but that's because the polynomial basis functions themselves have gigantic numbers in them.  The Bernstein basis for degree-100 involves 100 choose 50, which is already > 10^29.  You have to be careful calculating these polynomials or bits of your calculation exceed 64-bit floating point range, e.g. factorials of 10000, 2^10000.  See the formulas and table in this section: https://en.wikipedia.org/wiki/Legendre_polynomials#Rodrigues...\n \nreply",
      "https://arxiv.org/pdf/2503.02113This paper shows that polynomials show most features of deep neural nets, including double descent and ability to memorize entire dataset.It connects dots there - polynomials there are regularized to be as simple as possible and author argues that hundredths of billions of parameters in modern neural networks work as a regularizers too, they attenuate decisions that \"too risky.\"I really enjoyed that paper, a gem that puts light everywhere.\n \nreply",
      "The original paper about the bias variance tradeoff, that the double decent papers targeted, had some specific constraints.1) data availability and computer limited training set sizes.\n2) they could simulate infinite datasets.While challenging for our minds, training set sizes today make it highly likely that the patterns in your test set are similar to concept classes in your training set.This is very different than saying procedure or random generated test sets, both of which can lead to problems like over fitting with over parameterized networks.When the chances are that similar patterns exist, the cost of some memorization goes down and is actually somewhat helpful for generalization.There are obviously more factors at play here, but go look at the double decent papers and their citations to early 90's papers and you will see this.The low sensitivity of transformers also dramatically helps, with UHAT without CoT only having the expressiveness of TC0, and with log space scratch space having PTIME expressability.You can view this from autograd requiring a smooth manifold with the ability to approximate global gradient too if that works better for you.But yes all intros have to simplify concepts, and there are open questions.\n \nreply"
    ],
    "link": "https://alexshtf.github.io/2024/01/21/Bernstein.html",
    "first_paragraph": "\n        Jan 21, 2024\n      \n\u27ea\u00a0\u201cProximal Point to the Extreme - Factorization Machines\"\n\n\u201cKeeping the polynomial monster under control\"\u00a0\u27eb\nWhen fitting a non-linear model using linear regression, we typically generate new features using non-linear functions. We also know that any function, in theory, can be approximated by a sufficiently high degree polynomial. This result is known as Weierstrass approximation theorem. But many blogs, papers, and even books tell us that high polynomials should be avoided. They tend to oscilate and overfit, and regularization doesn\u2019t help! They even scare us with images, such as the one below, when the polynomial fit using the data points (in red) is far away from the true function (in blue):\nIt turns out that it\u2019s just a MYTH. There\u2019s nothing inherently wrong with high degree polynomials, and in contrast to what is typically taught, high degree polynomials are easily controlled using standard ML tools, like regularization. The source of the myth stems m",
    "summary": "### Are polynomial features the root of all evil? (2024) (alexshtf.github.io)\n\nIn a stroke of unmatched genius, a blog post dares to shatter the long-standing tradition of fearing high-degree polynomials like some kind of mathematical boogeyman. The author, flaunting a cavalier blend of optimism and polynomial coefficients, declares them not just benign but downright harmless when swaddled in the cozy confines of the Legendre basis. Meanwhile, the horde of _incredibly insightful_ commenters, armed with their freshly Googled knowledge, trip over themselves to one-up each other\u2014debating numerical limits, the inherent moral failings of non-[-1, 1] intervals, and whether machine learning textbooks are actually elaborate hoaxes. Truth, justice, and high-degree polynomial models prevail in another day of internet pedantry. \ud83c\udf93\ud83d\udca5\ud83d\udcda"
  },
  {
    "title": "Show HN: Durable Python Workflows (github.com/autokitteh)",
    "points": 23,
    "submitter": "itayd",
    "submit_time": "2025-04-22T22:41:08 1745361668",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43766979",
    "comments": [
      "> In addition, it is a durable execution platform for long-running and reliable workflows. It is based on Temporal, hiding many of its infrastructure and coding complexities.You only hide it for as long as things go smoothly. When a thing breaks then ur hosed. Having seen people try to self host cadence/temporal (not super easy) this makes me skeptical.\n \nreply",
      "In previous life I've been self hosting Cadence and it worked quite well. We then moved to Temporal cloud which was even easier.That said, if you use autokitteh.cloud, you don't need to worry about Temporal as we're doing all that management behind the scenes.\n \nreply",
      "My thoughts as well.. Libraries where the only reason to exist is 'hide this other complex library in the cupboard so you don't have to learn it' almost never achieve that goal, and are generally harmful to net complexity imho.\n \nreply",
      "Hiding is actually not the main thing here. AK allows you to \"deploy in a click\" instead of deploying the actual workers. We also provide integrations with built in authentication for external services such as Slack, JIRA, etc.\n \nreply"
    ],
    "link": "https://github.com/autokitteh/autokitteh",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Durable workflow automation in just a few lines of code\n      AutoKitteh is a developer platform for workflow automation and\norchestration. It is an easy-to-use, code-based alternative to no/low-code\nplatforms (such as Zapier, Workato, Make.com, n8n) with unlimited flexibility.You write in vanilla Python, we make it durable \ud83e\ude84In addition, it is a durable execution platform for long-running and\nreliable workflows. It is based on Temporal, hiding\nmany of its infrastructure and coding complexities.AutoKitteh can be self-hosted, and has a cloud offering as well.Once installed, AutoKitteh is a scalable \"serverless\" platform (with batteries\nincluded) for DevOps, FinOps, MLOps, SOAR, productivity tasks, critical\nbackend business processes, and more.\n\n\n\nPlatform: A scalable server that provides interfaces for building projects\n(workflows), d",
    "summary": "Title: Hacker News Discovers Magic Python Beans \ud83e\ude84\n\nAt last, the world is graced with AutoKitteh, <em>yet another</em> tool that promises to make coding as obsolete as your grandpa's Nokia. In a showcase of ultimate innovation, this platform intends to hide the horrors of Temporal behind the soothing simplicity of Python. Because <strong>nothing</strong> could ever go wrong by just wrapping complexity in a fluffy cloud and calling it serverless, right? Meanwhile, the comments section transforms into a compelling drama of skepticism and software nostalgia, offering tech folks a chance to humbly brag about their hosting conquests or simply mourn over the complexities 'hidden in the cupboard.' \ud83c\udfad"
  },
  {
    "title": "The complex origin story of domestic cats (phys.org)",
    "points": 83,
    "submitter": "gmays",
    "submit_time": "2025-04-22T18:07:32 1745345252",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=43764771",
    "comments": [
      "\"Taken together, these studies significantly alter our understanding of one of humanity's most familiar companions. Rather than silently trailing behind early farmers, slinking ever closer to human activity and community, cats likely moved into Europe in multiple waves post-domestication from North Africa, propelled by human cultural practices, trade networks, and religious reverence.\"Being treated like a god will get you everywhere.\n \nreply",
      "And of course being able to eliminate pest populations responsible for disease transmission, food spoilage, equipment/infrastructure damage, and other various harms has earned cats that seat in the pantheons of cultures around the globe.\n \nreply",
      "> equipment/infrastructure damageThe Cat has caused extensive damage in my house. I'm still working on repairing it.\n \nreply",
      "Unfortunately evolution does evolution:https://www.nhs.uk/conditions/toxoplasmosis/\n \nreply",
      "> Toxoplasmosis is a common infection that you can catch from the poo of infected cats, or infected meat. It's usually harmless but can cause serious problems in some peopleThe UK governments approach to using normal, simple language across all its web assets is fantastic.\n \nreply",
      "And if we're being honest, the whole soft tummies thing and purring probably helped too.... this thread needs pictures.\n \nreply",
      "Wolverines have soft tummies too.  That we see cats as cute is not so much that they have evolved to be cute. They look little different than wild cats.  We see the palus cat as cute but pet its tummy and you will lose some organs. Those humans who protected and nutured cats were better survivors.  Having cats around gave them an advantage over people who were indiferent to cats.  We finding them cute is a trait that has evolved in us.https://en.wikipedia.org/wiki/Pallas%27s_cat\n \nreply",
      "> eliminate ... equipment/infrastructure damageAnd cause it.\n \nreply",
      "Domestic cats are a contradiction in terms. They are small wild cats who have partially domesticated hairless apes, and still have a lot of work to do.\n \nreply",
      "Domestic cats are arguably the most successful mammalian carnivores anywhere.\n \nreply"
    ],
    "link": "https://phys.org/news/2025-04-complex-story-domestic-cats-tunisia.html",
    "first_paragraph": "",
    "summary": "**When Cats Conquered Humans: A Fur-tastic Journey**\n\nIn an article that should be titled \"Why Your House is a Mess: A Cat\u2019s Tale,\" phys.org attempts to unravel the complex origin story of the domestic cat, showing that humanity's furry overlords didn't just stalk humans for leftovers but were actually worshipped into domesticity across multiple European invasions. \ud83d\udc3e The comment section, a delightful litter box of insight and hysteria, dives deep into the pivotal role of cats in pest control and structural damage, with a touch of toxoplasmosis thrown in for good peril. One keen observer notes the ultimate truth: \"Domestic cats are small wild cats who have partially domesticated hairless apes,\" confirming our long-held suspicion that we\u2019re still very much in the middle of our performance review. Meanwhile, another comment insists on the necessity of soft tummy pictures, because if there's anything more crucial than scientific accuracy, it's fluffy bellies."
  },
  {
    "title": "Can a single AI model advance any field of science? (lanl.gov)",
    "points": 43,
    "submitter": "LAsteNERD",
    "submit_time": "2025-04-22T19:02:04 1745348524",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=43765207",
    "comments": [
      "Is this a recruiting attempt by Los Alamos? AI/ML for science as this broad field used to be known is interesting. Some five years ago there as a real craze where every STEM lab at my university was doing some form of ML project. I think by now people have learned what works and what doesn't. Climate models for example have been quite successful. Possibly the reason is that they learn directly from collected data, rather than trying to emulate the output of simulations. Attempts to build similiar models for fluid dynamics have been rather dismal. In general, big models and big data result in useful models, even if only because these models seem to be somehow interpolating based on similiar training data points. Trying to replace classical physics based models with ML models trained on simulation data does not seem to work. The model is only ever capable of emulating a physically plausible output when the input is close enough to the training data, and that too, only when the system isn't chaotic. For applications where you are generating a sample to be used in a downstream task, ML models trained on lots of data can be very useful. You only need a few lucky guesses, that you can verify downstream, to end up with some useful result. \nIn short, there is no magic to it. It's a useful tool that can be regarded as both a search algorithm and an optimization algorithm.\n \nreply",
      "I think an important question to ask is whether your scientific task is primarily one of interpolation, or one of extrapolation.  LLMs appear to be excellent interpolators.  They are bad at extrapolation.\n \nreply",
      "Climate models aren't LLMs.\n \nreply",
      "They're also not AI.It remains to be seen exactly how much a climate model can be improved by AI. They're already based on woefully sparse data points.\n \nreply",
      "Why are they not AI?\n \nreply",
      "Accessible, commercial grade, physics driven extrapolation isn't there yet, look at the vid gen AIs, they don't understand the world. Fluid dynamics, thermodynamics, radiation transfer. We don't have good physics driven AI yet, people are working on hybrid models, so that will be good, although my understanding climatology is under resourced from a comp sci brains perspective, the tech and people are locked up on other stuff. And as others have mentioned, data is not good. Personally, it's something that makes me quite itchy, I hope OSS is able to help here sometime soon.\n \nreply",
      "Check out Fourier Neural Operators, they claim to have a pretty solid solver for fluid flow equations (Navier Stokes etc).\n \nreply",
      "I am already acquainted with them but to be honest, I am no longer in the field so I am not able to comment on latest developments. However, as of two years ago, the consistent result was that you could get models that reproduce really good physics for problems in the same physical regimes as the training data, but such models had poor generalizability, so depending on the use case, they weren't of much use. The only exception I know is FourCastNet, which is a weather model FNO from NVIDIA.\n \nreply",
      "Yes. ML has advanced many fields related to modelling - meteorology, climate, molecular. Classification models have done much for genomics, particle physics, and other fields where experiments produce inhumane amounts of data.DeepVariant, Enformer, ParticleNet, DeepTau, etc. are some well-known individual models that are advanced branches of science. And there are the very famous ones, like AlphaFold (Nobel in Chemistry 2024).We need to think of AI not as a product (chats, agents, etc.), but as neural nets (AlexNet). Unfortunately, large companies are \"chat-washing\" these tremendously useful technologies.\n \nreply",
      "ML was used to sharpen the recent image of a black hole: https://physics.aps.org/articles/v16/63ML is more of a bag of techniques that can be applied to many things than a pure domain. Of course you can study the properties of neural networks for their own sake but it\u2019s more common as a means to an end.\n \nreply"
    ],
    "link": "https://www.lanl.gov/media/publications/1663/1269-earl-lawrence-ai",
    "first_paragraph": "Earl LawrenceStatistical ScientistMarch 31, 2025Download a print-friendly version of this article.I got the email on the Sunday after last Thanksgiving, just after returning home to Santa Fe from a family trip to Michigan. Jason Pruet, the director of the Lab\u2019s National Security AI Office, had invited me to a meeting to discuss leadership for the Lab\u2019s upcoming artificial intelligence investment. He didn\u2019t directly state that he was going to ask me to do it, but I had a sense that my career was about to pivot.By the fall of 2023, AI\u2019s potential had become too great to ignore. The latest iterations of industry-produced AI were performing tasks that were unthinkable just five years earlier. It wasn\u2019t just that they were writing coherent text or generating images of people who have never existed; they were also doing things that we should be doing at the national labs. Microsoft had just released a new AI climate model that performed well in both weather and short-term climate prediction.",
    "summary": "Title: Can a single AI model advance any field of science? (lanl.gov) \n\nThe intrepid minds at Los Alamos National Lab send an earth-shattering email that could *pivot careers*, including Earl\u2019s, a statistical scientist who suddenly thinks he\u2019s the next protagonist in a Dan Brown novel. According to the seismic buzz, Microsoft\u2019s new AI can predict weather *and* generate nonexistent people, exploding the not so profound notion that AI might just be magic for science. Commenters engage in the ritual dance of out-technobabbling each other, debating whether climate models are sentient, while subtly mourning the \"good old days\" when ML was merely an exotic pet at university STEM labs. They conclude, as all internet comment threads do, without discovering the Holy Grail of AI, mildly shocked that big data might just interpolate instead of innovate. \ud83e\udd16\ud83d\udc94"
  }
]