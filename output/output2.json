[
  {
    "title": "A conversation about AI for science with Jason Pruet (lanl.gov)",
    "points": 119,
    "submitter": "LAsteNERD",
    "submit_time": "2025-05-12T19:52:40 1747079560",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=43966843",
    "comments": [
      "> We certainly need to partner with industry. Because they are so far ahead and are making such giant investments, that is the only possible path.And therein lies the risk: research labs may become wholly dependent on companies whose agendas are fundamentally commercial. In exchange for access to compute and frontier models, labs may cede control over data, methods, and IP\u2014letting private firms quietly extract value from publicly funded research. What begins as partnership can end in capture.\n \nreply",
      "> research labs may become wholly dependent on companiesThey already are. Who provides their computers and operating systems? Who provides their HR software? Who provides their expensive lab equipment?Companies are not in some separate realm. They are how our society produces goods and services, including the most essential ones.\n \nreply",
      "I fail to understand the sentiment here.This is the intention of tech transfer. To have private-sector entities commercialize the R&D.What is the alternative? National labs and universities can't commercialize in the same way, including due to legal restrictions at the state and sometimes federal level.As long as the process and tech transfer agreements are fair and transparent -- and not concentrated in say OpenAI or with underhanded kickbacks to government -- commercialization will benefit productive applications of AI. All the software we're using right now to communicate sits on top of previous, successful, federally-funded tech transfer efforts which were then commercialized. This is how the system works, how we got to this level.\n \nreply",
      "What do you mean universities can't commercialize in the same way (I may have misunderstood what you meant)?  Due to Bayh-Dole, Universities can patent and license the tech they develop under contract for the government- often helping professors start up companies with funding, while simultaneously charging those companies to license the tech.  This is also true for National labs run by universities (Berkeley and a few others).  the other labs run under contract by external for-profit companies.\n \nreply",
      "> What is the alternative?Reasonably there should be a two way exchange? It might be okay for companies to piggyback on research funds if that also means that more research insight enters public knowledge.\n \nreply",
      "I\u2019d be happy if they just paid their fair share of tax and stopped acting like they were self-made when they really just piggybacked on public funds and research.There\u2019s zero acknowledgment or appreciation of public infra and research.\n \nreply",
      "> As long as the process and tech transfer agreements are fair and transparentI think that's the crux of the guy you're responding to's point. He does not believe it will be done fairly and transparently, because these AI corporations will have broad control over the technology.\n \nreply",
      "If so, yes indeed, fair point by him/her. It's up to ordinary folks like us to push against unfair tech transfer because yes, federal labs and research institutions would otherwise provide the incumbents an extreme advantage.Having been in this world though, I didn't see a reluctance in federal labs to work with capable entrepreneurs with companies at any level of scale. From startup to OpenAI to defense primes, they're open to all. So part of the challenge here is simply engaging capable entrepreneurs to go license tech from federal labs, and go create competitors for the greedy VC-funded or defense prime incumbents.\n \nreply",
      ">  I didn't see a reluctance in federal labs to work with capable entrepreneursMy reluctance is when we talk about fraud, waste, and corruptions in government, this is where it happens.The DoD's budget isn't $1T because they are spending $900B on the troops.  It's $1T because $900B of that ends up in the hands of the likes of Lockhead martin and Raytheon to build equipment we don't need.I frankly do not trust \"entrepreneurs\" to not be greedy pigs willing to 100x the cost of anything and everything.  There are nearly no checks in place to stop that from happening.\n \nreply",
      "Not that it fully takes away from your argument but a lot of that high price tag is also due to requiring much better controls on material to prevent supply chain attacks ala getting beepers with explosives in the hands of all your leadership\n \nreply"
    ],
    "link": "https://www.lanl.gov/media/publications/1663/0125-qa-jason-pruet",
    "first_paragraph": "",
    "summary": "Welcome to the AI-overlord-sponsored panel discussion at the Locked-in National Labs, where the brilliant minds of our era feel compelled to recite love poems to Big Tech. Here, Jason Pruet and a choir of apologists wax poetic about the downright *inspirational* symbiosis between public research and the industry titans\u2014because nothing screams \u201cpublic good\u201d quite like surrendering your intellectual property to the soothing embrace of commercial conglomerates. \u2697\ufe0f\ud83d\udc94 Commenters swoop in, some clad in capes made of dollar bills, claiming our only salvation lies in these benevolent corporations who just **might** exploit research meant for humanity's benefit. But hey, if some taxes get dodged and data gets monopolized along the way, it's all part of this magical journey called tech transfer\u2014sponsored by your friendly neighborhood megacorporation. \ud83c\udfa9\ud83d\udcb8"
  },
  {
    "title": "Understanding LucasArts' iMUSE System (github.com/meshula)",
    "points": 52,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-12T22:41:30 1747089690",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43968141",
    "comments": [
      "The iMuse system really is remarkable. Games like X-Wing took great advantage of the features, when a Star Destroyer jumps into the game the music would seamlessly transition to the imperial March and it felt just like being in the movies. I don't think any modern system even tries to do those seamless transitions from one music piece to another.One thing I wonder about .. he mentions CD-audio (Redbook?) as being one capability of the system. But the CD-Audio games like X-Wing vs Tie Fighter were much more limited in that sense. You'd literally just hear the music switch to the new track. And the Force Unleashed, the last game that used iMuse, wasn't particularly remarkable if memory serves. I wonder if that was a limitation they just couldn't quite make as seamless?I figure today you could do it, with a \"virtual MIDI\" system using MP3 audio of individual instrument sounds ..Edited to add: that last sentence is essential what a DAW provides.\n \nreply",
      "X-Wing just had great music. Even the original stuff was great. The music for the training run was perfect.Modern games have similar reactive music systems but I've never heard one I felt was better than X-Wing's. They got it right on the first try.\n \nreply",
      "Games today feature dynamic music with loops and transitions and individual stems that can be remixed at runtime. One prominent example (to me, at least) is \"Take Control\" playing over the Ashtray Maze in Control. This sounds like an absolutely seamless prog metal song while playing, but it is actually highly reactive to the gameplay - the rapid-fire sequence of battle arenas and fast paced corridors. The player stays in absolute control of the pacing the whole time.\n \nreply",
      "I was obsessed with the idea of music production as an engine within a game a long time ago. It was just something I came across in passing when I read about how Elder Scrolls Online created a soundtrack in a similar manner. This resurfaced in my mind again when I started digging into Suno and other AI-generated music recently and it's kind of fun to wonder what'll be possible with storytelling in games and visual novels with the ability to limitlessly adapt and change based on player interactions.\n \nreply",
      "If I remember correctly, another game with a similar music system is Deus Ex from 2000. It is pretty approachable. If you own a copy, open any of the s3m music files in your favorite mod tracker editor. Each song file contains  multiple versions of song sequences, depending on the mood (idle, battle, ...).\n \nreply",
      "I remember reading a PC Magazine article about Rogue Squadron for the N64. Apparently it was one of the first games to feature a context specific soundtrack.\n \nreply",
      "First one I remember it in was X-Wing (1993), five years before Rogue Squadron. Looks like Monkey Island 2 (1991) was the first to use the system. Dark Forces used it, too.\n \nreply",
      "I worked with Nick back in the ILM R&D group. He's an incredibly kind man and one of the best developers I've ever met; truly a genius.\n \nreply",
      "A music player that is able to change the music dynamically is neat in itself, but to me the true story behind systems like these is the tools and processes used to create the content for them. Making a technical system approachable to a creative mindset is at least as much of a challenge as the system itself.iMUSE was used for some really beautiful music in its time, so LucasArts had this figured out. But I'd be curious to learn how they did it.\n \nreply"
    ],
    "link": "https://github.com/meshula/LabMidi/blob/main/LabMuse/imuse-technical.md",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          ",
    "summary": "<h1>Amateurs Rediscover Soundtracks, Pretend It\u2019s Science</h1>\n<p>Today in \"groundbreaking\" discoveries, a merry band of GitHub wanderers have excavated the ancient LucasArts' iMUSE system, triggering a flood of <em>nostalgic epiphanies</em> about how MIDI might just be cooler than MP3. Commenters \u2013 fortified by years of fermenting in the musty basements of X-Wing and Monkey Island 2 nostalgia \u2013 engage in a circle-jerk over who can remember the oldest game featuring dynamic music transitions, all while mistaking personal anecdotes for <i>insightful contributions</i> to computer science. In a desperate reach for relevance, they equate fiddling with old game files to breaking codes at Bletchley Park, ignoring the fact that modern game designers probably just use Spotify playlists. Oh, and apparently, someone once met a developer and deemed him \u2018a genius\u2019, because installing SoundBlaster drivers without causing a system crash in 1995 was basically the Apollo Moon landing.</p>"
  },
  {
    "title": "The Barbican (arslan.io)",
    "points": 397,
    "submitter": "farslan",
    "submit_time": "2025-05-12T15:28:38 1747063718",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=43964136",
    "comments": [
      "It's one of my favorite places to spend time when in London. It's comfortable, clean, quiet, aesthetically striking, easy to loaf around at, and there's high brow art in numerous forms to enjoy \u2013 it's kinda like BBC Radio 3 if it were a neighborhood. It's also five minutes from the Elizabeth Line and the parking is good which is unusual for the City. It's strikingly non-commercial - there are no chains or even convenience stores there, though there is a fantastic music shop. It's one of those rare places you can feel more intelligent and cultured by merely being there.I'd love to retire there when the kids are gone, although there are a lot of oddities about Barbican living to contend with that are probably more fun to read about than deal with for real.\n \nreply",
      "I lived there for three years, rented a flat. Living in the Barbican was fantastic, livign in my flat was not fantastic. I used to joke it was a time machine to 1965. There was not only no dishwasher, there was literally no space for a dishwasher. Day one that seems funny, a few days later less so. I was spending a fortune in rent to spend 30 minutes every day handwashing my dishes. I did know people who had bought and renovated, they had amazing places. Oddly on my hall of 10 there were 10 flats of which 4 were empty. I don't mean someone just came occasionally I mean 100% empty with no furniture, with rich people just using it as an investment. Overall though was a greart experience, it's a fantasic place.\n \nreply",
      "> I was spending a fortune in rent to spend 30 minutes every day handwashing my dishes.Did you used to cook for the seven dwarves and their extended families, every day?\n \nreply",
      "\"kinda like BBC Radio 3 if it were a neighborhood\"Thanks for that, put a smile on my face.\n \nreply",
      "> It's also five minutes from the Elizabeth LineAnd about 200ft. Such is the maze-like nature of the Barbican.\n \nreply",
      "Actually the Barbican station has a lift that goes to the Elizabeth line at the far end.\n \nreply",
      "It also depends where you are in the Barbican. It spans between the Barbican station and Moorgate. If you are at the latter you can enter Elizabeth line from there too.\n \nreply",
      "\"There\u2019s an underground parking garage for the residents, but half of it is empty and filled with 20-30-year-old cars whose owners are no longer known.\"Years ago I bought a flat and it came with an underground parking garage. Once we were settled in I break the garage lock and inside was an old Peugot, cans of old motor oil, and all sorts of junk shoved in between the garage door cracks. It was hell to get rid of the thing. The tires were flat. No title meant no tow trucks wanted to touch it and no scrap yard was willing to accept it. After too many months I was able to get the city to declare the car derelict. And then I had to pay a scrap yard to accept it.\n \nreply",
      "I _really_ appreciate Tokyo's system of basically forcing you to affirmatively declare your parking spot when you buy a car, on top of the usual \"pay extra rent to have a parking space\" thing that happens in many spots.While it doesn't stop cars from being abandoned \"randomly\", just the entire principle of having a paper trail for these things and creating a bunch of incentives to make sure that parking spots don't turn into trash heaps[0].Especially now that I live in a place where street parking is a prime resource and yet people _who have garages_ still choose to street park out of convenience...[0]: not always of course, I know about the trash houses\n \nreply",
      "As a total noob about the cars or buying flats or the location you are from, my first though was that why didn't you get it fixed and drive it away. But you won't have the papers then. Do the scrap yards not accept it for the same reason?\n \nreply"
    ],
    "link": "https://arslan.io/2025/05/12/barbican-estate/",
    "first_paragraph": "Three years ago, while searching for Vitsoe setups, I stumbled upon Barbican. I delved deeper and discovered a building complex that was beyond my imaginations.Young Fatih would definitely find it ugly; current me finds it beautiful. The estate was built between 1965 and 1976. And, since I first saw it, I started watching and reading everything I could find. There are a few YouTube videos where you can peek into the lives of the residents. And there are a few books as well.Of course, my dream was to visit the actual place. Stay there for a few hours, and discover more about this grandiose place. When I had a chance to visit London a few weeks ago, I planned to visit the estate. I had to.While researching, I discovered that Barbican was far more than I had imagined. The residents offered architecture tours, and I couldn\u2019t resist the opportunity to attend one. I invited two friends along, and we embarked on a 2-hour-long tour.The tour, which took almost two hours, felt like it was just 1",
    "summary": "**The Glorious Wonders of Living in a Concrete Maze**: In a stunning discovery that'll shock absolutely nobody, a blogger falls head over heels for the Barbican, a brutalist dream that even its residents can't decide if they love or hate. Our fearless explorer, after years of feverishly consuming YouTube and books about a cluster of concrete, finally lumbers through its labyrinthine expanse, enchanted by how *fantastically non-commercial* and *strikingly chain-free* it is. Meanwhile, in the comments, armchair architects and wistful would-be residents engage in a humblebrag duel about subterranean garages filled with ghost cars and residences so authentic you wash dishes by hand as if teleported back to 1965. Every commenter agrees \u2013 living at Barbican is half prestige, half punishment, and a full-time job deciphering the maze-like corridors. But hey, it's *like BBC Radio 3*, if only you could live in a radio station. \ud83c\udfbb\ud83c\udfd9\ufe0f"
  },
  {
    "title": "FedRAMP 20x \u2013 One Month in and Moving Fast (fedramp.gov)",
    "points": 31,
    "submitter": "transpute",
    "submit_time": "2025-05-13T00:30:35 1747096235",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=43968713",
    "comments": [
      "You need to know what FedRAMP is. Don't even bother clicking on the link until you do.Founded in 2011.> The FedRAMP PMO mission is to promote the adoption of secure cloud services across the federal government by providing a standardized approach to security and risk assessment.Seems pretty bland to me. I'm not worried about this one.[1]: https://en.wikipedia.org/wiki/FedRAMP\n \nreply",
      "It's a bland title for a key thing: It sets compliance standards for government use of cloud computing. Even companies like Google have had massive projects to get FedRAMP compliant so that the federal government can use their services.See for example: https://fedscoop.com/google-earns-fedramp-high-authorization...And this announcement is basically just that they're going to massively lower the bar.\n \nreply",
      "I used to work on FedRamp and I\u2019m fine with it. It\u2019s just like SOC2 compliance, mostly dog and pony show for auditors who have conflict of interest and no clue what they are auditing.\n \nreply",
      "Sounds like a good time to invest in Russian VPS hosts.\n \nreply",
      "> And this announcement is basically just that they're going to massively lower the barThe FedRAMP bar was always dumb.I've been in the cybersecurity industry for more than a decade now, and while FedRAMP was envisioned as a way to streamline Fed cloud and security procurement, it ossified extremely quickly.To get FedRAMP you ended up having to work with a handful of dedicated FedRAMP partners, and your development velocity would dramatically decrease as you spent most of your time dealing with compliance BS that didn't actually affect your security posture.A lot of the innovation on the security vendor side is happening at early-mid stage startups, but sinking $15-20M and 1.5- \n2 years just to get FedRAMP compliance became too much of a lift, hence incentivizing consolidation amongst larger vendors.\n \nreply",
      "So, is there any evidence of actual improvement in anything that taxpayers care about?\n \nreply",
      "Better procurement rates.I've been a Engineer, PM, and VC in the cybersecurity for more than a decade now, and most of us would need to spend $15-20M and 1.5-2 years just to get FedRAMP compliance.In return, most vendors charge significantly higher than private sector rates despite selling the exact same product. And usually, an oligopoly forms per security feature.Making it easier to have multiple vendors makes it easier for federal agencies to negotiate a competitive price.\n \nreply",
      "The foxes are already in the hen house at the source of all USA gov data. This seems moot. Why do fedramp when you can just pay off the admin and not do it?\n \nreply",
      "If there\u2019s one thing I don\u2019t really want to see \u201caccelerated\u201d it\u2019s the pace at which our state secrets add new, unvetted technology.\u201cMove fast and break things\u201d is absolutely insane for the things that let us all sleep soundly at night\u2026\n \nreply",
      "FedRAMP services, even FedRAMP High services, aren't authorized to host classified material. My understanding is that those agreements are still largely negotiated directly between the intelligence / defense services and the big providers, such as the Joint Warfighting Cloud Capability contract. FedRAMP is a program for vetting SaaS services for use by government agencies broadly. FedRAMP Moderate and High certified services are qualified to host Controlled Unclassified Information, which might be sensitive and held by either the government or private companies like defense contractors, but I wouldn't call them state secrets per se\n \nreply"
    ],
    "link": "https://www.fedramp.gov/2025-04-24-fedramp-20x-one-month-in-and-moving-fast/",
    "first_paragraph": "An official website of the United States governmentHere\u2019s how you know Official websites use .gov  A .gov website belongs to an official government organization in the United States.  Secure .gov websites use HTTPS  A lock (  Lock A locked padlock    ) or https:// means you\u2019ve safely connected to the .gov website. Share sensitive information only on official, secure websites. 174545280017427744001737072000173707200017346528001734566400173344320017334432001733443200173344320017332704001731974400173102400017310240001731024000172912320017289504001727654400172765440017273952001725321600172480320017247168001723420800172316160017230752001723075200172229760017219520001721260800172065600017205696001719446400171745920017171136001717113600171443520017116704001711670400171158400017104608001709510400170951040017080416001707955200170795520017079552001707955200170769600017067456001706227200170562240017054496001702512000170130240016998336001698969600169871040016983648001697155200169525440016950816001",
    "summary": "The United States government, in its infinite capacity for bureaucratic acrobatics, proudly presents \"FedRAMP 20x \u2013 One Month in and Moving Fast,\" an exciting adventure into the world of cyber compliance that reads like the side effects section of a sleep aid manual. True to government form, the launch narrative is as thrilling as watching paint dry, but ensures every cyber snooper can sleep well knowing sensitive info is locked down tighter than Area 51. Commenters, in a dazzling display of missing the point, swiftly turn a mundane security update into a soapbox for conspiracy theories, tech industry grudges, and mournful reminiscences of simpler compliance times. It's clear from the discourse that everyone values speed over security until it's their data on the line. \ud83d\ude44"
  },
  {
    "title": "Air Traffic Control (computer.rip)",
    "points": 42,
    "submitter": "1317",
    "submit_time": "2025-05-12T00:39:22 1747010362",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43958562",
    "comments": [
      "Re WWII use of radio:My grandfather flew Typhoons, and they operated 'cab rank' as part of the 2nd Tactical Air Force. The Army would radio in coordinates of German tanks or fortified positions, and the Typhoons would come in with their rockets / bombs / cannons. I wish he was still around so I could ask him how that was done. A central dispatcher? Or did they talk to the Army directly? Not sure.\n \nreply",
      "Forward air control would get a map grid or other description of the target from Infantry and Aircraft would be dispatched to the target .   https://en.wikipedia.org/wiki/Forward_air_control_operations...\n \nreply"
    ],
    "link": "https://computer.rip/2025-05-11-air-traffic-control.html",
    "first_paragraph": "Air traffic control has been in the news lately, on account of my country's\ndeclining ability to do it. Well, that's a long-term trend, resulting from\ndecades of under-investment, severe capture by our increasingly incompetent\ndefense-industrial complex, no small degree of management incompetence in the\nFAA, and long-lasting effects of Reagan crushing the PATCO strike. But that's\njust my opinion, you know, maybe airplanes got too woke. In any case, it's an\ninteresting time to consider how weird parts of air traffic control are. The\ntechnical, administrative, and social aspects of ATC all seem two notches more\ncomplicated than you would expect. ATC is heavily influenced by its peculiar\nand often accidental development, a product of necessity that perpetually\ntrails behind the need, and a beneficiary of hand-me-down military practices\nand technology.In the early days of aviation, there was little need for ATC---there just\nweren't many planes, and technology didn't allow ground-based cont",
    "summary": "**Air Traffic Control: The Comedy of Errors**\n\nIn a world where no amount of radar can locate common sense, we get a delightful dissertation on how modern air traffic control is less about fancy blinking lights and more about bureaucratic bungles and military hand-me-downs. Isn't it hilarious that the system guiding thousands of metal birds across our skies is basically a vintage tech hoarder with a complex oligarchy at the helm? Commenters wax nostalgic about grandpa's heroic WWII radio antics, seemingly oblivious to the irony of their own idealization making zero impact on today's aerial spaghetti. Yet, these armchair experts, equipped with just enough second-hand stories to be dangerous, are convinced they can solve nationwide system failures between sips of coffee. \ud83d\udeeb\ud83d\udce1\ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "How to avoid P hacking (nature.com)",
    "points": 10,
    "submitter": "benocodes",
    "submit_time": "2025-05-09T07:53:54 1746777234",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43934682",
    "comments": [
      "> As any gambler knows, if you roll the dice often enough, eventually you\u2019ll get the result you want by chance aloneYou never count your results, when you're sitting at the lab bench, there will be time enough for counting, when the experiments are done.\n \nreply",
      "If the conclusion is \"be transparent\", I'm strongly supportive.And moreover, I would be even more supportive if we found a way to change the incentives for tenure and promotion such that reproducibility was an important factor in how we make decisions about grants, tenure, and promotion.\n \nreply"
    ],
    "link": "https://www.nature.com/articles/d41586-025-01246-1",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.AdvertisementBenjamin Tsang is a PhD candidate in the Department of Cell and Systems Biology at the University of Toronto, Canada.You can also search for this author in PubMed\n\u00a0Google Scholar\n\n                You have full access to this article via your institution.Credit: MirageC / GettyIt can happen so easily. You\u2019re excited about an experiment, so you sneak an early peek at the data to see if the P value \u2014 a measure of statistical significance \u2014 has dipped below the threshold of 0.05. Or maybe you\u2019ve tried analysing your results in several different ways, hoping one will give you that significant finding. These temptati",
    "summary": "**How to Pretend You Understand Statistics**\n\nToday on nature.com, Benjamin Tsang graciously interrupts your struggle with outdated Internet Explorer to dish out a groundbreaking revelation: peeking early at P values in your experiments might skew results! Shocker \ud83d\ude44! This pioneering insight, sprinkled with some juicy \"you can also google me\" vibes, dives deep into the addictive casino of manipulating data until significance is magically achieved. Meanwhile, the comment zone transforms into a beacon of accidental wisdom as veteran lab-rats confess how gaming the system is just part of the day\u2019s work. One enlightened soul even dreams of a world where academic metrics favor reproducibility over flashy, non-reproducible flukes. Oh, the Utopia of accountability!"
  },
  {
    "title": "HealthBench \u2013 An evaluation for AI systems and human health (openai.com)",
    "points": 120,
    "submitter": "mfiguiere",
    "submit_time": "2025-05-12T17:42:37 1747071757",
    "num_comments": 109,
    "comments_url": "https://news.ycombinator.com/item?id=43965608",
    "comments": [
      "I have no doubt that a lot of garden-variety diagnoses and treatments can be done by an AI system that is fine-tuned and vetted to accomplish the task. I recently had to pay $93 to have a virtual session with a physician to get prescription for a cough syrup, which I already knew what to take before talking to her because I did some research/reading. Some may argue, \"Doctors studied years in med school and you shouldn't trust Google more than them\", but knowing human's fallibility and knowing that a lot of doctors do look things up on places like https://www.wolterskluwer.com/en/solutions/uptodate to refresh/reaffirm their knowledge, I'd argue that if we are willing to take the risk, why shouldn't we be allowed to take that risk on our own? Why do I have to pay $93 (on top of the cough syrup that costed ~$44) just so that the doctor can see me on Zoom for less than 5 mins and submit an order for the med?With the healthcare prices increasing at the breakneck speed, I am sure AI will take more and more role in diagnosing and treating people's common illnesses, and hopefully (doubt it), the some of that savings will be transferred to the patients.P.S. In contrast to the US system, in my home city (Rangoon, Burma/Myanmar), I have multiple clinics near my home and a couple of pharmacy within two bus stops distance. I can either go buy most of the medications I need from the pharmacy (without prescription) and take them on my own (why am I not allowed to take that risk?) OR I can go see a doctor at one of these clinics to confirm my diagnosis, pay him/her $10-$20 for the visit, and then head down to the pharmacy to buy the medication. Of course, some of the medications that include opioids will only be sold to me with the doctor's prescription, but a good number of other meds are available as long as I can afford them.\n \nreply",
      "The only reason this worked is because your situation was exceedingly simple.The trouble is you are not educated enough to tell what is simple and what isn't. A cough could be a cough or it could be something more serious, only a \"real\" examination will reveal that. And sometimes even that's not enough, you need an examination by a specialists.I'll tell you a story. Once upon a time I got pain in my balls. I went to a doctor and he felt around and he said he didn't feel anything. I went to another doctor and he felt something, but he had no idea what it was. He said could be a cyst, could be a swollen vein, could be an infection - he didn't even know if it was on the testicle or on the tube thingy.Then I went to a Urologist. You can tell this man has felt up a lot of balls. He felt me up and said, \"yup, that's a tumor\" almost immediately. He was right, of course, and he ended up being the one to remove it too. Since I caught the cancer pretty early the chemotherapy wasn't too intense.Point is, expertise matters when things aren't straight forward. Then, experience and perspective gets to shine.\n \nreply",
      "I don't think this is the slam-dunk you think it is. You had to go to three doctors before someone recognized something was wrong. ChatGPT is the substitute for the first two doctors, not the third.And even there, I bet ChatGPT would have told you to go see a doctor, since it can't feel your balls. And after your first appointment, if you had told it that you still thought something was wrong, it would probably have told you to go see a urologist.\n \nreply",
      "I fall slightly on the \u201cdoomer\u201d side of the safety spectrum, but if I\u2019m being honest I\u2019ll be first in line - rain, sleet, or snow - to buy the chatgpt that can!\n \nreply",
      "> Why do I have to pay $93 (on top of the cough syrup that costed ~$44) just so that the doctor can see me on Zoom for less than 5 mins and submit an order for the med?Because you're paying for the expertise of someone who studied for more than a decade which you won't get from a random web search.An AI system with today's technology should be less trustworthy for medical diagnosis than a web search. At least with a web search you might stumble upon a site with content from experts, assuming you trust yourself to be able to discern expert advice from bot-generated and spam content. Even if a doctor is doing the searching instead of me, I would pay them only for their knowledge to make that discernment for me. Why you think an AI could do better than a human at that is beyond me.Your question reminds me of that famous Henry Ford GE invoice story:> Making chalk mark on generator: $1.> Knowing where to make mark: $9,999.\n \nreply",
      "You answered why someone would want to pay $93, but not why they have to pay $93.\n \nreply",
      "There's a limit though right? How about, if you can accurately predict the doctor's diagnosis it's free? If not you pay. This person needed a doctor's approval presumably for a prescription they couldn't get without authorization which leads to this gatekeeping. Not to mention also contributes to the insane medical costs in the US. $93 for 5 minutes is over 1k _an hour_ for what amounts to a rubber stamp (because how much can you really dig into a person's medical history and condition in 5 mins).\n \nreply",
      "AI systems have been improving. O3 now has the capability to decide to search multiple times as part of its response.\n \nreply",
      "Even o4-mini (free) uses web searches and runs Python scripts very eagerly. I'm not sure how long they'll be able to afford giving all of that away.\n \nreply",
      "An AI system with today's technology should be less trustworthy for medical diagnosis than a web search.This is the problem with reasoning from first principles. This statement is easily proven false by giving it a try, whether it \"should\" be true or not.\n \nreply"
    ],
    "link": "https://openai.com/index/healthbench/",
    "first_paragraph": "",
    "summary": "OpenAI, in an uncharacteristic burst of optimism, unveils HealthBench\u2014a potent tool if you believe replacing years of medical training with AI is like substituting a chef with a microwave. Comment warriors, quick to signal their cybernetic sympathies, spin tales of outrageously priced telemedicine sessions and prescriptions that could've been Googled, because apparently, medical degrees are now as downloadable as PDFs. While one commenter recounts a harrowing journey from two clueless docs to a life-saving urologist, underscoring that not all coughs are created equal, others are set to enshrine their Chrome histories in lieu of diplomas. Brace for a future where your health diagnostics are a mere alt-tab away from your last YouTube binge\u2014what could possibly go wrong? \ud83e\udd16\ud83d\udc68\u200d\u2695\ufe0f\ud83d\udcb8"
  },
  {
    "title": "RIP Usenix ATC (dtrace.org)",
    "points": 132,
    "submitter": "joecobb",
    "submit_time": "2025-05-12T16:29:05 1747067345",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=43964827",
    "comments": [
      "I think people are glossing over the fact that Usenix ATC wasn\u2019t even a good academic conference. If you\u2019re submitting high-quality academic work to Usenix, you\u2019re sending it to one of the higher prestige Usenix conferences in a specific subfield, such as Usenix Security or NSDI.\n \nreply",
      "Computers and the Internet started with academics. Distributed systems were theoretical before they were practical. The first sites of the Internet were universities and the first non-academic site was a milestone. Harvard and Dartmouth and MIT all contributed to computer design and programming advances. UNIX is a product of a quasi-academic research lab.The Center of Gravity shifted in the late 90s/early 2000s, I reckon. Real world needed distributed systems, and neural networks, and crypto math, and hardware design, and \u2026There\u2019s still a place for academic research but it\u2019s a much different place in the Gilded CPU Age. It\u2019s the god of the non-commercial gaps: for a while there if it seemed like it\u2019d make money, someone .com giant with gigabucks of free cash had someone on it. Why work for a university if gigabucks.com is hiring?Not saying USENIX didn\u2019t have factions. I just question the word \u201ccapture\u201d about academics, who invented all this stuff before there was money in it. It feels like retiring the Annual Technical Conference is the final step in Usenix\u2019s capture by industry.\n \nreply",
      "I think what they're saying is this:USENIX ATC at its best was always focused on operations and real use of Unix.  It was the Unix User's Group.But academic CS has a shortage of real publishing opportunities, causing less immediately practical things to flood USENIX ATC and smother it.\n \nreply",
      "Academic CS has no shortage of prestigious or even good publishing venues. Just within systems and networking, there is SOSP, OSDI, NSDI, EuroSys, PODC, DISC, FAST, SIGCOMM, CIDR. If you extend to databases there's also VLDB, SIGMOD, etc. These are all Tier-1/Tier-1.5 venues.Academic CS didn't take over ATC because of a shortage of venues.\n \nreply",
      "You made this comment twice, so I guess I'll reply to it twice, but the reality is that the acceptance rates of ATC was very, very low (again, under 13% in 2004). Low acceptance rates actually do indicate a shortage of good publishing venues (certainly relative to the number of submissions), but it would be interesting to look at the ATC acceptance rate over time; if it was much (much) higher in its final years, it would be easier to accept your assertion.\n \nreply",
      "IMO the implication that the \"in-person conference\" nature of the forum is what led to its demise rather than its capture by academics/abandonment by industry practitioners is incorrect. I think a better summary of the cause is that most academic research is not that relevant to industry but academics need the prestige of forums like Usenix ATC in a way that industry does not which naturally leads to squeezing industry (and thus much of the relevance) out of the forum over time. I think the same would have happened to a virtual conference/printed journal/etc.\n \nreply",
      "I definitely agree with you, and apologies if that didn't come through in the piece! (Perhaps the rare case where I undershot on the metaphors?)One thing I didn't mention but certainly believe: academics were attracted to USENIX ATC because of its attendance count (especially at the height of the Dot Com boom when it had nearly 2,000 attendees!) -- but no one really took apart who was attending or what they were looking for.  So the conference became more academic because of the attendee count -- but that it became more academic also drove the attendee count down. (I heard from a lot of practitioners who attended that they struggled to find sessions that were relevant to their work in even the broadest sense.)  I know I link to it in the piece, but I think Rik Farrow's piece[0] really got right to the heart of all of this.[0] https://www.usenix.org/system/files/login/articles/login_fal...\n \nreply",
      "I didn't mean to imply that I disagreed with anything other than that specific point (which perhaps is more nostalgia for the halcyon days of in-person conferences than anything else).I do think that you correctly identified a major source of the problem back in 2004 as economic factors. I was at Mozilla when Rust was built (though not directly involved) and the sum of Mozilla's investment into Rust over the years easily broke into the 8 digits. Google's investment in Go I'm sure is an order of magnitude or two more than that. This is simply beyond the capacity of any academic institution or grant process. The only academic efforts that get to this level require Acts of Congress (e.g. LIGO, the Human Genome Project, the James Webb Space Telescope, etc).And anyone who can afford to drop $10M+ into development can find channels for distributing and publicizing their work that don't go through program committees. Open source is definitely a big part of that but I don't think that's the whole story. I'd certainly count CUDA as a major advance in systems software since 2004, for instance.\n \nreply",
      "I don't feel like I have a good understanding of all the context but I don't think that's the implication at all. The post does offhandedly mention that doing online only conferences get \"more bang for the buck\" but the rest of the post and linked articles and talks discuss other aspects and, in my opinion, mirror your last sentiment about it happening to all conferences and journals.For example, one of the linked talks from 2016 \"A Wardrobe for the Emperor\" [0] talks about adding a more social website features (stars, likes, comments, feedback, etc.) to arXiv to improve it. He also talks about the \"Papers we love\" meetups, which are in person.I think the Usenix ATC is a highlighting a deeper problem and that it is happening across other conferences, journals and disciplines. I don't have a good theory of why. I think Cantrill's observations are part of the puzzle but I don't have a good sense for why things changed so drastically (and I think they have, compared to 20-40 years ago).[0] https://www.youtube.com/watch?v=gAEiXWO44bQ&t=3457s\n \nreply",
      "The big issue may be companies clamping down on divulging details regarding vulnerabilities and mitigations. Cybersecurity is significantly more weaponized now than it was in the 90s and companies might be facing immense pressure to limit the wrong kind of attention.\n \nreply"
    ],
    "link": "https://bcantrill.dtrace.org/2025/05/11/rip-usenix-atc/",
    "first_paragraph": "USENIX made the decision this week to\ndiscontinue its flagship\nAnnual Technical Conference.  When USENIX was started in 1975\u2009\u2014\u2009before the\nInternet, really\u2009\u2014\u2009conferences were the fastest vector for practitioners to\nformally share their ideas, and USENIX ATC flourished.  Speaking for myself, I\ncame up lionizing ATC:  I was an undergraduate in the early 1990s, and\nprograms like the\nUSENIX\nSummer 1994 conference felt like Renaissance-era Florence for systems\npractitioners.When we developed DTrace in the early 2000s, we knew that there was no better\nvenue to announce it to the world than USENIX ATC.  We put a lot of effort\ninto the resulting paper,\nDynamic\nInstrumentation of Production Systems\u2009\u2014\u2009and I was elated when it was\naccepted for USENIX ATC 2004.The conference itself was\u2026\u200b surprising.  This was not the USENIX of a decade\nprior; the conference was decidedly academic, and in the strictest sense: all\nof the presentations were from PhD students seeking out academic work.  I\nwrote about t",
    "summary": "**Title: The Grand Funeral of USENIX ATC: A Satirical Eulogy**\n\nIn a world bereft of floppy disks and dial-up tones, the USENIX Annual Technical Conference takes its last breath, opting out of the relentless march of technology. Mourners, a.k.a. tech aficionados from the '90s, trade teary-eyed eulogies over IRC, reminiscing about the golden age of conferences like it was Renaissance Florence\u2014but with more Unix and less actual painting. Commenters spar heroically beneath the blog post, wielding nostalgia and academic snobbery like antique swords, battling over whether USENIX ATC was a fallen titan of tech wisdom or just another casualty in the academia-industry cold war. Meanwhile, the real world scrolls past these digital Don Quixotes, double-tapping on TikTok and forgetting USENIX ever existed. \ud83c\udfbb\ud83d\udc53\ud83d\udcdc"
  },
  {
    "title": "Can you trust that permission pop-up on macOS? (wts.dev)",
    "points": 156,
    "submitter": "nmgycombinator",
    "submit_time": "2025-05-12T18:26:44 1747074404",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=43966089",
    "comments": [
      "On the off-chance someone at Apple reads this, I'll repeat my perennial beg that Apple stops popping up 'Give me your (local admin) password right now' dialogs randomly throughout the day because the computer has a hankering to install updates or something.Anyone with basic skills can whip up a convincing replica of that popup on the Web, and the \"bottom 80%\" (at least) of users in technical savvy would not think to try dragging it out of the browser viewport or switching tabs to see if it is fake or real.The only protection against this kind of stuff is to NOT teach users that legitimate software pops up random \"enter your password\" dialogs in front of your work without any prompting. That's what these dialogs are doing.Display a colorful flashing icon in the menu bar. Use an interstitial secure screen like Windows does. Whatever. But the modern macOS 'security' UI is wildly bad.\n \nreply",
      "The thing I hate about these things is I have no idea why they\u2019re asking this, and no idea what happens if I say No, even how to \u201cmanage\u201d these settings should I wish to change it.The UX is different from the apps saying \u201cHey, open the preferences panel and give us XXX\u201d and there you can see the app, the capability toggle, decide to turn it on, or even go back to turn it back off.These experiences have been why I\u2019ve not a big fan of \u201ccapabilities\u201d as a concept. The UX around them is awful, and almost has to be.\u201cEnable <root my computer> to enjoy your new app fully. \u201c This is what you get if the vendors have any say in what the messages should be.Not really helpful.\n \nreply",
      "> These experiences have been why I\u2019ve not a big fan of \u201ccapabilities\u201d as a concept. The UX around them is awful, and almost has to be.I don't think the UX has to be awful. The problem is just that they're kinda half baked on macos, and bolted on, and not really a first class citizen. There's no reason you couldn't have:- A preferences dialog showing which long-lived capabilities you've granted to which application. (Which is almost exactly what the accessibility preferences pane already is.) Ideally this UI could have a log of ways in which the application has used that capability recently and the ability to revoke it. Maybe even the ability to review the app's use of the capability. Show the review score to other users when the app asks for the permission.- A little blob of text saying why the application is asking for the specified permission. iOS requires this from all 3rd party apps. So its kinda weird that MacOS is missing explanatory text entirely on these popups.- Clear indication of what would happen if you said no.- Interdiction. In a good capability systems (eg SeL4), a capability object doesn't tell you what you can do with it. Eg, you can't ask a file handle which file its actually associated with. This means you can craft your own \"virtual\" capability which fakes the expected behaviour and pass that to an app instead. Any calls made using the capability come to you. Whenever phone apps ask for access to my contact address book, I'd love to be able to say yes, but give them access to like 100mb of fake entries instead.- And on top of interdiction: logging, call whitelisting, \"Little snitch\", etc.- More fine grained capabilities. I don't want to give any app a \"root my computer\" capability. I don't want that to be a thing applications ever need or get access to.I think macos's problem is that its trying to bolt on capabilities after the fact. POSIX isn't built around capabilities. As a result, app developers don't think in terms of capabilities, and they expect their apps (new and old) to work without them. In a real capability based OS, fopen() should probably take a capability as a parameter. But making that change would require changing just about every program ever written for the platform. And modifying the standard library of all programming languages.\n \nreply",
      "As someone who's looked into the internals of macOS for a bit now, this is all incredibly fascinating. However, I am curious: do you think capabilities could be implemented like this at a really low level? Part of me thinks we have the security models we do in POSIX is because they're simple enough to represent in C code.The capability systems you're mentioning sound cool, but they sound a lot more complex. And if that's true, and they aren't built with irreducible complexity, then it would be possible to work around it by just pulling out bits and pieces from the system and abusing them.\n \nreply",
      "Capabilities themselves can certainly be implemented at a very low level; you might implement them as an array of capabilities associated with each process: https://en.wikipedia.org/wiki/C-list_%28computer_security%29As that page points out, POSIX file descriptors are effectively c-lists. A capability operating system would use similar mechanisms to control access to resources other than just open files.The other things GP mentioned (logging, interdiction, UIs for visibility/control, etc) are layers that you would implement on top of the lowest-level capability system.",
      "Capabilities are what lets you open a file picker from an app that cannot read your files, giving you a seamless and secure interaction with no extra UI.They are definitely not always awful.\n \nreply",
      "I really appreciate the integrated fingerprint reader in these cases. I usually run with my laptop screen closed (with external monitor) but open it specifically to authenticate in system dialogs.\n \nreply",
      "What is the threat model of clicking on a fake popup? Isn't it a no-op because it isn't actually coming from the system?\n \nreply",
      "Just realized that it asked for your system password if you don't have Touch ID.\n \nreply",
      "When logging into iCloud, they show a pop-up asking for the local password to the computer. And then they upload that password to the iCloud servers.\n \nreply"
    ],
    "link": "https://wts.dev/posts/tcc-who/",
    "first_paragraph": "It's time to update your Macs again! This time, I'm not burying the lede. CVE-2025-31250, which was patched in today's releases of macOS Sequoia 15.5 et al., allowed for\u2026These did not have to be different applications. In fact, in most normal uses, they would all likely be the same application. Even a case where Applications B and C were the same but different than Application A would be relatively safe (if somewhat useless from Application A's perspective). However, prior to this vulnerability being patched, a lack of validation allowed for Application B (the app the prompt appears to be from) to be different than Application C (the actual application the user's consent response is applied to).Spoofing these kinds of prompts is not exactly new. In fact, the HackTricks wiki has had a tutorial on how to perform a similar trick on their site for a while. However, their method requires:This vulnerability requires none of the above.As I explained in my first ever article on this site, TCC ",
    "summary": "Welcome to another \"groundbreaking\" expose on macOS annoyances, where we dive into the dark, twisted bowels of CVE-2025-31250 exploits and the yawn-inducing world of permission pop-ups. Spoiler alert: the pop-up might be a big fat phony \u2014 a concept so novel it surely warrants an entire article. In the comments, armchair experts lament the security theatre with shouts into Apple's void and dreams of UI that doesn\u2019t treat users like hyperactive, click-happy toddlers. Can you trust macOS permission pop-ups? Maybe stick to asking the Magic 8-Ball for a more reliable answer."
  },
  {
    "title": "Wtfis: Passive hostname, domain and IP lookup tool for non-robots (github.com/pirxthepilot)",
    "points": 26,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-12T22:15:12 1747088112",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=43967962",
    "comments": [
      "Nice i like it ,, stared so i can check it out later\n \nreply",
      "Pretty!\n \nreply"
    ],
    "link": "https://github.com/pirxthepilot/wtfis",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Passive hostname, domain and IP lookup tool for non-robots\n      \nPassive hostname, domain and IP lookup tool for non-robotswtfis is a commandline tool that gathers information about a domain, FQDN or IP address using various OSINT services. Unlike other tools of its kind, it's built specifically for human consumption, providing results that are pretty (YMMV) and easy to read and understand.This tool assumes that you are using free tier / community level accounts, and so makes as few API calls as possible to minimize hitting quotas and rate limits.The project name is a play on \"whois\".The primary source of information. Retrieves:Optionally used if creds are provided. Retrieves:IP2Whois is recommended over Virustotal for whois data for a couple of reasons:Default Geolocation and ASN lookup source for IP addresses. Retrieves:IPWhois s",
    "summary": "**WTFis: It's Not Just a Clever Name**\n\nThe Github universe witnesses yet another groundbreaking revolution: a *command-line tool* that deciphers IP addresses, _presumably_ because the typical UI-driven tools are just too mainstream for the modern-day hipster hacker. The brilliantly named _wtfis_ promises to save the free world by making fewer API calls - an absolute lifeline for anyone who\u2019s ever feared the tyranny of the API rate limit. Comment sections blossom with groundbreaking insights like \"Nice i like it\" and intellectual heavyweight contributions like \"Pretty!\", clearly indicating that the audience is equally enthused and focused on the grave cybersecurity implications at hand. Isn't it refreshing to see the internet community unite over such a life-changing tool?"
  },
  {
    "title": "Launch HN: ParaQuery (YC X25) \u2013 GPU Accelerated Spark/SQL",
    "points": 96,
    "submitter": "winwang",
    "submit_time": "2025-05-12T16:01:31 1747065691",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=43964505",
    "comments": [
      "Congrats on the launch.This reminds me of https://www.heavy.ai/ (previously MapD back in 2015/16?)\n \nreply",
      "Many of us have been using GPU accelerated Spark for years:https://developer.nvidia.com/rapids/https://github.com/NVIDIA/spark-rapidsAnd it's supported on AWS:\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spar...\n \nreply",
      "Indeed, Spark-RAPIDS has been around for a while! And it's quite simple to have a setup that works. Most of the issues come after the initial PoC, especially for teams not wanting to manage infra, not to mention GPU infra.\n \nreply",
      "This could be incredibly useful for me.  Currently struggling to complete jobs with massive amounts of shuffle with Spark on EMR (large joins yielding 150+ billion rows).  We use Glue currently, but it has become cost prohibitive.\n \nreply",
      "You should try using an S3 based shuffle plugin: https://github.com/IBM/spark-s3-shuffleThen mount FSX for Lustre on all of your EMR nodes and have it write shuffle data there. It will massively improve performance and shuffle issues will disappear.Is expensive though. But you can offset the cost now because you can run entirely Spot instances for your workers as if you lose a node there's no recomputation of the shuffle data.\n \nreply",
      "Is the shuffle the biggest issue? Not too sure about joins but one of the datasets we're currently dealing with has a couple trillion rows. Would love to chat about this!\n \nreply",
      "Congrats on the launch!\nInteresting application.\n \nreply",
      "Congrats on the launch!Im curious about what kinds of workloads you see GPU-accelerated compute have a significant impact, and what kinds still pose challenges. You mentioned that I/O is not the bottleneck, is that still true for queries that require large scale shuffles?\n \nreply",
      "Large scale shuffles: Absolutely. One of the larger queries we ran saw a 450TB shuffle -- this may require more than just deploying the spark-rapids plugin, however (depends on the query itself and specific VMs used). Shuffling was the majority of the time and saw 100% (...99%?) GPU utilization. I presume this is partially due to compressing shuffle partitions. Network/disk I/O is definitely not the bottleneck here.It's difficult to say what \"workloads\" are significant, and easier to talk about what doesn't really work AFAIK. Large-scale shuffles might see 4x efficiency, assuming you can somehow offload the hash shuffle memory, have scalable fast storage, etc... which we do. Note this is even on GCP, where there isn't any \"great\" networking infra available.Things that don't get accelerated include multi-column UDFs and some incompatible operations. These aren't physical/logical limitations, it's just where the software is right now: https://github.com/NVIDIA/spark-rapids/issuesMulti-column UDF support would likely require some compiler-esque work in Scala (which I happen to have experience in).A few things I expect to be \"very\" good: joins, string aggregations (empirically), sorting (clustering). Operations which stress memory bandwidth will likely be \"surprisingly\" good (surprising to most people).Otherwise, Nvidia has published a bunch of really-good-looking public data, along with some other public companies.Outside of Spark, I think many people underestimate how \"low-latency\" GPUs can be. 100 microseconds and above is highly likely to be a good fit for GPU acceleration in general, though that could be as low as 10 microseconds (today).\n \nreply",
      "8TB/s bandwidth on the B200 helps :-) [yes, yes, that is at the high end, but 4.8TB/s@H200, 4TB/s@H100, 2TB/s@A100 is nothing to sneeze at either).\n \nreply"
    ],
    "link": "item?id=43964505",
    "first_paragraph": "",
    "summary": "Title: Launch HN: ParaQuery (YC X25) \u2013 Another Day, Another GPU Acceleration\n\nWelcome to yet another <em>groundbreaking</em> startup, ParaQuery, boldly entering the arena <i>already</i> populated by titans of GPU-accelerated data shuffling. Their fresh take? Doing <em>exactly what several other services and major cloud providers have been doing</em> for years, but with the shiny veneer of being \"YC X25\" stamped on it. In the comments, tech enthusiasts engage in the Silicon Valley equivalent of comparing Pok\u00e9mon cards, sharing links and humblebragging about bandwidths like it\u2019s the key to unlocking the ultimate universe's secrets. Truly, a revolution. \ud83d\ude44"
  },
  {
    "title": "The Beam (erlang-solutions.com)",
    "points": 28,
    "submitter": "Alupis",
    "submit_time": "2025-05-09T18:48:03 1746816483",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.erlang-solutions.com/blog/the-beam-erlangs-virtual-machine/",
    "first_paragraph": "",
    "summary": "In an attempt to out-nerd the rest of the internet, a fresh blog post on erlang-solutions.com praises the almighty <i>Beam</i>\u2014the technology equivalent of using a unicycle to commute. Commenters, in a display of one-upmanship, engage in a pseudo-intellectual brawl to prove who can misinterpret the content most creatively while flexibly adjusting their online personas from \"humble coder\" to \"distributed systems philosopher-king.\" Meanwhile, real problems persist, unaddressed, as everyone is too busy splitting hairs over which programming language Jesus would have used to orchestrate the Last Supper if it were a distributed system. \ud83d\ude44"
  },
  {
    "title": "Build your own Siri locally and on-device (thehyperplane.substack.com)",
    "points": 112,
    "submitter": "andreeamiclaus",
    "submit_time": "2025-05-12T19:32:19 1747078339",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=43966694",
    "comments": [
      "I love the idea and I would like to build something like this. But the few attempts i have made using whisper locally has so far been underwhelming. Has anyone gotten results with small whisper models that are good enough for a use case like this?Maybe I've just had a bad microphone.\n \nreply",
      "Man, I'd really love it if this were just a product/app I could download and use a UI to configure/teach.But this guide gives me what I need to make that, I think, so a big thank you for this!\n \nreply",
      "I\u2019ve noticed recently (maybe I missed an announcement) that Siri now functions locally for at least some commands. Try putting an Apple watch in airplane mode and asking it to set a timer or reminder\n \nreply",
      "Why haven\u2019t Apple taken a look at the data then hardcoded handlers for the top ~1000 usages???\n \nreply",
      "This is great, thanks for putting this together.Haven't followed it through yet, but does this model run successfully on an iPhone?My 9 year old ran a Qwen 0.6B model using ollama quite well, anything else was too slow to offer a good UX.\n \nreply",
      "Oh, a nine year old PHONE.I was thinking there was a fourth grader out there deploying models when at that age I was still learning multiplication tables.\n \nreply",
      "My son just turned 9 today so I was like, \"Wow! I wonder if my kid would be interested in doing this?\"\n \nreply",
      "MLC[0] indicates that it can run models in the 8B range on iOS, but 1-3B sounds more reasonable to me.[0] https://llm.mlc.ai/docs/deploy/ios.html#bring-your-own-model\n \nreply",
      "Cool project and nice write-up!\n \nreply",
      "Does Apple even allow you to replace Siri with another assistant? For the longest time on android, all non-Google assistants were crippled by not being able to listen in the background or use the assistant hardkey, gestures, or shortcuts. I'm not sure if the Google assistant still has privileges others don't, but I wouldn't be surprised in the least.\n \nreply"
    ],
    "link": "https://thehyperplane.substack.com/p/build-your-own-siri-locally-on-device",
    "first_paragraph": "",
    "summary": "**Build Your Own Siri, or Maybe Don't**\n\nIn the latest heroic dive into DIY tech, an enterprising spirit at \"thehyperplane\" sows confusion among the masses with an *enticing* guide to build your own budget Siri. Unfortunately, it appears their readership struggles with more than just basic software installation. Between complaints about the subpar performance of small models and daydreams of a magical, prepackaged app, it\u2019s clear that readers may have skipped a prerequisite or two. Meanwhile, one commenter can't decide if he's at a children's birthday party or a tech symposium. \ud83c\udf89\ud83d\udcbb"
  },
  {
    "title": "A community-led fork of Organic Maps (comaps.app)",
    "points": 270,
    "submitter": "maelito",
    "submit_time": "2025-05-12T11:40:24 1747050024",
    "num_comments": 181,
    "comments_url": "https://news.ycombinator.com/item?id=43961908",
    "comments": [
      "The concern seems to be they want a bunch of guarantees about what will be done with the project - not because there is a change happening from Organic, but because they're afraid of a change happening in the future. If such a change happens in the future, they can fork then. I mean, hell, this already happened; they had Maps.ME, it was sold, Roman forked it to Organic. If it gets sold again they can fork again. This seems like it'll hurt the community more than if they'd waited until it was necessary.\n \nreply",
      "I think that the open source community is too quick to make \"just fork it later\" the answer to all our governance woes.Look at the state of WordPress: the (B?)DFL actively bans people from the community for critiquing his self-described \"nuclear war\" waged against his biggest competitor in the hosting space, which \"nuclear war\" has caught thousands of members of the community in the crossfire. And yet we see no fork. Why? Because forking is hard and fragments the community, so people would rather put up with a tyrant than deal with the risk of instability. This is no different than tyrants in any other environment.If a project has good governance established from the beginning, including a reasonably democratic process for contributors to elect the executive function, then the community can be reasonably sure that they won't feel the need to fork in the future because they have recourse if things go sour.\n \nreply",
      "A difference between Wordpress and Organic Maps, though, is that Wordpress is a framework whereas Organic Maps is an application. Switching to a fork of Wordpress means a different extension marketplace, various config files that may need to be changed, etc. Switching to a fork of Organic Maps is just downloading a different app that does the same thing.Completely irrespective of the governance structure of Organic Maps, by its nature it is much more easily forkable than something like Wordpress.\n \nreply",
      "There needs to be a server generating up to date map files. Which isn't complicated in comparison, but it's a decent bit of resources.\n \nreply",
      "> Apes Together StrongAbsolutely 100% agree with your statement, Linux desktop is the perfect example of that. You get a billion different distribution that all comes from debian, arch and maybe fedora but that's all.In my opinion, there should be 3 Linux distribution. That's all.For instance Ubuntu: Yeah Ubuntu gnome suck, yeah canonical push snap package when flappack are better but do you really need a new distribution because of that ?Perfection is the enemy of progress. And when things go all bad and you have used all other alternative, then and only then forking should be considered. Like a nuclear button.Currently i feel like it's more often used by newcomer that want to get to the lead position of a project they are passionate about but didn't start, so they fork and get a fraction of the community behind. It's not much but it's still a bit.\n \nreply",
      "Un(?)fortunately, us Organic Maps forkers have been with the project since Organic Maps was OMaps, and before. The only people with more commits than the senior fork member are the OM co-owners themselves. We really tried getting OM to deliver on their promises, but it seems silence is preferable to accountability for them.\n \nreply",
      "Forkability is an underused metric when evaluating an open source tool.\n \nreply",
      "If I understand correctly, there is work going in inside the WordPress community. Not sure if and when things will happen, I am not involved personally.\n \nreply",
      "> If such a change happens in the future, they can fork then...Did such a change not already happen with the addition of Kayak affiliate links without any community consultation? It seems to me that there has already been enough to justify a fork.Not to mention, there was a promise of electing and changing boardmembers which has never happened, and hiding the use of OrganicMaps project donations for personal vacations as alleged by the initial open letter.\n \nreply",
      "> hiding the use of OrganicMaps project donations for personal vacations as alleged by the initial open letterWere those donations intended to support the core developers generally? Or were they specifically intended to pay for servers, equipment, etc.?If it\u2019s the former, a vacation seems like a totally legitimate use. If the latter, not so much.\n \nreply"
    ],
    "link": "https://www.comaps.app/news/2025-05-12/3/",
    "first_paragraph": "\n                        Download now\n                    This is a copy of the update to the Open Letter to Organic Maps Shareholders from openletter.earth:Work on a community fork is progressing rapidly!The project's core principles are Transparency, Community Decision-making, Not-for-profit & for Public Interest, Fully Open Source and Privacy-focused. Read more about the project's essence and direction at our new \"home\" at https://codeberg.org/comaps.The project has a strong start! And now we are focusing on building the foundation, setting up the technology, and many areas are in-progress. Work is continuing on the first release!The name for the project - CoMaps (community, collaborative, common, collective, etc.) - is provisional. We think it's a good one, but would like more people in the community to take part in choosing the final name together!The project name voting happens here and will conclude on May 20th.\nSign into Codeberg to vote or suggest a name. People are already si",
    "summary": "Title: Fork Yes or Fork No: A Community Drama Unfolds\n\nIn a world where every download is a revolutionary act, the comrades at CoMaps strike back against the apparently oppressive regime of Organic Maps, bearing torches of \"transparency\" and \"community decision-making.\" Everyone's invited to the ultimate democratic exercise of renaming their newborn digital baby, because, as we all know, new names solve *all* problems in software development. Commenters spiral into civil war, one camp donning foil hats fearing the apocalypse needs a pre-planned fork, while another insists that too many forks spoil the broth, and we should only press the nuclear fork button as a last resort. Meanwhile, the ghost of past forks looms over, whispering, \"If you fork it, they will leave... or maybe they\u2019ll stay. Whatever, just fork it!\" \ud83c\udf74\ud83d\udca5"
  },
  {
    "title": "Byte latent transformer: Patches scale better than tokens (arxiv.org)",
    "points": 96,
    "submitter": "dlojudice",
    "submit_time": "2025-05-12T16:55:39 1747068939",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=43965099",
    "comments": [
      "Great to see our paper here again! Since the paper release, we've also released model weights here for anyone interesting in building on top of it: https://huggingface.co/facebook/blt. We also added HF Hub code to easily load the model https://github.com/facebookresearch/blt?tab=readme-ov-file#l....\n \nreply",
      "The thing that stood out for me was the use of ngram hashes as an additional feature set. My understanding of this is that its typically used as a positional feature.Is this a limitation of the byte patches in that the positional information needs to be augmented?\n \nreply",
      "This BLT approach is why \"AI research is stalling\" takes are wrong. Dynamic byte-level patches instead of tokens seems genuinely innovative, not just scaling up the same architecture. Better efficiency AND handling edge cases better? Actual progress. The field is still finding clever ways to rethink fundamentals.\n \nreply",
      "This paper is very cool, comes from respected authors, and is a very nice idea with good experiments (flop controlled for compute). It shouldn't be seen as a wall-breaking innovation though. From the paper:> Existing transformer libraries and codebases are designed to be highly efficient for tokenizer-based transformer architectures. While we present theoretical flop matched experiments and also use certain efficient implementations (such as FlexAttention) to handle layers that deviate from the vanilla transformer architecture, our implementations may yet not be at parity with tokenizer-based models in terms of wall-clock time and may benefit from further optimizations.And unfortunately wall-clock deficiencies mean that any quality improvement needs to overcome that additional scaling barrier before any big runs (meaning expensive) can risk using it.\n \nreply",
      "It is pretty much the same scaling, though: https://arxiv.org/pdf/2412.09871#page=10 It just lets you avoid some of the pathologies of BPEs.\n \nreply",
      "I think DeepSeek (v3 and r1) showed us that there\u2019s still a ton of meat on the bone for fundamental research and optimization.\n \nreply",
      "Absolutely, I have seen so many good ideas that have not yet made it into notable trained models.A lot of that is because you need to have a lot more faith than \"seems like a good idea\" before you spend a few million in training that depends upon it.Some of it is because when the models released now began training, a lot of those ideas hasn't been published yet.Time will resolve most of that,  cheaper and more performant hardware will allow a lot of those ideas to be tested without the massive commitment required to build the leading edge models.\n \nreply",
      "The big guys are almost certainly incinerating millions a day on training \"maybe it could show some promise\" techniques. With the way things are right now, they are probably green lighting everything to find an edge.\n \nreply",
      "I think the sentiment (at least my sentiment) is that \"mainstream ML\" has fallen into the transformer local minimum, and given the weight of the players in that space it will take a huge amount of force to move them out of it.The likes of this, Mercury Coder, and even RKWV are definitely hopeful - but there's a pitch black shadow of hype and speculation to outshine.\n \nreply",
      "I disagree. Most AI innovation today is around things like agents, integrations, and building out use cases. This is possible because transformers have made human-like AI possible for the first-time in the history of humanity. These use-cases will remain the same even if the underlying architecture changes. The number of people working on new architectures today is way more than were working on neural networks in 2017 when 'attention is all you need' came out. Nevertheless, actual ML model researchers are only a small portion of the total ML/AI community, and this is fine.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2412.09871",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "**The Byte Latent Transformer Saga: Biting the Byte!**\n\nIn an exhilarating turn of events, arXiv showcases another groundbreaking paper where the \"Byte latent transformer\" decides that patches are the new black, overruling tokens with the ferocity of a mid-90s fashion critic. Commenters, reveling in their moment of technical glory, juggle URLs and GitHub repos like hot potatoes, ensuring everyone knows they're on the bleeding edge of making computers comprehend bytes better than a toddler does alphabet soup. One audacious commenter draws a line in the sand claiming this isn't just another attempt to scale Mount Obvious, but rather a bona fide reinvention of the silicon wheel. Meanwhile, the skeptics in the corner grumble about theoretical vs. real-time performance, because, clearly, what's a scientific debate without some good old fashion clock-watching? What a time to be alive and byte-slinging! \ud83d\ude80\ud83e\udd13"
  },
  {
    "title": "University of Texas-led team solves a big problem for fusion energy (utexas.edu)",
    "points": 210,
    "submitter": "signa11",
    "submit_time": "2025-05-12T12:21:33 1747052493",
    "num_comments": 150,
    "comments_url": "https://news.ycombinator.com/item?id=43962148",
    "comments": [
      "https://arxiv.org/abs/2410.02175v2",
      "> We report on a data-driven method for learning a nonperturbative guiding center model from full-orbit particle simulation data.> Then we describe a data-driven method for learning  from a dataset of full-orbit \u03b1-particle trajectories. We apply this method to the \u03b1-particle dynamics shown in Fig. 1 and find the learned non-perturbative guiding center model significantly outperforms the standard guiding center expansion. Our proposed method for learning  applies on a per-magnetic field basis; changing  requires re-training.Is this interpolation at its heart? A variable transformation then a data fit?Anyone know which functionals of these orbits are important? I don't know the space. I am wondering why the orbits with such nuance should be materially important when accessed via lower-order models.\n \nreply",
      "Haven\u2019t read the article yet, yet alone the paper but based on what you\u2019ve quoted these are ongoing challenges with regards to confinement. Think tokamak vs stellarator. Magnetoplasmahydrodynamics is hard because you have all the complexities of the navier-stokes combined with Maxwell and thats just scratching the surface. Sensitive dependence on initial conditions has never been so sinister as in plasma confinement. Orbital perturbations quickly lead to turbulent instabilities which lead to containment breach which can lead to multi-million degree hyper velocity jets tearing a hole through your multi-billion dollar toy in seconds.\n \nreply",
      "Are these sorts of instabilities harder to control in a tokamak as compared to a stellarator, or did you just bring those up as examples of magnetic confinement?\n \nreply",
      "It is a little jarring to hear \"data-driven\" and \"nonperturbative\" in the same sentence. It sounds a little bit like saying you designed a boat with a better lift-to-drag ratio. \"Wait, is it a boat or a plane?\". So, I opened the paper fully expecting to not understand anything, and I was pleasantly surprised.> First we deduce formally-exact non-perturbative guiding center\n equations of motion assuming a hidden symmetry with associated conserved quantity J. We refer to J as the non-perturbative adiabatic invariant.Simply: this is not just some kind of unsupervised ML black-box magic. There is a formal mathematical solution to something, but it has a certain gap, namely precisely what quantity is conserved and how to calculate it.> Then we describe a data-driven method for learning J from a dataset of full-orbit \u03b1-particle trajectories. [...] Our proposed method for learning J applies on a per-magnetic field basis; changing B requires re-training. This makes it well-suited to stellarator design assessment tasks, such as \u03b1-loss fraction uncertainty quantification.With the formal simplification of the dynamics in hand, the researchers believe that a trained model can then give a useful approximation of the invariant, which allows the formal model, with its unknown parameters now filled in, to be used to model the dynamics.In a crude way, I think I have a napkin-level sketch of what they're doing here. Suppose we are modeling a projectile, and we know nothing of kinematics. They have determined that the projectile has a parabolic trajectory (the formal part) and then they are using data analysis to find the g coefficient that represents gravitational acceleration (the data-driven part). Obviously, you would never need machine learning in such a very simple case as I have described, but I think it approximates the main idea.\n \nreply",
      "Often in physics the equations are already known or can be derived. However, taking a formula, generally a PDE, and solving it efficiently is the real trick. Also as you point out, formulating the equation in terms of core invariants you wish to hold, plays an important part.Finding simplified easy to solve solutions and using them to estimate solutions and using adjustments to get closer to the real solution is a baselime technique. That's the core of the pertubative approach in physics which uses : https://en.wikipedia.org/wiki/Perturbation_theory#:~:text=Pe...However, now it's possible to train AI models to learn much more complex approximations that allow them to run much quicker and more accurately. A prime example is DeepMinds AlphaFold, IMHO.I haven't read up on the research to much, but I'd place firm bets that AI models will be critical in controlling any viable fusion technology.\n \nreply",
      "One of the nice things about LLMs/ML, is that they can pound away at something for a billion cycles, and do exactly the same things that you or I would do.for _ in 0..<1000000000000 {\n    do_something_complicated()\n}\n \nreply",
      "Isn\u2019t that one of the nice things of computers in general not a feature of llm?\n \nreply",
      "The difference is the complexity of the repeated task\n \nreply",
      "But don't those complexities still boil down to machine level instructions??Or can/do llms operate outside of a CPU? Thanks\n \nreply"
    ],
    "link": "https://news.utexas.edu/2025/05/05/university-of-texas-led-team-solves-a-big-problem-for-fusion-energy/",
    "first_paragraph": "AUSTIN, Texas \u2014\u00a0\u00a0Abundant, low-cost, clean energy \u2014 the envisioned result if scientists and engineers can successfully produce a reliable method of generating and sustaining fusion energy \u2014 took one step closer to reality, as a team of researchers from The University of Texas at Austin, Los Alamos National Laboratory and Type One Energy Group solved a longstanding problem in the field.One of the big challenges holding fusion energy back has been the ability to contain high-energy particles inside fusion reactors. When high-energy alpha particles leak from a reactor, that prevents the plasma from getting hot and dense enough to sustain the fusion reaction. To prevent them from leaking, engineers design elaborate magnetic confinement systems, but there are often holes in the magnetic field, and a tremendous amount of computational time is required to predict their locations and eliminate them.In their paper published in Physical Review Letters, the research team describes having discover",
    "summary": "**University Teams Up To Put Plasma In A Bottle, Internet Explodes Anyway**\nIn a thrilling demonstration of academic buzzword bingo, the University of Texas announces a step forward in fusion energy that's sure to solve all our problems, just as soon as they figure out how to keep their alpha particles in check. Comment sections across the internet light up with armchair physicists who didn\u2019t read past the headline but confidently equate magnetic field dynamics with their high school science fair projects. Expect a flood of \"just like in Star Trek!\" insights from people who remember the word \"plasma\" from that one episode they saw. Meanwhile, real scientists continue to wrestle with the universe's most complex game of 3D pinball."
  },
  {
    "title": "Embeddings are underrated (2024) (technicalwriting.dev)",
    "points": 440,
    "submitter": "jxmorris12",
    "submit_time": "2025-05-12T15:05:44 1747062344",
    "num_comments": 130,
    "comments_url": "https://news.ycombinator.com/item?id=43963868",
    "comments": [
      "Hello, I wrote this. Thank you for reading!The post was previously discussed 6 months ago: https://news.ycombinator.com/item?id=42013762To be clear, when I said \"embeddings are underrated\" I was only arguing that my fellow technical writers (TWs) were not paying enough attention to a very useful new tool in the TW toolbox. I know that the statement sounds silly to ML practitioners, who very much don't \"underrate\" embeddings.I know that the post is light on details regarding how exactly we apply embeddings in TW. I have some projects and other blog posts in the pipeline. Short story long, embeddings are important because they can help us make progress on the 3 intractable challenges of TW: https://technicalwriting.dev/strategy/challenges.html\n \nreply",
      "> Discoveryness. Even if the needed content exists, it\u2019s hard to guarantee that users will find it.I'm curious as to what you'll think of the UX layer I applied to embeddings for public perusal. I call it \"semantic scrolling\" since it's not searching exactly, but moving through the cluster by using <summary>/<details> as a tree.[1] is a single starting point (press the animated arrow to \"wiki-hole\") and [2] is the entire collection (books, movies, music, animations, etc.)[1] - https://www.sharecommonbase.com/synthesize/1009?id=1009\n[2] - https://www.sharecommonbase.com/\n \nreply",
      "Cool. I kinda grok the idea of semantic scrolling but I'm having trouble seeing it in action in the site. I think it would be useful in cases where I want to become an expert in a given topic and therefore I want to peruse lots of related ideas and create the possibility of serendipitous new neural connections. As for technical documentation, usually people want to find certain information as quickly as possible so that they can get on with their work, so I don't think semantic scrolling would be a good fit on most docs sites. I.e. they won't have the patience to semantically scroll in order to find the info they need.\n \nreply",
      "Perhaps you should make the post more appealing to tech writers and less to ML experts. That would help increase the reach for the intended target audience. For example, you can expand on \"the ability to discover connections between texts at previously impossible scales\". There's an applications section, but it's easy to overlook. Frontload value for tech writers with examples.\n \nreply",
      "Yes, definitely need to follow through on the follow-up posts and projects that show exactly how we apply embeddings to TW. Examples (in all their forms) are truly magical in how effective they are as a teaching aid.\n \nreply",
      "> I know that the post is light on details regarding how exactly we apply embeddings in TW.More significantly, after having read the first 6 or 8 paragraphs, i still have no clue what an \"embedding\" is. From the 3rd paragraph:> Here\u2019s an overview of how you use embeddings and how they work.But no mention of what they are (unless perhaps it's buried far deeper in the article).\n \nreply",
      "A word embedding is a representation of a word using many numbers, where each numbers represents some property of the word. Usually we do not know what those properties are because the numbers are learned by a model during processing of a large number of texts.\n \nreply",
      "I was worried that introducing the formal concept too quickly would feel a bit overwhelming for my fellow technical writers who are learning about embeddings for the first time, but I know that it's also annoying when a post makes you wait too long to get an answer to a question. So I'll find a way to provide a quick answer upfront. Thanks for the feedback.\n \nreply",
      "Also, re: direct applications of embeddings in technical writing, see https://www.tdcommons.org/dpubs_series/8057/\n \nreply",
      "Thanks for the write-up!I\u2019m curious how you found the quality of the results? This gets into evals which ML folks love, but even just with \u201cvibes\u201d do the results eyeball as reasonable to you?\n \nreply"
    ],
    "link": "https://technicalwriting.dev/ml/embeddings/overview.html",
    "first_paragraph": "Machine learning (ML) has the potential to advance the state of the\nart in technical writing. No, I\u2019m not talking about text generation models\nlike Claude, Gemini, LLaMa, GPT, etc. The ML technology that might end up\nhaving the biggest impact on technical writing is embeddings.Here\u2019s an overview of how you use embeddings and how they work.\nIt\u2019s geared towards technical writers who are learning about\nembeddings for the first time.Someone asks you to \u201cmake some embeddings\u201d. What do you input? You input\ntext.1 You don\u2019t need to provide the same amount of text every time.\nE.g. sometimes your input is a single paragraph while at other times it\u2019s\na few sections, an entire document, or even multiple documents.What do you get back? If you provide a single word as the\ninput, the output will be an array of numbers like this:Now suppose your input is an entire set of documents. The output\nturns into this:One input was drastically smaller than the other, yet they both produced\nan array of 3 number",
    "summary": "**Embeddings: The Magic Beans of Technical Writing?**\n\nIn an act of brave defiance against the tyranny of ignorance, another heroic tech blogger at *technicalwriting.dev* boldly proclaims the underrated nature of embeddings in technical writing. Because, obviously, what the world of letters-on-a-screen really needs is another technical writer discovering that putting text into a black-box and getting mystery numbers out is _the next big thing_ \ud83d\ude44. The comment section becomes a merry-go-round of back-patting and mutual enlightenment, as readers either marvel at this newfound silver bullet or gently remind our valiant scribe that perhaps, just maybe, SEO-friendly summaries might require an explanation of the term 'embeddings' before the third scroll-down. The rest of us, meandering through the semantic scrolling tip, are still waiting for the punch line where we find the 'Immediately Useful' button. Meanwhile, in a nearby digital universe, Claude, Gemini, LLaMa, and GPT sip digital martinis, chuckling into the void. \ud83c\udf78"
  },
  {
    "title": "NASA Study Reveals Venus Crust Surprise (nasa.gov)",
    "points": 37,
    "submitter": "mnem",
    "submit_time": "2025-05-09T19:25:05 1746818705",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=43940172",
    "comments": [
      "- \"Several upcoming missions, including NASA\u2019s DAVINCI (Deep Atmosphere Venus Investigation of Noble gases, Chemistry, and Imaging)\"DAVINCI is actually cancelled in the latest budget request. For obvious reasons, the NASA press office (the OP) won't talk about this. But 50% of NASA's science funding is gone.https://spacenews.com/white-house-proposal-would-slash-nasa-...\n \nreply",
      "The full proposed budget is available at https://www.whitehouse.gov/wp-content/uploads/2025/05/Fiscal... folks are interested in seeing the whole picture and reasoning.\n \nreply",
      "> reasoningThe justification? I don't think they are open about their reasoning.\n \nreply",
      "Typo in your URL. replace the last l with a d ....\".pdf\" not \".plf\"\n \nreply",
      "HTTP 404\n \nreply",
      "> But 50% of NASA's science funding is gone.The proposed cuts to science are catastrophic, but there\u2019s still time to call your Congressperson.https://aas.org/posts/news/2025/05/week-of-action\n \nreply",
      "> Scientists expected the outermost layer of Venus\u2019 crust would grow thicker and thicker over timeI recall watching this NOVA episode in 1995 where scientists had no idea whether the lithosphere is thick or thin. Seek to 36 minutes: https://archive.org/details/VenusUnveiled/NOVA.S22E10.Venus....\n \nreply",
      "I'm blown away at the number of huge volcanos and relative lack of craters. If that's right, Venus must recycle its surface relatively often.\n \nreply",
      "Are you just looking at the photo to determine volcanoes and craters?  Wouldn't we expect that with so much atmosphere on Venus, that meteors would have a much harder time reaching the surface?  That is, much larger ones would burn up or get deflected than on Earth.\n \nreply",
      "I wonder if Venus could be terraformed via a sun shield placed in orbit around it. How big would it have to be to reverse the runaway greenhouse effect?\n \nreply"
    ],
    "link": "https://science.nasa.gov/science-research/astromaterials/nasa-study-reveals-venus-crust-surprise/",
    "first_paragraph": "3 min readNew details about the crust on Venus include some surprises about the geology of Earth\u2019s hotter twin.New details about the crust on Venus include some surprises about the geology of Earth\u2019s hotter twin, according to new NASA-funded research that describes movements of the planet's crust.Scientists expected the outermost layer of Venus\u2019 crust would grow thicker and thicker over time given its apparent lack of forces that would drive the crust back into the planet\u2019s interior. But the paper, published in Nature Communications, proposes a crust metamorphism process based on rock density and melting cycles.Earth\u2019s rocky crust is made up of massive plates that slowly move, forming folds and faults in a process known as plate tectonics. For example, when two plates collide, the lighter plate slides on top of the denser one, forcing it downward into the layer beneath it, the mantle. This process, known as subduction, helps control the thickness of Earth\u2019s crust. The rocks making up t",
    "summary": "<b>NASA Discovers Venus Might Need a New Foundation</b>\n\nIn an earth-shattering display of redundancy, NASA scientists have announced that Venus, unlike your high-school personality, is not as thick as previously suspected. This revelation, sourced from a space agency desperate to justify its existence in the face of budget cuts, has stirred the collective imaginations of basement-dwelling astrophysicists across forums. Commenters, quick to flaunt their Google-acquired PhDs, oscillate between mourning the cancellation of missions they didn't know existed until today and proposing sci-fi solutions to terraform Venus with fantastical space umbrellas. Debates rage on, fueled by a charming mixture of misinformation and typo-ridden URLs, inadvertently proving that internet access is perhaps as unchecked as Venusian volcanism. \ud83c\udf0b\ud83d\ude80\ud83d\udd25"
  },
  {
    "title": "Reviving a modular cargo bike design from the 1930s (core77.com)",
    "points": 129,
    "submitter": "surprisetalk",
    "submit_time": "2025-05-12T14:29:33 1747060173",
    "num_comments": 100,
    "comments_url": "https://news.ycombinator.com/item?id=43963397",
    "comments": [
      "These comments just read like a whole bunch of people who never rode a bike and yet are somehow experts on everything about bikes.This thing is not for Le Tour and you don't go fast on it and you don't go up giant hills on it.    That makes a lot of the concerns here go out the window.These types of bikes shaped objects often have all kinds of issues with trying to use bike parts designed for standard bikes on something that is very different.   Issues with needing enormous chains, huge cable runs, etc.. when designers try things like this they are worrying about issues like that more than whether you can climb a mountain on it or stuff it into a corner at high speed without going out of control.The thing with these is the cost to design & manufacture components that need to be different than normal bikes can be astronomical, so anything they can do to design the frame to use normal components in a normal/non-compromised way pays off in a huge way.The ideas behind this aren't that different than the Cruzbike being front wheel drive to get rid of a lot of the component/drivetrain issues that recumbent bikes are famous for.\n \nreply",
      "The comment asking about riding positions drop bars was hilarious.Lots of Americans in the comments is probably why, plus the Hacker News tendency toward armchair expertise.The American grid system tends to produce hills that are unmanageable on a town or cargo bike, so bicycles are a fitness/enthusiast thing. And US cities are much more spread out, too. In more walkable European cities, these bikes make a lot of sense.\n \nreply",
      "People comparing these with racing bikes is so unbelievable.\n \nreply",
      "Tricycles are inherently unstable in turns, especially when not loaded down, because they cannot lean. And when carrying a load, the same rules of physics apply, resulting in a lot of torsional forces on the frame. There's a reason we see so few of those on the roads, amidst an explosion of various human-powered modes of transport.Here's the original: http://youtube.com/watch?v=RuPwRQOUhl4Here's the reimagined modern version: https://www.youtube.com/watch?v=kA7qGYNFuY0In both cases, the rider effectively sits atop of where the handlebars would be on a traditional trike. You can see in the first video the lads have a hard time keeping all wheels on the ground.One notable difference between the new model and the old is that they seem to have changed the geometry of the frame so that the driver doesn't lean into the turn (the turning wheel stays upright). They don't demonstrate it in motion very well, but that kind of turn action will tend to throw the rider \"out\" of the turn, making the trike fall over opposite of the direction of the turn. The old version tends to fall \"into\" the turn.I can't think of many advantages to this design, other than the driving unit and cargo are modular. Even then, the rider would not be able to travel without the cargo portion.Trikes are tricky, they don't go very fast, they don't turn well, and they're wider than most other pedal-powered vehicles, making them hard to use on existing cycle infrastructure.\n \nreply",
      "> There's a reason we see so few [tricycles] on the roads, amidst an explosion of various human-powered modes of transport.Perhaps in the US and western Europe, but tricycle tuk-tuks and cycle rickshaws are extremely common in other parts of the world.\n \nreply",
      "In The Netherlands 3 wheel bikes are fairly common to haul kids and dogs.I can easily get mine on two wheels if I take a sharp, fast, turn - but after you do it once you learn the limits and it's a very stable bike.\n \nreply",
      "Same in Denmark. It is almost a must have for inner city families.\nMuch more common than two wheel Long Johns.Two very common models are:https://www.christianiabikes.com/classic/https://www.ladcyklen.dk/ladcykel/nihola-ladcykler.html\n \nreply",
      "The common aspect is that both the Netherlands and Denmark are flat. The danish/Dutch trikes are really just unsafe when you pick up significant speed like down a hill. I am a long time cyclist, who raced for years. I've never felt as unsafe on a bike as when I tried riding and a fast bike speed on a Christiania.\n \nreply",
      "Luckily cargo bikes aren't for speeding and racing but for carrying cargo. It would be like using a truck to race instead of a mid-engine super light sports car.Durch cargo trikes are generally assumed safer than their two-wheeled alternatives here.\n \nreply",
      "You mean like this DAF Turbo Twin overtaking a 405t16 in the Paris-Dakar?\nhttps://m.youtube.com/watch?v=AWCNnlk8rkU\n \nreply"
    ],
    "link": "https://www.core77.com/posts/136773/Reviving-a-Modular-Cargo-Bike-Design-from-the-1930s",
    "first_paragraph": "Your average cargo bike typically has a long frame divided into two areas: One for the cargo, and one for the rider. The rider is positioned just forward of the rear wheel, with a chain delivering power from the pedals to that rear wheel.  French mobility company Cyclauto has gone through the history books to revive an alternate design from the 1930s.  This design, originally by French industrialist Auguste Reymond, places the rider directly above the front wheel. They pedal this wheel directly; there's no chain, reducing maintenance needs. A three-speed gearbox in the hub makes starting easier.    The cargo-carrying part, in Cyclauto's modern rendition, is detachable, recalling a semi-trailer. This means a wide variety of trailer styles can be fitted to the drive unit, creating a modular system that can handle a wide variety of user needs. The Cyclauto can be configured to move cargo, people or even commercial fixtures (think food cart).   An additional benefit to the two-piece frame ",
    "summary": "**Hacker News Discovers Cargo Bikes, Saves World**\n\nIn an exhilarating display of historical awe, Cyclauto drags a dusty 1930s cargo bike design into the 21st century, much to the delight of design blog commenters who wouldn\u2019t know a chainring from a napkin ring. The bike, which places the rider suspiciously close to the front wheel in a throwback to tricycle days, apparently eliminates chains because who needs efficiency when you can have hipster points? The real entertainment, however, explodes in the comments section where armchair engineers battle fiercely over physics principles that were clearly not in play when they were skimming their physics textbooks back in high school. Between comparing these hulking trikes to racing bikes and confusing cargo hauling with the Tour de France, the collective misunderstanding could probably power a small cargo bike itself."
  },
  {
    "title": "Ruby 3.5 Feature: Namespace on read (ruby-lang.org)",
    "points": 174,
    "submitter": "ksec",
    "submit_time": "2025-05-12T13:39:34 1747057174",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=43962770",
    "comments": [
      "I feel like this is a solution to a problem nobody really has in practice, by simply following conventions, and I've been using Ruby for over 10 years. If byroot -- who works at Shopify on the largest Rails codebase in existence -- echos the same sentiment, then maybe it should be scrapped.\n \nreply",
      "I will share a concrete example where I've recently run into this problem.In order to make use of OpenStruct, `require 'ostruct'` first needs to be declared. Our code neglected to make that declaration, and we saw failures when it was deployed. This code, however, passed all of our tests. We discovered it was because our testing framework included rspec-expectations, which has a dependency on diff-lcs[1], and diff-lcs itself declares `require 'ostruct'`[2]. Because of this, ostruct was loaded globally before our code was tested, which silently masked the underlying issue.This being said, I do understand the sentiment that this feature seems superfluous and may introduce unnecessary complication, especially from a Rubyist's point of view. The underlying mental model of Ruby dependency management is different from many other languages, and it's something to keep in mind when coming from other languages that do have scope for declared dependencies.[1] https://github.com/rspec/rspec-expectations/blob/v3.13.3/rsp...\n[2] https://github.com/halostatue/diff-lcs/blob/v1.5.1/lib/diff/...\n \nreply",
      "Does that mean you develop code without running it locally?The first execution of new code is in CI?\n \nreply",
      "They didn't say where it was deployed. I'd often only test some changes in staging rather than locally, because they rely on some specific other service I don't want to spend time setting up. It's very common.\n \nreply",
      "`diff-lcs` no longer uses `ostruct` as of 1.6.0 (granted, that was released in February).\n \nreply",
      "> I've been using Ruby for over 10 yearsMaybe after a decade or two I wouldn't care, but as someone who has only used Ruby casually I would steer clear of it for anything serious, largely due to the lack of namespaces.\n \nreply",
      "I really like Ruby, I do sort of wish it could focus on it's own identity a bit.Some new features feel almost prescribed from other languages? Like, RBS and Namespaces specifically... they don't really fit the model of Ruby exactly, but they need to be there so Ruby can be presented as \"having\" type-safety and a-solution-for-monkey-patching. I'm all for taking inspiration from other places, but Ruby wasn't quite built around the same axioms that other programming languages started from.\n \nreply",
      "Almost every recent (in years) Ruby feature is not a novel idea. Even the origin was a Perl/Smalltalk/Lisp crossover so the axioms were pretty well known.\n \nreply",
      "Yea generally agree with this take. Feels like we\u2019re getting features shoehorned in.\n \nreply",
      "I would like to second (or _third_) this opinion.I've been working with Ruby for 20 years, and I've not needed something like this. This feels like adding a lot of complexity for little practical benefit. The trade-off feels off. I don't think this is worth the additional complexity.\n \nreply"
    ],
    "link": "https://bugs.ruby-lang.org/issues/21311",
    "first_paragraph": "\n        Added by tagomoris (Satoshi Tagomori) 7 days ago.\n        Updated about 2 hours ago.\n        DescriptionThis replaces #19744This proposes a new feature to define virtual top-level namespaces in Ruby. Those namespaces can require/load libraries (either .rb or native extension) separately from other namespaces. Dependencies of required/loaded libraries are also required/loaded in the namespace.This feature will be disabled by default at first, and will be enabled by an env variable RUBY_NAMESPACE=1 as an experimental feature.\n(It could be enabled by default in the future possibly.)The \"on write\" approach here is the design to define namespaces on the loaded side. For example, Java packages are defined in the .java files and it is required to separate namespaces from each other. It can be implemented very easily, but it requires all libraries to be updated with the package declaration. (In my opinion, it's almost impossible in the Ruby ecosystem.)The \"on read\" approach is to crea",
    "summary": "In the latest episode of \"Solving Problems That Don't Exist\", Ruby introduces *Namespace on read*, a feature that's as eagerly awaited as rain in a flood. Satoshi Tagomori, presumably having exhausted all other hobbies, proposes a system that requires developers to upend existing code under the guise of \"organization\". Commenters, in a dazzling display of missed points, argue passionately over the semantics of a feature that their grandmas will use to manage zero libraries. Meanwhile, everyone quietly neglects basic dependency management, proving Ruby's community is adept at keeping traditions \u2014 unnecessary complexity and philosophical debates over features borrowed from languages that actually needed them. \ud83c\udf89"
  }
]