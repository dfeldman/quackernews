[
  {
    "title": "Nvidia Is Full of Shit (sebin-nyshkim.net)",
    "points": 314,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-04T21:58:25 1751666305",
    "num_comments": 161,
    "comments_url": "https://news.ycombinator.com/item?id=44468175",
    "comments": [
      "I'm so happy to see someone calling NVIDIA out for their bullshit. The current state of GPU programming sucks, and that's just an example of the problems with the GPU market today.The lack of open source anything for GPU programming makes me want to throw my hands up and just do Apple. It feels much more open than pretending that there's anything open about CUDA on Linux.reply",
      "Seems a bit calculated and agreed across the industry. What can really make sense of Microsoft's acquisitions and ruining of billion dollar IPs? It's a manufactured collapse of the gaming industry. They want to centralize control of the market and make it a service based (rent seeking) sector.I'm not saying they all got together and decided this together but their wonks are probably all saying the same thing. The market is shrinking and whether it's by design or incompetence, this creates a new opportunity to acquire it wholesale for pennies on the dollar and build a wall around it and charge for entry. It's a natural result of games requiring NVidia developers for driver tuning, bitcoin/ai and buying out capacity to prevent competitors.The wildcard I can't fit into this puzzle is Valve. They have a huge opportunity here but they also might be convinced that they have already saturated the market and will read the writing on the wall.reply",
      "I think the reason you see things like Blizzard killing off Overwatch 1 is because the Lindy effect applies in gaming as well. Some things are so sticky and preferred that you have to commit atrocities to remove them from use.From a supply/demand perspective, if all of your customers are still getting high on the 5 (or 20) year old supply, launching a new title in the same space isn't going to work. There are not an infinite # of gamers and the global dopamine budget is limited.Launching a game like TF2 or Starcraft 2 in 2025 would be viewed as a business catastrophe by the metrics most AAA studios are currently operating under. Monthly ARPU for gamers years after purchasing the Orange Box was approximately $0.00. Giving gamers access to that strong of a drug would ruin the demand for other products.reply",
      "I purchased \"approximately $0.00\" in TF2 loot boxes. How much exactly? Left as an exercise to the reader.reply",
      "The video game industry has been through cycles like this before. One of them (the 1983 crash) was so bad it killed most American companies and caused the momentum to shift to Japan for a generation. Another one I can recall is the \"death\" of the RTS (real-time strategy) genre around 2010. They have all followed a fairly similar pattern and in none of them that I know of have things played out as the companies involved thought or hoped they would.reply",
      "I worked in the video game industry from the 90s through to today.  I think you are over generalizing or missing the original point.  It's true that there have been boom and busts.  But there are also structural changes.  Do you remember CD-ROMs?  Steam and the iPhone were structural changes.What Microsoft is trying to do with Gamepass is a structural change.  It may not work out the way that they plan but the truth is that sometimes these things do change the nature of the games you play.reply",
      "Not in the game industry but as a consumer this is very true. One example: ubiquitous access to transactions and payment systems gave a huge rise to loot boxes.Also mobile games that got priced at $0.99 meant that only the unicorn level games could actually make decent money so In-App Purchases were born.But also I suspect it is just a problem where as consumers we spend a certain amount of money on certain kinds of entertainment and if as a content producer you can catch enough people\u2019s attention you can get a slice of that pie. We saw this with streaming services where an average household spent about $100/month on cable so Netflix, Hulu, et al all decided to price themselves such that they could be a portion of that pie (and would have loved to be the whole pie but ironically studios not willing to license everything to everyone is what prevented that).reply",
      "But the thing is that Steam didn't cause the death of physical media. I absolutely do remember PC gaming before Steam, and between the era when it was awesome (StarCraft, Age of Empires, Unreal Tournament, Tribes, etc.) and the modern Steam-powered renaissance, there was an absolutely dismal era of disappointment and decline. Store shelves were getting filled with trash like \"40 games on one CD!\" and each new console generation gave retailers an excuse to shrink shelf space for PC games. Yet during this time, all of Valve's games were still available on discs!I think Microsoft's strategy is going to come to the same result as Embracer Group. They've bought up lots of studios and they control a whole platform (by which I mean Xbox, not PC) but this doesn't give them that much power. Gaming does evolve and it often evolves to work around attempts like this, rather than in favor of them.reply",
      "As much as they've got large resources, I'm not sure what projects they could reasonably throw a mountain of money at and expect to change things, and presumably benefit from in the future instead of doing it to be a a force of chaos in the industry. Valve's efforts all seem to orbit around the store, that's their main business and everything else seems like a loss-leader to get you buying through it even if it comes across as a pet project of a group of employees.The striking one for me is their linux efforts, at least as far as I'm aware they don't do a lot that isn't tied to the steam deck (or similar devices) or running games available on steam through linux. Even the deck APU is derived from the semi-custom work AMD did for the consoles, they're benefiting from a second later harvest that MS/Sony have invested (hundreds of millions?) in many years earlier. I suppose a lot of it comes down to what Valve needs to support their customers (developers/publishers), they don't see the point in pioneering and establishing some new branch of tech with developers.reply",
      "Valve is a private company so doesn\u2019t have the same growth at all costs incentives. To Microsoft, the share price is everything.reply"
    ],
    "link": "https://blog.sebin-nyshkim.net/posts/nvidia-is-full-of-shit/",
    "first_paragraph": "Since the disastrous launch of the RTX 50 series, NVIDIA has been unable to escape negative headlines: scalper bots are snatching GPUs away from consumers before official sales even begin, power connectors continue to melt, with no fix in sight, marketing is becoming increasingly deceptive, GPUs are missing processing units when they leave the factory, and the drivers, for which NVIDIA has always been praised, are currently falling apart. And to top it all off, NVIDIA is becoming increasingly insistent that media push a certain narrative when reporting on their hardware.Just like with every other GPU launch in recent memory, this one has also been ripe with scalper bots snatching up stock before any real person could get any for themselves. Retailers have reported that they\u2019ve received very little stock to begin with. This in turn sparked rumors about NVIDIA purposefully keeping stock low to make it look like the cards are in high demand to drive prices. And sure enough, on secondary m",
    "summary": "**Nvidia Is Full Of It, And So Are The Comment Warriors**\n\nIn a dazzling display of corporate incompetence, Nvidia somehow manages to squander public goodwill faster than their GPUs catch fire \u2013 literally. Cue the *scalper bot apocalypse* and mysteriously vanishing GPU stock, setting the stage for Nvidia to play the underdog in a tragedy of their own scripting. Meanwhile, the comment section morphs into a battleground where tech enthusiasts and conspiracy theorists alike wrestle over the bones of decency in technological reporting, each more eager than the last to proclaim the end of gaming as we know it. It's a spectacle of misplaced outrage and armchair economics, with a dash of apocalypse now \u2013 because who doesn\u2019t love a good tech dystopia?"
  },
  {
    "title": "OBBB signed: reinstates immediate expensing for U.S.-based R&D (kbkg.com)",
    "points": 59,
    "submitter": "tareqak",
    "submit_time": "2025-07-05T00:24:00 1751675040",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=44469124",
    "comments": [
      "If correct, this is a good thing on a generally bad, overstuffed bill. Immediate expensing never should have been changed in the first place, and it was always weird seeing people twist themselves in knots defending it.reply",
      "> Foreign R&D must still be amortized over 15 yearsreply",
      "Awesome, this literally could not be better for American tech workers.reply",
      "There's also H-1B (and other worker visa) restrictions/costs imposed. Overall, quite good for the American tech workerreply",
      "I don't see anything supporting this in the text of OBBB, nor in the definition of domestic research expense (https://www.irs.gov/pub/irs-regs/research_credit_basic_sec41...). Where did you see this?reply",
      "Source?reply",
      "Extra $250 fee for visa applications: https://judiciary.house.gov/media/press-releases/big-beautif...3.5% remittance fees on sending money out of the US: https://www.globalimmigrationblog.com/2025/06/what-are-the-i...Also (in above source), no ACA subsidies for H-1B visa holders (and others), which likely means employers they will have to pay more for health care if they want to cover their immigrant workersreply",
      "Quoting all the fees in https://judiciary.house.gov/media/press-releases/big-beautif...> Expansion of Immigration Fees:>    $1,000 asylum application fee \u2014 first in U.S. history>    $1,000 fee for individuals paroled into the U.S.>    $3,500 fee for sponsors of unaccompanied children>    $5,000 fee for sponsors of unaccompanied children who fail to appear in court>    $550 fee for work permits>    $500 application fee for Temporary Protected Status (TPS)>    $400 fee to file a diversity immigrant visa application>    $250 fee to register for the Diversity Visa Lottery>    $250 visa integrity fee>    $100 year fee while asylum applications remain pending>    $100 fee for continuances granted in immigration court>    $5,000 fee for individuals ordered removed in absentia>    $1,500 fee to adjust status to lawful permanent resident (green card)>    $1,050 fee for inadmissibility waivers>    $900 fee to appeal a decision by an immigration judge>    $900 fee to appeal a decision by DHS>    $1,325 fee to appeal in practitioner disciplinary cases>    $900 fee to file motions to reopen or reconsider>    $600 application fee for suspension of deportation>    $600 application fee for cancellation of removal (permanent residents)>    $1,500 application fee for cancellation of removal (non-permanent residents)>    $30 fee for Form I-94 (arrival/departure record), up from $6reply",
      "So payroll for R&D is now entirely tax deductible? Businesses get to choose to pay taxes or do R&D for themselves?reply",
      "Tax deductible is a weird way of phrasing it. It's not like these software companies were counting their money at the end of the quarter, and then deciding to do R&D instead of paying taxes.  They had already paid R&D expenses to build the product, which gained them revenue.  Previously they weren't allowed to actualize the cost of R&D all at once, so the business could be losing money, and still have to pay taxes on top of the loss (which is nuts).This fixes the problem, so now if you spend $100 on software developers, and you make $100 from the software, then you have $0 income, instead of $80 income.reply"
    ],
    "link": "https://www.kbkg.com/feature/house-passes-tax-bill-sending-to-president-for-signature",
    "first_paragraph": "",
    "summary": "**Weekday Legislators Pass Bloated Bill; Armchair Economists Rejoice**\n\nIn what seems to be a miraculous moment of clarity, Congress throws a bone to U.S.-based R&D, affirming that yes, companies can actually write off the money they dump into developing yet another redundant tech tool. Internet commenters, bursting with their usual expertise distilled from swiftly skimmed headlines, rally in celebration as if the law is mana from haven\u2014conveniently ignoring the barrage of fees set to handcuff anyone entering the U.S. with a dream and a suitcase. Amidst cheers of patriotism and Silicon Valley back-pats, several distraught souls attempt to verify claims with sources, only to get buried under a deluge of more cheering. In the echo chamber of online discourse, who needs facts when you have \ud83d\udd25 hot takes \ud83d\udd25?"
  },
  {
    "title": "Mini NASes marry NVMe to Intel's efficient chip (jeffgeerling.com)",
    "points": 291,
    "submitter": "ingve",
    "submit_time": "2025-07-04T15:21:51 1751642511",
    "num_comments": 142,
    "comments_url": "https://news.ycombinator.com/item?id=44465319",
    "comments": [
      "Intel N150 is the first consumer Atom [1] CPU (in 15 years!) to include TXT/DRTM for measured system launch with owner-managed keys. At every system boot, this can confirm that immutable components (anything from BIOS+config to the kernel to immutable partitions) have the expected binary hash/tree.TXT/DRTM can enable AEM (Anti Evil Maid) with Qubes, SystemGuard with Windows IoT and hopefully future support from other operating systems. It would be a valuable feature addition to Proxmox, FreeNAS and OPNsense.Some (many?) N150 devices from Topton (China) ship without Bootguard fused, which _may_ enable coreboot to be ported to those platforms. Hopefully ODROID (Korea) will ship  N150 devices. Then we could have fanless N150 devices with coreboot and DRTM for less-insecure [2] routers and storage.[1] Gracemont (E-core): https://chipsandcheese.com/p/gracemont-revenge-of-the-atom-c... | https://youtu.be/agUwkj1qTCs (Intel Austin architect, 2021)[2] \"Xfinity using WiFi signals in your house to detect motion\", 400 comments, https://news.ycombinator.com/item?id=44426726#44427986reply",
      "Where are you seeing devices without Bootguard fused? I'd be very curious to get my hands on some of those...reply",
      "As a Schr\u00f6dinger-like property, it may vary by observer and not be publicly documented.. One could start with a commercial product that ships with coreboot, then try to find identical hardware from an upstream ODM. A search for \"bootguard\" or \"coreboot\" on servethehome forums, odroid/hardkernel forums, phoronix or even HN, may be helpful.reply",
      "While it may be tempting to go \"mini\" and NVMe, for a normal use case I think this is hardly cost effective.You give up so much by using an all in mini device...No Upgrades, no ECC, harder cooling, less I/O.I have had a Proxmox Server with a used Fujitsu D3417 and 64gb ecc for roughly 5 years now, paid 350 bucks for the whole thing and upgraded the storage once from 1tb to 2tb. It draws 12-14W in normal day use and has 10 docker containers and 1 windows VM running.So I would prefer a mATX board with ECC, IPMI 4xNVMe and 2.5GB over these toy boxes...However, Jeff's content is awesome like alwaysreply",
      "I think you're right generally, but I wanna call out the ODROID H4 models as an exception to a lot of what you said. They are mostly upgradable (SODIMM RAM, SATA ports, M.2 2280 slots), and it does support in-band ECC which kinda checks the ECC box. They've got a Mini-ITX adapter for $15 so it can fit into existing cases too.No IPMI and not very many NVME slots. So I think you're right that a good mATX board could be better.reply",
      "Well, if you would like to go mini (with ECC and 2.5G) you could take a look at this one:https://www.aliexpress.com/item/1005006369887180.htmlNot totally upgradable, but at least pretty low cost and modern with an optional SATA + NVMe combination for Proxmox. Shovel in an enterprise SATA and a consumer 8TB WD SN850x and this should work pretty good. Even Optane is supported.IPMI could be replaced with NanoKVM or JetKVM...reply",
      "Not sure about the odroid but I got myself the nas kit from friendly elec. With the largest ram it was about 150 bucks and comes with 2,5g ethernet and 4 NVME slots. No fan and keeps fairly cool even under load.Running it with encrypted zfs volumes and even with a 5bay 3.5 Inch HDD dock attached via USBhttps://wiki.friendlyelec.com/wiki/index.php/CM3588_NAS_Kitreply",
      "Another thing is that unless you have a very specific need for SSDs (such as heavily random access focused workloads, very tight space constraints, or working in a bumpy environment), mechanical hard drives are still way more cost effective for storing lots of data than NVMe. You can get a manufacturer refurbished 12TB hard drive with a multi-year warranty for ~$120, while even an 8TB NVMe drive goes for at least $500. Of course for general-purpose internal drives, NVMe is a far better experience than a mechanical HDD, but my NAS with 6 hard drives in RAIDz2 still gets bottlenecked by my 2.5GBit LAN, not the speeds of the drives.reply",
      "Don\u2019t forget about power.  If you\u2019re trying to build a low power NAS, those hdds idle around 5w each, while the ssd is closer to 5mw. Once you\u2019ve got a few disks, the HDDs can account for half the power or more.  The cost penalty for 2TB or 4TB ssds is still big, but not as bad as at the 8TB level.reply",
      "such power claims are problematic - you're not letting the HDs spin down, for instance, and not crediting the fact that an SSD may easily dissipate more power than an HD under load.  (in this thread, the host and network are slow, so it's not relevant that SSDs are far faster when active.)reply"
    ],
    "link": "https://www.jeffgeerling.com/blog/2025/mini-nases-marry-nvme-intels-efficient-chip",
    "first_paragraph": "",
    "summary": "Title: Mini NASes marry NVMe to Intel's efficient chip\n\nIn a stunning turn of events, techno-hobbyists finally _embrace_ the 21st century by shoehorning Intel's latest Atom chip into cramped closets of Mini NAS devices, enabling the \"highly necessary\" TXT/DRTM feature that you obviously can't live without. As usual, tech enthusiasts spill octabytes of digital ink waxing poetic on the internet forums about potential security enhancements and dreamy hardware scenarios that will almost certainly revolutionize their already efficient use of five-year-old refurbished laptop drives. Cue the inevitable comments about cost-benefit analysis and upgrades (or the tragic lack thereof), because if you're using a commercial product without being able to swap parts like a seasoned mechanic, are you even trying? Brace yourself for the thread where everyone misses the point and debates whether a Mini NAS with an NVMe SSD can indeed replace a 1998 beige box desktop."
  },
  {
    "title": "Being too ambitious is a clever form of self-sabotage (maalvika.substack.com)",
    "points": 102,
    "submitter": "alihm",
    "submit_time": "2025-07-04T21:11:24 1751663484",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=44467912",
    "comments": [
      "> the \"taste-skill discrepancy.\" Your taste (your ability to recognize quality) develops faster than your skill (your ability to produce it). This creates what Ira Glass famously called \"the gap,\" but I think of it as the thing that separates creators from consumers.This resonated quite strongly with me. It puts into words something that I've been feeling when working with AI. If you're new to something and using AI for it, it automatically boosts the floor of your taste, but not your skill. And you end up never slowing down to make mistakes and learn, because you can just do it without friction.reply",
      "This is the disconnect between proponents and detractors of AI.Detractors say it's the process and learning that builds depth.Proponents say it doesn't matter because the tool exists and will always exist.It's interesting seeing people argue about AI, because they're plainly not speaking about the same issue and simply talking past each other.reply",
      "> It's interesting seeing people argue about AI, because they're plainly not speaking about the same issue and simply talking past each other.It's important to realize this is actually a general truth of humans arguing. Sometimes people do disagree about the facts on the ground and what is actually true versus what is bullshit, but a lot of the time what really happens is people completely agree on the facts and even most of the implications of the facts but completely disagree on how to frame them. Doesn't even have to be Internet arguments. A lot of hot-button political topics have always been like this, too.It's easy to dismiss people's arguments as being irrelevant, but I think there's room to say that if you were to interrogate their worldview in detail you might find that they have coherent reasoning behind why it is relevant from their perspective, even if you disagree.Though it hasn't really improved my ability to argue or even not argue (perhaps more important), I've definitely noticed this in myself when introspecting, and it definitely makes me think more about why I feel driven to argue, what good it is, and how to do it better.reply",
      "If anything it's the opposite, except maybe at the very low end: AI boosts implementation skill (at least by increasing speed), but not {research, coding, writing} taste. Hence slop of all sorts.reply",
      "In the spirit of July 4, John Lewis Gaddis explores a similar theme in \"On Grand Strategy\". This is one of my favourite explorations, where he compares Abraham Lincoln and John Quincy Adams:> Compare Lincoln\u2019s life with that of John Quincy Adams. Great expectations inspired, pursued, and haunted Adams, depriving him, at critical moments, of common sense. Overestimations by others\u2014which he then magnified\u2014placed objectives beyond his reach: only self-demotion brought late-life satisfaction. No expectations lured Lincoln apart from those he set for himself: he started small, rose slowly, and only when ready reached for the top. His ambitions grew as his opportunities expanded, but he kept both within his circumstances. He sought to be underestimated.The point -- being too ambitious can slow you down if you're not strategic.reply",
      "It almost seems like a tautology.e.g. By definition the 99.9th percentile person cannot live a 99.999th percentile life, if they did they would in fact be that amazing.reply",
      "> e.g. By definition the 99.9th percentile person cannot live a 99.999th percentile life, if they did they would in fact be that amazing.This seems far too deterministic and I think is contrary to what you're replying to.It sounds more like a 99.999th percentile person[0] that constantly reaches too far too early, before being prepared, will not have a 99.999th percentile life. A 99th percentile person who, on the other hand, does not constantly fail due to over-reach, can easily end up accomplishing more. (And there are many other things that might hold them back too - they might get hit by a car while crossing the street.)[0] in whatever measurement of \"capability\" you have in mindreply",
      "Well the critical thing is that we can\u2019t determine who is at what percentile until after the fact. So for example an early bloomer genius type, who is 99.999th percentile among everyone in the same birth year cohort, could suddenly crash back down towards the average.There\u2019s no practical way to determine that looking forwards in time.reply",
      "The first two sections reminded me of an observation I've made about myself: the more I delay \"doing the thing\" and spend time \"researching\" or \"developing taste\", the more I turn into a critic instead of a creator.> Your taste develops faster than your skill> \"the quality group could tell you why a photograph was excellent\"They are critics now. People with a huge taste-skill gap are basically critics \u2014 first towards themselves and gradually towards others. I don't want to generalize by saying \"critics are just failed creators\", but I've certainly found it true for myself. Trying to undo this change in me and this article kind of said all the words I wanted to hear. :)It's both dense and beautifully written. Feels like every paragraph has something profound to say. This kind of \"optimizing-for-screenshot-shares\" writing usually gets overdone, but since this actually had substance, it was amazing to read.(See how I turned into a critic?)reply",
      "For those who haven\u2019t run across it, I like the man in the arena speech from Theodore Roosevelt to but things in perspective when I turn into a critic, or get harsh feedback from a critic.\u201cIt is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who neither know victory nor defeat.\u201dreply"
    ],
    "link": "https://maalvika.substack.com/p/being-too-ambitious-is-a-clever-form",
    "first_paragraph": "",
    "summary": "**Being too ambitious is a clever form of self-sabotage**\n\nThe latest blogosphere eruption comes from someone brave enough to admit that \"ambition might just be the trendy way to fail.\" Spewing wisdom that your average Substack philosopher might charge you a Patreon tier for, we learn about the \"taste-skill discrepancy\" \u2014 a fancy term for realizing you're not as good as you think. Comment sects quickly transform this idea into an intellectual mud-wrestling match, tossing around Ira Glass quotes as if they were sacred texts. Meanwhile, through the cacophony of AI proponents, TED Talk refugees, and self-help rejects, no one has yet dared to ask the real question: but does it make you happy?"
  },
  {
    "title": "The ITTAGE indirect branch predictor (nelhage.com)",
    "points": 13,
    "submitter": "Bogdanp",
    "submit_time": "2025-07-04T23:57:21 1751673441",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44468999",
    "comments": [
      "I must be missing something here. How would this help predict interpreter dispatch? Those won\u2019t be a function of previous branch history or pc, which may very well be independent of the next opcode. They\u2019d be a function of state in memory or registers.reply",
      "In a hot loop, the next opcode can be predicted quite well from the history of previous opcodes executed, especially once have a couple iterations available in your history. And the opcodes executed in an interpreter are generally equivalent to the dispatch branch target.reply",
      "\"very well may be\" but oftentimes isn't. Branch history does in practice do a very good job of predicting what target you're going to take for an indirect branch.reply",
      "Sure. I can easily see that often being the case for arbitrary code but not interpreter dispatch loops.reply"
    ],
    "link": "https://blog.nelhage.com/post/ittage-branch-predictor/",
    "first_paragraph": "\n\n      Jul  4, 2025\n    \nWhile investigating the performance of the new Python 3.14 tail-calling interpreter, I learned (via this very informative comment from Sam Gross) new (to me) piece of performance trivia: Modern CPUs mostly no longer struggle to predict the bytecode-dispatch indirect jump inside a \u201cconventional\u201d bytecode interpreter loop. In steady-state, assuming the bytecode itself is reasonable stable, modern CPUs achieve very high accuracy predicting the dispatch, even for \u201cvanilla\u201d while / switch-style interpreter loops1!Intrigued, I spent a bit of time reading about just how branch predictors achieve this feat. I found the answer pretty fascinating, so I\u2019m going to try to share the salient high-level features \u2013 as I understand them \u2013 as well as some interesting connections and ideas that came up in response.A quick caveat: I am not a hardware engineer or CPU designer, and I\u2019m mostly going to be focused on some high-level ideas I find interesting. I\u2019ll probably get some th",
    "summary": "Title: **The Amazing Tales of Predicting the Unpredictable Opcode**\n\nToday on <em>nelhage.com</em>, we delve into the riveting world of indirect branch prediction with the intrigue of a sloth crossing the road. Watch in awe as our intrepid author, armed with the mighty Python 3.14 tail-calling interpreter, unravels the mysteries of CPU behavior that most of us glossed over in CompSci 101. It's a wild ride through bytecode jungles and switch-style loops, explained with enough caveats to make a lawyer blush. Meanwhile, the comment section morphs into the blind leading the blind, featuring a spectacular miss-the-point fest. Could their next opcode predict the usefulness of this discussion? \ud83d\ude44\ud83e\udd37\u200d\u2642\ufe0f"
  },
  {
    "title": "EverQuest (filfre.net)",
    "points": 169,
    "submitter": "dmazin",
    "submit_time": "2025-07-04T16:10:13 1751645413",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=44465731",
    "comments": [
      "I was there.My first pay stub had Verant on it, I joined shortly before the SOE transition.One thing maybe not well known outside of the company was that the MMO subscription revenue enabled a hotbed of experimentation. There was an MMO RTS which never shipped, and several other takes on \u201ccan we make genre X an MMO?\u201d that I can\u2019t remember. And then SWG, obviously.EQ2 had all kinds of interesting people on it as a result - Ken Perlin did the lip sync work (driving facial animations from dialog), Brian Hook worked on the rendered for a while. I\u2019m sure there were others.Then there\u2019s all the things we didn\u2019t do. I read the complete Harry Potter series specifically because we were in talks with JK Rowling to do a HP MMO, but negotiations failed.Crazy times.[addendum]\nSeveral of the people in the article are no longer with us (Brad McQuaid, and Kelly Flock at least)The office park that SOE was located in on Terman Court was also demolished years ago. I remember standing at the door to my office on my last day, looking out the window at the eucalyptus trees and thinking I was never going to see the place again.I was right.reply",
      "Comments like this is why I love HN. Thanks for sharing. And RIP to your former colleagues.reply",
      "Thanks, but trust me - they were no saints.Kelly Flock threw the project I was leading under the bus with Sony Pictures on his way out the door when they fired him. I barely saved it.Brad McQuaid, as CEO of Sigil famously didn\u2019t even show up for the meeting where the whole Vanguard team was told the company had failed and they had no jobs, and no severance.The games industry definitely has its heroes, but they ain\u2019t it.reply",
      "I absolutely loved EverQuest and it\u2019s still probably holds some of my fondest gaming memories. My favorite feeling about it is that it felt like a real world first, gameplay second. It had a real sense of danger and wonder that I think will be almost impossible to recreate.Going from Qeynos to Freeport, or crossing the ocean on a boat felt absolutely epic and dangerous. It was wonderful, but not something I would want to play today now that I have real life obligations.reply",
      "It was also at the perfect moment in time where you couldn't just pull up the game's wiki on a second monitor and have fully detailed maps and quest details on hand. You actually had to learn things for yourself by exploration and trial and error. You had to learn things from other people by talking to them in game.In my mind back then, I was in awe of people that even had the knowledge of how to get across certain zones safely. You know it took effort/skill for them to gain that knowledge. You couldn't just look it up.I've been thinking how you could possibly replicate a similar thing nowadays, but unless the world constantly randomly changes over time, rendering any created guides/maps/etc moot, I think that window has closed.reply",
      "I've made an effort in recent years to actively avoid researching wikis and guides on games as I play them.  I've come to think that a lot of the joy in gaming is the discovery and unraveling the systems that make the game tick.  Finding the optimal ways to level or complete some mission through exploration and experimentation is always so much more fulfilling than finding the first result the comes up in google where the answer is already there for you.Admittedly, it does take a degree of willpower and sometimes I will still do some online research when a game gets particularly frustrating.  The biggest obstacle to my approach of avoiding online information is that some games feel like they're designed with that in mind and don't provide enough information in the games for an isolated player to really figure everything out.reply",
      "100% agreed with games being designed for online aids. Some of the quests in Oldschool Runescape make me wonder if I'd ever have completed them without guides - it's like they're designed to be a challenge for the whole community upon release, rather than for individual players.reply",
      "2007 Quest Cape with no guides is a long-standing childhood dream of mine. One I think I will never complete, but still!reply",
      "I too formed memories by playing EQ in a way that was, in retrospect, dumb, and learning from the experience.e.g. I created an Erudite wizard (who could not see in the dark) and insisted on leveling up in Toxxulia forest, the default \"newbie\" zone for Erudites.  It was dark there, even during the day, and pitch black at night.  I kept my monitor at the calibrated brightness level because I didn't want to \"cheat\". Monsters of an appropriate level were spread out and often hard to find.  A troll NPC roamed the forest and randomly killed players.  I spent many hours getting lost (and killed) there before leaving the island, only to discover the comparatively easy newbie zone that stood outside Qeynos, a short, safe, free, ship voyage away.The game was full of stuff like this.  If you wanted to do something, there was usually a very bad way to go about it and other ways that were much better.  Finding those gave you a sense of accomplishment that was far sweeter than mere levels.Modern games tend to be more balanced so you can be assured that, however you're doing something, there probably isn't another way to do it that is vastly  easier unless you're doing something really weird.  This \"wastes\" less of your time, but somehow feels less realistic.  In real life, different strategies for doing things are seldom equal.reply",
      "You have to make the world big and uncharted enough that it can't be picked over quickly. I have some hope that Light No Fire might pull it off.Probably an uncommon experience, but I felt something similar playing Final Fantasy XV. The semi-realistic scale and emptiness of the world map that people complained about actually contributed to the consistent feeling of being out in the wilderness, stumbling on dungeons and whanot. Most open-world games feel like theme parks, Eos felt like a national park. I'm told RDR2 and Death Stranding carry similar vibes.I'd like devs to get a bit more bold about real-world scaling environments. Let a long-ass walk between towns be a long-ass walk between towns. And no mini-maps.reply"
    ],
    "link": "https://www.filfre.net/2025/07/everquest/",
    "first_paragraph": "It isn\u2019t always or even usually the pioneers who reap the rewards of the trails they blaze. As often as not, some pragmatic Johnny-come-lately pops in to make off with the booty.Such was the case in the MMORPG space in the late 1990s. There Ultima Online demonstrated that there was an audience for a persistent fantasy world where people could live out alternative existences together through the magic of the Internet. Yet it was another game called EverQuest that turned the proof of concept into a thriving business that enthralled hundreds of thousands of players for years on end, generating enormous amounts of money in the process. For, while the first-mover advantage should not be underestimated, there\u2019s something to be said for being the second mover as well. EverQuest got to watch from backstage as Ultima Online flubbed line after line and stumbled over assorted pieces of scenery. Then, with a list in hand of what not to do, it was able to stride confidently onto center stage to a s",
    "summary": "<h1>Internet Dragons and Cash Cows: The Legacy of EverQuest</h1>\n\n<i>EverQuest</i>, a name that trembles with the nostalgia of a thousand late-'90s twee fantasies, righteously snatches the baton from the fumbling hands of <i>Ultima Online</i> in the overly earnest MMORPG relay race. Replete with elves, orcs, and an alarming willingness to charge first into the pitfalls so determinedly graced by its predecessors, this digital timesink cunningly transformed rookie mistakes into a wizard's chest of cash. Commenters descend into a wistful haze of \"back in my day\" anecdotes, gleefully swapping tales of virtual heroism and draconian office politics, apparently confusing the comment section for a therapy couch. <em>Remember when</em> edges out <em>what actually mattered</em> as every nostalgic betrayal is laid out like loot in a high-level raid, proving once more that even in epic fantasy, the devil truly is in the detail. \ud83d\udc09\ud83d\udcb0\ud83d\ude22"
  },
  {
    "title": "Version Control for AI Coding (branching.app)",
    "points": 11,
    "submitter": "sheremetyev",
    "submit_time": "2025-07-01T10:00:33 1751364033",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=44432272",
    "comments": [
      "I watched the video and I don't really understand how this maps to the underlying Git operations and what it can do. What happens if I make changes locally while Cursor is doing something? Is this detected properly? (That might be useful.) Can I use it with Claude Code too in some way? Is it primarily for syncing with external tools like Lovable?Also, the ChatGPT generated copy for the landing page is somewhat off-putting.reply",
      "Not sure calling the product Branching is a good idea.  May cause confusion.reply",
      "I\u2019m confused on what this is based on the landing page.Version control isn\u2019t specific to A.I workflows, what does this add on top of git?Is this a worktree type solution so you could make parallel edits?reply",
      "Take a look at my thoughts on version control and vibecodinghttps://github.com/TZubiri/keyboard-transpositions-checkerMy idea is that we should not commit LLM written code, but rather we should commit the prompts. The LLM prompts are source code, the LLM code is target code. If you use typescript and scss, you would commit that, not the generated js and css.That LLMs are typically non-deterministic introduces some issue, but surely you can achieve determinism with seeds, specific model revisions and 0 temperature.reply"
    ],
    "link": "https://branching.app",
    "first_paragraph": "Code now comes from you, your teammates, and AI helpers\u2014changes happen every few seconds, but the old Git ritual keeps breaking the rhythm.Branching brings the flow back.Your working copy stays current: Branching fetches and shares changes continuously, no push/pull delay.You work on the latest state and avoid broken builds or surprise rewinds\u2014flow preserved, no context-switches.When edits overlap, Branching resolves the conflict automatically and lets you review or tweak the result if you want.You keep momentum instead of wrestling with merge markers, and your history stays clean and linear.Branching reads and writes a plain Git repo, so PRs, Actions, and every CLI or IDE workflow still work.Zero migration risk\u2014install or remove Branching at any time without touching your repo history or pipeline.\"Finally, version control feels effortless.\"Free signup, no credit card required",
    "summary": "Welcome to the future of AI coding with Branching, where your codebase morphs faster than your caffeine-fueled typing can handle, and Git rituals are as outdated as dial-up internet. Branching promises a utopian \"flow\" where merges solve themselves like magic, assuming you believe in unicorns and effortless tech. Meanwhile, the commenters play a merry-go-round of confusion - worried about compatibility with their favorite AI overlords and horrified by marketing copy likely penned by a rogue ChatGPT with a sense of humor. Does anyone else smell the burning aroma of overpromising tech? \ud83e\udd14 \ud83c\udfaa"
  },
  {
    "title": "Robots move Shanghai city block [video] (youtube.com)",
    "points": 55,
    "submitter": "surprisetalk",
    "submit_time": "2025-07-03T14:02:16 1751551336",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44455196",
    "comments": [
      "Hao's moving castlereply",
      "A few years ago they moved a (historic) train station where I lived. It needed to be moved for some underground tube construction, but also a few meters to make the new buildings fit. I witnessed, it was awesome.https://www.e-architect.com/images/jpgs/leipzig/bayerischer_... / https://www.e-architect.com/leipzig/bayerischer-bahnhof-buil...reply",
      "This title is misleading. It makes it seem like the robots did this autonomously, when in reality hundreds if workers were involved. The \u201crobots\u201d were \u201csmart jacks\u201d I would say. Humans couldn\u2019t have done this without hydraulic jacks, they used fancy hydraulic jacks.reply",
      "I was not really lead to believe they did this autonomously. It seemed to me like either (a) they were doing the lockstep in a pre-programmed way that required timing of the equipment working together or (b) the same but with humans operating the timing. In either case I find the use of robots impressive.reply",
      "It is still a very impressive feat of engineering.reply",
      "I dont understand this. I always thought houses/buildings have underground supports on which the structure is erected. Doesn't have to be tall towers, all small buildings have underground support too.How come these buildings don't have any of that? Or is the support in form of metal rods which these structures are freely screwed to?reply",
      "I found this because I had a similar question, I think it might be hard to gauge how much prep work was done from the video.https://parametric-architecture.com/shanghai-relocates-7500-...The houses: https://shanghaistreetstories.com/?page_id=1288reply",
      "This is incredible -- serious question -- has anything of this scale been done in the US or Europe? Do we even have the technology?reply",
      "Check out the raising of Chicago (https://en.wikipedia.org/wiki/Raising_of_Chicago). From buildings up to entire city blocks were raised, moved on rollers, or both, usually while businesses and residents stayed in them for normal day-to-day life.reply",
      "In 1930 they moved an entire telephone exchange in Indianapolis without even taking it offline: https://indianahistory.org/blog/instead-of-moving-mountains-...The technology in this video appears to be computer control of the many pistons underneath the raised block. I would estimate that could be done with roughly 1970s-level of technology.reply"
    ],
    "link": "https://www.youtube.com/watch?v=7ZccC9BnT8k",
    "first_paragraph": "",
    "summary": "**When Robots Play LEGO with Shanghai**\n\nShanghai's recent block-moving extravaganza manages to be utterly <em>unremarkable</em> yet sensationalized by a YouTube video claiming \"Robots move city block,\" sparking an outburst of equally gullible and pedantic comments. Commenters are torn between awe and architectural ignorance, debating whether the feat involved autonomous robots or an army of under-credited humans maneuvering glorified hydraulic jacks. Meanwhile, historical flexing ensues as users compare the event to moving a train station in Leipzig, hoisting Chicago, or teleporting an entire telephone exchange in Indianapolis\u2014all done with what seems like a string and a prayer from our robot overlords. Who knew relocating buildings could make armchair engineers out of YouTube viewers? \ud83e\udd16\ud83c\udfd9\ufe0f"
  },
  {
    "title": "How to Incapacitate Google Tag Manager and Why You Should (2022) (backlit.neocities.org)",
    "points": 123,
    "submitter": "fsflover",
    "submit_time": "2025-07-04T18:12:25 1751652745",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=44466697",
    "comments": [
      "I have an idea that another way of preventing being tracked is just massively spamming trash in the data layer object, pushing thousands of dollars worth of purchase events and such, pushing randomly generated user details and other such events. Perhaps by doing this your real data will be hard to filter out. A side effect is also that data becomes unreliable overall, helping less privacy aware people in the process.reply",
      "Now there\u2019s a fun idea!! I wonder how difficult it would be to spoof events.Edit: looks like this might exist already:\nhttps://addons.mozilla.org/en-US/firefox/addon/adnauseam/reply",
      "Since installing it on firefox on this computer (18 months ago or so) Ad Nauseam has clicked ~$38,000 worth of ads, that i never saw.Between this and \"track me not\" i've been fighting back against ads and connecting my \"profile\" with any habits since 2016 or so. I should also note i have pihole and my own DNS server upstream, so that's thiry-eight grand in ad clicks that got through blacklists.https://www.trackmenot.io/faqreply",
      "[Preface: I hate ads, I love uBlock origin, I use pihole, I'm a proponent of ad blockers]I manage a Google Ads account with a $500,000 budget. That budget is spent on a mix of display ads, google search, and youtube ads.If I knew that 10% of our budget was wasted on bot clicks, there's nothing I can do as an advertiser. We can't stop advertising... we want to grow our business and advertising is how you get your name out there. We also can't stop using Google Ads - where else would we go?$38,000 in clicks boosts Google's revenue by $38k (Google ain't complaining). The only entity you're hurting are the advertisers using Google. Advertisers might see their campaigns performing less well, but that's not going to stop them from advertising. If anything, they'll increase budgets to counteract the fake bot clicks.I really don't understand what Ad Nauseam is trying to achieve. It honestly seems like it benefits Google more than it hurts them. It directly hurts advertisers, but not enough that it would stop anyone from advertising.Google has a system for refunding advertisers for invalid clicks. The $500k account that I manage gets refunded about $50/month in invalid clicks. I'm guessing if bot clicks started making a real dent in advertiser performance, Google would counter that by improving their bot detection so they can refund advertisers in higher volumes. If there's ever an advertiser-led boycott of Google Ads, Google would almost certainly respond by refunding advertisers for bot clicks at much higher rates.reply",
      "You know, I'm not too worried that I'm making the lives of people who spy on me harder and wasting their money.You don't have to buy privacy violating ads. You don't have to buy targetted adsreply",
      "> I really don't understand what Ad Nauseam is trying to achieve. It honestly seems like it benefits Google more than it hurts them.Google is part of the problem, but they're neither the only ones nor best to target through bottom-up approaches.> It directly hurts advertisers, but not enough that it would stop anyone from advertising.You know the saying about XML - if it doesn't solve the problem, you are not using enough of it.> there's nothing I can do as an advertiser. We can't stop advertising...We know. The whole thing is a cancer[0], a runaway negative feedback loop. No single enlightened advertiser can do anything about it unilaterally. Which is why the pressure needs to go up until ~everyone wants change.--[0] - https://jacek.zlydach.pl/blog/2019-07-31-ads-as-cancer.htmlreply",
      "> I'm guessing if bot clicks started making a real dent in advertiser performance, Google would counter that by improving their bot detection so they can refund advertisers in higher volumes.They already have methods to detect a lot. Like you said yourself, customers have no alternative, so why would they refund money they don't have to?reply",
      "Hopefully it puts my browsers on an bot blocklist, which then invalidates the tracking profile and eliminates targeted advertising entirely.reply",
      "The problem with being on google's bot blocklist is you'll suddenly discover that recaptcha is used in a heck of a lot of places.reply",
      "My assumption with something as hostile as ad nauseum is that you were running the risk of Google profile bansreply"
    ],
    "link": "https://backlit.neocities.org/incapacitate-google-tag-manager",
    "first_paragraph": "\"We're long past the days when it was possible to simply say \"no\" to corporate stalking without consequence. Today, when we say \"no\", we get punished for it. But that only goes to show WHY, more than ever, we should be saying \"no\".\"Google Tag Manager. It's a product which, by design, cloaks a range of the Internet's most invasive and unethical scripts in an opaque closet, then springs them out in disguise. Combining immense power with obfuscation and vast scale of use, Google Tag Manager is the WWW's single most destructive tool to public privacy and online ethicism.And it's getting worse. Google is now driving Tag Manager into the first-party domain, switching from third-party to first-party cookie usage, for example. Whilst this may look like a warm-hearted bid to increase privacy protection for the public, it's really just part of Google's relentless string of attempts to circumvent third-party content-blocking by shifting surveillanceware into a first-party container.This probably ",
    "summary": "<h1>The Digital Rebel's Guide to Pretend Privacy</h1>\n\nIn an exhilarating display of cyber-rebellion, the tired souls at backlit.neocities.org treat us to an expos\u00e9 on Google's personal hobby: peeping Tom on a global scale via Google Tag Manager. Marvel as the author details how this insidious tool not only tracks your every move but probably steals the leftovers from your fridge too! Meanwhile, commenters chime in with their own superhero antics, like turning their browsers into disinformation machines that would make even a Cold War spy blush. \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcbb Spoiler: Google still ends up with all the money, and your cat videos remain perilously under-protected. \ud83d\udc31\ud83d\udcb8"
  },
  {
    "title": "The story behind Caesar salad (nationalgeographic.com)",
    "points": 67,
    "submitter": "Bluestein",
    "submit_time": "2025-07-04T19:45:08 1751658308",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=44467312",
    "comments": [
      "https://archive.is/2025.07.04-200114/https://www.nationalgeo...reply",
      "Ave! :)reply",
      "Hint - it existed long before they claim it did. I have found similar recipes for dressing going back hundreds of years.Also what's with the lazy restauranteurs allowing their employees to serve lettuce without even chopping it? That's a deal breaker for me, if I am expected to chop the lettuce myself I'm ordering tap water only and no food and never ever EVER going back lol.reply",
      "A classic Caesar uses whole leaves; the dish was originally meant to be eaten with  hands. You can have whatever preferences you like, but I don't think the attitude you're expressing it with is helpful.reply",
      "Same energy as complaining their pizza and steak isn\u2019t cut for them.reply",
      "Some people are hungry so they want it cut into more slices.reply",
      "It\u2019s worth having the original at Caesar\u2019s in Tijuana. Absolutely delicious.reply",
      "The one I make - no idea if it is authentic, but the family gobble it up https://www.bbcgoodfood.com/recipes/chicken-caesar-saladreply",
      "I think it\u2019s worth it making your own dressing.https://www.instagram.com/reel/DEBMPGxxn_t/reply",
      "The first chicken Caesar salad I ever had was, I believe, at Metro Grill during the summer of 2006. I was not (and still am not) much of a salad fan, but that was the salad that made me say \"maybe I can learn to like salad.\"reply"
    ],
    "link": "https://www.nationalgeographic.com/travel/article/story-behind-caesar-salad",
    "first_paragraph": "Caesar salad celebrates its 101st birthday in July 2025. We look at the origins of the popular dish \u2014 surprisingly, it doesn\u2019t involve a certain Roman emperor.Crisp, fresh and satisfying, Caesar salad is a dish that\u2019s conquered dining outlets the world over, from your neighbourhood bistro and Pret A Manger to Michelin-starred marvels like Osteria Mozza in Los Angeles. While mayonnaise-heavy iterations haunt room-service menus in hotels far and wide, Caesar salad purists live and breathe its original recipe: whole romaine lettuce leaves, crunchy garlic croutons tossed in a tangy, raw-egg-based dressing of minced anchovies and garlic, dijon mustard, lemon, salt and pepper, topped with shaved parmesan.This punchy salad\u2019s basic, accessible ingredients mean it\u2019s a fabulously flexible dish, easy to spruce up, adding extras to the core ingredients. That\u2019s maybe why, in 1953, the Paris-based International Society of Epicures hailed the recipe as \u2018the greatest to originate in the Americas in 50",
    "summary": "<h1>Caesar Salad Conquers the Clueless</h1>\n\n<i>Another year, another haphazard nod to the enigmatic origins of Caesar salad by National Geographic</i>, proving once again that food history is best served half-baked. According to the culinary sleuths who can't differentiate between a Roman emperor and a Tijuana chef, the salad's centennial is as vague and overdressed as the dish itself. Meanwhile, the comment section is a veritable comedic feast, topped with mouth-breathers who debate the existential crisis of cutting lettuce while others provide unsolicited links to their homemade abominations. God save us from foodies who believe ancient Romans dashed Worcestershire sauce on their romaine. \ud83e\udd57\ud83d\ude02"
  },
  {
    "title": "The Amiga 3000 Unix and Sun Microsystems: Deal or No Deal? (datagubbe.se)",
    "points": 28,
    "submitter": "wicket",
    "submit_time": "2025-07-04T21:11:53 1751663513",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.datagubbe.se/amix/",
    "first_paragraph": "datagubbe.se \u00bb\nthe amiga 3000 unix and sun microsystems: deal or no deal?\nSummer 2025\nAmiga lore is full of exciting tales. Many of them are retold to demonstrate how the incompetence of Commodore's management destroyed a platform that, by rights, was destined for success. Coulda, shoulda, and the Amiga woulda risen as rightful ruler of all other computer platforms, forever and ever. Amen.\n\nOne of those stories is about how Sun Microsystems allegedly showed interest in the Amiga 3000 during the early 1990s. It's a classic Amiga anecdote, usually recounted without much reflection, and one I've certainly helped perpetuate.\n\nAlas, the more I think about it, the less it adds up. Fact or factoid? Let's speculate!\n\nThe Amiga 3000 was launched in 1990. Featuring on-board SCSI, several high speed expansion slots, a 25 MHz Motorola 68030 CPU and a 68882 FPU clocked at the same speed, it was certainly a very competent Amiga model. During 1991, Commodore launched a rebadged version dubbed Amiga 3",
    "summary": "**The Ghosts of Technology Past: The Amiga 3000 Saga**\n\nIn yet another exhilarating dive into the annals of tech nostalgia, the intrepid historian at datagubbe.se dusts off the cryptic tales of the Amiga 3000\u2014history's favorite \"what if\" machine. This piece, steeped in the tears of former Commodore aficionados, boldly speculates (again) on whether Sun Microsystems once flirted with the idea of hooking up with the Amiga 3000. Spoiler: they didn't, but don't let the lack of a happy ending or factual basis stop a good yarn. Meanwhile, commenters reenact the Hundred Years War, armed only with CAPS LOCK and semi-functional memories, insisting that, in an alternate universe, they are dictating this comment from their Amiga-powered moon bases. \ud83d\ude80\ud83c\udf1b"
  },
  {
    "title": "Why I left my tech job to work on chronic pain (sailhealth.substack.com)",
    "points": 273,
    "submitter": "glasscannon",
    "submit_time": "2025-07-04T12:50:04 1751633404",
    "num_comments": 174,
    "comments_url": "https://news.ycombinator.com/item?id=44464068",
    "comments": [
      "I felt like I was dying at 35 years old, my body was completely betraying me, exhausted, constant pain, no life as absolutely no energy on days off and still exhausted starting the next week.  Even years in the Army never left me feeling like thatI had no idea it was the misery of the IT job that was causing most of my pain and suffering, and it had nothing to do with the job itself, it was the endless insanity of everyone else around me doing exactly what they were informed would cause problems instead of having discussions with people that actually knew how shit worked.  I was endlessly picking up everyone elses mess and treated worse than a pile of shit all because people were incapable of having a speck of respect for other people since all their hatred for computers fell on meI GTFO of the career of misery and took half a decade to finally start feeling betterI have now spent years and countless hours working on software and I greatly enjoy doing this work again and find I get even more done than I used to simply by doing life the way I need to instead of how some backwards/abusive control freak \"needs it done\"reply",
      "I ended up in a similar situation last year. Amazing job, but typical startup stresses combined with some situational stuff in my personal life (moving, new jobs for partner, kids, day-care changes, etc, etc, etc) left me completely broken.  I ended up leaving my job to take care of my family (thought I was done with my career, but it ended up being a sabbatical - back at the old job and doing great now)It took about 6 months for the brain zaps to start fading. Then another 6 months for me to start feeling capable of really doing my job well.  I'm 18 months into \"recovery\" and I still think I have another 6 to 12 months before I feel like my old self again (so about 2 to 2.5 years in total).Time is really the only solution. You can't just think your way through it. You have to left your body's rewards systems re-adapt and re-learn how to be a healthy, happy human.reply",
      "My grandfather said he experiences stupidity as physically painful. I suppose pain is an indicator that some kind of damage is actually occurring.That's my experience at least, that it's not healthy to be in environments like that for any length of time. In such a place, my regret is always not leaving sooner...reply",
      "Stress is so damaging to our bodies.  Glad you got relief!reply",
      "Can you tell us how you recovered?reply",
      "overall it took time away from all that to recover, I also changed careers for a while to more fulfilling part time workI have always had a passion for computing so I eventually found my way back with a project of my ownreply",
      "Sometimes it requires taking a step back to move forward. Healing takes time. There\u2019s so many odd jobs, side hustles, or simply - working a no-brainer warehouse job, for you to find yourself again.reply",
      "What do you do for a living post-escaping the IT career?reply",
      "I did part time work as a mentor which was way more fulfilling than the IT work.  I eventually found my way back to programming my own projectreply",
      "Roaming a labyrinth and savaging young Athenians might seem like a positive change in the short term, but ultimately it\u2019s probably just as unfulfilling as corporate IT.reply"
    ],
    "link": "https://sailhealth.substack.com/p/why-i-left-my-tech-job-to-work-on",
    "first_paragraph": "",
    "summary": "**The Great Tech Escape**  \nA burnt-out IT martyr discovers that escaping the labyrinth of tech support is the true path to healing body and soul. Cue spinoff of a digital-age Odyssey, where every commenter chimes in with their sob stories, mistaking correlation with causation like it\u2019s an Olympic sport. \ud83e\udd47 Amidst boasts of rediscovery through \u201cfulfilling\u201d part-time gigs and backhanded compliments aimed at warehouse jobs, readers rally around the narrative that everything awful must be your employer\u2019s fault. Don\u2019t miss the riveting saga, complete with a chorus of armchair health experts and the inevitable half-baked return to coding\u2014because <i>freelance desperation</i> is always healthier than structured misery, right? \ud83d\ude43"
  },
  {
    "title": "Larry (cat) (wikipedia.org)",
    "points": 247,
    "submitter": "dcminter",
    "submit_time": "2025-07-04T09:43:04 1751622184",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=44462947",
    "comments": [
      "See also previous Chief Mousers to the Cabinet Office, going back approx 100 years:https://en.wikipedia.org/wiki/Sybil_(cat)https://en.wikipedia.org/wiki/Humphrey_(cat)https://en.wikipedia.org/wiki/Wilberforce_(cat)https://en.wikipedia.org/wiki/Peta_(cat)https://en.wikipedia.org/wiki/Peter_III_(cat)https://en.wikipedia.org/wiki/Peter_II_(cat)https://en.wikipedia.org/wiki/Nelson_(cat)https://en.wikipedia.org/wiki/Munich_Mouserhttps://en.wikipedia.org/wiki/Peter_(chief_mouser)https://en.wikipedia.org/wiki/Rufus_of_Englandreply",
      "> At a cost of about \u00a3100 a year (paid for from the Cabinet Office's budget), most of which went towards food, Humphrey was said to be of considerably better value than the Cabinet's professional pest controller, who charged \u00a34,000 a year and is reported to have never caught a mouse.[3]reply",
      "I like how https://en.wikipedia.org/wiki/Chief_Mouser_to_the_Cabinet_Of... has a timeline and highlights if the Chief Mouser was under a Conservative or Labour government.reply",
      "Of course, as a civil servant, the Chief Mouser is expected to implement government policy impartially.reply",
      "Aww :)The \"rival\" cat at the Foreign & Commonwealth house down the street also has his own wiki lol:\nhttps://en.wikipedia.org/wiki/Palmerston_(cat)reply",
      "Reassuring to see none of them were intentionally killed and only Peter II passed due to an accident.Where I live its exceedingly rare to have an outdoor cat that lives past 10. And they are not even related to unpopular public figures...reply",
      "> Humphrey was found as a stray by a Cabinet Office civil servant and named in honour of Humphrey Appleby, the archetypal civil servant of Yes Minister and Yes, Prime Minister.Love it. Thatcher was famously a big fan of \"Yes Minister\"reply",
      "Wonderfully, the official government webpage[1] lists his duties as:  Larry spends his days greeting guests to the house, inspecting security defences and testing antique furniture for napping quality. His day-to-day responsibilities also include contemplating a solution to the mouse occupancy of the house. Larry says this is still \u2018in tactical planning stage\u2019.\n\n[1] https://www.gov.uk/government/history/10-downing-street#larr...reply",
      "Putting the \"Larry with Boris Johnson in 2019\" photo under the heading of \"Relationships with other animals\" is hilarious, intentional or not.reply",
      "Boris the animal?It's just Boris!reply"
    ],
    "link": "https://en.wikipedia.org/wiki/Larry_(cat)",
    "first_paragraph": "\nLarry (born c.\u2009January 2007) is a British domestic tabby cat who has served as Chief Mouser to the Cabinet Office at 10 Downing Street since 2011. He is cared for by Downing Street staff, and is not the personal property of the prime minister of the United Kingdom. Larry has lived at 10 Downing Street during the premierships of six prime ministers: David Cameron, Theresa May, Boris Johnson, Liz Truss, Rishi Sunak and Keir Starmer. He is the first cat to be officially given the title of Chief Mouser, according to the Downing Street government website.[1]\nLarry was born as a stray cat around January 2007 and later came into the possession of Battersea Dogs & Cats Home. In 2011 he was adopted by Downing Street staff, initially intended to be a pet for Cameron\u2019s children.[2] He was described by Downing Street sources as a \"good ratter\" and as having \"a high chase-drive and hunting instinct\".[3] In 2012 the Home said Larry's publicity had resulted in a 15% increase in cat adoptions.[4]\nSoo",
    "summary": "**The Incomparable Adventures of Larry, the Bureaucratic Feline**\n\nIn an era where global politics swing between tragic and farcical, blessed be we to have the monumental Wikipedia entry of Larry the cat, offering a crucial respite with his heroic tales of rodent control since 2011. Larry, the tabby \"not owned\" by the Prime Minister but supported by British taxpayers, has handled the reigns of chief mouser under six prime ministers, emerging as a beacon of stability in the political chaos of Downing Street. Comment sections, as always a pit of despair and misinformation, serve up dollops of useless trivia about past chief mousers and heartfelt relief that none were murdered due to their policies \u2014 truly, a representation of where our societal priorities lay. Armed with a government-issued title and the Cabinet Office's budget for his gourmet feasts, Larry navigates security, greeting duties, and the infinite mouse-problem, remaining a critical and unreplaceable asset, celebrated in the <em>highest echelons</em> of online encyclopedia entries. \ud83d\udc08\ud83d\udcbc"
  },
  {
    "title": "Show HN: AirBending \u2013 hand gesture based macOS app MIDI controller (nanassound.com)",
    "points": 45,
    "submitter": "bepitulaz",
    "submit_time": "2025-07-04T19:40:02 1751658002",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=44467279",
    "comments": [
      "Not just a \u2018theremin\u2019 \u2014 that totally downplays the power of midi.\nSomeone else mentioned the mimu gloves, and I love the idea of a vision based controller - almost everyone has a phone, tablet or laptop with a camera, especially if theyre making music.I also love that this could blur the lines of music playing and dance.Great job OP, thanks for sharing.reply",
      "I wonder if Linux version will be available.reply",
      "Theremin.Here's someone playing a theremin who's reasonably good.[1][1] https://www.youtube.com/watch?v=K6KbEnGnymkreply",
      "This instruments timbre and tone are literally dream shit to me, so wavy and I dont know\u2014unearthly/worldlyreply",
      "From the article: \"When using AirBending for pitch control, you can lock your gestures to specific musical scales and keys. This ensures every note you play is perfectly in tune with your composition\"Reminds me of the Moog Theremini - that was a fun bit of kit.https://en.wikipedia.org/wiki/Thereminireply",
      "Imogen Heap demonstrating her Mi.Mu gloves: https://www.youtube.com/watch?v=ci-yB6EgVW4Using the gloves during an NPR Tiny Desk concert: https://www.youtube.com/watch?v=3QtklTXbKUQ&t=555sreply",
      "Seems like the same software could be used as a soundtrack for Tai Chi exercises. Would be pretty neat.reply",
      "Great work but wouldn't the iPhone with the lidar depth sensor be a better device?reply",
      "It\u2019s on the plan to expand this app for iPhone. But, I haven\u2019t tried lidar, so I decided to release for macOS first.Also with iPhone, I have to think how to transmit MIDI data to DAW on laptop. Well, most likely via USB or network.reply",
      "Apple devices can do MIDI over Bluetooth. I've used this in the past to send VisionPro hand tracking data as MIDI.reply"
    ],
    "link": "https://www.nanassound.com/products/software/airbending",
    "first_paragraph": "\nGesture-based MIDI Controller\n\nTransform your hand gestures into expressive MIDI control. Using only your Mac's built-in camera and advanced computer vision, it tracks your hand movements in real-time.\n\nAs a standard MIDI controller, AirBending works seamlessly with any digital audio workstation (DAW) including Logic Pro, Ableton Live, Pro Tools, Cubase, and more. Connect to external hardware synthesizers, software instruments, and effects processors. No special drivers or plugins required\u2014just standard MIDI communication that every music production setup understands.\n\nAirBending offers a sophisticated preset system that allows you to customize every aspect of your gesture-to-music mapping. Configure it to use either one hand or both hands for controlling X/Y axis movements, giving you complete flexibility in your musical expression.\nAssign specific MIDI channels for output, allowing you to control multiple hardware or software instruments at once. Perfect for complex studio setups an",
    "summary": "Welcome to the future of making music by flailing wildly at your MacBook! With <i>AirBending</i>, every hopeful DJ can now pretend their erratic hand gestures are controlling epic soundscapes, instead of just confusing onlookers. This groundbreaking app tracks your hand movements via the built-in camera\u2014no extra hardware needed, just pure, unadulterated flapping. Commenters are ecstatic, comparing it to everything from theremins to Tai Chi soundtracks, proving once again that you too can fool people into thinking your dance moves are \"musical expression.\" \ud83c\udfb6\ud83d\ude02"
  },
  {
    "title": "Continue (YC S23) is hiring software engineers in San Francisco (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-07-04T21:00:13 1751662813",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/continue/jobs",
    "first_paragraph": "Create, share, and use custom AI code assistantsWe believe there is an opportunity to create a future where developers are amplified, not automated. This is why we are enabling developers to create, share, and use custom AI code assistants with our open-source IDE extensions and hub of models, rules, prompts, docs, and other building blocks.Headquartered in San Francisco, Continue (YC S23) is funded by Heavybit and angels like Julien Chaumond (co-founder of Hugging Face), Lisha Li (founder of Rosebud AI), and Florian Leibert (co-founder of Mesosphere).With 26k+ GitHub stars, 2M+ downloads, and many large organizations like Siemens rolling out Continue, we are building a team to tackle the biggest challenges at the intersection of AI and developer tools.\u00a9 2025 Y Combinator",
    "summary": "Title: Continue (YC S23) bravely battles to bury developers in buzzwords\n\nIn a valiant effort to prevent developers from doing any real coding, Continue (YC S23) offers the revolutionary chance to play digital Lego with AI code assistants instead. Funded by everyone\u2019s favorite angel investors and headquartered in the haven of unaffordable housing, San Francisco, Continue is the new beacon of hope for those who believe the true path to software nirvana is paved with \"open-source IDE extensions\" and over-hyped GitHub stats. Apparently, having 26k+ GitHub stars equates to market dominance, convincing heavy hitters like Siemens that they, too, need more AI in their semicolons. Meanwhile, enthusiastic commenters are busy debating whether this will make their coffee brew faster or just help them automate their job into oblivion. \ud83e\udd16\ud83d\udcac\ud83d\ude80"
  },
  {
    "title": "Everything around LLMs is still magical and wishful thinking (dmitriid.com)",
    "points": 167,
    "submitter": "troupo",
    "submit_time": "2025-07-04T21:16:57 1751663817",
    "num_comments": 175,
    "comments_url": "https://news.ycombinator.com/item?id=44467949",
    "comments": [
      "One thing I find frustrating is that management where I work has heard of 10x productivity gains. Some of those claims even come from early adopters at my work.But that sets expectation way too high. Partly it is due to Amdahl's law: I spend only a portion of my time coding, and far more time thinking and communicating with others that are customers of my code. Even if does make the coding 10x faster (and it doesn't most of the time) overall my productivity is 10-15% better. That is nothing to sneeze at, but it isn't 10x.reply",
      "Maybe it's due to a more R&D-ish nature of my current work, but for me, LLMs are delivering just as much gains in the \"thinking\" part as in \"coding\" part (I handle the \"communicating\" thing myself just fine for now). Using LLMs for \"thinking\" tasks feels similar to how mastering web search 2+ decades ago felt. Search engines enabled access to information provided you know what you're looking for; now LLMs boost that by helping you figure out what you're looking for in the first place (and then conveniently searching it for you, too). This makes trivial some tasks I previously classified as hard due to effort and uncertainty involved.At this point I'd say about 1/3 of my web searches are done through ChatGPT o3, and I can't imagine giving it up now.(There's also a whole psychological angle in how having LLM help sort and rubber-duck your half-baked thought makes many task seem much less daunting, and that alone makes a big difference.)reply",
      "This, and if you add in a voice mode (e.g. ChatGPT's Advanced Mode), it is perfect for brainstorming.Once I decide I want to \"think a problem through with an LLM\", I often start with just the voice mode. This  forces me to say things out loud \u2014 which is remarkably effective (hear hear rubber duck debugging) \u2014 and it also gives me a fundamentally different way of consuming the information the LLM provides me. Instead of being delivered a massive amount of text, where some information could be wrong, I instead get a sequential system where I can stop/pause the LLM/redirect it as soon as something gets me curious or as I find problems with it said.You would think that having this way of interacting would be limiting, as having a fast LLM output large chunks of information would let you skim through it and commit it to memory faster. Yet, for me, the combination of hearing things and, most of all, not having to consume so much potentially wrong info (what good is it to skim pointless stuff), ensures that ChatGPT's Advanced Voice mode is a great way to initially approach a problem.After the first round with the voice mode is done, I often move to written-form brainstorming.reply",
      "> One thing I find frustrating is that management where I work has heard of 10x productivity gains. Some of those claims even come from early adopters at my work.Similar situation at my work, but all of the productivity claims from internal early adopters I've seen so far are based on very narrow ways of measuring productivity, and very sketchy math, to put it mildly.reply",
      "I don't disagree with your assessment of the world today, but just 12 months ago (before the current crop of base models and coding agents like Claude Code), even that 10X improvement of writing some-of-the-code wouldn't have been true.reply",
      ">  just 12 months ago (before the current crop of base models and coding agents like Claude Code), even that 10X improvement of writing some-of-the-code wouldn't have been true.You had to paste more into your prompts back then to make the output work with the rest of your codebase, because there weren't good IDEs/\"agents\" for it, but you've been able to get really really good code for 90% of \"most\" day to day SWE since at least OpenAI releasing the ChatGPT-4 API, which was a couple years ago.Today it's a lot easier to demo low-effort \"make a whole new feature or prototype\" things than doing the work to make the right API calls back then, but most day to day work isn't \"one shot a new prototype web app\" and probably won't ever be.I'm personally more productive than 1 or 2 years ago now because the time required to build the prompts was slower than my personal rate of writing code for a lot of things in my domain, but hardly 10x. It usually one-shots stuff wrong, and then there's a good chance that it'll take longer to chase down the errors than it would've to just write the thing - or only use it as \"better autocomplete\" - in the first place.reply",
      "> I don't disagree with your assessment of the world today, but just 12 months ago (before the current crop of base models and coding agents like Claude Code), even that 10X improvement of writing some-of-the-code wouldn't have been true.So? It sounds like you're prodding us to make an extrapolation fallacy (I don't even grant the \"10x in 12 months\" point, but let's just accept the premise for the sake of argument).Honestly, 12 months ago the base models weren't substantially worse than they are right now. Some people will argue with me endlessly on this point, and maybe they're a bit better on the margin, but I think it's pretty much true. When I look at the improvements of the last year with a cold, rational eye, they've been in two major areas:  * cost & efficiency\n\n  * UI & integration\n\nSo how do we improve from here? Cost & efficiency are the obvious lever with historical precedent: GPUs kinda suck for inference, and costs are (currently) rapidly dropping. But, maybe this won't continue -- algorithmic complexity is what it is, and barring some revolutionary change in the architecture, LLMs are exponential algorithms.UI and integration is where most of the rest of the recent improvement has come from, and honestly, this is pretty close to saturation. All of the various AI products already look the same, and I'm certain that they'll continue to converge to a well-accepted local maxima. After that, huge gains in productivity from UX alone will not be possible. This will happen quickly -- probably in the next year or two.Basically, unless we see a Moore's law of GPUs, I wouldn't bet on indefinite exponential improvement in AI. My bet is that, from here out, this looks like the adoption curve of any prior technology shift (e.g. mainframe -> PC, PC -> laptop, mobile, etc.) where there's a big boom, then a long, slow adoption for the masses.reply",
      "12 months ago, if I fed a list of ~800 poems with about ~250k tokens to an LLM and asked it to summarize this huge collection, they would be completely blind to some poems and were prone to hallucinating not simply verses but full-blown poems. I was testing this with every available model out there that could accept 250k tokens. It just wouldn't work. I also experimented with a subset that was at around ~100k tokens to try other models and results were also pretty terrible. Completely unreliable and nothing it said could be trusted.Then Gemini 2.5 pro (the first one) came along and suddenly this was no longer the case. Nothing hallucinated, incredible pattern finding within the poems, identification of different \"poetic stages\", and many other rather unbelievable things \u2014 at least to me.After that, I realized I could start sending in more of those \"hard to track down\" bugs to Gemini 2.5 pro than other models. It was actually starting to solve them reliably, whereas before it was mostly me doing the solving and models mostly helped if the bug didn't occur as a consequence of very complex interactions spread over multiple methods. It's not like I say \"this is broken, fix it\" very often! Usually I include my ideas for where the problem might be. But Gemini 2.5 pro just knows how to use these ideas better.I have also experimented with LLMs consuming conversations, screenshots, and all kinds of ad-hoc documentation (e-mails, summaries, chat logs, etc) to produce accurate PRDs and even full-on development estimates. The first one that actually started to give good results (as in: it is now a part of my process) was, you guessed it, Gemini 2.5 pro. I'll admit I haven't tried o3 or o4-mini-high too much on this, but that's because they're SLOOOOOOOOW. And, when I did try, o4-mini-high was inferior and o3 felt somewhat closer to 2.5 pro, though, like I said, much much slower and...how do I put this....rude (\"colder\")?All this to say: while I agree that perhaps the models don't feel like they're particularly better at some tasks which involve coding, I think 2.5 pro has represented a monumental step forward, not just in coding, but definitely overall (the poetry example, to this day, still completely blows my mind. It is still so good it's unbelievable).reply",
      "Your comment warrants a longer, more insightful reply than I can provide, but I still feel compelled to say that I get the same feeling from o3. Colder, somewhat robotic and unhelpful. It's like the extreme opposite of 4o, and I like neither.My weapon of choice these days is Claude 4 Opus but it's slow, expensive and still not massively better than good old 3.5 Sonnetreply",
      "12 months ago, we had no reasoning models and even very basic arithmetic was outside of the models' grasp.  Coding assistants mostly worked on the level of tab-completing individual functions, but now I can one-shot demo-able prototypes (albeit nothing production-ready) of webapps. I assume you consider the latter \"integration\", but I think coding is so key to how the base models are being trained that this is due to base model improvements too. This is testable - it would be interesting to get something like Claude Code running on top of a year-old open source model and see how it does.If you're going to call all of that not substantial improvement, we'll have to agree to disagree. Certainly it's the most rapid rate of improvement of any tech I've personally seen since I started programming in the early '00s.reply"
    ],
    "link": "https://dmitriid.com/everything-around-llms-is-still-magical-and-wishful-thinking",
    "first_paragraph": "",
    "summary": "In the latest blogosphere extravaganza, <em>\"Everything around LLMs is still magical and wishful thinking,\"</em> we discover the shockingly novel idea that promising 10x productivity gains from AI might just be a tad overstated\u2014a real jaw-dropper there. Commenters furiously tap away, alternating between mild skepticism and breathless awe, as they dive into tortuous explanations of how their AI overlords are miraculously handling both their code and their morning coffee routine. Some gem in the rough declares a newfound synergy in \"thinking\" thanks to LLMs, likening it to the profound innovation previously known as 'using a search engine.' Meanwhile, another brave soul still reeling from the miseries of yesteryear's tech continues to plead for sanity in productivity metrics, holding a faint hope that maybe, just this once, AI might not just be a glorified autocomplete on steroids. \ud83e\udd16\ud83d\udcab"
  },
  {
    "title": "Show HN: I AI-coded a tower defense game and documented the whole process (github.com/maciej-trebacz)",
    "points": 199,
    "submitter": "M4v3R",
    "submit_time": "2025-07-04T12:34:49 1751632489",
    "num_comments": 117,
    "comments_url": "https://news.ycombinator.com/item?id=44463967",
    "comments": [
      "I'm really enjoying reading over the prompts used for development: (https://github.com/maciej-trebacz/tower-of-time-game/blob/ma...)A lot of posts about \"vibe coding success stories\" would have you believe that with the right mix of MCPs, some complex claude code orchestration flow that uses 20 agents in parallel, and a bunch of LLM-generated rules files you can one-shot a game like this with the prompt \"create a tower defense game where you rewind time. No security holes. No bugs.\"But the prompts used for this project match my experience of what works best with AI-coding: a strong and thorough idea of what you want, broken up into hundreds of smaller problems, with specific architectural steers on the really critical pieces.reply",
      "I tried to build a simple static HTML game for the board game Just One, where you get a text box, type a word in, and it's shown full screen on the phone. There's a bug where, when you type, the text box jumps around, and none of the four LLMs I tried managed to fix it, no matter how much I prompted them. I don't know how you guys manage to one-shot entire games when I can't even stop a text box from jumping around the screen :(reply",
      "Browser text entry on mobile phones is notoriously hard to get right and some bugs are literally unfixable [1]. I'm a frontend developer in my day job and I struggled with this even before AI was a thing. I think you just accidentally picked one of the hardest tasks for the AI to do for you.[1] Example: https://www.reddit.com/r/webdev/comments/xaksu6/on_ios_safar...reply",
      "Huh, that's actually my exact bug. I didn't realize this was so hard, thank you.reply",
      "I have a reasonably good solution for this project of mine you might find useful:https://grack.com/demos/adventure/The trick for me was just using a hidden input and updating the state of an in game input box. The code is ancient by today's standards but uses a reasonably simple technique to get the selection bounds of the text.It works with auto complete on phones and has been stable for a decade.reply",
      "That's promising, thank you! I'll ask the LLM to implement it.reply",
      "> what works best with AI-coding: a strong and thorough idea of what you want, broken up into hundreds of smaller problems, with specific architectural steers on the really critical piecesThis has worked extremely well for me.reply",
      "I have been working on an end-to-end modeling solution for my day job and I'm doing it entirely w/Claude.I am on full-rework iteration three, learning as I go on what works best, and this is definitely the way. I'm going to be making a presentation to my team about how to use AI to accelerate and extend their day-to-day for things like this and here's my general outline:1. Tell the LLM your overall goal and have it craft a thoughtful product plan from start to finish.2. Take that plan and tell it to break each of the parts into many different parts that are well-planned and thoroughly documented, and then tell it to give you a plan on how to best execute it with LLMs.3. Then go piece by piece, refining as you go.The tool sets up an environment, gets the data from the warehouse, models it, and visualizes it in great detail. It took me about 22 hours of total time and roughly 2 hours of active time.It's beautiful, fast, and fully featured. I am honestly BLOWN AWAY by what it did and I can't wait to see what others on my team do w/this. We could have all done the setup, data ingestion, and modeling, no question; the visualization platform it built for me we absolutely could NOT have done w/the expertise we have on staff--but the time it took? The first three pieces probably were a few days of time, but the last part, I have no idea. Weeks? Months?Amazing.reply",
      "I wrote a whole PRD for this very simple idea, but still the bug persisted, even though I started from scratch four times. Granted, some had different bugs.reply",
      "Have you tried with both Claude opus 4 and Gemini 2.5 pro?reply"
    ],
    "link": "https://github.com/maciej-trebacz/tower-of-time-game",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Vibe coded Tower Defense type of game made for a game jam\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.Entry for Beginner's Jam Summer 2025A time-traveling tower defense game where you defend your base against waves of enemies using the power to rewind time.\ud83c\udfae Play the game at https://m4v3k.itch.io/tower-of-timeTower of Time is a unique tower defense game that combines strategic building with time manipulation mechanics. When enemies overwhelm your defenses, use your time-travel powers to rewind and rebuild your strategy. The game features multiple tower types, energy management, and wave-based enemy spawning.Game supports both keyboard and gamepad.This game serves as a proof of concept for AI-assisted game development. Approximately 95% of the codebase wa",
    "summary": "**Today on Hacker News:** Yet another game developer unleashes their \"groundbreaking\" AI-coded tower defense time traveler, now with a <em>whopping</em> 95% AI-written codebase\u2014a miraculous feat if you ignore the basic functionality hitches that plague every second indie tech demo on itch.io. Full of hopeful bytes and waves of commenters astounded by the sheer notion of AI not burning down a virtual house, they exchange GitHub stars like Pok\u00e9mon cards, blissfully ignoring that their time-travel game is as bogged down in loading errors as their career prospects. Meanwhile, the comments section has evolved into a support group for those battling the pervasive horrors of mobile input fields\u2014a testament to the resilience of human denial.  Can't wait to see this tossed onto the ever-growing pile of half-finished projects that are \"just one more tweak\" from greatness. \ud83c\udfae"
  },
  {
    "title": "Writing a Game Boy Emulator in OCaml (linoscope.github.io)",
    "points": 212,
    "submitter": "ibobev",
    "submit_time": "2025-07-04T09:34:56 1751621696",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=44462896",
    "comments": [
      "Would anyone here assert that there's any particular programming language that's better for writing emulators, virtual machines, bytecode interpreters, etc?Where, when I say \"better\", I'm not so much talking about getting results that are particularly efficient/performant; nor in making fewer implementation errors... but more in terms of the experience of implementing an emulator in this particular language, being more rewarding, intuitive, and/or teaching you more about both emulators and the language.I ask because I know that this sort of language exists in other domains. Erlang, for example, is particularly rewarding to implement a \"soft-realtime nine-nines-of-uptime distributed system\" in. The language, its execution semantics, its runtime, and its core libraries, were all co-designed to address this particular problem domain. Using Erlang \"for what it's for\" can thus teach you a lot about distributed systems (due to the language/runtime/etc guiding your hand toward its own idiomatic answers to distributed-systems problems \u2014 which usually are \"best practice\" solutions in theory as well); and can lead you to a much-deeper understanding of Erlang (exploring all its corners, discovering all the places where the language designers considered the problems you'd be having and set you up for success) than you'd get by trying to use it to solve problems in some other domain.Is there a language like that... but where the \"problem domain\" that the language's designers were targeting, was \"describing machines in code\"?reply",
      "Haskell excels at DSLs and the sort of data manipulation needed in compilers. OCaml, Lisp, and really any language with support for ADTs and such things do the trick as well. You can even try hard with modern C++ and variant types and such, but it won't be as pretty.Of course, if you actually want to run games on the emulator, C or C++ is where the game is. I suppose Rust would work too, but I can't speak much for its low-level memory manipulation.reply",
      "Haskell and OCaml are excellent for compilers, because - as you suggest - you end up building, walking, and transforming tree data structures where sum types are really useful. Lisp is an odd suggestion there, as it doesn\u2019t really have any built-in support for this sort of thing.At any rate, that\u2019s not really the case when building an emulator or bytecode interpreter. And Haskell ends up being mostly a liability here, because most work is just going to be imperatively modifying your virtual machine\u2019s state.reply",
      "I\u2019d also point out, that even in the compiler space, there are basically no production compilers written in Haskell and OCaml.I believe those two languages themselves self-host. So not saying it\u2019s impossible. And I have no clue about the technical merits.But if you look around programming forums, there\u2019s this ideas that\u201dOcaml is one of the leading languages for compiler writers\u201d, which seems to be a completely made up statistic.reply",
      "Obviously C is the ultimate compiler of compilers.But I would call Rust, Haxe and Hack production compilers. (As mentioned by sibling, Rust bootstraps itself since its early days. But that doesn't diminish that OCaml was the choice before bootstrapping.)reply",
      "I don't know that many production compilers are in them, but how much of that is compilers tending towards self hosting once they get far enough along these days? My understanding is early Rust compilers were written in Ocaml, but they transitioned to Rust to self-host.reply",
      "> And Haskell ends up being mostly a liability here, because most work is just going to be imperatively modifying your virtual machine\u2019s state.That sounds odd to me. Haskell is great for managing state, since it makes it possible to do so in a much more controlled manner than non-pure languages.reply",
      "Yeah, I don't understand what the \"liability\" here is. I never claimed it was going to be optimal, and I already pointed out C/C++ as the only reasonable choice if you actually want to run games on the thing and get as much performance as possible. But manipulating the machine state in Haskell is otherwise perfect. Code will look like equations, everything becomes trivially testable and REPLable, and you'd even get a free time machine from the immutability of the data, which makes debugging easy.reply",
      "If you're effectively always in a stateful monad, Haskell's purity offers nothing. Code doesn't look like equations, things aren't trivially testable and REPLable, you don't get a free time machine, and there's syntactic overhead from things like lifting or writes to deeply nested structures and arrays, since the language doesn't have built-in syntactic support for them.reply",
      "On the other hand, it does have support for things like side-effectful traversals, folds, side effects conditional on value existing, etc. In most other languages you have to write lower-level code to accomplish the same thing.reply"
    ],
    "link": "https://linoscope.github.io/writing-a-game-boy-emulator-in-ocaml/",
    "first_paragraph": "For the past few months, I have been working on a project called CAMLBOY, a Game Boy emulator written in OCaml that runs in the browser. You can try it out on the following demo page:Demo PageI included several homebrew ROMs in the demo, so please try them out (I recommend Bouncing ball and Rocket Man Demo). You can also play with it in your mobile browser as it runs at 60 FPS on recent smartphones.You can find the repository here:https://github.com/linoscope/CAMLBOYHave you ever felt like the following when learning a new programming language?These were exactly my thoughts when I started to study OCaml seriously a few months ago. I understood the basics of the language by reading books and implementing simple algorithms, but the above two \u201cdon\u2019t know\"s prevented me from feeling like I could really write OCaml. I knew that the only way to get out of this situation was practice, so I started looking for a project to work on.I choose a Game Boy emulator as the project for the following r",
    "summary": "In a daring attempt to escape real-world productivity, a brave OCaml enthusiast decides to resuscitate the Game Boy's ghost with CAMLBOY\u2014an emulator that swaps bullets and race cars for academic purity and type safety. The commenter gallery, ever eager to flaunt niche knowledge, dives into a thrilling discussion about the pros and cons of using Haskell to build stateful emulators\u2014because clearly, conceptual purity trumps playable frame rates any day on Hacker News. Throw in a dash of armchair language design and you've got yourself the perfect recipe for avoiding deadlines while arguing about the best tool to reinvent the wheel. \ud83d\ude44 Meanwhile, Game Boys everywhere remain blissfully unaware of their new existential wrapper."
  },
  {
    "title": "Kepler.gl (kepler.gl)",
    "points": 122,
    "submitter": "9woc",
    "submit_time": "2025-07-04T13:58:03 1751637483",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=44464641",
    "comments": [
      "Nice to see kepler.gl getting some love here!Funny enough, I just released an open-source, opinionated map editor built on top of kepler.gl with their DuckDB integration \u2014 yesterday! If anyone's curious to see how it all fits together, feel free to check it out here: https://github.com/mountayaapp/insight-editor.reply",
      "DuckDB integration is a game-changer for geospatial visualization since it enables client-side processing of massive datasets without server roundtrips, significantly reducing the performance bottlenecks typical in browser-based GIS tools.reply",
      "Thanks. I didnt know this about DuckDB. I will have to check it out.reply",
      "Great to see this. Indeed very useful.\nI am new learner here. Would you kindly share Which version of kepler you are using as base?reply",
      "Glad to see it can be useful. The project relies on the latest kepler.gl version, which is v3.1.8.reply",
      "Foursquare has another open source project worth noting on DuckDB - SQLRoomshttps://sqlrooms.org/\u201cBuild data-centric apps with DuckDB An Open Source React Framework for Single-Node Data Analytics powered by DuckDB\u201dreply",
      "Which is powered by deck.gl, also a very nice library.https://deck.gl/reply",
      "As someone to whom these kind of awesome visualizations are often presented, let me tell you something. Real decisions do not depend on these nice stuff. I sometimes feel sorry for the folks who spend great effort in producing these, like a children amusing themselves with the great sand castles they built. A lot of times, simple text or numbers could also have more effect on the decisions.reply",
      "I agree that these visuals rarely drive decisions on their own. They\u2019re more like supporting tools\u2026 useful for framing an argument or guiding a narrative in a presentation, especially when static.Unless it\u2019s interactive or tied to live data the usefulness of the visualization produced is limited.. it\u2019s a shame and it\u2019s something this tools should pivot towardsreply",
      "It\u2019d be great to have contributors join our collaborator summit [0] in the Seattle area this fall!It\u2019s a free event, supported by the OpenJS Foundation, kepler.gl\u2019s host foundation.We\u2019re expecting to have a session from Shan, the creator of Kepler.gl as well as Ilya, the creator of SQLRooms. We\u2019re still accepting session proposals as well [1]![0] https://deck.gl/events/seattle-summit-2025/\n[1] https://github.com/openjs-foundation/summit/issues/450reply"
    ],
    "link": "https://kepler.gl/",
    "first_paragraph": "",
    "summary": "In the latest tech circlejerk, kepler.gl receives a fleeting pat on the back from enthusiastic hobbyists pretending their marginal tweaks to map tools rival the Apollo moon landing. Amid the revelry, a comment bravely declares DuckDB a \"game-changer\" elevating it from a mere database to the geek community\u2019s equivalent of sliced bread. Apparently, in the thrilling world of open-source GIS tools, dropping buzzwords and GitHub links is all it takes to mask the bitter truth: these shiny toys hardly sway real-world decisions. No senior executive ever leaned forward in an earnings call, mesmerized by pretty heatmaps\u2014instead, they likely nod off dreaming of excel sheets. Meanwhile, another commenter, fresh from an introductory coding bootcamp, eagerly inquires about software versions, blissfully unaware they've embarked on a voyage of Titanic-proportions aboard the HMS Overhyped. \ud83d\ude80\ud83d\udcbb"
  },
  {
    "title": "Compression Dictionary Transport (developer.mozilla.org)",
    "points": 72,
    "submitter": "todsacerdoti",
    "submit_time": "2025-07-04T15:07:33 1751641653",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=44465206",
    "comments": [
      "Why can\u2019t browsers/servers just store a standard English dictionary and communicate via indexes?. Anything that isn\u2019t in the dictionary can be sent raw. I\u2019ve always had this thought but don\u2019t see why it isn\u2019t implemented. Might get a bit more involved with other languages but the principle remains the same.Thinking about it a bit more, we are doing this at the character level- a Unicode table, so why can\u2019t we lookup words or maybe even common sentences ?reply",
      "Compression algorithms like Brotli already do this:https://www.rfc-editor.org/rfc/rfc7932#page-28reply",
      "Brotli has a built-in dictionary.reply",
      "Seems like this would result in quite a lot of increased server load.Previously servers would cache compressed versions of your static resources.Whereas now they either have to compress on-the-fly or have a massive cache of not only your most recent static JavaScript blob, but also all past blobs and versions compressed using different combinations of them as a dictionary.This could easily 10x resources needed for serving static html/CSS/js.reply",
      "Presumably you'd generate a standalone compressed form (or forms) as usual, and also compressed forms using several dictionaries.Then the server is doing more work at request time, but it's not meaningfully more work --- just checking if the request path has a dictionary compressed form that matches the dictionary hash provided by the client.reply",
      "The past versions stored clientside are the dictionaries. Serverside, just keep the diffs against, say, the last five versions around if storage is an issue, or whatever gets you some high percentage of returning clients, then rebuild when pushing a new release.reply",
      "This seems like a lot of added complexity for limited gain. Are there cases where gzip and br at their highest compression levels aren\u2019t good enough?reply",
      "Some examples here: https://github.com/WICG/compression-dictionary-transport/blo...show significant gain of using dictionary over compressed w/o dictionary.It seems like instead of sites reducing bloat, they will just shift the bloat to your hard-drive. Some of the examples said dictionary of 1MB which doesn't seem big, but could add up if everyone is doing this.reply",
      "Every piece of information or file that is compressed sends a dictionary along with it. In the case of, say, many HTML or CSS files, this dictionary data is likely nearly completely redundant.There's almost no added complexity since zstd already handles separate compression dictionaries quite well.reply",
      "The standard compressed formats don't literally contain a dictionary. The decompressed data becomes its own dictionary while its being decompressed. This makes the first occurrence of any pattern less efficiently compressed (but usually it's still compressed thanks to entropy coding), and then it becomes cheap to repeat.Brotli has a default dictionary with bits of HTML and scripts. This is built in into the decompressor, and not sent with the files.The decompression dictionaries aren't magic. They're basically a prefix for decompressed files, so that a first occurrence of some pattern can be referenced from the dictionary instead of built from scratch. This helps only with the first occurrences of data near the start of the file, and for all the later repetitions the dictionary becomes irrelevant.The dictionary needs to be downloaded too, and you're not going to have dictionaries all the way down, so you pay the cost of decompressing the data without a dictionary whether it's a dictionary + dictionary-using-file, or just the full file itself.reply"
    ],
    "link": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Compression_dictionary_transport",
    "first_paragraph": "Web technology reference for developersStructure of content on the webCode used to describe document styleGeneral-purpose scripting languageProtocol for transmitting web resourcesInterfaces for building web applicationsDeveloping extensions for web browsersBuild web projects usable for allWeb technology reference for developersLearn web developmentLearn web developmentLearn to structure web content with HTMLLearn to style content using CSSLearn to run scripts in the browserLearn to make the web accessible to allA customized MDN experienceGet real-time assistance and supportAll browser compatibility updates at a glanceLearn how to use MDN PlusFrequently asked questions about MDN PlusWrite, test and share your codeScan a website for freeGet real-time assistance and supportExperimental: This is an experimental technologyCheck the Browser compatibility table carefully before using this in production.Compression Dictionary Transport is a way of using a shared compression dictionary to drama",
    "summary": "**Today in Hacker Sorrows**: Web developers, disparate in talent but uniform in desperation, have stumbled across an \"experimental\" piece of wizardry called Compression Dictionary Transport. Here at developer.mozilla.org, the home of web devs so ancient they remember how to center a <div>, we've concocted a new scheme that involves using shared dictionaries to compress web data. Genius commenters, unable to grasp the basics of how zippers work on their trousers, suggest reinventing compression by turning it into a glorified game of Scrabble. \"Why not use the entire Oxford Dictionary?\", they ponder while aggressively missing the point. Yes, because every server admin dreams of indexing more dictionaries than a linguistics PhD. Surely this will end well. \ud83d\ude44"
  }
]