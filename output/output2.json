[
  {
    "title": "Gorgeous-GRUB: collection of decent community-made GRUB themes (github.com/jacksaur)",
    "points": 73,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-03T22:57:58 1746313078",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=43883040",
    "comments": [
      "What I really would like: something that mimicked the old SGI start up, complete with boot audio and a micro distro for OS setup. These days, with snapshot filesystems, that shouldn't be too hard. Also, I've had to chroot to fix my system in my life a few times; I can't believe that's hard to automate.\n \nreply",
      "On Arch, I think you could install a second \"backup\" copy of Arch Linux on a recovery partition that your motherboard firmware can boot into directly, and then use the `arch-chroot` program to recover your main OS. I'm sure something similar exists for other distros?\n \nreply",
      "I can finally make my computer look like the ones from hackers! Awesome.\n \nreply",
      "Haha, my first thought when I saw the themes was, wow this is exactly like the movie!Might rewatch it soon, been listening to the soundtrack while working lately :)\n \nreply",
      "Stuff like this is why I fell in love with Linux. Some amazing creativity in here. Almost makes me wish I dual booted with something so I\u2019d have an excuse to see grub!\n \nreply",
      "While these are cool, I honestly wish GRUB was silent unless you\u2019re holding a key during boot. The 5 seconds it takes to go away and just boot the OS by default is really unnecessary.\n \nreply",
      "Why would you actually want this? Such a weird desire to hide this kind of stuff because it's so inconsequential in my mind.\n \nreply",
      "I think you can get that by setting `GRUB_TIMEOUT_STYLE=hidden` and `GRUB_TIMEOUT=0`. Then you can hold `Shift` to see the GRUB menu, otherwise it will boot the default option immediately.\n \nreply",
      "Good tip. I've always felt like I'd like to be able to boot directly into the Second Option via a key, etc. like Shift + 2 etc\n \nreply",
      "I'm pretty sure there is an option for this, holding shift makes it show the menu.\n \nreply"
    ],
    "link": "https://github.com/Jacksaur/Gorgeous-GRUB",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Collection of decent Community-made GRUB themes. Contributions welcome!\n      There are many great community made GRUB themes to spice up your bootloader before booting into your system proper. Unfortunately, they're spread across multiple sites and it can be difficult to find good ones. As another user told me, the majority of themes on Pling (the largest host of GRUB themes currently) are fairly low effort and can be boring to trawl through. Hence, I decided to put together this page to bring attention to some decent themes I've found around the internet over time. They aren't all absolute masterpieces of course: But they've all at least had a fair amount of effort put into them, with custom backgrounds, fonts, and colours.And don't forget, themes are extremely easy to customize! Like a theme's layout but prefer a different backgr",
    "summary": "**Gorgeous-GRUB: Bootloader Beautification for Bored Booters**\nIn a stunning display of focus on the _absolutely critical_ aspects of computing, a heroic GitHub user corrals a herd of \"decent\" community-made GRUB themes into one conveniently underwhelming repository. Enthusiasts who find joy in staring at their boot screen three seconds longer, now rejoice! Comments reveal an adorable blend of nostalgia and needless complexity, as users dream of re-living the '90s startup screens while pondering the existential dread of an automated chroot recovery\u2014all while arguing over how many seconds of their lives are wasted by an unhidden GRUB menu. Really, if your bootloader looks nicer than your desktop, it might be time to reconsider your priorities. \ud83d\ude02"
  },
  {
    "title": "Show HN: Free, in-browser PDF editor (breezepdf.com)",
    "points": 331,
    "submitter": "philjohnson",
    "submit_time": "2025-05-03T18:15:45 1746296145",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=43880962",
    "comments": [
      "I tossed a legal document at it which was recently of passing interest to me, and it looks like embedded fonts still need some work. I'm not inclined to share a test case from what I have, which relates to a change of name and in any case was not really prepared by anyone especially competent when it comes to PDFs and their content; I tested with the first, facially void, version I was given. But it is possible I'll find more use for this tool, and if a shareable test case does come along then I'll do so. (And heaven knows with this document format, embedded fonts are a total nightmare always, even somehow in programmatic authoring. I'm not criticizing!)On a similar note, a downloadable (single-file HTML or so, although these days some kind of HTTP service is a practical necessity) version would be nice to have. Low pri even from my perspective; it isn't that I spend a lot of time in places with no cell signal, so much as just that tethering on an a la carte plan gets out of hand pretty quick, since applications aren't at all required to honor or even notice the existence of the \"data saver\" option.This is really neat! Thanks for posting it, I've bookmarked it for later use in the \"just need a quick tweak\" kind of case. I'll look forward to seeing how it develops!\n \nreply",
      "Thank you!I'll take a look at improving rendering embedded font support. And that's a neat idea to be able to download it for offline, I'll give some thought to that. Appreciate your feedback!\n \nreply",
      "Fonts on the web are super hard. There\u2019s hardly any useful browser APIs for the kind of typesetting you need for PDFs: itemization, character bounding boxes, font file sub setting, etc.  To solve or properly, you need to include a lot of code, e.g. all of harfbuzz compiled to wasm.\n \nreply",
      "Nice work. I've been building something lately to manipulate PDFs in the browser for privacy, although it's quite a different use case.I think I see you're using pdf-lib and jspdf - both great libraries, and I'm using both, but:(1) Have you seen the recent WASM compilation of MuPDF? I am also using it for some functions and find it really excellent with accessible APIs and highly functional. Worth an try!(2) We chose different forks of the (unmaintained) pdf-lib - is there a reason you went with `pdf-lib-plus-encrypt`? I chose the cantoo fork, which seemed well-maintained to me - but I didn't research many others so would be interested to know if there is a good reason.\n \nreply",
      "(1) Yes, I am very familiar with the WASM compilation of MuPDF. It's got a lot of great features. I actually built another product pdfredactoronline.com that does redaction fully in the browser using the MuPDF WASM compilation. The reason I don't use it in BreezePDF is MuPDF has an APGL license which requires open-sourcing any code that uses their software. Which, I guess technically anything fully browser based is essentially open sourced :) so perhaps I could use it here.Since a lot of the basic functionality I've added so far is also covered by more permissible packages like the ones you mentioned, I've started out just using those. But thanks for bringing that up, I'll revisit using MuPDF for redaction and other features(2) I went with pdf-lib-plus-encrypt because the original pdf-lib doesn't have functionality for password-protecting PDFs, and since pdf-lib-plus-encrypt does, I used it so I could have that feature\n \nreply",
      "Thanks for the insight. Yes, the AGPL isn't a problem for me - I didn't check your licence as I could just view source :)\n \nreply",
      "Is your code available?\n \nreply",
      "I would assume not based on their objection to AGPL libraries.\n \nreply",
      "not yet available as open source. Open-source has two main purposes I think: trust, and customizing/integrating it into other products.On the first part, since everything happens in the browser, anyone can see the html/javascript and inspect the Network tab and see that no network requests are made that send their PDF anywhere.And on the second part, I think most people who use the software aren't developers and won't want to modify it, and I don't particularly see a use case for integrating this software into another one, outside maybe an internal corporate scenario.Though, maybe I'll add something where you can pay to get the desktop version, similar to what Sejda does.\n \nreply",
      "I use stirling-pdf for this, ran locally withdocker run -d -p 8080:8080 -e DOCKER_ENABLE_SECURITY=false --name stirling-pdf frooodle/s-pdf:latestLots of features, OP you should definitely check this out\n \nreply"
    ],
    "link": "https://breezepdf.com",
    "first_paragraph": "Breeze PDF is a powerful, free PDF editor that works entirely offline in your browser. No uploads, 100% privacy guaranteed. Your files never leave your computer.Everything you need to manage your PDFs, securely and easily.Easily type and add text anywhere on your PDF document.Insert images (JPG, PNG) directly into your PDF pages.Sign documents digitally by drawing, typing, or uploading.Add interactive text input boxes for fillable forms.Combine multiple PDF files into a single document.Remove unwanted pages from your PDF file easily.Secure your sensitive PDFs with strong password encryption.Works offline directly in your web browser. No installation needed.With Breeze PDF, your documents are processed locally in *your* browser using JavaScript. They are never uploaded to any server. This means complete confidentiality and control over your sensitive information. What happens on your computer, stays on your computer.Quick answers to common questions.Yes, it\u2019s 100% free with no hidden fe",
    "summary": "Title: Show HN: BreezePDF - Redefining the PDF Game... Offline!\n\nAnother day, another \"revolutionary\" tool launched into the PDF editing void by a hopeful hacker who thinks they can solve all document woes with JavaScript and local storage \ud83e\udd13. Breeze PDF promises to never breach your trust by sending your documents into the mystical cloud, because who would want their sweaty gym contract PDF to ever escape their own hard drive? Meanwhile, the comment section becomes a battleground for coding samurais to flex obscure knowledge about embedded fonts and API limitations. Everyone's excited about not needing to Google \"how to split a PDF\" anymore, but the real fight over privacy settings and open-source licenses remains as unresolved as your last PDF merge attempt. \ud83d\ude44"
  },
  {
    "title": "DuckDB is probably the most important geospatial software of the last decade (dbreunig.com)",
    "points": 155,
    "submitter": "dbreunig",
    "submit_time": "2025-05-03T19:30:38 1746300638",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=43881468",
    "comments": [
      "> Prior to this, getting up and running from a cold-start might\u2019ve required installing or even compiling severall OSS packages, carefully noting path locations, standing up a specialized database\u2026 Enough work that a data generalist might not have bothered, or their IT department might not have supported it.I've been able to \"CREATE EXTENSION postgis;\" for more than a decade. There have been spatial extensions for PG, MySQL, Oracle, MS SQL Server, and SQLite for a long time. DuckDB doesn't make any material difference in how easy it is to install.\n \nreply",
      "That requires data to already be in Postgres, otherwise you have to ETL data into it first.DuckDB on the other hand works with data as-is (Parquet, TSV, sqlite, postgres... whether on disk, S3, etc.) with requiring an ETL step (though if the data isn't already in a columnar format, things are gonna be slow... but it will still work).I work with Parquet data directly with no ETL step. I can literally drop into Jupyter or a Python REPL and duckdb.query(\"from '*.parquet'\")Correct me if I'm wrong, but I don't think that's possible with Postgis. (even pg_parquet requires copying? [1])[1] https://www.crunchydata.com/blog/pg_parquet-an-extension-to-...\n \nreply",
      "Yeah, if you want to work with GeoParquet, and you want to keep your data in that format. I can see how that's easer to use your example. That's not what a lot of geospatial data is in. You might have shapefiles, geopackages, geojsons, who knows? There is a lot of software, from QGIS to ESRI to work with different formats to solve different problems. I don't think GeoParquet, even though it might be the fastest geospatial vector data format right now, is that common, and the article did not claim that either. So, given an average user trying to answer some GIS question, some ETL is pretty much a given, on average. And given that, installing PostGIS and installing DuckDB, both require some ETL, and learning some query and analytics language. DuckDB might be an improvement, but it's certainly not as much of a leap as quote is making it out to be.\n \nreply",
      "Yeah, just an example of a QoL issue with DuckDB: even though it can glob files in other cases, the way it passes parameters to GDAL means that globs are taken literally instead of expanded. So I can't query a directory with thirty million geojson files. This is not a problem in geopandas because ipython, being a full interactive development environment, allows me to produce the glob any way I choose.I think this is a fundamental problem with the SQL pattern. You can try to make things just work, but when they fall then what?\n \nreply",
      "I think this is just cause it hasn't been implemented in spatial yet. DuckDB is currently going through a pretty big refactor of the way we glob/scan/union multiple files with all the recent focus on data lake formats, but my plan is to get to it in spatial after next release when that part of the code has stabilized a bit.\n \nreply",
      "Not wrong. Load to PG, then query. Duck UVP is like bringing 8 common tools/features under one tent.\n \nreply",
      "I'm a big fan of DuckDB and I do geospatial analysis, mostly around partitioning geographies (into Uber H3 hexagons), calculating Haversine distances, calculating areas of geometries, figuring out which geometry a point falls in, etc. Many of these features have existed in some form or other in geopandas or postgis, so DuckDB's spatial extensions bring nothing new.But what DuckDB as an engine does is it lets me work directly on parquet/geoparquet files at scale (vectorized and parallelized) on my local desktop. It beats geopandas in that respect. It's a quality of life improvement to say the least.DuckDB also has an extension architecture that admits more exotic geospatial features like Hilbert curves, Uber H3 support.https://duckdb.org/docs/stable/extensions/spatial/functions....https://duckdb.org/community_extensions/extensions/h3.html\n \nreply",
      "\u201cimport geopandas\u201d also exists  and has for some time. Snark aside, WHAT is special about duckDB? I wish the author had actually shown some practical examples so I could understand their claims better.\n \nreply",
      "I replied to another comment, but I think a big part is that duckdbs spatial extension provides a SQL interface to a whole suite of standard foss gis packages by statically bundling everything (including inlining the default PROJ database of coordinate projection systems into the binary) and providing it for multiple platforms (including WASM). I.E there are no transitive dependencies except libc.Yes, DuckDB does a whole lot more, vectorized larger-than-memory execution, columnar compressed storage and a ecosystem of other extensions that make it more than the sum of its parts. But while Ive been working hard on making the spatial extension more performant and more broadly useful (I designdd a new geometry engine this year, and spatial join optimization just got merged on the dev-branch), the fact that you can e.g. convert too and from a myriad of different geospatial formats by utilizing GDAL, transforming through SQL, or pulling down the latest overture dump without having the whole workflow break just cause you updated QGIS has probably been the main killer feature for a lot of the early adopters.(Discmaimer, I work on duckdb-spatial @ duckdblabs)\n \nreply",
      "I'm not the OP, but thank you for such a detailed answer.  The integration and reduced barriers to entry you mention mirror my own experiences with tooling in another area, and your explanation made parallels clear.\n \nreply"
    ],
    "link": "https://www.dbreunig.com/2025/05/03/duckdb-is-the-most-impactful-geospatial-software-in-a-decade.html",
    "first_paragraph": "\nMay 3, 2025\n    What happens when you embed geospatial capabilities in generalist data tools? More people engaging with geo data.I just returned from the inaugural Cloud-Native Geospatial conference. It was fantastic, I highly recommend you jump in if Jed and team organized another.One of the core questions discussed in the breakouts and in the halls was how to broaden the geospatial audience. How can we better communicate geo data\u2019s utility, in all industries and domains? Many tactics and case studies were debated, but the one I kept coming back to is that of DuckDB.This chart could\u2019ve been pretty bleak! Interest in \u201cgeospatial\u201d (a term that functions well as a proxy for similar terms) declined and flatlined until late 2023 \u2013 right when DuckDB released their spatial extension.Now, all the standard caveats about correlation and causation apply, but I\u2019m inclined to believe this chart.DuckDB lowers the barriers to working with geo data to two lines:If the extension is already installed,",
    "summary": "In a world starved for yet another geospatial tool, an article drops celebrating <em>DuckDB's godsend spatial extension</em> as if it's the second coming for data hipsters. While the geek chorus in the comments section performs mental gymnastics to justify why *DuckDB vastly outperforms existing solutions like PostGIS*, a few brave souls dare to ask: has anyone actually used this in the wild, or are we just clapping for compression algorithms again? A commenter eager to flaunt 'I can query 30 million geojson files in my Jupyter notebooks!' discovers, to their dismay, that DuckDB deflates their data dreams faster than they can say \"SQL pattern issues.\" Meanwhile, another helpful wizard from DuckDB coven hints at upcoming magical patches \u2013 because perpetual beta is definitely what enterprises adore for their critical infrastructure! \ud83c\udf89\ud83d\udcc9"
  },
  {
    "title": "Google Gemini has the worst LLM API (venki.dev)",
    "points": 51,
    "submitter": "indigodaddy",
    "submit_time": "2025-05-03T22:29:54 1746311394",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43882905",
    "comments": [
      "I still don't really understand what Vertex AI is.If you can ignore Vertex most of the complaints here are solved - the non-Vertex APIs have easy to use API keys, a great debugging tool (https://aistudio.google.com), a well documented HTTP API and good client libraries too.I actually use their HTTP API directly (with the ijson streaming JSON parser for Python) and the code is reasonably straight-forward: https://github.com/simonw/llm-gemini/blob/61a97766ff0873936a...You have to be very careful when searching (using Google, haha) that you don't accidentally end up in the Vertext documentation though.Worth noting that Gemini does now have an OpenAI-compatible API endpoint which makes it very easy to switch apps that use an OpenAI client library over to backing against Gemini instead: https://ai.google.dev/gemini-api/docs/openaiAnthropic have the same feature now as well: https://docs.anthropic.com/en/api/openai-sdk\n \nreply",
      "Google Cloud Console's billing console for Vertex is so poor. I'm trying to figure out how much i spent on which models and I still cannot for the life of me figure it out. I'm assuming the only way to do it is to use the gemini billing assistant chatbot, but that requires me to turn on another api permission.I still don't understand the distinction between Gemini and Vertex AI apis. It's like Logan K heard the criticisms about the API and helped push to split Gemini from the broader Google API ecosystem but it's only created more confusion, for me at least.\n \nreply",
      "Gemini\u2019s is no better. Their data can be up to 24h stale and you can\u2019t set hard caps on API keys. The best you can do is email notification billing alerts, which they acknowledge can be hours late.\n \nreply",
      "OpenAI compatible API is missing important parameters, for example I don't think there is a way to disable flash 2 thinking with it.Vertex AI is for grpc, service auth, and region control (amongst other things). Ensuring data remains in a specific region, allowing you to auth with the instance service account, and slightly better latency and ttft\n \nreply",
      "I find Google's service auth SO hard to figure out. I've been meaning to solve deploying to Cloud Run via service with for several years now but it just doesn't fit in my brain well enough for me to make the switch.\n \nreply",
      "If you're on cloud run it should just work automatically.For deploying, on GitHub I just use a special service account for CI/CD and put the json payload in an environment secret like an API key. The only extra thing is that you need to copy it to the filesystem for some things to work, usually a file named google_application_credentials.jsonIf you use cloud build you shouldn't need to do anything\n \nreply",
      "GCP auth is terrible in general. This is something aws did well\n \nreply",
      "From the linked docs:> If you want to disable thinking, you can set the reasoning effort to \"none\".For other APIs, you can set the thinking tokens to 0 and that also works.\n \nreply",
      "Wow thanks I did not know\n \nreply",
      "When I used the openai compatible stuff my API\u2019s just didn\u2019t work at all. I switched back to direct HTTP calls, which seems to be the only thing that works\u2026\n \nreply"
    ],
    "link": "https://venki.dev/notes/google-gemini-is-bad",
    "first_paragraph": "",
    "summary": "<h3>The Sad State of Gemini vs. The Eternal Confusion of Vertex AI</h3>\n\nThe latest blog post on venki.dev serves its intended purpose: whipping tech enthusiasts into a frenzy over Google's newest API debacle, cleverly hidden behind the pseudonym 'Gemini'. In an uproar of confusion that could only be likened to a senior citizen navigating Snapchat, esteemed commenters lament their labyrinthine journey through Vertex AI's billing - a modern-day tribute to Dante's circles of Hell, only less organized and more expensive. Meanwhile, the revelation of a stale, 24-hour behind dataset prompts developer gurus to offer heartfelt eulogies to real-time data. Amid nods to subpar authorization mechanisms and nostalgic sighs for AWS, the tech community finds solace in the irony of using Google to search how not to use Google. \ud83d\ude02\ud83d\udd25"
  },
  {
    "title": "Numerical Linear Algebra Class in Julia TUM (venkovic.github.io)",
    "points": 50,
    "submitter": "darboux",
    "submit_time": "2025-05-03T21:22:28 1746307348",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://venkovic.github.io/NLA-for-CS-and-IE.html",
    "first_paragraph": "\u00a0Home, Research, TeachingThe course is organized into 18 lectures. All lectures consist of a theoretical presentation followed by homework problems [pdf]. Most lectures are also followed by Julia coding assignments.00. \nIntroduction\n\u00a0\u00a0\u00a0\nslides [tex, pdf]01. \nEssentials of linear algebra\n\u00a0\u00a0\u00a0\nslides [tex, pdf]02. \nEssentials of the Julia language\n\u00a0\u00a0\u00a0\nslides [tex, pdf]03. \nFloating-point arithmetic and error analysis\n\u00a0\u00a0\u00a0\nslides [tex, pdf], \nnotebook [ipynb, pdf]04. \nDirect methods for dense linear systems\n\u00a0\u00a0\u00a0\nslides [tex, pdf], \nnotebook [ipynb, pdf]05. \nSparse data structures and basic linear algebra subprograms\n\u00a0\u00a0\u00a0\nslides [tex, pdf], \nnotebook [ipynb, pdf]06. \nIntroduction to direct methods for sparse linear systems\n\u00a0\u00a0\u00a0\nslides [tex, pdf]07. \nOrthogonalization and least-squares problems\n\u00a0\u00a0\u00a0\nslides [tex, pdf], \nnotebook [ipynb, pdf]08. \nBasic iterative methods for linear systems\n\u00a0\u00a0\u00a0\nslides [tex, pdf], \nnotebook [ipynb, pdf]09. \nBasic iterative methods for eigenvalue problems\n\u00a0\u00a0\u00a0\nslides [t",
    "summary": "Title: \"Another Unbearable Semester of Julia\"\n\nDid you miss torturing yourself with niche programming languages and high-concept math problems? Fear not, the Numerical Linear Algebra class at TUM is here to drag you through an excruciating ordeal disguised as education. Brace yourself for 18 lectures, each packed with enough theoretical jargon to make your brain plead for the sweet release of a Segfault. If watching academic hopefuls discuss the specifics of Julia's garbage collection in the required <em>humorless tone</em> wasn't enough, the comment section is ready to serve as a playground for masochists and pedants alike, debating trivialities as if their very sanctity depended upon it. \ud83e\udd13\ud83d\udcbb\ud83d\udcd8"
  },
  {
    "title": "Legendary Bose Magic Carpet Suspension Is Finally Going Global (thedrive.com)",
    "points": 82,
    "submitter": "PaulHoule",
    "submit_time": "2025-04-30T10:27:37 1746008857",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=43843241",
    "comments": [
      "> The original version replaced traditional dampers with linear electric motors that used sensor data to literally move the wheels up and down and cancel out bumps. ClearMotion adapted the control software and applied to active valve dampers with a magnetic fluid.So, in other words, ClearMotion is producing a technology that other OEM's have been doing for years. Just off the top of my head, Cadillac has their magnetic suspension (which uses a fluid that changes viscosity in the presence of a magnetic field. I guess this is the same as what TFA claims is brand new.) The Ford Raptor with their live valve by Fox has a solenoid valve that regulates the shim pack. (Funny enough, I've spent all morning doing a FEA analysis of their valve.) The latest Mercedes Gelandewagen also has solenoid valves in their dampers to switch between soft and hard damping. Citroen has been doing it since the 50's with a purely mechanical system.The basic idea is very simple: you want a computer to regulate the damper between soft and firm, as the road dictates. The implementation of this can become very complicated and there's a number of very different implementations. If I remember right, the Bose implementation required too much electricity to be practical. Most other implementations have some type of solenoid valve to control the pressure drop of the hydraulic fluid across an orifice. Again, the theory is simple, but mass producing a system that is  cheap, reliable, yet can respond in milliseconds is difficult.\n \nreply",
      "> Citroen has been doing it since the 50's with a purely mechanical system.I remember watching Citroens demonstrating losing a wheel, and continuing to drive, in the early 1970s.Citroens are cool. Maybe their build quality wasn't so good, or they were too expensive, as I've not seen them on this side of the pond.\n \nreply",
      "I can confirm. Was driving for a while, impervious to any sign of having a flat tyre before someone on the street signalled me to stop. And that was on a Citroen Saxo, which I don\u2019t think had the suspension of C5 etc. Amazing car, lasted for 15 years with me, with only battery and frankly inconsistent oil changes. The lady who bought it when we immigrated still runs it 15 years later. Build quality is a hit and miss per model I think.\n \nreply",
      "It's really smart though: Why spend a lot of energy moving the wheel up and down, when you can just control the damper and rely the spring to store energy.\n \nreply",
      "It's not the same.For a simple example, let's say you are simply driving in a circle.  The car wants to lean toward the outside.  The linear motors can provide a countering force, lifting the outside, lowering the inside, so the car stays level.  Variable damping can only control the rate that it rolls.  It will still roll in sub-second timescales, unless it completely locks down the suspension, which is terrible for both handling and comfort.For another simple example: going over a speed bump.  Linear motors can lift the front wheels over the bump, and then the rear wheels, so the body stays level the whole time.  An active damper can go full-soft the moment the wheel hits the bump, but the compressed spring will still start lifting the front of the car.  An active damper can do a better job managing the rebound on the far side so it doesn't oscillate, but it can't entirely prevent the bump from pitching the body up and down in the first place.That's not to say it's worthless.  Very fast active dampers can improve both handling and comfort.  It's just nowhere near the level which is possible with linear motors.\n \nreply",
      "Precision, repeatability, responsiveness.\n \nreply",
      "It's easy to underestimate just how slow digital systems are in many real time scenarios compared to analog systems.Consider it in the context of camera based self-driving cars, it's tangential to this discussion but it's an easy to visualize metaphor:- A car traveling 60mph is traveling at 88 feet per second- Assuming a 60hz camera, there would be a 16.67 ms gap between each frame- The car is traveling 1.5 feet between each frame interval- A certain amount of exposure time is necessary for the camera to generate even 1 frame or it will be blurry- High framerate cameras often work around this by staggering/interlacing multiple sensors, but doing this implicitly increases the latency of each frame- A 120hz camera might deliver double the frames per second, but each frame could be arriving 4 frames late in exchange- 4 frames would be imperceptible to humans, it would be 3 feet for the car- You haven't even processed any of these frames yet.- Your off the shelf library introduces a random 1 second delay for some reason and costs you 88 miles in processing time- The car can drive as fast as 120mphAll digital sensors implicitly have a sampling frequency, and the fundamental disconnect is always high sampling frequency =/= low latency. People constantly make this mistake over and over, and by the time you notice you are already too deep into development to make a change.Decreasing latency is expensive, and requires specialized knowledge. Often you get expert software engineers who end up bottlenecked by the hardware limitations they can't even comprehend or the reverse, hardware guys bottlenecked by the software they can't introspect. The latency is only truly understood when you get to integration testing.Nearly every step of the way you discover you need specialized hardware, software, operating systems, sensors. Every part of the chain each costing you more latency. It's like it's own ecosystem where almost everyone writes everything from scratch and doesn't share anything. It's gotten better in recent years though.Full disclosure: I work in medtech and don't actually deal with cars, but it's a very similar problem space. We often use the same hardware/software cars use for this reason.\n \nreply",
      "88ft not 88 miles. I don't understand your 120hz camera line, I also have a degree in imaging tech and that part I do not understand, would you be so kind as to expand upon it??\n \nreply",
      "I can\u2019t quite understand your comment.> Assuming a 60hz camera, there would be a 16.67 ms gap between each frame. The car is traveling 1.5 feet between each frame interval.Ok? So? You are just stating this as if we should understand the implications. I do in fact work with self-driving cars. What you say is true, but it is not a big deal. Why do you feel this maters? Or what is your point?> A certain amount of exposure time is necessary for the camera to generate even 1 frame or it will be blurryThis is a confused statement. A certain amount of exposure is necessary for the camera to collect light. If you don\u2019t have long enough exposure the picture will be dark, not blurry. Your statement makes it sound as if avoiding blur is why we need exposure time, which is a complete nonsense.In reality none of this is a problem. There are automotive grade cameras which can collect enough light fast enough that the images are not blurry in practice. Yes, these cameras have a non-zero exposure time. Yes, this adds latency. No, this is not a problem.> Your off the shelf library introduces a random 1 second delay for some reason and costs you 88 miles in processing timeYou mean 88 feet. If my off the shelf library introduces a random 1 second delay i will chuck it in the bin post haste. Use stuff whose performance characteristics are well understood by you and is not terrible.> Nearly every step of the way you discover you need specialized hardware, software, operating systems, sensors.I do not recognise the world you describe.\n \nreply",
      "Exactly what I came here to post. Mag fluid suspension has been mainstream for a while. Cadillacs aren't exotic.\n \nreply"
    ],
    "link": "https://www.thedrive.com/news/legendary-bose-magic-carpet-suspension-is-finally-going-global",
    "first_paragraph": "\nBy Beverly Braga\n\n\t\t\t\t\tPublished Apr 21, 2025 7:05 AM EDT\nGet The Drive\u2019s daily newsletterTerms of Service & Privacy Policy.Over twenty years ago, a video of a Lexus LS400 seemingly defying the laws of physics shocked the world. Riding on a prototype electromechanical proactive suspension developed by the audio company Bose, the luxury sedan bunny-hopped over curbs, skipped individual wheels over potholes, cornered completely flat, and held its body perfectly level at speed over undulating pavement. It looked like magic, and to some degree it was.Sadly it was totally impractical for mass production in 2004, and Bose eventually gave up on the project and sold off the rights to the tech. Meanwhile, the demo clips lodged themselves in the internet as a prime example of amazing technology that didn\u2019t make it. Every few years, the videos go viral again as a new round of people ask why we can\u2019t get something this cool in new cars today.And at last, now you can. OK, almost.Last month, the pr",
    "summary": "<h1>Bose's Suspended Disbelief: Now With Actual Believers!</h1>\n<p>In a stunning turn of events that's sure to rock the three people still using MySpace, Bose has waved its techno-wand again, making cars hop like caffeinated rabbits over potholes. Reveling in tech that fizzled out when flip phones were cool, the legendary audio brand has re-embarked on its quest to eradicate road bumps\u2014because regular shocks are just too <i>mainstream</i>. Meanwhile, armchair engineers in the comments rehash suspension technologies that have been around since the era of leaded gasoline, flexing their Google-fu as if it were a doctoral thesis. ClearMotion springs into action, sprinkling some magnetic fairy dust on the old bones of Bose\u2019s dreams, ensuring that car enthusiasts can now <em>almost</em> not feel the road.</p>"
  },
  {
    "title": "Run LLMs on Apple Neural Engine (ANE) (github.com/anemll)",
    "points": 217,
    "submitter": "behnamoh",
    "submit_time": "2025-05-03T15:29:10 1746286150",
    "num_comments": 93,
    "comments_url": "https://news.ycombinator.com/item?id=43879702",
    "comments": [
      "I wonder if Apple ever followed up with this: https://github.com/apple/ml-ane-transformersThey claim their ANE-optimized models achieve \"up to 10 times faster and 14 times lower peak memory consumption compared to baseline implementations.\"AFAIK, neither MLX nor llama.cpp support ANE. Though llama.cpp started exploring this idea [0].What's weird is that MLX is made by Apple and yet, they can't support ANE given its closed-source API! [1][0]: https://github.com/ggml-org/llama.cpp/issues/10453[1]: https://github.com/ml-explore/mlx/issues/18#issuecomment-184...\n \nreply",
      "Whisper.cpp has a coreml option which gives 3x speed up over cpu only according to the docs: https://github.com/ggml-org/whisper.cpp?tab=readme-ov-file#c...\n \nreply",
      "Some outdated information about bare-metal use of the ANE is available from the Whisper.cpp pull req: https://github.com/ggml-org/whisper.cpp/pull/1021 Even more outdated information at: https://github.com/eiln/ane/tree/33a61249d773f8f50c02ab0b9fe... In short, the early (M1/M2) versions of ANE are unlikely to be useful for modern LLM inference due to their seemingly exclusive focus on statically scheduled FP16 and INT8 MADDs.More extensive information at https://github.com/tinygrad/tinygrad/tree/master/extra/accel... (from the Tinygrad folks, note that this is also similarly outdated) seems to basically confirm the above.(The jury is still out for M3/M4 which currently have no Asahi support - thus, no current prospects for driving the ANE bare-metal.  Note however that the M3/Pro/Max ANE reported performance numbers are quite close to the M2 version, so there may not be a real improvement there either. M3 Ultra and especially the M4 series may be a different story.)\n \nreply",
      "I wouldn't say that they aren't useful for inference (there are pretty clear performance improvements even from the asahi effort you linked) - it's just that you have to convert the model ahead of time to be compatible with the ANE which is explained in the readme docs for whisper.cpp that I linked above.I would say though that this likely excludes them from being useful for training purposes.\n \nreply",
      "Note that I was only commenting on modern quantized LLM's that basically avoid formats like FP16 or INT8, preferring lower precision wherever feasible.  When in-memory model values must be padded to FP16/INT8 this slashes your effective use of memory bandwidth, which is what determines token generation speed.  So the only feasible benefits are really in the prompt pre-processing phase, and even then only in lower power use compared to GPU, not really in higher speed.\n \nreply",
      "That's really interesting! I didn't know that about the padding behavior here. I am interested to know which models this would include? I know Gemma 3 raw is bf16 - are you just talking about the quantized versions of these? Or are models being released purely as quantized versions these days? I know Google just released a QAT (Quantization Aware Training) model of Gemma 3 27b - but that base model was already released.\n \nreply",
      "Models may be released as unquantized (and even then they are gradually shifting towards lower precisions over time), but most people are going to be running them in a quantized version simply because that gives you the best bang for your buck (you can fit more interesting models on the same hardware).  Of course this is strictly about local LLM inference, though one may reasonably assume that the big players are also doing something similar.\n \nreply",
      "My understanding is that model throughput is fundamentally limited at some point by the fact that the ANE is less wide than the GPU.At that point, the ANE loses because you have to split the model into chunks and only one fits at a time.\n \nreply",
      "What do you mean by less wide? The main bottleneck for transformers is memory bandwidth. ANE has a much lower ceiling than CPU/GPU (yes, despite unified memory).Chunking is actually beneficial as long as all the chunks can fit into the ANE\u2019s cache. It speeds up compilation for large network graphs and cached loads are negligible cost. On M1 the cache limit is 3-4GB, but it is higher on M2+.\n \nreply",
      "I was referring to both the lower memory bandwidth and lower FLOPs. The GPU can just do\u2026 more at once? For now. Or is that changing?I had also assumed that loading a chunk from the cache was not free because I\u2019ve seen cache eviction on my M1, but it\u2019s good to know that it\u2019s no longer as big of a limitation.also, I\u2019m a big fan of your work! I played around with your ModernBERT CoreML port a bit ago\n \nreply"
    ],
    "link": "https://github.com/Anemll/Anemll",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Artificial Neural Engine Machine Learning Library\n      ANEMLL (pronounced like \"animal\") is an open-source project focused on accelerating the porting of Large Language Models (LLMs) to tensor processors, starting with the Apple Neural Engine (ANE).The goal is to provide a fully open-source pipeline from model conversion to inference for common LLM architectures running on ANE.\nThis enables seamless integration and on-device inference for low-power applications on edge devices, ensuring maximum privacy and security.\nThis is critical for autonomous applications, where models run directly on the device without requiring an internet connection.We aim to:See update Roadmap.md for more detailsANEMLL provides five main components for Apple Neural Engine inference development:LLM Conversion Tools - Scripts and code to convert models direc",
    "summary": "**Innovating Obsolescence with \"ANEMLL\"**\n\nIn a world desperately craving for yet another way to make their shiny Apple devices do more, the \"ANEMLL\" project promises to transform your overpriced iPhone into an edge computing wonder. Pitched with the subtlety of a sledgehammer on glass, this open-source marvel (that practically no one asked for) aims to let LLMs run directly on Apple's Neural Engine - because _obviously_, what we all need is to process our social media rants and cat photo descriptions faster and more privately. The GitHub comment section erupts with the confused euphoria of developers stumbling over memory specifications and outdated links, while debating the existential bandwidth constraints of Apple's chip architecture as if uncovering the lost city of Atlantis. Meanwhile, the rest of us ponder why our phones need to be smarter than we are. \ud83e\udd16\ud83c\udf4f\ud83d\udca5"
  },
  {
    "title": "Why I Am Not Going to Buy a Computer (1987) [pdf] (matthewjbrown.net)",
    "points": 37,
    "submitter": "bookofjoe",
    "submit_time": "2025-05-03T22:16:11 1746310571",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=43882809",
    "comments": [
      "An amusing anecdote: I was in grad school in 1987. The university had a campus computer store, which gave out a pamphlet: \"Should I get a computer?\" It listed many pro's and con's, nothing surprising for the era. I already had a computer. The thing that stuck with me, was the advice: \"Don't expect a computer to organize you. If you have a messy desk, you will have a messy computer.\"Sure as shootin', even to this day, I still have a messy computer.\n \nreply",
      "Nevertheless, I go through life with the persistent unconscious belief that if I buy products-for-organizing I will become organized. :p\n \nreply",
      "Past discussionshttps://news.ycombinator.com/item?id=42636195 - 3 months ago, 10 commentshttps://news.ycombinator.com/item?id=31808269 - 3 years ago, 169 commentshttps://news.ycombinator.com/item?id=2108463 - 14 years ago, 11 comments\n \nreply",
      "huh. I\u2019m aware of this author for other reasons - he\u2019s popular among some carbon-capture enthusiasts. This\u2026 colors my opinion.\n \nreply",
      "I'd never heard of them before! But I'm sure some people that disapprove will come along soon enough to balance things out.\n \nreply",
      "I wonder if- in the 2030s -the anti AI people of 2025 will look as obnoxious as the anti computer people of the 80s.\n \nreply",
      "Anti computer people are blissfully unaware of your feelings toward them, maybe they're onto somethingWendell Barry had a perfectly successful career without having to touch computers, and there will be people in twenty years who lead careers without having to interact with AI personalities\n \nreply",
      "Hopefully there will be careers for people in 20 years.\n \nreply",
      "kinda hope there won\u2019t be and we all retire to leisure\n \nreply",
      "With what money? My kids aren't going to have enough money to retire to leisure in 20 years.\n \nreply"
    ],
    "link": "https://classes.matthewjbrown.net/teaching-files/philtech/berry-computer.pdf",
    "first_paragraph": "",
    "summary": "Another day on Hacker News, where a dusty old PDF throws tech enthusiasts into the throes of existential reminiscence. A user dredges up Wendell Berry's wild notion from 1987, \"Why I Am Not Going to Buy a Computer,\" causing an uproarious blend of nostalgia and condescension to bubble up. Commenters gleefully mock Berry\u2019s archaic views, competing to show who can most effectively warp their sense of superiority into a tool for ironically proving Berry's point. Amidst the quips about chaotic desktops both physical and virtual, there's a lurking fear: What if those anti-AI folks are <i>right</i>? No worries, more tech will fix everything, right? \ud83e\udd16\ud83d\ude02"
  },
  {
    "title": "When Flat Rate Movers Won't Answer Your Calls (aphyr.com)",
    "points": 109,
    "submitter": "kevincox",
    "submit_time": "2025-05-03T20:44:56 1746305096",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=43882150",
    "comments": [
      "Boy, if robotics could learn how to bubble wrap, tape, box and stack both hard and soft items\u2026 They could hitch a ride on the back of a panel truck. Somewhat similar to forklifts on the tails of Home Depot delivery trucks.\n \nreply",
      "Those robot movers would be recording every item in your home, evaluating the age and condition of those items, logging every member of your family, mapping out the floor plans of your old and new home, and streaming that data back to the moving company who would sell it to data brokers.\n \nreply",
      "Today I learned there is a national database (for now) so you can bypass and file appropriately. That is nice. I\u2019m happy op got somewhat whole again.But that sucks, luckily I\u2019ve been able to just do U-Haul solo but lately (also facing a move) man - it is tiresome the older you get.\n \nreply",
      "Honestly the best way to move is to sell or donate everything and buy new for your new place. Get down to suitcases or stuff you can pack and ship via UPS.Yeah some stuff has sentimental value but try to get past that as much as you can. It\u2019s just stuff.When my uncle moved when he retired he took what would fit in his car. I have never gotten that lean but I admire him for it.\n \nreply",
      "Yes, next time I move I will sell or donate my $100,000 piano and simply buy a new one in the new place.\n \nreply",
      "This is very situational. Many people have tens or hundreds of thousands of dollars of items, especially furniture.\n \nreply",
      "For me it isn't about the cost as much as it is that I've tailored my possessions to be what I like. If I have it, it brings value, and I probably like the details enough that it's hard to replace without getting an exact replica.\n \nreply",
      "He paid $14k for the movers. That would buy a fair bit of new furniture unless it\u2019s really high end.\n \nreply",
      "That\u2019s a fairly significant amount of furniture.Unless you\u2019re buying mid-grade or lower IKEA or purchasing used, you\u2019ll almost always come out behind by selling and repurchasing.\n \nreply",
      "I see many scenarios where the items would add up to less than this. I see more where it doesn't. e.g.: What you say makes sense for a single person who owns a PC, a TV, and Ikea + amazon furniture.Now imagine a couple or family, who's been (criticize the capitalist/consumer culture or not) buying nice wood furniture, has a well-stocked kitchen, multiple computers, hobby equipment etc.I encourage you to run a rough estimate of your own household's items. Do the same for your a neighbor's; a friend's.\n \nreply"
    ],
    "link": "https://aphyr.com/posts/381-when-flat-rate-movers-wont-answer-your-calls",
    "first_paragraph": "Back in 2023 I went through an inter-state move with Flat Rate Movers,\nLTD. (US DOT 488466, MC 254356). They bungled the job, damaged my stuff\nand my house, and then refused to talk to me or their own insurance company. I\nplaced dozens of calls, wrote letters, emails, nothing worked. I finally got\nsome money for the damage by working with their insurer directly. I know a lot\nof folks struggle with moving companies, and figured I\u2019d write up a bit of this\nstory in case it helps. You can skip ahead if you\njust want to know about the FMCSA insurer database.In November 2022 I scheduled the move with Flat Rate. We took a detailed video\ninventory of everything to be moved. I paid over $14,000 for an \u201call inclusive\nmove\u201d, which, per the move plan, included full packing services, blankets for furniture, boxes for electronics, lamps, and bicycles, a custom crate for a tabletop, and \u201cextended valuation coverage\u201d\u2014their insurance policy.A few days before the move in March 2023, Flat Rate called to ",
    "summary": "<b>An Odyssey of Misery: When Flat Rate Movers Ghost You</b>\n\nToday on the disaster blog, a soul braves the seventh circle of bureaucratic hell known as moving with Flat Rate Movers. For a mere \ud83d\udcb8 $14,000 \ud83d\udcb8, what did our brave hero get? Damaged goods, ghosted calls, and a lifetime subscription to stress. The comment section becomes a tragicomic coliseum where armchair logisticians debate whether to sell grandma's urn for a newer, shinier urn or just stuff one\u2019s life into a 1995 Subaru and bond with fast food leftovers. Some suggest robotic overlords should handle our moves until they remember that means giving Skynet the keys to their house. Ah, technology \u2013 simultaneously our savior and doom."
  },
  {
    "title": "Metagenomics test saves woman's sight after mystery infection (bbc.co.uk)",
    "points": 14,
    "submitter": "neversaydie",
    "submit_time": "2025-04-30T07:56:22 1745999782",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.bbc.co.uk/news/articles/czx45vze0vyo",
    "first_paragraph": "Dr Ellie Irwin's sight has been saved after a test pinpointed the cause of her long-lasting eye inflammationA 29-year-old doctor from Bristol has had her eyesight saved after a \"game-changing\" test identified a mystery infection that had plagued her health for five years.Ellie Irwin suffered persistent inflammation in her right eye that didn't go away, resulting in blurred vision. No treatment helped and at one point she even considered having her eye removed.It was only after Ellie was offered a \"last resort\" analysis called metagenomics, that she was diagnosed with a rare bacterial infection which was cured with antibiotics.\"It's been transformative,\" Ellie told the BBC. \"I feel so fortunate.\"Professor Carlos Pavesio, consultant ophthalmologist at Moorfields Eye Hospital in London, says Ellie's case is a \"breakthrough in the diagnosis of infectious diseases\". \"There are many patients we treat with chronic infections for years, but despite multiple tests we cannot identify the bug res",
    "summary": "**The Future of Healthcare: Clickbait Edition**  \nIn a stunning display of modern science akin to finding Nemo in the Pacific, Dr. Ellie Irwin essentially played microbiological Where's Waldo in her eye for five years. Thank goodness for metagenomics, the magical mystery tour that finally pinpointed her eye bug after the more traditional method of random guessing proved, shockingly, ineffective. Comments are a delightful mix of amateur epidemiologists and future Nobel laureates, convinced they, too, could have swabbed an eyeball and achieved the same results with their high school chemistry set. What a time to be <em>alive</em>! \ud83e\udd13\ud83d\udd2c\ud83d\udc8a"
  },
  {
    "title": "QModem 4.51 Source Code (github.com/aaronfriel)",
    "points": 160,
    "submitter": "AaronFriel",
    "submit_time": "2025-05-03T15:30:33 1746286233",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=43879715",
    "comments": [
      "This brought back memories. I remember dialling into BBSes using Qmodem, downloading QWKs (compressed email packets) from \u201cconferences\u201d (similar to newsgroups).I would read/reply offline using OLX (Offline Express, a QWK reader also part of the Qmodem suite), and then batch upload my replies (.REPs, also compressed) to the BBS.This was back in the day when you weren't connected 24/7, and when dial-up wasn\u2019t unlimited (in my country \u2014 even if it was, BBSes were node limited so you couldn\u2019t stay connected forever). So participating in BBS conferences meant quick dial-ins and uploads, where most of the messaging/replies was done offline.Although bandwidth is abundant these days, I still think the QWK/REP idea is an attractive one. There is an art and a beauty to crafting replies offline from the cozy Turbo Vision UI that was OLX.\n \nreply",
      "Similar story here. I used RoboMail for MS-DOS as the offline reader back in maybe 1992-1993?, connecting to a BBS that interfaced with RelayNet/RIME, which was similar to FidoNet.RoboMail wasn't TurboVision, but it was very nice as TUIs went at the time. It seems mostly vanished from Google. I was a Turbo Pascal developer myself at the time, and I made an offline reader that I thought was far superior (multiple Turbo Vision windows etc.), but by the time I had gotten close to the point of release, the Internet arrived and I completely lost interest.I still wish there was an archive of RelayNet, because I used to post a lot, and of course I never kept anything myself. I've never found any archive of the content since it was shut down in 2007.Also, it's sad to hear that the QWK format's creator died in a swatting incident in 2020, of all things.\n \nreply",
      "> Also, it's sad to hear that the QWK format's creator died in a swatting incident in 2020, of all things.Oh, man. That sucks. I knew about that tragedy[0] but I never read deep enough to realize that Mark Herring (the gentleman who, arguably, was killed in the incident) was the creator of the QWK format.It was already a horrific story. Now it just feels that much closer to home. Ugh. RIP Mark Herring.[0] https://en.wikipedia.org/wiki/2020_Tennessee_swatting\n \nreply",
      "I loved QWK packets. That saved me a ton of long distance telephone charges. I didn't use OLX (\"Silly Little Mail Reader\" was my jam) but the concept was wonderful.\n \nreply",
      "Yeah. My primary BBS was relatively nearby in the same state. But in those days intrastate US calls outside of your very local area could actually cost more than interstate.\n \nreply",
      "Ah SLMR is OLX.Mustang Software (Wildcat! BBS) bought Qmodem and SLMR (and renamed the latter to OLX).\n \nreply",
      "Wildcat! Was great\n \nreply",
      "Oh, wow. I didn't realize that. Very cool. I used the heck out of it.At one point I learned about the QWK format and wrote some code to build a QWK packet out of text files. My idea was to distribute an e-zine as a QWK packet. I successfully made QWK packets but never had any actual content to release.\n \nreply",
      "> My idea was to distribute an e-zine as a QWK packet.What  a cool idea! I would have loved something like that.This hearkens back to the day of Byte and PC Magazine where I would actually buy paper magazines to learn about the latest tech -- and I wondered, why couldn't there be an electronic version of this? (the QWK downloadable e-zine idea sounds so ergonomic). But eventually the Internet happened and we got these in the form of websites.But I feel websites still lack the nice offline, self-contained natures of a magazine. Links on a website feel dispersed. Whereas an offline packetized magazine would have a linear nature to it, and you'd be able to browse in one sitting. And look at full page glossy ads (hey, I looked at the ads -- they were so cool back in the day with Gateway and Dell feuding).Ah water under the bridge now...\n \nreply",
      "I use slrn and mutt with mbsync/msmtp like that, among RSS.\n \nreply"
    ],
    "link": "https://github.com/AaronFriel/qmodem-4.51",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          The source code release of QModem 4.51, an MS-DOS telecommunications program authored by John Friel III (1960\u20132024). This source snapshot reflects the state of QModem \"Test-Drive\" edition, version 4.51, as it existed in early 1992. The release is presented in the hope it may prove valuable as a historical artifact, for telecommunications enthusiasts, retrocomputing hobbyists, or anyone interested in the inner workings of a classic DOS comms package.QModem was a widely-used terminal communications program for MS-DOS, supporting a rich array of modem protocols, scripting, user customization, modem auto-configuration, and even a \"Host Mode\" for basic BBS-like operation.QModem was developed throughout the 1980s and early 1990s as a competitor to programs such as Procomm, Telix, and others. It provided robust support for:This repository contains ",
    "summary": "In a stunning revelation of misplaced nostalgia, the GitHub repository for QModem 4.51 pokes its head into the 21st century, dragging behind it a slew of sentimental techies who reminisce about the golden age of 2400 baud modems and getting excited over downloading text files. These noble guardians of forgotten tech waste no time diving into the comments to swap tales of yesteryear, where they battled against time and phone bill monsters, just to partake in \"high-speed\" offline email packet exchanges. Four sentences to confirm: tech nostalgia might be less about utility and more about missing youth's simplicity. Meanwhile, any outsiders stumbling upon this discussion might wonder if they've cracked open a digital tomb, where skeletons of QWK and BBS still rattle for attention. \ud83d\udcdf\ud83d\udc74\ud83d\udcbe"
  },
  {
    "title": "Why can't HTML alone do includes? (frontendmasters.com)",
    "points": 228,
    "submitter": "susam",
    "submit_time": "2025-05-03T12:50:55 1746276655",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=43878728",
    "comments": [
      "HTML was historically an application of SGML, and SGML could do includes. You could define a new \"entity\", and if you created a \"system\" entity, you could refer to it later and have it substituted in.    <!DOCTYPE html example [\n      <!ENTITY myheader SYSTEM \"myheader.html\">\n    ]>\n    ....\n    &myheader;\n\nSGML is complex, so various efforts were made to simplify HTML, and that's one of the capabilities that was dropped along the way.\n \nreply",
      "We also had a brief detour into XML with XHTML, and XML has XInclude, although it's not a required feature.\n \nreply",
      "Neat reference, going to look into that.The <object> tag appears to include/embed other html pages.An embedded HTML page:<object data=\"snippet.html\" width=\"500\" height=\"200\"></object>https://www.w3schools.com/tags/tag_object.asp\n \nreply",
      "This was the rabbit hole that I started down in the late 90s and still haven\u2019t come out of. I was the webmaster of the Analog Science Fiction website and I was building tons of static pages, each with the same header and side bar. It drove me nuts. So I did some research and found out about Apache server side includes. Woo hoo! Keeping it DRY (before I knew DRY was a thing).Yeah, we\u2019ve been solving this over and over in different ways. For those saying that iframes are good enough, they\u2019re not. Iframes don\u2019t expand to fit content. And server side solutions require a server. Why not have a simple client side method for this? I think it\u2019s a valid question. Now that we\u2019re fixing a lot of the irritation in web development, it seems worth considering.\n \nreply",
      "I've become a fan of https://htmx.org for this reason.A small 10KB lib that augments HTML with the essential good stuff (like dynamic imports of static HTML)\n \nreply",
      "> Iframes don\u2019t expand to fit contentActually, that was part of the original plan - https://caniuse.com/iframe-seamless\n \nreply",
      "I used the seamless attribute extensively in the past, it still doesn't work the way GP intended, which is to fit in the layout flow, for example to take the full width provided by the parent, or automatically resize the height (the pain of years of my career)It worked rather like a reverse shadow DOM, allowing CSS from the parent document to leak into the child, removing borders and other visual chrome that would make it distinguishable from the host, except you still had to use fixed CSS layouts and resize it with JS.\n \nreply",
      "The optimal solution would be using a template engine to generate static documents.\n \nreply",
      "> The optimal solution would be using a template engine to generate static documents.This helps the creator, but not the consumer, right?  That is, if I visit 100 of your static documents created with a template engine, then I'll still be downloading some identical content 100 times.\n \nreply",
      "True for any server side solution, yes.On the other hand it means less work for the client, which is a pretty big deal on mobile.\n \nreply"
    ],
    "link": "https://frontendmasters.com/blog/seeking-an-answer-why-cant-html-alone-do-includes/",
    "first_paragraph": "I\u2019m obsessed with this basic web need. You\u2019ve got three pages:You need to put the same header on all three pages. Our developer brains scream at us to ensure that we\u2019re not copying the exact code three times, we\u2019re creating the header once then \u201cincluding\u201d it on the three (or a thousand) other pages.We don\u2019t need to list them all here. I documented some of them one time, but there are many more. We\u2019ve got JavaScript to go fetch the HTML and insert it. We\u2019ve got old school web server directives. Any static site generator can do it. Task runners can do it. Templating languages tend to have include functionality. Any backend language can generate HTML on the fly. I\u2019ve seen several Web Components purpose-built for this. We\u2019ve got <iframe>, which technically is a pure HTML solution, but they are bad for overall performance, accessibility, and generally extremely awkward here, but we can extract them. We can just not worry about includes at all and trust our code editors powerful find and re",
    "summary": "Title: Why can't HTML alone do includes? (frontendmasters.com)\n\nWelcome to another thrilling episode of \"How Many Ways Can We Reinvent the Wheel?\" starring all your favorite web development tricks to achieve something as mystical as reusing a header! In today\u2019s feature, we discover that HTML, the backbone of the web, is as incapable of handling includes as a spoon is of cutting steak. Cue a chorus of developers reminiscing about the good old days of SGML and server side includes, and preaching about the hidden virtues of iframes, while ignoring their many sins like performance hits and poor accessibility. Meanwhile, the comment section turns into a nostalgic trip down memory lane with XML and XInclude, spiced up with modern solutions that still don\u2019t solve the simple desire for clean, efficient client-side includes. \ud83e\udd21\ud83c\udfad\n\nHTML, meet DRY principle. DRY principle, meet HTML. Spoiler: They don\u2019t get along. \ud83c\udfbb"
  },
  {
    "title": "Time saved by AI offset by new work created, study suggests (arstechnica.com)",
    "points": 192,
    "submitter": "amichail",
    "submit_time": "2025-05-03T13:14:59 1746278099",
    "num_comments": 202,
    "comments_url": "https://news.ycombinator.com/item?id=43878850",
    "comments": [
      "I can't find the article anymore but I remember reading almost 10 years ago an article on the economist saying that the result of automation was not removal of jobs but more work + less junior employment positions.The example they gave was search engine + digital documents removed the junior lawyer headcount by a lot. Prior to digital documents, a fairly common junior lawyer task was: \"we have a upcoming court case. Go to the (physical) archive and find past cases relevant to current case. Here's things to check for:\" and this task would be assigned to a team of junior (3-10 people). But now one junior with a laptop suffice. As a result the firm can also manage more cases.Seems like a pretty general pattern.\n \nreply",
      "Dwarkesh had a good interview with Zuck the other week. And in it, Zuck had an interesting example (that I'm going to butcher):FB has long wanted to have a call center for its ~3.5B users. But that call center would automatically be the largest in history and cost ~15B/yr to run. Something that is cost ineffective in the extreme. But, with FB's internal AIs, they're starting to think that a call center may be feasible. Most of the calls are going to be 'I forgot my password' and 'it's broken' anyways. So having a robot guide people along the FAQs in the 50+ languages is perfectly fine for ~90% (Zuck's number here) of the calls. Then, with the harder calls, you can actually route it to a human.So, to me, this is a great example of how the interaction of new tech and labor is a fractal not a hierarchy. In that, with each new tech that your specific labor sector finds, you get this fractalization of the labor in the end. Zuck would have never thought of a call center, denying the labor of many people. But this new tech allows for a call center that looks a lot like the old one, just with only the hard problems. It's smaller, yes, but it looks the same and yet is slightly different (hence a fractal).Look, I'm not going to argue that tech is disruptive. But what I am arguing is that tech makes new jobs (most of the time), it's just that these new jobs tend to be dealing with much harder problems. Like, we''re pushing the boundaries here, and that boundary gets more fractal-y, and it's a more niche and harder working environment for your brain. The issue, of course, is that, like a grad student, you have to trust in the person working at the boundary is actually doing work and not just blowing smoke. That issue, the one of trust, I think is the key issue to 'solve'. Cal Newport talks a lot about this now and how these knowledge worker tasks really don't do much for a long time, and then they have these spats of genius. It's a tough one, and not an intellectual enterprise, but an emotional one.\n \nreply",
      "I worked in automated customer support, and I agree with you. By default, we automated 40% of all requests. It becomes harder after that, but not because the problems the next 40% face are any different, but because they are unnecessarily complex.A customer who wants to track the status of their order will tell you a story about how their niece is visiting from Vermont and they wanted to surprise her for her 16th birthday. It's hard because her parents don't get along as they used to after the divorce, but they are hoping that this will at the very least put a smile on her face.The AI will classify the message as order tracking correctly, and provide all the tracking info and timeline. But because of the quick response, the customer will write back to say they'd rather talk to a human and ask for a phone number they can call.The remaining 20% can't be resolved by neither human nor robot.\n \nreply",
      "Between the lines, you highlight a tangental issue: execs like Zuckerberg think easy/automatable stuff is 90%. People with skin in the game know it is much less (40% per your estimate).This isn't unique to LLMs. Overestimating the benefit of automation is a time-honored pastime.\n \nreply",
      "This reminds me how Klarna fired their a large part of their customer support department to replace it with ai, only to eventually realize they couldn't do the job primarily using ai and had to rehire a ton of people.\n \nreply",
      "> But because of the quick response, the customer will write back to say they'd rather talk to a humanIs this implying it's because they want to wag their chins?My experience recently with moving house was that most services I had to call had some problem that the robots didn't address. Fibre was listed as available on the website but then it crashed when I tried \"I'm moving home\" - turns out it's available in the general area but not available for the specific row of houses (had to talk to a human to figure it out). Water company, I had an account at house N-2, but at N-1 it was included, so the system could not move me from my N-1 address (no water bills) to house N (water bill). Pretty sure there was something about power and council tax too. With the last one I just stopped bothering, figuring that it's the one thing that they would always find me when they're ready (they got in touch eventually).\n \nreply",
      "> Most of the calls are going to be 'I forgot my password' and 'it's broken' anyways. So having a robot guide people along the FAQs in the 50+ languages is perfectly fine for ~90% (Zuck's number here) of the calls.No it isn't. Attempts to do this are why I mash 0 repeatedly and chant \"talk to an agent\" after being in a phone tree for longer than a minute.\n \nreply",
      "And you don't think that this won't improve with better bots?\n \nreply",
      "> And you don't think that this won't improve with better bots?Actually, now that I think about it, yeah.The whole purpose of the bots is to deflect you from talking to a human. For instance: Amazon's chatbot. It's gotten \"better\": now when I need assistance, it tries three times to deflect me from a person after it's already agreed to connect me to one.Anything they'll allow the bot to do can probably can be done better by a customer facing webpage.\n \nreply",
      "I try to enunciate very clearly: \"What would you like to do?\" - \"Speak to a fcuking human. Speak to a fcuking human. Speak to a fcuking human. Speak to a fcuking human.\"\n \nreply"
    ],
    "link": "https://arstechnica.com/ai/2025/05/time-saved-by-ai-offset-by-new-work-created-study-suggests/",
    "first_paragraph": "\n        Survey of 2023\u20132024 data finds that AI created more tasks for 8.4 percent of workers.\n      A new study analyzing the Danish labor market in 2023 and 2024 suggests that generative AI models like ChatGPT have had almost no significant impact on overall wages or employment yet, despite rapid adoption in some workplaces. The findings, detailed in a working paper by economists from the University of Chicago and the University of Copenhagen, provide an early, large-scale empirical look at AI's transformative potential.In \"Large Language Models, Small Labor Market Effects,\" economists Anders Humlum and Emilie Vestergaard focused specifically on the impact of AI chatbots across 11 occupations often considered vulnerable to automation, including accountants, software developers, and customer support specialists. Their analysis covered data from 25,000 workers and 7,000 workplaces in Denmark.Despite finding widespread and often employer-encouraged adoption of these tools, the study con",
    "summary": "**Another Day in AI Paradise: Workers Still Work, Bots Still Botch.** A recent survey joyfully reveals that the AI revolution is as <em>disappointing</em> as a soggy toast, failing to free humanity from the shackles of the 9-to-5, but oh boy, it sure can create extra checkboxes on your to-do list! In a hilarious turn of events, the AI that was supposed to handle our mundane tasks now has everyone scrambling to fix the new mess it creates\u2014<i>progress</i>, they said? Meanwhile, the ars technica comment section becomes an echo chamber of tech bros nostalgically reminiscing about the good old days of human inefficiency, completely oblivious to their own irrelevance in a world where even pseudo-intelligent bots can't make them more useful. \ud83e\udd16\ud83d\ude44"
  },
  {
    "title": "Understanding-j: An introduction to the J programming language that gets to the (github.com/bugsbugsbux)",
    "points": 33,
    "submitter": "todsacerdoti",
    "submit_time": "2025-05-03T20:40:59 1746304859",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=43882118",
    "comments": [
      "The title stops just short of the\n \nreply",
      "To the author: you can add a .devcontainer directory with a Dockerfile, allowing folks to try this in their browser with GitHub CodespacesShameless plug, feel free to copy my setup! https://github.com/jdan/try-j\n \nreply",
      "Unintentional humour with how the title got cut off?\n \nreply"
    ],
    "link": "https://github.com/bugsbugsbux/understanding-j",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        An introduction to the J programming language that gets to the point.\n      Found anything wrong? File an issue at\nhttps://github.com/bugsbugsbux/understanding-j/issues.An introduction to the J programming language that gets to the point.It is intended for those with (some) programming experience, but others\nshould be mostly fine after looking up some basic programming terms like\nfunction, argument, class, instance, inheritance, statement, expression,\netc.Don't treat this as a reference: Section titles do not introduce an\nexhaustive explanation of a certain topic, but serve to give this\ndocument a rough structure. Individual sections are not intended to be\nread in isolation from the others and assume the knowledge of previous\nsections.Run the examples and read the comments!If you have J installed you can open this file in JQt via th",
    "summary": "Welcome to the *exciting* world of J programming, explained so clearly that you'll need only half a dozen prerequisites and an insatiable desire to decipher cryptic syntax from a pre-emoji era. The friendly creators of this guide suggest a casual stroll through the Github issues page if anything bewilders you more than J itself (which is quite an accomplishment \ud83e\udd2f). Meanwhile, our heroic commenters ride to the rescue with indispensable advice like dockerizing your whole development environment, because obviously, what better way to learn a programming language than configuring containerized systems? And don't miss the incidental comedy gold from a title that chops itself off more abruptly than your enthusiasm when you actually start learning J."
  },
  {
    "title": "Helpcare AI (YC F24) Is Hiring (docs.google.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-05-03T21:00:18 1746306018",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://docs.google.com/forms/d/e/1FAIpQLScpzOyP_mk3muEpbKrnW8UTZB_yP5SJwjbeT8_6A6fhdvpJCg/viewform?usp=preview",
    "first_paragraph": "",
    "summary": "On this week's episode of Silicon Valley's Mad Libs, Helpcare AI, yet another YC prodigy, is hiring! Dive into the doc (if you *dare*) to discover a labyrinth of buzzwords and vague promises of changing the healthcare game. Commenters are lining up in the rare expert fashion only Hacker News can muster, providing free consulting while simultaneously seeking validation for their untested startup theories. Come for the revolutionary 'AI', stay for the intense scuffle over whether React or Vue is the *real* MVP of frontend frameworks. \ud83d\ude44"
  },
  {
    "title": "RethinkDNS Resolver That Deploys to CF Workers, Deno Deploy, Fastly, Fly.io (github.com/serverless-dns)",
    "points": 69,
    "submitter": "indigodaddy",
    "submit_time": "2025-05-03T18:04:14 1746295454",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=43880883",
    "comments": [
      "I doubt their choice of implementing something so performance-critical as a DNS server in JavaScript. It seems like a slow language for that purpose.When I was looking for ad-block solutions on Android, Rethink DNS was actually on the top of my list. However, when I found out that their server was written in JavaScript, I did some benchmarking.Rethink's server processed DNS requests in 400-500ms, which could potentially make a new webpage render up to half a second slower the first time:  ~ > curl -w \"DNS Time: %{time_namelookup}s\\nConnect: %{time_connect}s\\nTotal: %{time_total}s\\n\" -H \"accept: application/dns-message\" -H \"content-type: application/dns-message\" --data \"<binary-data>\" -o nul -s https://sky.rethinkdns.com/1:6AcDACIBLIDAAFQwIAAACA==\n  DNS Time: 0.004995s\n  Connect: 0.142332s\n  Total: 0.462496s\n\nWhile the Cloudflare's server took just 5-10ms, as seen below:  ~ > curl -w \"DNS Time: %{time_namelookup}s\\nConnect: %{time_connect}s\\nTotal: %{time_total}s\\n\" -H \"accept: application/dns-message\" -H \"content-type: application/dns-message\" --data \"<binary-data>\" -o nul -s 1.1.1.1\n  DNS Time: 0.000026s\n  Connect: 0.006273s\n  Total: 0.008822s\n\nIn the end, I chose AdAway and have stuck to that choice.\n \nreply",
      "> Rethink's server processed DNS requests in 40-60ms, which could potentially make a new webpage render up to 50ms slower the first time, while the system default took just 5-10msYou can choose \"system default\" in the Rethink Android app from Configure -> DNS -> System DNS. You don't have to use Rethink's DoH (DNS over HTTPS) / DoT (DNS over TLS) servers. You can setup any DNSCrypt, Oblivious DoH, or plain old DNS endpoint with Rethink to forward DNS requests to.> I chose AdAway and have stuck to that choice for nowA decent choice.A word of caution when using it in non-root mode, though: AdAway borrows code from another (now discontinued but pretty solid) project viz. DNS66. From when I looked at the code, DNS66 never handled DNS over TCP. You can test this with Termux: dig +tcp example.com A.> doubt their choice of implementing something so performance-critical as a DNS server in JavaScriptFor a remotely-hosted DNS resolver, network latency & concurrency is likely to dominate than performance of the code itself. You're right that JS is slow (as compared to Rust, say), but looking at metrics from serving 300m to 3bn queries/day on Cloudflare, our median (CPU time) spent processing a DNS request was consistently less than 2ms, and IO wait (there are no \"disks\" so, this is mostly hitting the caches or upstream resolvers) was estimated to be less than 7ms.Also, we employ a bunch of optimizations on client & the server, as appropriate (if we're missing any we should implement, let us know).- Coalesce multiple requests into one upstream request: For example, if there's 40 clients all wanting to resolve the same domain (say, ipv4only.arpa) within milliseconds of each other, only one request is upstreamed.- Utilize in-process LFU cache and Cloudflare's Cache API to avoid upstreaming where possible.- Connection pool egress to avoid reconnect overheads.- Race response for a query from multiple upstreams in parallel.- Support TLS session resumption.- Prefer the lighter TLS_AES_128_GCM_SHA256 cipher suite.- Dynamically adjust the TLS frame size. DNS queries aren't that big.- Disable Nagling on TCP.- Rehydrate responses for top domains stored in the in-process cache in the background.- Admission Control (load shedding) & queuing disciplines (CoDel etc)---> curl -w \"DNS Time: %{time_namelookup}s\\nConnect: %{time_connect}s\\nTotal: %{time_total}s\\n\" -H \"accept: application/dns-message\" -H \"content-type: application/dns-message\" --data \"<binary-data>\" -o nul -s 1.1.1.1Shouldn't the switch \"-s\" be \"https://one.one.one.one/dns-query\"? For DoH perf specifically, I use https://github.com/ameshkov/godnsbench as it supports parallel and cache-busting (random) queries.Ex:Rethink on Workers:  ./godnsbench -a https://sky.rethinkdns.com/dns-query -p 10 -c 300 -t 1 -q {random}.dnsleaktest.com\n  godnsbench with the following configuration:\n  {\n    \"Address\": \"https://sky.rethinkdns.com/dns-query\",\n    \"Connections\": 10,\n    \"Timeout\": 1,\n    \"QueriesCount\": 300,\n  }\n\n  The test results are:\n  Elapsed: 6.120078941s\n  Average QPS: 49.018394\n  Processed queries: 300\n  Average per query: 20.400765ms\n  Errors count: 0\n\nCloudflare 1.1.1.1:  ./godnsbench -a https://cloudflare-dns.com/dns-query -p 10 -c 300 -t 1 -q {random}.dnsleaktest.com\n  godnsbench with the following configuration:\n  {\n    \"Address\": \"https://cloudflare-dns.com/dns-query\",\n    \"Connections\": 10,\n    \"Timeout\": 1,\n    \"QueriesCount\": 300,\n  }\n\n  The test results are:\n  Elapsed: 6.780474724s\n  Average QPS: 44.244383\n  Processed queries: 299\n  Average per query: 22.677588ms\n  Errors count: 1\n \nreply",
      "Cool!\n\"Pi-hole -esque\" is a nice descriptor.Tangent: Bunny.net is my new favorite CDN / cloud service provider. They have scriptable DNS too.\n \nreply",
      "The Rethink DNS mobile app hasn't seen a release in years. Is the project still alive?https://github.com/celzero/rethink-app\n \nreply",
      ">in years.not even past 1 year.See https://github.com/celzero/rethink-app/discussions/1884\n \nreply",
      "> the development happens on the lead developer's feature branchOof. This makes me cringe so hard. I once took over a project (but the developer didn't know they were getting fired) and the guy was doing everything on his laptop, from his laptop. Deployments and builds were from his laptop. Even dependencies weren't checked into the code (using global installs of them on unknown versions). The owner had me come in because after talking to several people realized he was in a bad place.It took me ~2 months to learn everything and document all the things. Then the owner fired him. That guy kept development back for so long by simply not documenting/sharing code and configuration. Now there's an entire team with a healthy development flow. But wow, I had some flashbacks reading that...\n \nreply"
    ],
    "link": "https://github.com/serverless-dns/serverless-dns",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        The RethinkDNS resolver that deploys to Cloudflare Workers, Deno Deploy, Fastly, and Fly.io\n      serverless-dns is a Pi-Hole esque content-blocking, serverless, stub DNS-over-HTTPS (DoH) and DNS-over-TLS (DoT) resolver. Runs out-of-the-box on Cloudflare Workers, Deno Deploy, Fastly Compute@Edge, and Fly.io. Free tiers of all these services should be enough to cover 10 to 20 devices worth of DNS traffic per month.RethinkDNS runs serverless-dns in production at these endpoints:Server-side processing takes from 0 milliseconds (ms) to 2ms (median), and end-to-end latency (varies across regions and networks) is between 10ms to 30ms (median).\u2003The Rethink DNS resolver on Fly.io is sponsored by FOSS United.Cloudflare Workers is the easiest platform to setup serverless-dns:For step-by-step instructions, refer:To setup blocklists, visit http",
    "summary": "**High-Tech DNS Blocking or Clever Bloatware?**\n\nThe world of DNS resolution is abuzz with the latest \"innovation\" from RethinkDNS, a service which combines the zippy overhead of JavaScript with the breakneck speeds of serverless architecture to deliver web pages at the pace of a leisurely Sunday drive. This groundbreaking technology ensures that, with every DNS request, you have just enough time to question all your life choices before the content actually loads. The RethinkDNS community, comprising true masochists of web performance, debates fiercely over milliseconds while the rest of the world wonders why their cat videos aren\u2019t loading. Who knew turning browsing into a time travel to the dial-up era could be met with such enthusiasm? \ud83d\udc22\ud83d\udca4"
  },
  {
    "title": "Vygotsky's Zone of Proximal Development (simplypsychology.org)",
    "points": 4,
    "submitter": "Anon84",
    "submit_time": "2025-05-04T00:40:59 1746319259",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.simplypsychology.org/zone-of-proximal-development.html",
    "first_paragraph": "",
    "summary": "Another day, another desperate scrape at the crumbling cliff face of educational theory. Today on SimplePsychology.org, we venture into the \"Zone of Proximal Development,\" a mystical realm conjured by Vygotsky, where learners magically achieve more with a helper than alone - like doing your taxes with a calculator. Commenters, in a dazzling display of missing the point, are fiercely debating whether Pok\u00e9mon or Minecraft is the better analogy, thereby demonstrating their own placement just outside the Zone of Useful Contribution. \ud83c\udf93\ud83d\udcc9"
  },
  {
    "title": "We fell out of love with Next.js and back in love with Ruby on Rails (hardcover.app)",
    "points": 274,
    "submitter": "mike1o1",
    "submit_time": "2025-05-03T18:26:06 1746296766",
    "num_comments": 256,
    "comments_url": "https://news.ycombinator.com/item?id=43881035",
    "comments": [
      "Just my opinion but, server\u2011side rendering never really went away, but the web is finally remembering why it was the default. First paint and SEO are still better when markup comes from the server, which is why frameworks as different as Rails + Turbo, HTMX, Phoenix LiveView, and React Server Components all make SSR the baseline. Those projects have shown that most dashboards and CRUD apps don\u2019t need a client router, global state, or a 200 kB hydration bundle\u2014they just need partial HTML swaps.The real driver is complexity cost. Every line of client JS brings build tooling, npm audit noise, and another supply chain risk. Cutting that payload often makes performance and security better at the same time. Of course, Figma\u2011 or Gmail\u2011class apps still benefit from heavy client logic, so the emerging pattern is \u201cHTML by default, JS only where it buys you something.\u201d Think islands, not full SPAs.So yes, the pendulum is swinging back toward the server, but it\u2019s not nostalgia for 2004 PHP. It\u2019s about right\u2011sizing JavaScript and letting HTML do the boring 90 % of the job it was always good at.\n \nreply",
      "> they just need partial HTML swaps.Been a web dev for over a decade, and I still use plain JS. I have somehow managed to avoid learning all the SPAs and hyped JS frameworks. I used HTMX for once project, but I prefer plain JS still.I was a JQuery fan back in the day, but plain JS is nothing to scoff at these days. You are right though, in my experiences at least, I do not need anything I write to all happen on a single page, and I am typically just updating something a chunk at a time. A couple of event listeners and some async HTTP requests can accomplish more than I think a lot of people realize.However, if I am being honest, I must admit one downfall. Any moderately complex logic or large project can mud-ball rather quickly -- one must be well organized and diligent.\n \nreply",
      "Having a server provide an island or rendering framework for your site can be more complex than an SPA with static assets and nginx.You still have to deal with all the tooling you are talking about, right? You\u2019ve just moved the goalpost to the BE.And just like the specific use cases you mentioned for client routing I can also argue that many sites don\u2019t care about SEO or first paint so those are non features.So honestly I would argue for SPA over a server framework as it can dramatically reduce complexity. I think this is especially true when you must have an API because of multiple clients.I think the DX is significantly better as well with fast reload where I don\u2019t have to reload the page to see my changes.People are jumping into nextjs because react is pushing it hard even tho it\u2019s a worse product and questionable motives.\n \nreply",
      "I think the DX is significantly better as well with fast reload\u2026\n\nAs a user, the typical SPA offers a worse experience. Frequent empty pages with progress bars spinning before some small amount of text is rendered.\n \nreply",
      "> As a user, the typical SPA offers a worse experience.Your typical SPA has loads of pointless roundtrips.  SSR has no excess roundtrips by definition, but there's probably ways to build a 'SPA' experience that avoids these too.  (E.g. the \"HTML swap\" approach others mentioned ITT tends to work quite well for that.)The high compute overhead of typical 'vDOM diffing' approaches is also an issue of course, but at least you can pick something like Svelte/Solid JS to do away with that.\n \nreply",
      "My biggest annoyance with SPAs is that they usually break forward/back/history in various subtle (or not so subtle) ways.Yes, I know that this can be made to work properly, in principle. The problem is that it requires effort that most web devs are apparently unwilling to spend. So in practice things are just broken.\n \nreply",
      "I still have some ptsd from payment gateway integrations via iframes about 6-7 years ago. If you thought SPAs are bad by themselves for history tracking imagine those banking iframes randomly adding more entries via inside navigation/redirection that you have to track manually.\n \nreply",
      "A lot can be said for just putting a \"back\" button a page. I still do it occasionally for this very reason. Then again, my user base for the apps I write are the most non-technical folks imaginable, so many of them have no concept of a browser back button to begin with. I am not being hyperbolic either.",
      "An additional one for me is stale state. I can leave most webpages open for days, except SPAs. Especially on mobile.\n \nreply",
      "> Your typical SPA has loads of pointless roundtripsThis is an implementation choice/issue, not an SPA characteristic.> there's probably ways to build a 'SPA' experience that avoids these tooPWAs/service workers with properly configured caching strategies can offer a better experience than SSR (again, when implemented properly).> The high compute overhead...I prefer to do state management/reconciliation on the client whenever it makes sense. It makes apps cheaper to host and can provide a better UX, especially on mobile.\n \nreply"
    ],
    "link": "https://hardcover.app/blog/part-1-how-we-fell-out-of-love-with-next-js-and-back-in-love-with-ruby-on-rails-inertia-js",
    "first_paragraph": "By Adam FortunaThis is part 1 of a series documenting Hardcover\u2019s Alexandria release. We recently migrated our codebase from Next.js to Ruby on Rails, and it\u2019s been amazing so far! It was a learning experience, and I\u2019m excited to share some of our takeaways. I\u2019ll link each article here as it\u2019s written.Today\u2019s focus is on the main reason for the move from Next.js to Ruby on Rails. This is the first question anyone asks, and the most important one. So let\u2019s dive into it.Just a side note: this is going to be a software development related post, not a book related posts. If you\u2019re here for the Book Vibes, I\u2019d encourage you read about the release first.When Hardcover started, I was primarily a Ruby on Rails developer. I had experience with JS frontends, but Rails was my jam. I\u2019ve been building things in it since before Rails 1.0, worked at multiple startups that use it, built courses to teach it, spoken at meetups about it, and been to conferences.I\u2019ve been all in with Rails for a while.In ",
    "summary": "**We Fell Out Of Love With Next.js And Back In Love With Ruby On Rails**\n\nIn an unsurprising twist of nostalgia and practicality, the digital artisans at Hardcover find themselves abandoning the shiny modernity of Next.js for the comforting, elderly embrace of Ruby on Rails. Surely, this retrograde motion is chronicled as a bold 'learning experience'\u2014an enlightening journey from the fads of JavaScript back to good ol' server-rendered HTML, which apparently never really went out of style (just like flared jeans and comeback tours). Comment sections ignite with tech hipsters and server-side aficionados arrhythmically clapping, reminiscing over the 'good times' when websites were websites, and JavaScript was just a nice-to-have for scrolling text and pop-up ads. The debate resembles a tech-renaissance fair, where everyone\u2019s arguing over the best way to turn back time, while inadvertently proving that new isn't always better\u2014but it certainly keeps the blog mills running. \ud83c\udfa0\ud83d\udc74\ud83d\udcbe"
  },
  {
    "title": "Buffett to step down following six-decade run atop Berkshire (bloomberg.com)",
    "points": 277,
    "submitter": "mfiguiere",
    "submit_time": "2025-05-03T18:16:54 1746296214",
    "num_comments": 196,
    "comments_url": "https://news.ycombinator.com/item?id=43880973",
    "comments": [
      "https://archive.ph/zXRmj",
      "He's 94 years old, it's honestly impressive he was still there.\nStill, end of an era.\n \nreply",
      "> Still, end of an era.That is an enormous understatement. At 94, Buffet has witnessed history and has represented a vision of betterment, a just society, and sound economic stewardship. These things were not and are not hallucinations.But for many, it seems, the lessons about co-operation, stable economic growth. the sane application of democratic principles, and the rule of law must be learned again.It's the end of an era bona fides and respect for truth and accountability as the structural integrity of democratic society.\n \nreply",
      "Buffet reminded me a lot of Angela Merkel.They stood clearly and simply for good moral judgment, fair systems and looked at the bigger picture to carry most people forward.  They also based all their decision in facts, truth and science. They learn't their trades (economics & politics) over time and weren't afraid to adapt as times changed.Their slow and steady presence did more for equality and fairness than many others. We will need to find these values again after the current times have played out.\n \nreply",
      "That has to be about a different Angela Merkel, the one I know had one priority: preserving status quo.\n \nreply",
      "Which recent German politician in power wasn't about preserving the status quo? The entire country and culture is all about the status quo.\n \nreply",
      "Well when the alternative is right wing fascism, I'll take the status quo thank you.\n \nreply",
      "Feh, Merkel created conditions for the rise of right wing fascism... IMO.She and her finance minister championed austerity uber alles, squeezing the middle and lower class across the EU, and then she said refugees were welcome. I agree with the idea of helping people fleeing bombs and bullets, but after decades of saying there's money, we need to watch our budgets, suddenly there's money? Nooo fucking wonder the populist right managed to grasp on to the disillusionment of the lower/middle class.\n \nreply",
      "Measured governance. I feel the same, although I'm sure the Europeans and Germans not at all, but they also talked Angela out of nuclear, so one might argue the Germans are a bit too measured compared to their Chancellor.\n \nreply",
      "> They stood clearly and simply for good moral judgmentMerkel has severely underestimated Putin. She played a role in the continuous betting on Russian oil. Merkel has called the internet \u201cneuland\u201d and wasn\u2019t her government also the one starting with hydrogen subsidies. I donno about you but to this day I only hear lots of talk about hydrogen but near zero results. So all the wrong bets.Also I don\u2019t know whether you noticed but Germany is expected to be in a recession for three years in a row now.About equity and fairness okay I guess you are right. Everybody in Germany will be poor if things continue like this.\n \nreply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2025-05-03/warren-buffett-to-step-down-from-berkshire-hathaway-at-year-end",
    "first_paragraph": "",
    "summary": "The Oracle of Omaha finally decides to retire at 94, launching a flood of internet know-it-alls into one last tearful, overwrought adulation session. Buffet's stepping down is somehow both an epoch-ending event and a reminder that yes, people get old and retire, even demi-god investors. The ensuing comments awkwardly equate Buffet to Merkel with emotional acrobatics worthy of Cirque du Soleil, arguing over her legacy with passion usually reserved for canceled TV series. Watch as everyone suddenly becomes an expert in economic policy, German politics, and the moral fabric of society, all while lamenting the terrifying void left by an old man who decided to enjoy his billions in peace. \ud83d\ude44\ud83d\udcb8\ud83d\udcc9"
  },
  {
    "title": "Creating the Commodore 64: The Engineers' Story (ieee.org)",
    "points": 43,
    "submitter": "amichail",
    "submit_time": "2025-04-30T01:04:24 1745975064",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=43840057",
    "comments": [
      "I'm currently reading \"Too Much Fun: The Five Lives of the Commodore 64 Computer\" by Jesper Juul [1]. An interesting look at the history and influence of the C64 in games, the Demoscene and as a retro computer. He also has a companion website [2].[1] https://direct.mit.edu/books/monograph/5883/Too-Much-FunThe-...[2] https://jesperjuul.net/c64/\n \nreply",
      "(1985)Previous discussion: https://news.ycombinator.com/item?id=30895210If you're interested in the history of Commodore, I thoroughly recommend the Commodore International Historical Society at https://commodore.international/.  Dave has pulled together many of the people who were there at the time.  For example, here's an Amiga panel from the recent VCF East: https://www.youtube.com/watch?v=r_AYDkuMg-U\n \nreply",
      "And the part 2 of the Amiga Panel, https://www.youtube.com/watch?v=vvTOFYykBTQThere is an older one from VCF, which were a couple of sessions, where I was proven wrong, some of the engineers actually wanted the Amiga to be UNIX like first, the way it turned out to be was a sequence of decisions that eventually made it the way it was in the end. Thankfully, as it had plenty of cool ideas, many of which never got out of the Amiga into other platforms.Looking forward to watch these.\n \nreply",
      "There are also Brian Bagnall's Commodore history books, of course.\n \nreply",
      "> Running a 5-micrometer-technology chip at an 8-MHz clock rate caused it to dissipate a great deal of power\u2014nearly 1.5 watts.So quaint in our era of 500w graphics cards.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/commodore-64",
    "first_paragraph": "The May issue of IEEE Spectrum is here!The daring and design that went into the best-selling computer of all timeIn January 1981, a handful of semiconductor engineers at MOS Technology in West Chester, Pa., a subsidiary of Commodore International Ltd., began designing a graphics chip and a sound chip to sell to whoever wanted to make \u201cthe world\u2019s best video game.\u201d In January 1982, a home computer incorporating those chips was introduced at the Winter Consumer Electronics Show in Las Vegas, Nev. By using in-house integrated-circuit-fabrication facilities for prototyping, the engineers had cut the design time for each chip to less than nine months, and they had designed and built five prototype computers for the show in less than five weeks. What surprised the rest of the home-computer industry most, however, was the introductory price of the Commodore 64: $595 for a unit incorporating a keyboard, a central processor, the graphics and sound chips, and 64 kilobytes of memory instead of th",
    "summary": "**Title: Electronics Nostalgia: Out-engineering the '80s with Good Old Commodore**\n\nThis month, IEEE Spectrum takes readers on a magically mundane tour back to the daring days of the early '80s, where a handful of engineers in a quaint Pennsylvanian town (yes, engineers from *Pennsylvania*) concocted the Commodore 64, throwing the whole home computer industry into a moderate tizzy with their \"world's best video game\" maker. The engineering prowess displayed by compressing prototypes timelines and somehow connecting keyboards to processors is dissected with breathless awe, managing to make the introductory price tag of $595 seem like the deal of the century\u2014even though it could probably only get you half a bitcoin nowadays. Meanwhile, the comments section morphs into a digital museum as various netizens engage in over-excitable tech necromancy, resuscitating dead URLs and dusting off old books that revisit every delightful transistor of the machine. Whether it's discussing old panels from 'recent' VCF East conferences or the \"quaint\" power consumption of 5-micrometer chips, every commenter is seemingly trapped in a wistful loop of 8-bit nostalgia, forever reliving the glory days when pixels were big, and user manuals were bigger. \ud83d\udd79\ufe0f\ud83d\udcbe\ud83d\udc7e"
  }
]