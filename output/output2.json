[
  {
    "title": "Show HN: X11 desktop widget that shows location of your network peers on a map (github.com/h2337)",
    "points": 29,
    "submitter": "h2337",
    "submit_time": "2025-07-21T00:16:25 1753056985",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=44630648",
    "comments": [
      "Pretty cool! Reminds me of the game Uplink.reply",
      "Forgive my ignorance, but I'm not sure what this is showing me. I'm running it on my home linux system, which is connected to the Internet via Verizon FIOS. The map shows three red dots, none of which are near me.reply",
      "Those 3 dots are your peers, the other end of the TCP connection :)So you basically have some apps running in the background (or foreground) that are making those connections.reply",
      "Maybe they were expecting first hops like from traceroute.  Maybe traceroute is an interesting way to continue developing.reply",
      "Okay, got it, thanks. I suppose it could also be the FIOS router itself making those connections, or any of the other systems on my local network.reply",
      "No, for normal network configurations they wouldn't show. It's most likely your system connmap is running on making those connections.reply",
      "That's a really neat idea, damn.reply",
      "No basically secure:char mapFilename[256];\n  strcat(strcpy(mapFilename, getenv(\"HOME\")), RESOURCES);\n  strcat(mapFilename, mapName);reply",
      "While that's indeed a bug, for it to be a security vulnerability, wouldn't there also have to be a security boundary involved? Specifically, mapName is always either \"w1000b.png\" or \"w1000.png\", so the only way to trigger the buffer overflow would be through the HOME environment variable. But if an attacker can run commands as you with arbitrary environment variables, aren't you already pwned? What would anyone gain by running your program and exploiting it to do something, rather than just doing the thing directly? https://devblogs.microsoft.com/oldnewthing/20060508-22/?p=31...reply",
      "What's insecure? Can you explain what's the vulnerability here and how and by whom can it be exploited?reply"
    ],
    "link": "https://github.com/h2337/connmap",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        connmap is an X11 desktop widget that shows location of your current network peers on a world map in real-time\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.connmap is an X11 desktop widget that shows location of your current network peers on a world map.\n(Works on Wayland as well!)\n\nClone the repository git clone https://github.com/h2337/connmap --depth 1, install the dependencies (see below), run make install, then run the resulting executable ./connmap.elf.If you want to run it without attaching it to the terminal then add ampersand at the end of the command: ./connmal.elf &.You can also add it to your i3wm config to run at startup (make sure it is in the PATH):Build dependencies: xlib, libxext, libxfixes, libcairo2Installation dependencies: unzipRuntim",
    "summary": "**HN Launches DIY NSA Kit**\n\nIn a daring display of nostalgia for the early 2000s, a lone hero gifts Hacker News a new X11 widget called \"connmap,\" which lets you pinpoint the exact locations of your network foes\u2014or friends\u2014like some kind of low-budget Bond villain. This groundbreaking tool not only crashes consistently but also comes packaged with a potential security blunder that allows it to blend into the elite circles of HN\u2019s software showcase. The comment section, a veritable circus of confusion, sees users flexing their traceroute knowledge and casually diagnosing security vulnerabilities. Meanwhile, someone is bound to suggest running it in Docker **just because**. \ud83c\udf0d\ud83d\udd0d\ud83d\udca5"
  },
  {
    "title": "Staying cool without refrigerants: Next-generation Peltier cooling (samsung.com)",
    "points": 164,
    "submitter": "simonebrunozzi",
    "submit_time": "2025-07-20T20:18:54 1753042734",
    "num_comments": 121,
    "comments_url": "https://news.ycombinator.com/item?id=44628930",
    "comments": [
      "For those worried about tiny COPs from these gizmos, trawling through the actual paper -- as well as the PR from JHU APL -- in this HN post [1] shows claims of COPs of ~15 for Delta Ts of 1.3\u00b0C.A compressor based cooler gets a COP of about 4 in the real world. I'm pretty sure this is an apples to oranges comparison to an expert (I am not one of those) but a factor of 3+ increase in COP is fairly noteworthy -- if it holds up.[1] https://news.ycombinator.com/item?id=44424087reply",
      "ugh, reading the paper, their methodology is kinda crap. They basically just guess what the thermal resistances in their system are, and use air temperature measurements to figure out the heat flow. This is not how to measure heat flow accurately. It might be OK as a comparison with whatever TEC they tested with, maybe, but it's not at all something I would trust to compare to another test setup. If their box is more insulative than they think it is, their results are gonna look better than reality. This can be validated at least approximately by just putting a heater in the box that's dissipating a known amount of heat and looking at the temperature rise, but it seems they didn't even do this. And in general the regime where you've got small temperature differences is where your systematic error in a system like this can become huge and distort the results by multiples.(This is an area which is really hard and details matter. Heat is basically impossible to measure directly, and the indirect measurements are fraught with peril. Getting it wrong was a large part of why people thought they had demonstrated cold fusion)reply",
      "For anyone wondering (like me) what COP is in this context, it\u2019s Coefficient of Performance: https://en.m.wikipedia.org/wiki/Coefficient_of_performancereply",
      "I believe the \u201capples to oranges\u201d is the temperature gradient. AC units would routinely manage 15-20c and are rated for more than that. And some freezers manage up to 50c. The greater the gradient the worse the efficiency in general.reply",
      "COP has to be compared at the same delta T. The COP of a loss less refrigerator is infinite at Delta T = 0And you dont get to stack Peltiers to increase COP, only to increase delta T.Still, Peltiers are super cool and I have some ideas for their use od they get slightly better. Advances are super welcome.reply",
      "One can only hope some kind of phonon diode material can exist that a slight voltage can overcome something so inescapable as entropy by providing it only lanes that suit us.reply",
      "The idea was just so astonishing that I ordered some from American Science and Surplus. I connected the leads to a battery and poof, one side got hot and the other got cold. Blew my mind.I didn't actually have a use for it. It was just neat that it actually worked.I understand the basic physics of it perfectly well. It's just one of those things where you expect basic physics to be overwhelmed by friction or something.reply",
      "What about a \u0394T of say 20\u00b0C? I'd reckon most refrigerators and air conditioners are around there (temp difference of refrigerant between evaporator and condenser).Stacking a bunch of these Peltiers  to give more temperature difference would give a pretty low CoP. Say, for a 13\u00b0C temperature difference you'd have to stack 10 of them and use 10x the power. It's even worse actually as the hotter ones have to also pump the waste heat from the cooler ones.reply",
      "Note that a small temperature difference that is sustained very consistently over a long time using a tiny amount of electricity (let's say half of what the parent post cited, so like a COP of 8) could add up to a lot of nearly-free cooling. You'd chill your walls for weeks and when a heat wave comes with hot nights for a week, if you(r home automation) close(s) the blinds during the heat of the day, the more-powerful AC might barely have to do anythingJust an idea of course, but I'd not write new tech off as \"ok but just 1.3 degrees who cares\" when the claimed COP is so insanely good without first trying it outreply",
      "The \u0394T between evaporator (typically 2-6\u00b0C while cooling) and condenser (often 40-50\u00b0C in cooling mode) is much higher than 20\u00b0C. The condenser is often almost 20\u00b0C above ambient outside temperature.The design \u0394T of ~10\u00b0C is the typical return-to-supply air \u0394T.reply"
    ],
    "link": "https://news.samsung.com/global/interview-staying-cool-without-refrigerants-how-samsung-is-pioneering-next-generation-peltier-cooling",
    "first_paragraph": "",
    "summary": "In a breathtaking display of technological wizardry, Samsung graciously unveils a Peltier cooler that promises to save the world from refrigerants, by moving heat from here to slightly over there (\ud83c\udf21\ufe0f1.3\u00b0C difference, hold the applause). Commenters dive into a mosh pit of technical jargon, passionately debating COPs, \u0394Ts, and whether comparing this to traditional cooling systems is like comparing apples to mutated, tech-savvy oranges. Amidst a storm of links and poorly understood physics lessons, one brave soul discovers the revolutionary concept of **Coefficient of Performance** and shares it with the class. Meanwhile, everyone misses the underlying miracle: a device that turns electricity into a slightly cooler spot on your desk, potentially changing nothing but the discourse in cooling technology forever. \ud83c\udf4f\ud83c\udf4a"
  },
  {
    "title": "LLM Alloying Improves Performance over Single Model (xbow.com)",
    "points": 20,
    "submitter": "summarity",
    "submit_time": "2025-07-21T00:33:04 1753057984",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=44630724",
    "comments": [
      "I'm curious if this would also improve small local models. E.g. if I \"alloy\" Qwen3-8B and OpenThinker-7B is it going to be \"better\" than each models? I'll try testing this in my M1 Pro.reply",
      "For an internal workflow where we have an LLM looking at relatively simple data (where the conclusions the LLM may make vary widely depending on what the LLM believes the data represents) we found that taking a consortium approach, where you have multiple models approach the same problem at once and then essentially argue about the results, yields far better outcomes than if you have a single model performing the analysis, or even a single model arguing against itself multiple times. Somewhat adjacent to what\u2019s done here, but it\u2019s clearly true that having model diversity is a plus.reply",
      "The article talks about that at the end, then says:> Let models talk to each other directly, making their own case and refining each others\u2019 answers. Exemplified in patterns like Multi-Agent Debate, this is a great solution for really critical individual actions. But XBOW is basically conducting a search, and it doesn\u2019t need a committee to decide for each stone it turns over whether there might not be a better one.In general, this seems reasonable to me as a good approximation of what works with humans, but with _much_ faster feedback loops in communication.reply",
      "Stack 3 models together, then 4...Congratulations you just have a very expensive simulation of a Baysian function (ish, close enough that one should get the point).reply",
      "Anyone else try this?reply"
    ],
    "link": "https://xbow.com/blog/alloy-agents/",
    "first_paragraph": "",
    "summary": "In the latest technological marvel, <i>LLM Alloying Improves Performance over Single Model</i>, xbow.com enchants the tech community with the groundbreaking discovery that if you smash two AI models together, they magically get \u201cbetter.\u201d \ud83e\udd16\u2728 One bright spark in the comments suggests mixing \u201cQwen3-8B\u201d with \u201cOpenThinker-7B\u201d on his M1 Pro just to see what happens, because clearly his MacBook is the ideal proving ground for cutting-edge AI research. Another commenter muses on the efficacy of AI group therapy, proving we\u2019re just one step away from AIs holding weekly meetings to debate if cats or dogs make for better viral content. Meanwhile, someone rightly equates stacking models to a pricey approximation of a Bayesian function, because why pay less when you can overcomplicate things and pay more? \ud83e\udd11 Truly, we stand on the shoulders of giants!"
  },
  {
    "title": "XMLUI (jonudell.net)",
    "points": 444,
    "submitter": "mpweiher",
    "submit_time": "2025-07-20T14:03:39 1753020219",
    "num_comments": 235,
    "comments_url": "https://news.ycombinator.com/item?id=44625292",
    "comments": [
      "This was tried with Polymer way back in 2014 too, ex. making network requests with <iron-ajax> - https://github.com/PolymerElements/iron-ajaxThere was also Adobe Flex of the similar era that exists these days as Apache Royale:\nhttps://apache.github.io/royale-docs/features/mxmlThere was also XAML and inside Microsoft they made NetUI and FlexUI to make Office 2007+ too.It all seems great on paper, but in practice the markup abstraction turned out to be worse than code first solutions like JSX even for novices.reply",
      "And coldfusion shudderreply",
      "Jon has been around for a long time, and I've been a long-time fan. He's a bit of an elder: he's seen a lot of things, and he's worth listening to.> I\u2019m a fan of web components but it\u2019s the React flavor that dominate and they are not accessible to the kind of developer who could productively use Visual Basic components back in the day.I think this is the most important statement in the piece. The rest of the post explains the technical details, but this explains _why_ this exists. This is a bold statement, but I don't think he's wrong.Now, is XMLUI the _right_ set of abstractions to help these developers? We'll have to wait an see. But I love to see the attempt!reply",
      "code currently only works on JS evergreen browsers. i guess like VB only worked with windows and if shipped with the right DLLs.reply",
      "Windows is a narrower platform than evergreen JS-enabled browsers, and the relevance of Windows as a platform is only going to go down. So the choice of the platform is right.What are some GUI-enabled devices you wish you were able to address, but which cannot run Firefox, Safari, or a Chromium derivative?reply",
      "So, exactly like VB? (From memory it required a msvbvm DLL).reply",
      "I am simultaneously in the seemingly opposite camps of \"haha we reinvent HTML lol\" and \"Actually this sounds immediately useful to me\".To be human is to be multitudes.reply",
      "Thank you putting me onto this concept. I had not heard of Walt Whitman or his work before.\"Do I contradict myself? Very well, then I contradict myself.\"reply",
      "Beautifully said. All that matters in the end: will it be immediately useful to people like you who imagine that it might be?reply",
      "I wrote Qt C++ for 7 years as an open source contributor to KDE. This reminds me of QtWidgets\u2019 .ui files\u2014custom XML files following a specific schema. Later, Qt introduced QML, which I personally found unintuitive, and over time I lost interest in Qt altogether. That said, I still think XML for UI definitions makes sense, and it\u2019s understandable that some larger environments continue to use it.reply"
    ],
    "link": "https://blog.jonudell.net/2025/07/18/introducing-xmlui/",
    "first_paragraph": "Jon UdellStrategies for Internet citizens\nIn the mid-1990s you could create useful software without being an ace coder. You had Visual Basic, you had a rich ecosystem of components, you could wire them together to create apps, standing on the shoulders of the coders who built those components. If you\u2019re younger than 45 you may not know what that was like, nor realize web components have never worked the same way. The project we\u2019re announcing today, XMLUI, brings the VB model to the modern web and its React-based component ecosystem. XMLUI wraps React and CSS and provides a suite of components that you compose with XML markup. Here\u2019s a little app to check the status of London tube lines.\n\n\n\nA dozen lines of XML is enough to:\n\nThis is a clean, modern, component-based app that\u2019s reactive and themed without requiring any knowledge of React or CSS. That\u2019s powerful leverage. And it\u2019s code you can read and maintain, no matter if it was you or an LLM assistant who wrote it. I\u2019m consulting for ",
    "summary": "**XMLUI: The Revolutionary Non-Revolution in Web Development**\n\nJon Udell, the Internet's beloved grandpa, heaves another relic at the web development world with XMLUI\u2014essentially Visual Basic's ghost wrapped in a modern web costume. Apparently unaware that the world moved past XML when we stopped wearing cargo shorts, Udell insists that *binding React with XML* is the lever we've all been missing. Cue the nostalgia-fueled commentariat, reminiscing about the \"golden era\" with terrifying flashbacks to Adobe Flex and ColdFusion, as if any sane person would catch feelings for days when JavaScript was just a glint in Brendan Eich\u2019s eye. Meanwhile, in a universe where <em>useful</em> might be objective, we're supposed to believe that XMLUI is not just another episode of \"Tech Hipsters Try It Again.\""
  },
  {
    "title": "New colors without shooting lasers into your eyes (dynomight.net)",
    "points": 224,
    "submitter": "zdw",
    "submit_time": "2025-07-17T16:01:17 1752768077",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=44594808",
    "comments": [
      "For whatever reason, evolution decided those wavelengths should be overlapping. For example, M cones are most sensitive to 535 nm light, while L cones are most sensitive to 560 nm light. But M cones are still stimulated quite a lot by 560 nm light\u2014around 80% of maximum.The reason is simple: genes coding the long wave opsins (light-sensitive proteins) in these cones have diverged from copies of the same original gene. The evolution of this is very interesting.Mammals in general have only two types of cones: presumably they lost full color vision in the age of dinosaurs since they were primarily small nocturnal animals or lived in habitats with very limited light (subterranean, piles of leaves, etc.) Primates are the notable exception, and have evolved the third type of cone, enabling trichromatic color vision, as a result of their fruitarian specialization and co-evolution with the tropical fruit trees (same as birds, actually).So, what's interesting is that New World and Old World primates evolved this cone independently. In Old World primates the third cone resulted from a gene duplication event on the X chromosome, giving rise to two distinct (but pretty similar) opsin genes, with sensitivity peaks at very close wavelengths. As a note, because these genes sit on the X chromosome, colorblindness (defects in one or both of these genes) is much more likely to happen in males.New World primates have a single polymorphic opsin gene on the X chromosome, with different alleles coding for different sensitivities. So, only some (heterozygous) females in these species typically have full trichromatic vision, while males and the unlucky homozygous females remain dichromatic.Decent wikipedia article on the subject: https://en.wikipedia.org/wiki/Evolution_of_color_vision_in_p...Types of opsins in vertebrates:\nhttps://en.wikipedia.org/wiki/Vertebrate_visual_opsinreply",
      "This is only tangentially related, but I have always wondered why chlorophyll absorbs blue and red, but reflects green--green being sunlight's brightest component.It's almost as if there was some evolutionary pressure towards being very visible in sunlight which is more important than evolving ways to collect as much sun energy as possible. When I guess at this I end up with something along the lines of reflected green being used as a signal to a neighboring plant: \"I'm already here, grow in some other direction instead.\"  There is some evidence that plants do this (https://en.wikipedia.org/wiki/Crown_shyness, https://onlinelibrary.wiley.com/doi/10.1111/1365-3040.ep1160...) but it's not clear that the need to do so is so strong that it would overshadow the drive to collect as much energy as possible.Or perhaps there's something to do with the physics of absorbing light to drive a chemical reaction that makes it better to absorb at red and blue while passing on green (450nm and 680nm are not harmonics--so if this is the case it's more complex than which sorts of standing waves would fit in some chemical gap or other).reply",
      "This is fascinating, I\u2019d never realized there is this seeming-paradox! Thanks for mentioning itreply",
      "Have you looked into band-gaps?Also remember that these are random processes with selection pressure keeping those who survive to reproduce. Assigning a will to such processes makes them and the results harder to understand- imho.Theres probably something more efficient at converting light into simple sugars.reply",
      "It could also be to prevent overstimulation; \"maximize energy\" is not really the goal. A lot of plants can die from too much Sun unless their other inputs are just right (plenty of water, etc.).reply",
      "Maybe it was in response to an extinction level event that filtered sunlight for a long time, removing green but allowing primarily only blue or red.reply",
      "It still doesn't explain the need to reflect green, though. They could have evolved to be black and absorb all energy.reply",
      "Here's a recent take: https://www.quantamagazine.org/why-are-plants-green-to-reduc...TLDR: Plants are running an energy-harvesting system that can only respond so quickly to changes in light input. Making use of green would cause variance to be large enough that the gains would not offset the losses. So, avoid green and have lower variance --> higher energy capture on average.reply",
      "Red is nature's warning signal, and blue was already taken by the sky, so the only option left was green.Just kidding of course, it is an interesting question.reply",
      "This is a good biological explanation. The physical explanation is, if the sensitivities didn't overlap, our spectral sensitivity would not be continuous. There would be valleys of zero sensitivity between the cones, and a continuous wavelength sweep would result in us seeing black bands between colors.reply"
    ],
    "link": "https://dynomight.net/colors/",
    "first_paragraph": "\n\n    Jul 2025\n    \n\n\n\n    Mistakes?\n  \n\n\n\n\n\n\n\nfix plz\n\n(Just want to see what happens?)\n\n\nComments at \n\n  lemmy, \n\n  substack.\n\n\n\n\n\n\n\nYour eyes sense color. They do this because you have three different kinds of cone cells on your retinas, which are sensitive to different wavelengths of light.For whatever reason, evolution decided those wavelengths should be overlapping. For example, M cones are most sensitive to 535 nm light, while L cones are most sensitive to 560 nm light. But M cones are still stimulated quite a lot by 560 nm light\u2014around 80% of maximum. This means you never (normally) get to experience having just one type of cone firing.So what do you do?If you\u2019re a quitter, I guess you accept the limits of biology. But if you like fun, then what you do is image people\u2019s retinas, classify individual cones, and then selectively stimulate them using laser pulses, so you aren\u2019t limited by stupid cone cells and their stupid blurry responsivity spectra.Fong et al. (2025) choose fun.W",
    "summary": "<b>New Colors Without Needing New Eyes</b> \u2013 Because who in their right mind would accept the biological mediocrity of human vision? <i>Dynomight.net</i> decides it\u2019s high time we stop letting our eyeballs have all the say in what colors we can see. Through a series of likely eye-watering laser exercises, they outline a method to bypass evolutionary laziness and tickle each cone cell in isolation. The comments swiftly devolve into a tangential ramble about plant evolution, proving once more the internet\u2019s unparalleled ability to veer off-topic in the name of pseudo-scientific banter. \ud83c\udf08\ud83d\udc40"
  },
  {
    "title": "Stdio(3) change: FILE is now opaque (OpenBSD) (undeadly.org)",
    "points": 97,
    "submitter": "gslin",
    "submit_time": "2025-07-20T18:18:40 1753035520",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=44627793",
    "comments": [
      "If you've ever done this to a C library, the first thing that you'll look at when someone else does it is not the FILE type, but how stdin, stdout, and stderr have changed.The big breaking change is usually the historical implementation of the standard streams as addresses of elements of an array rather than as named pointers.  (Plauger's example implementation had them as elements 0, 1, and 2 of a _Files[] array, for example.)  It's possible to retain binary compatibility with unrecompiled code that uses the old getc/putc/feof/ferror/fclearerr/&c. macros by preserving structure layouts, but changing stdin, stdout, and stderr can make things not link.And indeed that has happened here.reply",
      "I think FreeBSD tried to opaque FILE[1], but it was reverted[2] and still non-opaque in main[3].[1]: https://github.com/freebsd/freebsd-src/commit/c17bf9a9a5a3b5...[2]: https://github.com/freebsd/freebsd-src/commit/19e03ca8038019...[3]: https://github.com/freebsd/freebsd-src/blob/main/include/std...reply",
      "OpenBSD tends to commit to breaking changes much more aggressively than others. Something tells me they're not reverting.reply",
      "I think FreeBSD is also more concerned with performance regression than OpenBSD is.reply",
      "FreeBSD's implementation of FILE is a nice object-oriented structure which anyone could derive from. Super-easy to make FILE point to a memory buffer or some other user code. I used that a bunch a long time ago.Obviously making FILE opaque completely breaks every program that used this feature, so no surprise it was reverted.reply",
      "So many words in the commit message and the announcement article but not a single mention of rationale? I have a bad feeling about their practice.reply",
      "Can someone elaborate? I always treated FILE as opaque, but never imagined people could poke into it?reply",
      "The MH and nmh mail clients used to directly look into FILE internals. If you look for LINUX_STDIO in this old version of the relevant file you can see the kind of ugliness that resulted:https://cgit.git.savannah.gnu.org/cgit/nmh.git/tree/sbr/m_ge...It's basically searching an email file to find the contents of either a given header or the mail body. These days there is no need to go under the hood of libc for this (and this code got ripped out over a decade ago), but back when the mail client was running on elderly VAXen this ate up significant time. Sneaking in and reading directly from the internal stdio buffer lets you avoid copying all the data the way an fread would. The same function also used to have a bit of inline vax assembly for string searching...The only reason this \"works\" is that traditionally the FILE struct is declared in a public header so libc can have some of its own functions implemented as macros for speed, and that there was not (when this hack was originally put in in the 1980s) yet much divergence in libc implementations.reply",
      "In gnulib, there is code that patches FILE internals for various platforms to modify behavior of <stdio.h> functions, or implement new functionality.https://cgit.git.savannah.gnu.org/cgit/gnulib.git/tree/lib/s...Yes, it's not a good idea to do this. There are more questionable pieces in gnulib, like closing stdin/stdout/stderr (because fflush and fsync is deemed too slow, and regular close reports some errors on NFS on some systems that would otherwise go unreported).reply",
      "Yes, that part of Gnulib has caused some problems previously. It is mostly used to implement <stdio_ext.h> functions on non-glibc systems. However, it is also needed for some buggy implementations of ftello, fseeko, and fflush.P.S. Hi Florian :)reply"
    ],
    "link": "https://undeadly.org/cgi?action=article;sid=20250717103345",
    "first_paragraph": "OpenBSD JournalHome\nArchives\nAbout\nSubmit Story\nCreate Account\nLogin\nContributed by\nrueda\non 2025-07-17\nfrom the more-opacity,-igor dept.In -current,\r\nthe struct underlying\r\nstdio(3)'s\r\nFILE type\r\nhas been\r\nmade opaque, with library versions bumps across the board:Make no mistake, this is a major change, which touches a lot of components across the base system. The fallout can be seen in a lot of places, including libcrypto (which is itself subject to a major, sepearate cleanup), libtls, libssl, and includes touches to the profiling subsystem we mentioned earlier.Those who normally build from source are\r\nstrongly encouraged\r\nto use a snapshot upgrade\r\nto cross the bumps.Reply\n\nCopyright \u00a9\n2004-2008\nDaniel Hartmeier.\nAll rights reserved.\nArticles and comments are copyright their respective authors,\nsubmission implies license to publish on this web site.\nContents of the archive prior to\nApril 2nd 2004 as well as images\nand HTML templates were copied from the fabulous original\ndeadly.org ",
    "summary": "**OpenBSD Boldly Renovates the Achy Joints of the FILE Struct, and Programmers Recoil in Mock Horror**\n\nIn a move that will shock precisely nobody who knows how to spell \"OpenBSD,\" the project heroes have yanked the curtains off the internals of stdio(3)'s FILE type, forcing it to dress in full opaqueness. This tremendous uphease in the filesystem echelon is sure to have the OpenBSD userbase \u2013 all dozen of them \u2013 scurrying to update their personal web servers and email a friend about it. Meanwhile, commenters on the sideline engage in a nostalgic dissection of the C library's undergarments, reminiscing over FILE's transparent days like estranged lovers of long-lost struct layouts. Yes, everyone is very upset by this tragic denial of peeking rights, in a community gatherings typically characterized by spirited threading about the virtues of simplicity, presumably with a straight face. \ud83d\udcbe\ud83d\udd12"
  },
  {
    "title": "Simulating Hand-Drawn Motion with SVG Filters (camillovisini.com)",
    "points": 113,
    "submitter": "camillovisini",
    "submit_time": "2025-07-17T09:12:26 1752743546",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=44591305",
    "comments": [
      "I have played around with using SVG effects as they are stunningly powerful, but I wouldn't recommend deploying them for too much: even though apparently Firefox does some level of GPU acceleration for SVG filter graphs, in practice a lot of very simple looking graphs I've tried constructing seem to fall back to CPU even with webrender.all turned on. This is kind of a shame because feTurbulence in particular is pretty useful. You could use it for a lot of things. One case I wanted to use it for was to make a gradient grainier, to reduce the obvious banding. Unfortunately, I found that it pegs all of the CPU cores on my laptop immediately :)reply",
      "Charming wiggling! Similar to Wobblepaint (drawing tool made  by the creator of PICO-8).https://www.lexaloffle.com/bbs/?tid=40058reply",
      "I had hand-done mine on https://kurnell.ai - its really cool to see that I could have computationally done this instead :)I used rive.app to encode the frames and create a state machine to move between the states. Perhaps I can simplify this even more.reply",
      "Really cool! Reminded me of this post [1] from ~2 weeks ago ago. Could it be combined with your approach?[1] https://news.ycombinator.com/item?id=44498133reply",
      "The technique is cool to know. SVG has many non-obvious abilities.But the effect, due to the way it's produced, is more like a hot air distortion, only without the faint shimmering. It's completely raster in nature, AFAICT, and is likely implemented as a GPU shader (which is good from the performance POV).An effect more like an unsteady human hand could likely be achieved by oscillating nodes in the direction perpendicular to the curvature, and adding some random jitter to the control points.reply",
      "I had to re-read the second paragraph to make sure you weren't saying that heat haze is a GPU shader effect. Using the phrase \"in nature\" really didn't help me!reply",
      "Wonderful, svg has so manu unexplored capabilities that feels like a crime using it mostly for icons on the webreply",
      "Dr. Katz.reply",
      "Home Movies!reply",
      "Kids these days, not knowing Dr. Katz...reply"
    ],
    "link": "https://camillovisini.com/coding/simulating-hand-drawn-motion-with-svg-filters",
    "first_paragraph": "",
    "summary": "**Simulating Your Nostalgia-Stained Doodles With Math Rocks**\n\nIn today's lesson at the high-tech arts and crafts corner, we learn how to use the magical SVG filters to simulate hand-drawn tremors. One intrepid commenter recalls the tragic tale of his laptop's CPU cores burning up in a valiant attempt to render a slightly less banded gradient using <em>feTurbulence</em>. Meanwhile, others reminisce about long-forgotten drawing tools and obscure internet threads, misty-eyed and full of a burning desire to make wobbly lines without actually trembling. The ghost of Dr. Katz hovers silently, waiting to psychoanalyze why we're all still trying to make our stable vectors look like they have the shakes. \ud83c\udfa8\ud83d\udcbb\ud83d\udd25"
  },
  {
    "title": "What birdsong and back ends can teach us about magic (digitalseams.com)",
    "points": 14,
    "submitter": "nkurz",
    "submit_time": "2025-07-20T23:59:39 1753055979",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44630551",
    "comments": [
      "> Sometimes magic is just someone spending more time on something...I'd suggest the real metric is \"attention to detail\", not \"time spent\".  It just so happens that attention to detail often requires time.  The real magic happens if one can manage to take care of the details without blowing out the time.reply",
      "> I\u2019ve never seen Merlin\u2019s Sound ID produce an obviously-wrong identificationWell, good luck identifying mockingbirds, lol. But that's a tricky case.Anyway, it's a nice way to use AI, unlike so many others that pop up lately.I wish they'd open source it all though, especially if they are getting help from volunteers.reply"
    ],
    "link": "https://digitalseams.com/blog/what-birdsong-and-backends-can-teach-us-about-magic",
    "first_paragraph": "Have you ever had a magical experience with software? I have.With just 5 simple questions, you can almost identify any bird in North America. These questions are easy: location, time of year, size, color, and what the bird was doing. The Merlin app uses your answers to narrow down candidate birds to just a few options. From there, it\u2019s easy to pick the right bird from the shortlist - no need to mess with a camera to catch a bird mid-flight or deal with unreliable AI features.The Steller\u2019s Jay is one of the most flashy birds in the Bay Area, and it\u2019s the one that got me into identifying birds.But then the shocking, magical thing happened - they did introduce an AI-powered \u201cSound ID\u201d feature, and it was awesome. It was not dodgy in the slightest; I\u2019m no expert, but I\u2019ve never seen Merlin\u2019s Sound ID produce an obviously-wrong identification like I had with so many other AI-powered nature tools.\u00a0I thought they might have some crazy technical advancements, but I was wrong. They built that q",
    "summary": "**What *Birdsong* and Back Ends Can Teach Us About *Magic***\n\nIn the latest episode of Silicon Valley shamanism, digitalseams.com awes us with the heartwarming tale of a tech bro who confuses basic ornithology with \"magic.\" \ud83e\ude84\ud83d\udc26 Thrilling narratives detail the groundbreaking discovery that apps can use information to function, shattering our preconceived notions that technology works by pixie dust and good vibes. Comment sections burgeon with insights, one profound philosopher notes that \"magic is just attention to detail,\" inadvertently outlining the basic premise of every job ever. Meanwhile, another hopes for open-sourcing the sorcery, presumably to aid in their quest to turn water into craft beer with old Raspberry Pis. In the world of tech, the real magic is still convincing people that making things work as intended is revolutionary."
  },
  {
    "title": "Coding with LLMs in the summer of 2025 \u2013 an update (antirez.com)",
    "points": 412,
    "submitter": "antirez",
    "submit_time": "2025-07-20T11:04:02 1753009442",
    "num_comments": 285,
    "comments_url": "https://news.ycombinator.com/item?id=44623953",
    "comments": [
      "> Gemini 2.5 PRO | Claude Opus 4Whether it's vibe coding, agentic coding, or copy pasting from the web interface to your editor, it's still sad to see the normalization of private (i.e., paid) LLM models. I like the progress that LLMs introduce and I see them as a powerful tool, but I cannot understand how programmers (whether complete nobodies or popular figures) dont mind adding a strong dependency on a third party in order to keep programming. Programming used to be (and still is, to a large extent) an activity that can be done with open and free tools. I am afraid that in a few years, that will no longer be possible (as in most programmers will be so tied to a paid LLM, that not using them would be like not using an IDE or vim nowadays), since everyone is using private LLMs. The excuse \"but you earn six figures, what' $200/month to you?\" doesn't really capture the issue here.reply",
      "The models I can run locally aren't as good yet, and are way more expensive to operate.Once it becomes economical to run a Claude 4 class model locally you'll see a lot more people doing that.The closest you can get right now might be Kimi K2 on a pair of 512GB Mac Studios, at a cost of about $20,000.reply",
      "> Once it becomes economical to run a Claude 4 class model locally you'll see a lot more people doing that.By that time Claude 5 (or whatever) will be available over API.I am grateful for upward pressure from models with published binaries - I do believe this is fundamental floor-raising technology.Choosing frontier-1 for the sake of privacy, autonomy, etc will always be a hard sell and only ever to a pretty niche market. Even me - I'm ideologically part of this market, but I'm already priced out hardware wise.reply",
      "Have you considered the Framework Desktop setup they mentioned in their announcement blog post[0]? Just marketing fluff, or is there any merit to it?> The top-end Ryzen AI Max+ 395 configuration with 128GB of memory starts at just $1999 USD. This is excellent for gaming, but it is a truly wild value proposition for AI workloads. Local AI inference has been heavily restricted to date by the limited memory capacity and high prices of consumer and workstation graphics cards. With Framework Desktop, you can run giant, capable models like Llama 3.3 70B Q6 at real-time conversational speed right on your desk. With USB4 and 5Gbit Ethernet networking, you can connect multiple systems or Mainboards to run even larger models like the full DeepSeek R1 671B.I'm futsing around with setups, but adding up the specs would give 384GB of VRAM and 512GB total memory, at a cost of about $10,000-$12,000. This is all highly dubious napkin math, and I hope to see more experimentation in this space.There's of course the moving target of cloud costs and performance, so analysing break-even time is even more precarious. So if this sort of setup would work, its cost-effectiveness is a mystery to me.[0] https://frame.work/be/en/blog/introducing-the-framework-desk...reply",
      "Strix Halo does not run a 70B Q6 dense model at real-time conversational speed - it has a real-world MBW of about 210 GB/s. A 40GB Q4 will clock just over 5 tok/s. A Q6 would be slower.It will run some big MoEs at a decent speed (eg, Llama 4 Scout 109B-A17B Q4 at almost 20 tok/s). The other issue is its prefill - only about 200 tok/s due to having only very under-optimized RDNA3 GEMMs. From my testing, you usually have to trade off pp for tg.If you are willing to spend $10K for hardware, I'd say you are much better off w/ EPYC and 12-24 channels of DDR5, and a couple fast GPUS for shared experts and TFLOPS. But, unless you are doing all-night batch processing, that $10K is probably better spent on paying per token or even renting GPUs (especially when you take into account power).Of course, there may be other reasons you'd want to inference locally (privacy, etc).reply",
      "Yeah it's only really viable for chat use cases, coding is the most demanding in terms of generation speed, to keep the workflow usable it needs to spit out corrections in seconds, not minutes.I use local LLMs as much as possible myself, but coding is the only use case where I still entirely defer to Claude, GPT, etc. because you need both max speed and bleeding edge model intelligence for anything close to acceptable results. When Qwen-3-Coder lands + having it on runpod might be a low end viable alternative, but likely still a major waste of time when you actually need to get something done properly.reply",
      "The framework desktop isn't really that compelling for work with LLMs, it's memory bandwidth is very low compared to GPUs and Apple Silicon Max/Ultra chips - you'd really notice how slow LLMs are on it to the point of frustration. Even a 2023 Macbook Pro with a M2 Max chip has twice the usable bandwidth.reply",
      "The memory bandwidth is crap and you\u2019ll never run anything close to Claude on that unfortunately. They should have shipped something 8x faster at least 2 tb/s bandwidthreply",
      "I love Framework but it's still not enough IMO. My time is the most valuable thing, and a subscription to $paid_llm_of_choice is _cheap_ relative to my time spent working.In my experience, something Llama 3.3 works really well for smaller tasks. For \"I'm lazy and want to provide minimal prompting for you to build a tool similar to what is in this software package already\", paid LLMs are king.If anything, I think the best approach for free LLMs would be to run using rented GPU capacity. I feel bad knowing that I have a 4070ti super that sits idle for 95% of the time. I'd rather share an a1000 with bunch of folks and have that run at close to max utilization.reply",
      "> and a subscription to $paid_llm_of_choice is _cheap_ relative to my time spent working.In the mid to long term the question is, is the subscription covering the costs of the LLM provider. Current costs might not be stable for long.reply"
    ],
    "link": "https://antirez.com/news/154",
    "first_paragraph": "",
    "summary": "At antirez.com, the eternal summer of code-splaining continues. 2025 brings a steaming hot plate of \"here's why I still use Emacs,\" but now with extra LLMs because blindly depending on a third party makes the inner hipster coder scream in agony. Meanwhile, the commenters engage in the sacred ritual of techno-economic-mathletics, conjuring hardware setups that put NASA to shame just to avoid monthly fees. Gems like \"running Claude 5 over API by 2026\" and \"I used my supercomputer to toast bread\" fly around, proving once more that the only real requirement for being a programmer in 2025 is a solid credit line at the hardware store. \ud83d\udcb8\ud83d\udd28"
  },
  {
    "title": "Peep Show \u2013 The Most Realistic Portrayal of Evil Ever Made (2020) (mattlakeman.org)",
    "points": 46,
    "submitter": "Michelangelo11",
    "submit_time": "2025-07-20T21:05:19 1753045519",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=44629299",
    "comments": [
      "This is a good read for fans of the the show. I don\u2019t think the central premise of redefined evil holds up, but it\u2019s a fun read and the analysis of their characters is spot on.reply",
      "Yeah, I never empathized with Mark and Jez so his thesis falls apart for me. Maybe he\u2019s the baddie?reply",
      "If you haven't heard of Peep Show but you enjoyed Succession you should know that Jessie Armstrong, the creator and showrunner of Succession, was previously one half of the Jessie Armstrong and Sam Bain pair responsible for Peep Show.I find the similarities between the two shows fascinating: in particular the way they both revel in how flawed their central characters are.reply",
      "I\u2019ll tell you what, that crack is really moreish.reply",
      "please Jez, don\u2019t talk about crack!!reply",
      "I place it in the category of \u201ccringe humor,\u201d and by that standard it\u2019s more cringey than Seinfeld and curb - especially because of the first person shots. It is a great show! Well worth a watch.reply",
      "I always refer to it as cringe humour too but for the reason that it makes me physically cringe seeing how they humiliate themselves in social situations - Like NO MARK PLEASE DONT DO THAT! DONT ACT ON THAT IMPULSE PLEASE! kinda thing.reply",
      "\u00b7 Lies to a woman about accidentally killing her dog to try to sleep with her (also tries to burn the dog corpse and dispose of it)reply",
      "And then claims it\u2019s a barbecue turkey, and eats it in front of her in an attempt to save face.reply",
      "> Lies to a woman about stalking her so he can continue running into her in different locationsWe've all done that.reply"
    ],
    "link": "https://mattlakeman.org/2020/01/22/peep-show-the-most-realistic-portrayal-of-evil-ive-ever-seen/",
    "first_paragraph": "Matt LakemanPeep Show, a British tv series running from 2003 to 2015, starring David Mitchell and Robert Webb as a pair of miserable, co-dependent roommates living in Croydon, London,\u00a0is the most realistic portrayal of evil\u00a0I have ever seen.Admittedly, I\u2019m using \u201cevil\u201d in an unorthodox way. Most people think of \u201cevil\u201d as being synonymous with \u201cmalicious\u201d and \u201cdoing really, really bad things.\u201d But I have a broader view of \u201cevil.\u201d I consider a thing to be evil if it creates bad outcomes not just out of malice, but instinct or carelessness.By that standard,\u00a0Peep Shows\u2019s\u00a0protagonists, Mark Corrigan and Jeremy \u201cJez\u201d Usborne, are evil. They\u2019re not evil in quite the way serial killers and murderous dictators are, nor in the exaggerated cartoony manner of other comedic anti-heroes like the characters on\u00a0It\u2019s Always Sunny in Philadelphia, Arrested Development,\u00a0or\u00a0Archer.\u00a0Rather,\u00a0Peep Show\u2019s\u00a0characters are evil in the scariest way possible \u2013 they\u2019re\u00a0realistically evil. They embody the worst, wea",
    "summary": "<b>Peep Show</b> \u2013 Unveiling the Banality of <em>Real</em> Evil\n\nMatt Lakeman blesses us with his novel take on <em>\"evil\"</em> by dissecting the minutely malevolent lives of Mark and Jez, the pathetically parochial protagonists of <b>Peep Show</b>. Redefining \"evil\" in a way that perhaps even Satan wouldn't approve, he suggests that true malevolence lies in the mundane horrors of badly managed personal lives in Croydon. The blog commenters, in their tireless quest to both miss the point and one-up each other, juggle irrelevant show comparisons and haphazard humor theory, all while meticulously quoting the series. \ud83d\ude44 It\u2019s not just Peep Show that revels in flawed characters, but its analysis threads which revel in flawed debates; spoiler alert \u2013 everyone\u2019s the baddie here. \u2620\ufe0f"
  },
  {
    "title": "What My Mother Didn't Talk About (2020) (buzzfeednews.com)",
    "points": 35,
    "submitter": "NaOH",
    "submit_time": "2025-07-17T21:43:23 1752788603",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=44598578",
    "comments": [
      "Similar to when my mother died of cancer while I was in high school but one big difference was how rational and aware the mother in this story was compared to my mother on her last night and day while undergoing hallucinations from the incredible pain she was experiencing. There were moments where she would recognize you and grip your hand and then she would be lost and rambling and saying nonsense and completely separate from us in another world until finally she took her last breath.This was at-home hospice. Very different than when my father died of ALS a few decades later and the nurses were knowingly and purposefully giving him morphine so that he could suffocate in peace as his diaphragm stopped working but at the same time they slowly killed him with the morphine stopping his breathing, thankfully.reply",
      "> at the same time they slowly killed him with the morphine stopping his breathing, thankfully.Yeah, some places have two forms of assisted death available. Fast assisted death, or slow assisted death. Either way, you're getting medical assistance through the dying process. Not sure why some people feel like slow assisted death should be the only option.reply",
      "Its funny we don't have \"legal\" euthanasia but hospice stopping feeding and fluids, or overdosing painkillers, is a more horrible form of it.reply",
      "We do have ~~legal euthanasia~~ assisted suicide! Several countries and even some US states have some form of it! Just not widely advertised.https://en.wikipedia.org/wiki/Assisted_suicidehttps://en.wikipedia.org/wiki/Assisted_suicide_in_the_United...reply",
      "https://archive.ph/BI6Cl(The original link broke with my ad-blockers turned on, the archive though is missing one photo from the original article.)A very powerful read. I lost my mother two years ago and this resonates.Just realized I read one of the author's books, \"How to Get into the Twin Palms\".reply",
      "If you wondered too which condition her mother had I can spare you the read\n(or the copy paste in your favorite LLM).The condition is never named in the article.reply",
      "I am pretty sure it's metastatic lung cancer. The clues are various mention of lungs and the stage 4 in the beginning of the article.reply",
      "Breast cancer.https://www.legacy.com/us/obituaries/nhregister/name/krystyn...reply",
      "The internet has her death announcement, if you DDG it. All the information is on the task.reply",
      "[flagged]"
    ],
    "link": "https://www.buzzfeednews.com/article/karolinawaclawiak/what-my-mother-didnt-talk-about-karolina-waclawiak",
    "first_paragraph": "My mother and I were very close, but when she died last year there was still so much I didn\u2019t know about her.BuzzFeed News Culture Executive EditorA few weeks after my mother died I searched through her email for evidence of a lie. I input her doctors\u2019 names as keywords, because I knew my mother did not use the word for the illness she was diagnosed with 28 years ago. She had had this disease, at stage 4, for so long and yet she hardly ever used the word for what she had \u2014 only saying it for emphasis when she wanted us to understand how severe things were for her. Maybe, I wondered, I had become so desensitized to her condition that I did not take it seriously enough this last time.A few weeks after my mother died I searched through her email for evidence of a lie. I input her doctors\u2019 names as keywords, because I knew my mother did not use the word for the illness she was diagnosed with 28 years ago. She had had this disease, at stage 4, for so long and yet she hardly ever used the wo",
    "summary": "In \"What My Mother Didn't Talk About,\" a BuzzFeed News Culture Executive Editor turns amateur detective, mining her deceased mother's emails for health secrets, because personal boundaries vanish at death, apparently. Commenters, eager to share their own tales of medical woe and familial martyrdom, leap at the opportunity to out-sad each other, offering a smorgasbord of terminal anecdotes. It's a morbid contest where context collapses and everyone misses the point, competing for the 'most tragic backstory' award. Meanwhile, readers leaving comments seem confused whether they're on BuzzFeed or WebMD, revealing more about their own Google-fu skills and familial traumas than the vague article merits."
  },
  {
    "title": "Computational Complexity of Neural Networks (lunalux.io)",
    "points": 9,
    "submitter": "mathattack",
    "submit_time": "2025-07-21T00:23:33 1753057413",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://lunalux.io/introduction-to-neural-networks/computational-complexity-of-neural-networks/",
    "first_paragraph": "In order to motivate why we separate the training and inference phases of neural networks, it can be useful to analyse the computational complexity.This essay assumes familiarity with analytical complexity analysis of algorithms, and hereunder big-O notation. If you need a recap, you should read the essay on computational complexity before continuing.Time complexity might not be the best metric to reason about algorithms which are highly parallel which most linear algebra operations are.The time complexity does not capture the parallelism, and reports the same complexity for a fully parallel algorithm as a sequential one if they have the same number of total steps. But with GPUs being ubiquitous in deep learning algorithms, we should really look for better metrics such as work depth, and span. This article still has merits on its own, but the time complexity results should be understood with proper context.Looking at inference part of a feed forward neural network, we have forward prop",
    "summary": "**Hackers Rejoice: The Joy of Complexity**\n\nAt lunalux.io, another silicon-smitten techie has decided to flex their undergraduate understanding of big-O notation to explain why we divide neural networks into training and inference like our laundry into whites and colors. Thankfully, the author assumes you've already mastered computational complexity \u2013 because, of course, everyone reading this has a deep-seated passion for theoretical models before breakfast. Commenters have eagerly transformed the discussion into a playground of misunderstood concepts, each waving their GPUs like lightsabers while missing the point entirely that parallel processing might, just might, need new metrics. Dive into this fest of jargon and misplaced self-assurance where even the complex is overly simplified! \ud83d\ude44\ud83d\udcbb\ud83e\udd13"
  },
  {
    "title": "FFmpeg devs boast of another 100x leap thanks to handwritten assembly code (tomshardware.com)",
    "points": 159,
    "submitter": "harambae",
    "submit_time": "2025-07-20T20:51:41 1753044701",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=44629206",
    "comments": [
      "When I spent a decade doing SIMD optimizations for HEVC (among other things), it was sort of a joke to compare the assembly versions to plain c. Because you\u2019d get some ridiculous multipliers like 100x. It is pretty misleading, what it really means is it was extremely inefficient to begin with.The devil is in the details, microbenchmarks are typically calling the same function a million times in a loop and everything gets cached reducing the overhead to sheer cpu cycles.But that\u2019s not how it\u2019s actually used in the wild. It might be called once in a sea of many many other things.You can at least go out of your way to create a massive test region of memory to prevent the cache from being so hot, but I doubt they do that.reply",
      "Sorry for the derail, but it sounds like you have a ton of experience with SIMD.Have you used ISPC, and what are your thoughts on it?I feel it's a bit ridiculous that in this day and age you have to write SIMD code by hand, as regular compilers suck at auto-vectorizing, especially as this has never been the case with GPU kernels.reply",
      "Personally I\u2019ve never been able to beat gcc or icx autovectorization by using intrinsics; often I\u2019m slower by a factor of 1.5-2x.Do you have any wisdom you can share about techniques or references you can point to?reply",
      "> Have you used ISPCNo professional kernel writer uses Auto-vectorization.> I feel it's a bit ridiculous that in this day and age you have to write SIMD code by handYou feel it's ridiculous because you've been sold a myth/lie (abstraction). In reality the details have always mattered.reply",
      "ffmpeg is not too different from a microbenchmark, the whole program is basically just: while (read(buf)) write(transform(buf))reply",
      "the devil is in the details (of the holy assembly).thus sayeth the lord.praise the lord!reply",
      "> what it really means is it was extremely inefficient to begin withI care more about the outcome than the underlying semantics, to me thats kind of a givenreply",
      "The article somtimes says 100x, other times it says 100% speed boost. E.g. it says \"boosts the app\u2019s \u2018rangedetect8_avx512\u2019 performance by 100.73%.\" but the screenshot shows 100.73x.100x would be a 9900% speed boost, while a 100% speed boost would mean it's 2x as fast.Which one is it?reply",
      "It's definitely 100x (or 100.73x) as shown in the screenshot, which represents a 9973% speedup - the article text incorrectly uses percentage notation in some places.reply",
      "100x to the single function 100% (2x) to the whole filterreply"
    ],
    "link": "https://www.tomshardware.com/software/the-biggest-speedup-ive-seen-so-far-ffmpeg-devs-boast-of-another-100x-leap-thanks-to-handwritten-assembly-code",
    "first_paragraph": "But admit this boost is only seen in 'an obscure filter'. \nWhen you purchase through links on our site, we may earn an affiliate commission. Here\u2019s how it works.\nThe developers behind the FFmpeg project are again claiming major performance uplifts delivered by wielding the art of handwritten assembly code. With the latest patch applied, users should see a \u201c100x speedup\u201d in the cross-platform open-source media transcoding application. However, the developers were soon to clarify that the 100x claim applies to just a single function, \u201cnot the whole of FFmpeg.\u201dBREAKING: FFmpeg 100x speedup from handwritten assembly13:55:30 <\u2022haasn> rangedetect8_avx512: 121.2 (100.18x) that may be the biggest speedup I've seen so farJuly 16, 2025Last November, we reported on an FFmpeg performance boost that could speed certain operations by up to 94x. The latest handwritten assembly patch boosts the app\u2019s \u2018rangedetect8_avx512\u2019 performance by 100x. If your modern processor doesn\u2019t support AVX512, you should",
    "summary": "**BREAKING: FFmpeg Developers Harness Ancient Techniques for Minimal Gains**\n\nThe FFmpeg team, renowned magicians of obscure software tricks, have <em>once again</em> shocked the world by accelerating an already arcane function by 100 times! \ud83d\ude80 Yes, if you're one of the three people using 'rangedetect8_avx512', your life is about to change\u2014in a completely imperceptible and pointless way. Meanwhile, in comment land, armchair developers argue over whether this speedup means anything in real-world applications or if it's just another bragging right to confuse beginners. Spoiler: it's definitely the latter. \ud83e\udd13\ud83d\udcac"
  },
  {
    "title": "Logical implication is a comparison operator (btdmaster.bearblog.dev)",
    "points": 11,
    "submitter": "btdmaster",
    "submit_time": "2025-07-17T19:15:52 1752779752",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44597037",
    "comments": [
      "One quite useful application of this is that implication can play the role of the partial-order operator in a Galois Connection. A Gallois connection is an iff-and-only-if formula of the form    F(x) \u2264 y  iff x \u2264 G(y)\n\nOne form of this is the tautology when F(x) = (x and a),  G(y) = (a => y), and pick logical implication as the \"\u2264\".    ((x and a) => y) iff (x => (a => y))\n\nhttps://en.wikipedia.org/wiki/Galois_connection#Power_set;_i...reply",
      "Probably \u00e0 typing mistake in \"Denying the consequent\" section, which should rather state \"if P => Q then not-Q => not-P\"?reply",
      "Great catch, thanks!reply"
    ],
    "link": "https://btdmaster.bearblog.dev/logical-implication-as-comparison/",
    "first_paragraph": "",
    "summary": "Title: Hackers Discover Order in Chaos with Logic, Claim World Peace is Next\n\nIn a stunning display of keyboard wizardry, btdmaster.bearblog.dev leads us down a rabbit hole where logical implication doubles as a VIP at the club of comparison operators. According to the salivating crowd in the comments, this revelation is especially mind-blowing because it can mimic the partial-order operator in a Galois Connection \u2013 a concept surely discussed at every bar worldwide. As if one tautology on a Friday night wasn't enough, a commenter\u2019s **heroic** identification of a typo shifts the academic landscape significantly, prompting another to graciously bestow a digital nod of approval. Trust the Internet to make a mathematical blog post about implications sound like the advent of telepathy. \ud83d\ude44\ud83d\udcab"
  },
  {
    "title": "Speeding up my ZSH shell (scottspence.com)",
    "points": 134,
    "submitter": "saikatsg",
    "submit_time": "2025-07-20T15:51:44 1753026704",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=44626363",
    "comments": [
      "With articles like these popping up all the time, oh-my-zsh is seriously harming zsh's reputation. It's giving the wrong impression of zsh being slow and bloated.zsh doesn't need configuration frameworks or plugins. All it needs is a change in the default settings so that its powerful completion works out the box. It currently needs more than ideal amount of tweaks to the defaults, which is probably why people flock to these frameworks.reply",
      "I don't want to spend ages figuring out which knobs to turn to get a half decent shell. If there's an alternative to oh-my-zsh that looks halfway decent, has that nice fzf integrated, and the 'ghost text' history suggestions, then I welcome it!reply",
      "If zsh has its completion fully configured by default, there will be no need for most people to turn knobs nor will it be a \"half decent shell.\" It'll be the best shell, if it isn't already.I wonder why ghost text history suggestions are popular though, I'd rather not have it. Shell history search works better, and I don't want my shell always showing me or whoever else is looking at the screen random commands that I've previously typed.reply",
      "Look into zgen, it's a zsh plugin system but much more static (but less ridiculous out of the box)reply",
      "IMO, that alternative is Fish shell.reply",
      "Zim has been my choice since I got sick of multi second startups with OMZ.reply",
      "Once you have everything figured out, you just keep using the config you have.  It might be worth the investment.  I stopped using oh-my-zsh when I realized it was what was causing multi-second delays on Raspberry Pis.I think auto suggestions and syntax highlighting plugins can be installed separately from oh-my-zsh.I use starship for a better prompt, and it works on more shells than just Zsh.I also have Atuin installed to share history across machines, and as a benefit the history search is a lot more powerful.https://github.com/zsh-users/zsh-autosuggestionshttps://github.com/zsh-users/zsh-syntax-highlightinghttps://starship.rshttps://atuin.shreply",
      "You should try fish shell. Great user experience out of the box, including history suggestions.https://fishshell.comreply",
      "The incompatible syntax of fish makes it a no go for me.As an SRE, at my day job I often need to copy/paste commands that are generated from a playbook.Our playbooks use Bash, and in practice Zsh is compatible. But a co-worker using fish often has to manually modify commands before running, and I'm not about that life.The problem with fish is mostly the different syntax for setting variables and lack of heredocs. Sometimes the string substitution differences come up too.reply",
      "Exactly. Variable assignment works different in Fish, and it\u2019s very hard to change that habit. With my usage of shell (with Fzf & zoxide), fish simply doesn\u2019t improve much for me. I\u2019m using bash constantly for my job, and having the same syntax in my shell is importantreply"
    ],
    "link": "https://scottspence.com/posts/speeding-up-my-zsh-shell",
    "first_paragraph": "",
    "summary": "Hobbyists with an aversion to defaults rejoice as <em>scottspence.com</em> hosts a seminar on \"Speeding up my ZSH shell,\" inspiring a tide of unsolicited advice and plugin recommendations. The comment section predictably transforms into a battleground where keyboard warriors argue over whether <em>zsh</em> needs rescuing from <em>oh-my-zsh</em> or if it's just a misunderstood beast begging for a minimalist's touch. Armed with links like sharpened lances, every commenter is ready to charge into the fray, each convinced that their plugin soup is less bloated than the next guy\u2019s. Meanwhile, the real question remains unanswered: Why does every shell user think they are a UI/UX expert destined to save <em>zsh</em>, one alias at a time? \ud83d\udc1a\ud83d\udca5"
  },
  {
    "title": "IPv6 Based Canvas (openbased.org)",
    "points": 14,
    "submitter": "tylermarques",
    "submit_time": "2025-07-20T22:29:04 1753050544",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://canvas.openbased.org/",
    "first_paragraph": "",
    "summary": "In a daring leap into the future, openbased.org publishes \"IPv6 Based Canvas,\" a thrilling expos\u00e9 revealing that, yes, IPv6 addresses can indeed be longer than Twitter bios. The comment section, a celebrated think tank of underachievers, springs into action, engaging in intellectual mud-wrestling about whether this innovation will finally make printers work or just help their Alexa spy more effectively. Expect groundbreaking applications like tracking your pet's movements through your smart toaster, because, why not convert excess technological capability into further erosion of privacy? Clearly, IPv6 is about to become as beloved and well-understood as quantum physics at a high school reunion."
  },
  {
    "title": "The Genius Device That Rocked F1 (youtube.com)",
    "points": 13,
    "submitter": "brudgers",
    "submit_time": "2025-07-20T23:42:26 1753054946",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.youtube.com/watch?v=FhmLb2DhNYM",
    "first_paragraph": "",
    "summary": "In another monumental moment for human achievement, YouTube engineers release a video celebrating a new device \u201c<i>rocking</i>\u201d Formula 1 \u2013 presumably because the sport was previously bereft of any technology, innovation, or excitement until now. Thrilled commenters, who clearly have too many graduate degrees in mechanical engineering, chirp en masse about how this device will revolutionize the ancient art of driving in circles faster than the other million-dollar cars. Critics are uncharacteristically silent, because after mastering TensorFlow tutorials last week, they are now too busy reverse-engineering space shuttles. Expect the device's schematic, redesigned in a suburban garage, hitting eBay under the listing \u201c<em>REAL F1 Tech, slightly used</em>,\u201d within the fortnight."
  },
  {
    "title": "Discovering what we think we know is wrong (science.org)",
    "points": 16,
    "submitter": "strangattractor",
    "submit_time": "2025-07-18T03:26:36 1752809196",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=44600899",
    "comments": [
      "> So let\u2019s ask ourselves: would AI have told us this?Why ask ourselves, when we can ask the AI? Here's the start of my conversation with Gemini:> Me: What is known about fatty acid combustion in the brain?> Gemini: The Brain's Surprising Use of Fat for Fuel\nFor a long time, the brain was thought to rely almost exclusively on glucose for its immense energy needs. However, a growing body of research reveals that fatty acid combustion, or beta-oxidation, plays a more significant and complex role in brain energy metabolism and overall neurological health than previously understood. While not the brain's primary fuel source under normal conditions, the breakdown of fatty acids is crucial for various functions, particularly within specialized brain cells and under specific physiological states....It cites a variety of articles going back at least to the 1990s.So> would AI have told us this?Yes and it didreply",
      "I think this could use a more informative title?  The title this was posted with is actually less informative than the original title.reply",
      "Derek has a little thought experiment at the end.reply",
      "Answer to his though experiment: \nYes, I believe a sufficiently advanced AI could told us that. Scientists who have been fed with wrong information can come up with completely new ideas. Making what we know less wrong.That being said, I don't think current token-predictors can do that.reply",
      "My read of this was that AI is fundamentally limited by the lack of access to the new empirical data that drove this discovery; that it couldn't have been inferred from the existing corpus of knowledge.reply",
      "Maybe an AI will be smart enough to realize that there's more than one explanation for a low level of triglycerides in neurons.The RICE myth and the lactic acid myth will surely be a part of the training material so the AI will realize that there's a fair amount of unjustified conclusions in the bioworldreply"
    ],
    "link": "https://www.science.org/content/blog-post/tell-me-again-about-neurons-now",
    "first_paragraph": "",
    "summary": "**Discovering What We Think We Know Is Wrong (Not Like We Knew Anything Anyway)**\n\nHere we are again at science.org, tossing another beleaguered science fact into the \"maybe not\" bin. This week it's learning that our precious brains might burn fat instead of just sugar\u2014color me shocked like a slug in a salt mine. Commenters are tripping over to marvel at this revelation with the kind of naive enthusiasm reserved for children discovering rain puddles. Derek ponders if AI would have spilled these fatty beans earlier, but honestly, leaning on <i>any</i> AI for groundbreaking hypotheses is like asking your cat for investment advice\u2014cute but catastrophic. Meanwhile, the debate clumsily dances around whether AI is just another overhyped bystander in the saga of human ignorance. Godspeed, brainiacs! \ud83e\udde0\ud83d\udd25"
  },
  {
    "title": "Show HN: Conductor, a Mac app that lets you run a bunch of Claude Codes at once (conductor.build)",
    "points": 125,
    "submitter": "Charlieholtz",
    "submit_time": "2025-07-17T15:43:04 1752766984",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=44594584",
    "comments": [
      "I'm in the midst of modernizing a 1994 K&R C 32-bit codebase to 64-bit C23, as a museum piece: the computer algebra system Macaulay that I coauthored, since replaced by Macaulay 2.The horror!On macOS my environment is Sublime Text as editor, iTerm as terminal. I open two full screen iTerm windows of four panes each, start Claude Code Opus 4 and type \"just task\" in each pane. CLAUDE.md tells the session to run this justfile command and follow the directions, which generally include instructions to continue by doing \"just task\" again. Each session may edit its <file>.c and <file>.h but not shared headers; if it has a problem with this I touch STOP in the task directory, the other sessions eventually are told to wait, and then the trouble child session can edit what it wants.I'm sure in a few years AI will read and fix my 57 C file code base all at once, but for now this workflow has proved invaluable.The irony? This is the dumbest parallel task manager imaginable yet it works. And our code base has a similar feel: The storage management is a set of fixed sized stacks that outperformed malloc() by 4x back in the day. Etc. Etc.I learned this gift from my Dad. He devised the Bayer filter for digital photography back in 1974, it looks like ten minutes work (it wasn't) but we're still using it.reply",
      "I was really excited to try this but this does NOT work the way I expected. I wanted a simple git worktree manager for my existing, already-checked-out repository. Instead, it requests Github permissions and clones the repo from Github. This is bad, because you need to run all the dependency installs, etc. for every workspace before being able to test anything. In other words it's like Codex or Cursor Background Agents, except worse, because it's on your machine. The reason I don't use Codex or Background Agents is because my project has way too many dependencies and would take way too long (and too complicated) to install everything required to have an isolated running instance, plus there would be DNS conflicts, external API conflicts, among other issues.What I do want is simple git worktree management for an already-checked-out repo on my machine, no Github permissions or dependency re-installation (copying node_modules, etc.).reply",
      "hey, co-creator of Claude Squad \u2013 imo the most popular+used of these \"claude code multiplexers\" and it's also open source and free :)works like you'd expecthttps://github.com/smtg-ai/claude-squadreply",
      "I just tried Claude Squad this morning , the instructions to use and interface was very clunky . There also was no uninstall instructions or scripts , so I had to write one and uninstall it.  Lamereply",
      "\"This is bad, because you need to run all the dependency installs\", you must be using Python or one of the other locally cached dependency language tookings. Is never understood why people wanto clone everything every time. Maven and its sibling use ~/.m2 so everything gets cached once. I ceinge every time I have to re-download the universe when I inevitably have to delete my venv and redo anything, what a waste of time and bandwidth and space.reply",
      "How do you suggest simultaneous independent branches be worked on concurrently from a single repository copy, or is it that you would simply prefer the local work tree method of duplication? And, for example, how do you suggest the same node_modules be used if part of the task requires changing, removing, or updating dependencies? Additionally, how much time do you expect a developer to spend implementing this new system your suggesting across all platforms for the benefit of it taking you a bit less time, especially when pulling dependencies from a local caches as I\u2019m sure you\u2019re doing? Or are you making a suggestion that you plan to contribute?I\u2019m sure the authors would appreciate well-thought alternative suggestions and assistance.reply",
      "I tried out git worktrees recently and while they do what's on the tin, I really didn't like them as much as I thought I would. I actually like the way cursor does it despite the fact that it is a VM - I wish I could have their same UI/UX but with local worktrees.reply",
      "Yes I had a similar experience. the thing that tripped me up with git worktrees, which is maybe obvious in retrospect, is that they don\u2019t include things that are not tracked by git - e.g. .env.development.localSo starting a new worktree requires additional setup and isn\u2019t as simple as just checking out a new branchreply",
      "Sorry about that! We\u2019re working on making this more intuitive.Internally our workflow looks like this: \n- we have a script that sets up a repo \u2014 copies env variables, runs pnpm i, inits a db, etc\n- we have a field in the repo settings called \u201csetup script.\u201d every time you make a new workspace, that script runsHopefully will be much improved over the next week or two!reply",
      "Could you group the main repo and its worktrees under a common parent directory that contains your .env.development.local?reply"
    ],
    "link": "https://conductor.build/",
    "first_paragraph": "Run a bunch of Claude Codes in parallel.Each Claude gets an isolated workspace. See who\u2019s working, see who\u2019s stuck, and what\u2019s changed.Download for MacDownload for MacIf you want to run multiple Claude Code agents at the same time, use Conductor \ud83d\udd25\n\nBeautiful UI and handles all the git worktree stuff for you!Ian Nuttallserial internet biz builderConductor is very good.Joe Choi-GreeneCo-Founder and CTO, Clearly AIOk this is kind of sickIlya SukharPartner at Matrix, Co-Founder of Parsegave this a try and holy shit. this is a new productivity unlock@nexxelCo-Founder mocha.emailLoving using Conductor so far! Do give it a try.Ovais TariqCo-Founder & CEO, Tigris DataThis is what the future looks like: anyone can be Steve Jobs, conducting their own agent orchestra.Evan WinelandCo-Founder Weave RoboticsI love Claude Code. I love Conductor.Jeff BargAI at ClayFeels like we're entering a new chapter for AI coding assistance: agent orchestration.Peter ZakinPartner, Upfront VenturesVery promising GU",
    "summary": "**Conductor: The Symphony of Coders or Just Noise?**\n\nWelcome to yet another groundbreaking app, <em>Conductor</em>, which promises to let you run multiple Claude Codes simultaneously without breaking a sweat. Join the horde of technocrats praising its revolutionary ability to replicate functions that several free software tools have done for years. Meanwhile in the comments, seasoned programmers engage in the time-worn ritual of bemoaning new tools while simultaneously missing the point, reminiscing about better days of lower-level programming. Witness this modern tragedy as they try to navigate the app's overhyped \"innovations,\" occasionally stopping to mourn the glory days of simpler systems, or perhaps just simpler times. \ud83c\udfbb\ud83d\ude43"
  },
  {
    "title": "Tough news for our UK users (janitorai.com)",
    "points": 234,
    "submitter": "airhangerf15",
    "submit_time": "2025-07-20T20:43:16 1753044196",
    "num_comments": 206,
    "comments_url": "https://news.ycombinator.com/item?id=44629134",
    "comments": [
      "I personally know how this works in Europe & UK. Not only government, this applies to big companies such as large banks as well. They recruit two kinds of staff. One that works to progress some work and one who puts an many hurdles as possible and call it risk management, compliance, security, regulatory etc (RCSR). They hire approximately 3 times more people into these RCSR positions compared to the technical and real work related positions. These RCSR guys dump thousands of pages of guidelines, making it impossible for any meaningful work to progress. My technical team has been running around for 4 months for approvals for testing an upgrade of a database.Top management can never go against the RCSR guys, who are like priests of the church in medieval ages. And the RCSR guys have no goals linked to the progress of the real work. The don't like any thing that moves. It's a risk.Management thinks that RCSR helps with controls around the work. But what happens is, you put more people in building controls, they deliver fort walls around your garbage bins.reply",
      "It's also interesting how much worse security gets with all this RCSR oversight.Because the policy teams don't understand the technical details, they get hung up on things that don't matter so all your \"security budget\" (in terms of dev effort available for improvement) gets spent on useless things.Because there is a CIS guideline recommending \"do not put secrets in environment variables\" in k8s, a team I work with recently spent two weeks modifying deployments of all their third party charts to mount all secrets as volumes, this included modifying / patching upstream helm charts. This will be a maintenance burden forever lol. Actual security benefit in context is approximately zero.Meanwhile they COULD have spent that time implementing broadly effective things like NetworkPolicy or CSP for the front-end but now there is no dev time for that.At the top levels of their process there are plenty of risk x impact matrices but there aren't corresponding effort / payoff assessments, so the things being done end up not being engineering.reply",
      "> This will be a maintenance burden forever lol.Not if they never ever ship an update.(i - briefly - worked for place that did things this way. They didn't update the chart, but they _did_ use `sed` to update the image tag from time to time.)reply",
      "America has a similar (if less severe) version of this problem where nobody can contradict any compliance-adjacent function. Because if you get sued, someone will ask you \"why did you ignore the guidance of your compliance team??\" and might even try to use that to justify piercing the corporate veil. Of course compliance types have no incentive to let business happen just like business types have a limited incentive to operate in a compliant fashion, but lawsuits favor compliance always taking precedent with a hyper-cautious approach.reply",
      "And yet uber and Airbnb and polymarket and\u2026reply",
      "These are at some level businesses that do well because they avoid regulatory regimes. That risk is built-in from day one and basically their raison d'etre. I happen to think that's fine and good, but it's still the case.In other words, the guys who actually believe in a normal level of compliance are hamstrung, defection is rewarded, classic failure mode of this kind of thing.reply",
      "IOW, America also has a stagnant, risk-averse business culture, except that superimposed on this culture there's a thin, fragile, layer of risk taking, one geography- and sector-constrained, but in which ~all US value creation and economic differentiation happens.It's like how the growth of a tree trunk happens in a thin layer beneath the bark and the rest is inert wood.reply",
      "This is not limited to Europe. I worked for a company like this in the US. The problem of course is the incentive structure: The penalty for violating some minor regulation was that the company would cease to exist (or so management assumed) while there was effectively no penalty for not getting the mission accomplished.Thus the only reasonable course of action was to do nothing.reply",
      "> My technical team has been running around for 4 months for approvals for testing an upgrade of a database.Jeez, and I was just grumbling how it takes up to 24 hours for a change I make to appear in prod,\ncompared to about 4 hours at my previous job.It should be possible to develop automated systems and processes to keep most changes on the \u201chappy path\u201d where approvals are quick. These sorts of organization responses were a new layer of people who can say \u201cno\u201d grow whenever a problem happens or a regulation changes are suffocating.reply",
      "Sounds like the rest of the world couldn't be bothered with learning code, but couldn't tolerate code being law, that they tried to dilute the code's stake by adding more laws, despite code still being the law.reply"
    ],
    "link": "https://blog.janitorai.com/posts/3/",
    "first_paragraph": "",
    "summary": "**Tough news for our UK users: A comedy of inefficiencies**\n\nIn a stunning revelation that shocks exactly no one, janitorai.com discovers that \"working\" at big companies involves less actual work and more bureaucratic black magic. Readers treated to the familiar tale of \"risk management\" where dedicated employees generate enough paper to reforest the Amazon, ensuring that simple database updates require more clearance than a moon launch. The comments section becomes an echo chamber where tech veterans one-up each other with tales of corporate paralysis, each anecdote more Kafkaesque than the last. As the corporate world fortifies garbage bins like national treasures, the unstoppable force of RCSR meets the immovable object of doing anything productive. Guess which wins? \ud83d\ude09"
  }
]