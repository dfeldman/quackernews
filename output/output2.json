[
  {
    "title": "Webb captures iconic Horsehead Nebula in unprecedented detail (esa.int)",
    "points": 129,
    "submitter": "rbanffy",
    "submit_time": "2024-04-29T15:31:49",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=40199624",
    "comments": [
      "Wow. The NIRCam image is probably going to be the most exciting new photo, but I can't get over how well MIRI reveals the internal structure of the nebula.NIRCam: https://www.esa.int/ESA_Multimedia/Images/2024/04/Horsehead_...MIRI: https://www.esa.int/ESA_Multimedia/Images/2024/04/Horsehead_...Comparison: https://www.esa.int/ESA_Multimedia/Images/2024/04/Slider_Too...",
      "I love that there are multiple sensors that can be compared to like this, but also love when the optical images from Hubble are compared as well.The images that combine all of the frequencies from Chandra X-rays, Hubble's optical, and now Webb's IR make for some truly fascinating images.",
      "The youtube link to a 'zoom' in video to the image:https://www.youtube.com/watch?v=TkVprNB5XbIWhat is really, really neat to notice isn't just the detail in that final image.... look behind it, and there are whole edge-on spiral galaxies in the distance.  Not stars.  Galaxies.The nebula is about 1375 light years away.  Those galaxies in the distance.... are billions of light years away.  It's hard to comprehend.",
      "> look behind it, and there are whole edge-on spiral galaxies in the distance. Not stars. Galaxies.just to add to the awe of that, pretty much every \"dot\" in one of these images is going to be another galaxy. individual stars from within the Milky Way will have diffraction spikes and very obvious as a single item.",
      "There really is a lot of stuff left to see for the first time",
      "\"A lot\" is the number of fish in a swarm maybe.This is so far away from our concept of counting things that the mind just gives up. There's no comparison, no dumbing down to X amount of football fields, just nothing.I find it depressing, confusing but also inspiring and fascinating at the same time.",
      "That's an incredibly detailed image.Every single time I see one of these amazing space pics, it's hard not to get all philisophical and wonder about the size of space & time on cosmic scale, how small our earth is and how insignificant our regular problems are.I don't care if I don't get to see flying cars or AGI in my lifetime but I will be very  disappointed if our knowledge of space remains more or less the same as today without much progress.",
      "The zoom-in video at the end is utterly unbelievable, don't miss it. What an engineering and scientific triumph.",
      "And it's in glorious 432p resolution!Edit: Here is the 2160p version: https://www.youtube.com/watch?v=UdHnF9Go_DQ",
      "I wonder how fast an observer would need to be traveling for it to look like that!"
    ],
    "link": "https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_captures_iconic_Horsehead_Nebula_in_unprecedented_detail",
    "first_paragraph": "The NASA/ESA/CSA James Webb Space Telescope has captured the sharpest infrared images to date of one of the most distinctive objects in our skies, the Horsehead Nebula. These observations show a part of the iconic nebula in a whole new light, capturing its complexity with unprecedented spatial resolution.",
    "summary": "In the latest episode of \"Astro-nerds Unite,\" the James Webb Space Telescope snaps yet another picture of space dust, this time the Horsehead Nebula, showcasing what $10 billion and a penchant for staring into the void can get you. Commenters dive headfirst into the cosmic pool of amazement, with links flying faster than light to various images that prove, indeed, galaxies exist beyond our own. The conversation quickly spirals from an appreciation of infrared photography to existential musings on the insignificance of human life, punctuated by complaints about video resolution. As if understanding the Universe could be hindered by anything less than 4K, our intrepid observers marvel at distant galaxies, quietly ignoring the fact that, just maybe, the vast emptiness mirrors their social calendars."
  },
  {
    "title": "Project Habbakuk: Britain's Ice \"Bergship\" Aircraft Carrier Project (99percentinvisible.org)",
    "points": 30,
    "submitter": "not_a_boat",
    "submit_time": "2024-04-29T15:35:31",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=40199679",
    "comments": [
      "This is one of those \u201cso cool yet so silly\u201d brainstorms that I\u2019m grateful someone was audacious enough to entertain. I\u2019m both relieved and saddened that it never came to fruition.There\u2019s a word, chindogu, to describe things that are less than useless. In some sense this project engendered more problems than it solved. Like so many other attractive brainstorms.",
      "The outcome of this project is an illustration of a rule of thumb from materials science:  many solid materials begin to lose their mechanical properties at about half their absolute melting point.  This is why (for example) ordinary steel should not be used above about 550 C; there's too much creep.",
      "It's probably worth the trouble to try making some DIY pykrete.Take a chain saw to it. Take a sledgehammer to it.",
      "I believe they didn't actually use pykrete in the Canadian test model. It was just ice."
    ],
    "link": "https://99percentinvisible.org/article/project-habbakuk-britains-secret-ice-bergship-aircraft-carrier-project/",
    "first_paragraph": "",
    "summary": "In a stunning display of historical whimsy, 99percentinvisible.org delves into Britain's *ambitious* yet fundamentally flawed plan to dominate the wartime seas with a colossal ice carrier, affectionately dubbed Project Habbakuk. It appears the collective minds behind this frosty fiasco momentarily forgot that ice, much like their hopes and dreams, has a pesky tendency to melt. Commenters, reveling in their retrospective wisdom, could not resist pointing out the sheer, mind-boggling chindogu of the entire endeavor, equating it to hitting icebergs with sledgehammers just for the novelty. With an almost poetic justice, this chilling tale of ambition melting away under the harsh sun of practicality, serves as a stark reminder: just because we *can* theoretically saw through ice with determination and a chainsaw, doesn't necessarily mean we *should*."
  },
  {
    "title": "Memary: Open-Source Longterm Memory for Autonomous Agents (github.com/kingjulio8238)",
    "points": 114,
    "submitter": "james_chu",
    "submit_time": "2024-04-29T11:12:51",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=40196879",
    "comments": [
      "This seems like its overloading the term knowledge graph from its origins. Rather than having information and facts encoded into the graph, this appears to be a sort of similarity search over complete responses. It's blog style \"related content\" links to documents rather than encoded facts.Searching through their sources, it looks like the problem came from Neo4j's blog post misclassifying \"knowledge augmentation\" from a Microsoft research paper with \"knowledge graph\" (because of course they had to add \"graph\" to the title).This approach is fine, and probably useful but its not a knowledge graph in the sense that its structure isn't encoding anything about why or how different entities are actually related. A concrete example in a knowledge graph you might have an entity \"Joe\" and a separate entity \"Paris\". Joe is currently located in Paris so would have a typed edge between the two entities of something like \"LocatedAt\".I didn't dive into the code but what I inferred from the description and referenced literature, it is instead storing complete responses as \"entities\" and simply doing RAG style similarity searches to other nodes. It's a graph structured search index for sure but not a knowledge graph by the standard definitions.",
      "Exactly. Glad to see this. I do think knowledge graphs are important to AI assistants and agents though and someone needs to build a knowledge graph solution for that space.The idea of actual entities and relationships defined like triples with some schema and appropriately resolved and linked can be useful for querying and building up the right context. It may even be time to start bringing back some ideas from the schema.org back the day to standardize across agents/assistants what entities and actions are represented in data fed to them.",
      "Yeah precisely. Knowledge graphs are simple to think about but as soon as you look into them you realize all the complexity is in the creation of a meaningful ontology and loading data into that ontology. I actually think LLMs can be massively useful for building up the ontology but probably not in the creation of the ontology itself (far too ambiguous and large/conceptual task for them right now).",
      "How do we build ontology using LLMs? Will the building blocks be like the different parts of a brain?\nP.S I am assuming that by \"creation of ontology itself\" means creation of AGI.",
      "Ontologies are just defining what certain category, words, and entity types mean. Commonly used in NLP for data representation (\u201cfacts\u201d/triples/etc.) in knowledge graphs and other places where the definition of an ontology helps provide structure.This doesn\u2019t have anything to do with AGI or brains. They are typically created or tuned by humans and then models fit/match/resolve entities to match the ontology.",
      "Yeah, one of the specific things I'd love to do is collaboratively bulking up WikiData more. It's missing a ton of low hanging fruit that people using an ML augmented tool could really make some good progress on, similar to ML assisted OpenStreetMapping work",
      "While I'm 100% on board with RAG using associative memory, I'm not sure you need Neo4J.  Associative recall is generally going to be one level deep, and you're doing a top K cut so even if it wasn't the second order associations are probably not going to make the relevance cut.  This could be done relationally, and then if you're using pg_vector you could retrieve all your rag contents in one query.",
      "I think there's a lot of cases where you don't want to just RAG it. If you're going for tool assisted, it's pretty neat to have agent write out queries for what it needs against the knowledge graph. There was an article recently about how LLMs are bad at inferring B is A from A is B. You can also do more precise math against it, which is useful for questions even people need to reason out.I need to dig into what they're doing here more with their approach, but I think using an LLM for both producing and consuming a knowledge graph is pretty nifty, which I wrote up about a year ago here, https://friend.computer/jekyll/update/2023/04/30/wikidata-ll... .I will say figuring out how to actually add that conversation properly into a large knowledge graph is a bit tricky. ML does seem slightly better at producing an ontology than humans though (look how many times we've had to revise scientific names for creatures or book ordering)",
      "Yes, but this doesn\u2019t seem to be an actual knowledge graph which is part of the issue imho. If you look at the Microsoft knowledge graph paper linked in the repo it looks like they build out a real entity-relationship based knowledge graph rather then storing responses and surface form text directly.",
      "My initial thought was \"building the knowledge graph is what LLMs and the embedding process does implicitly\", why the need for a graphdb like Neo4j?"
    ],
    "link": "https://github.com/kingjulio8238/memary",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In an audacious display of creativity that will surely change the face of AI as we know it, a GitHub repository proposes what they call \"Memary,\" an alleged solution for granting artificial agents the gift of long-term memory. Apparently, the term \"knowledge graph\" was too confining for their revolutionary concept, prompting internet commenters to embark on a brave quest to define exactly what makes a graph a *knowledge* graph. Readers are treated to a delightful maze of technical jargon, from \"RAG style similarity searches\" to \"entities\" that are really just glorified text snippets. As the techno-babble whirlwind ensues, some commenter champions assert the importance of building a \"real\" knowledge graph, heroically battling the nefarious forces of ambiguity and conceptual overload. Others daydream about ontologies and their glorious place in AI, blissfully unaware that the real punchline is the collective realization that we're all just desperately overcomplicating the note-taking app we downloaded for free."
  },
  {
    "title": "Husband and wife outed as GRU spies aiding bombings and poisonings across Europe (theins.ru)",
    "points": 149,
    "submitter": "dralley",
    "submit_time": "2024-04-29T15:00:15",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=40199193",
    "comments": [
      "This is my semi annual plug for all of you to watch the fantastic and somehow forgotten FX network TV show, The Americans, A spy drama set in Washington DC in the 1980s about KGB \u201cillegals\u201d posing as travel agents.It\u2019s way better than any basic cable TV show had any right to be. Plus, all seasons are streaming on Hulu, so you don\u2019t have to worry about whether the story will be completed.https://www.hulu.com/series/the-americans-6deba130-65fb-4816...",
      "I want to give some advice: Don't judge it by its first few episodes. When I first checked it out, the basic setup seemed rather inane and I stopped watching.Then, a couple years later, I needed something to watch during long exercise sessions and I checked it out again. It was getting much more interesting by the end of the first season.And every season got better and richer. By the very end, I experienced it as actually deep. It was a real pleasure and I'm very glad I had the chance to enjoy it. Recommended!",
      "I dunno. I watched the first few episodes over COVID and I thought it just the same as any US cable show: sex, crash-bangs and manufactured plot twists. IIRC there's a sex scene in the first five minutes.",
      "The most realistic Russian sleeper scenario imo was in Slow Horses. Brits don't insult your intelligence too greatly in their spook shows; the American variety always involves some sort of super-human characters. It's good as entertainment, the Americans, but just over the top.",
      "The Americans is pretty banal and follows the basic script you would expect. It's good background noise if you want 80s nostalgia though.",
      "I'm with you. I don't really agree with all the praise it gets. I liked it at first but it really seemed to run into the \"manufactured drama\" trap that a lot of TV shows run into when they try and keep it going. It really ruined my suspense of disbelief.",
      "I'd recommend Tinker Tailor Soldier Spy instead.",
      "Loosely based on an actual case.",
      "One of the best TV shows of all time, I second your recommendation. Rare show that gets better with each season.",
      "i don't know about that, the last 1 or to some extent the last 2 seasons were a bit lazy"
    ],
    "link": "https://theins.ru/en/politics/271205",
    "first_paragraph": "",
    "summary": "In a groundbreaking revelation that will surely leave the world aghast, a husband and wife duo have been exposed as GRU spies, single-handedly weaving a tapestry of espionage that covers bombings and poisonings across Europe. But hey, why focus on such trivial matters when the *real* issue at hand is determining if \"The Americans\" is a worthy binge-watch or a snoozefest worthy of serving as your background noise while you sort your sock drawer? Commenters, in a display of internet discourse at its finest, swiftly pivot from international intrigue to passionately debating the merits of fictional spies, with a side dish of nostalgia for 80s decor and a sprinkling of disdain for anything that doesn't come with a British accent. Because, as everyone knows, the authenticity of Cold War espionage is best gauged by a TV show's adherence to the hallowed trifecta of sex scenes, crash bangs, and manufactured drama."
  },
  {
    "title": "Answering Legal Questions with LLMs (hugodutka.com)",
    "points": 88,
    "submitter": "hugodutka",
    "submit_time": "2024-04-29T14:01:47",
    "num_comments": 57,
    "comments_url": "https://news.ycombinator.com/item?id=40198458",
    "comments": [
      "I saw a RAG demo from a startup that allows you to upload patient's medical docs, then the doctor can ask it questions like:> what's the patient's bp?even questions about drugs, histories, interactions, etc. The AI keeps in mind the patient's age and condition in its responses, when recommending things, etc. It reminded me of a time I was at the ER for a rib injury and could see my doctor Wikipedia'ing stuff - couldn't believe they used so much Wikipedia to get their answers. This at least seems like an upgrade from that.I can imagine the same thing with laws. Preload a city's, county's etc. entire set of laws and for a sentencing, upload a defendant's criminal history report, plea, and other info then the DA/judge/whoever can ask questions to the AI legal advisor just like the doctor does with patient docs.I mention this because RAG is perfect for these kinds of use cases, where you really can't afford the hallucination - where you need its information to be based on specific cases - specific information.I used to think AI would replace doctors before nurses, and lawyers before court clerks - now I think it's the other way around. The doctor, the lawyer - like the software engineer - will simply be more powerful than ever and have lower overhead. The lower-down jobs will get eaten, never the knowledge work.",
      "> It reminded me of a time I was at the ER for a rib injury and could see my doctor Wikipedia'ing stuffTo be honest, I'm much more comfortable with a doctor looking things up on wikipedia than using LLMs. Same with lawyers, although the stakes are lower with lawyers.If I knew my doctor was relying on LLMs for anything beyond the trivial (RAGS or not), I'd lose a lot of trust in that doctor.",
      "> I mention this because RAG is perfect for these kinds of use cases, where you really can't afford the hallucination - where you need its information to be based on specific cases - specific information.I think it's worth cautioning here that even with attempted grounding via RAG, this does not completely prevent the model from hallucinating. RAG can and does help improve performance somewhat there, but fundamentally the model is still autoregressively predicting tokens and sampling from a distribution. And thus, it's going to predict incorrectly some of the time even if its less likely to do so.I think its certainly a worthwhile engineering effort to address the myriad of issues involved, and I'd never say this is an impossible task, but currently I continue to push caution when I see the happy path socialized to the degree it is.",
      "I've 100% found AI to be super helpful in learning a new programming language or refreshing on one I haven't used in a while. Hey how do I this thing in Gleam? What's Gleams equivalent of y? I turn it first instead of forums/stackoverflow/google now and would say I only need to turn to other sources less than maybe 5% of the time.",
      "I think that is right. The sweat spot is twofold: 1) A replacement for general search on a topic where you have limited familiarity that can give you an answer for a concise question, or a starting point for more investigation or 2) For power-user use cases, where there already exists subject matter expertise, elaboration or extrapolation from a clear starting point to a clear end state, such as translation or contextualized exposition.The problem comes with thinking you can bridge both of those use cases - vague task descriptions to final output. The work described in the article of getting an LLM itself to break down a task seems to work sometime but struggles in many scenarios. Products that can define their domain narrowly enough, and embed enough domain knowledge into the system, and can ask the feedback at the right points, and going to be successful and more generalized systems will either need to act more like tools rather than complete solutions.",
      "Absolutely, I can't imagine doing Angular without an LLM sidekick.Curiosity + LLM = instant knowledge",
      "Yup. Entirely replaced the \"soft\" answers online like stack overflow for me. Now its LLM and if that isnt good enough then right to docs. I actually read documentation more often now because its pretty clear when I'm trying to do something common (LLM handle this well) vs uncommon (LLM often do not handle this well).",
      "I found this to be the case recently when I built something new in a framework I hadn't used before. The AI replaced Google most of the time and I learned the syntax very fast.",
      "> I used to think AI would replace doctors before nurses, and lawyers before court clerks - now I think it's the other way around.I've come to this conclusion as well.  AI is a power tool for those that know what questions to ask and will become a crunch for those that don't.  My concern is with the latter, as I think they will lose the ability develop critical thinking skills.",
      "I wonder if this \"AI will replace your job\" is like \"AI will drive your car\" in that where once something can solve 95% of the problem the general public assumes the last 5% will come very quickly.Rodney Brooks used to point out that self-driving was perceived by the public as happening very quickly, when he could show early examples in Germany from the 1950s.  We all know this kind of AI has been in development a long time and it keeps improving.    But people may be overestimating what it can do in the next five years -- like they did with cars."
    ],
    "link": "https://hugodutka.com/posts/answering-legal-questions-with-llms/",
    "first_paragraph": "",
    "summary": "In the latest display of tech optimism that mistakes legal advice and medical diagnosis for querying a souped-up version of Clippy, a blog at hugodutka.com dives headfirst into the fantasy where AI will save us from the dread of human error and study. One enthusiast recounts a heartwarming tale of a doctor turning to Wikipedia in the ER, cheerfully implying that a machine pulling the same stunt but faster is somehow a monumental improvement. Another, clearly aiming for the 'Tech Bro of the Year' award, dreams of a world where AI not only replaces your google search but also magically endows you with instant expertise, because who needs years of study when you've got a bot? The comment section, a delightful mix of misplaced trust and fear of obsolescence, oscillates between \"AI will steal all our jobs\" and \"AI is my new best friend,\" showcasing the kind of dichotomy only the internet can provide."
  },
  {
    "title": "Claude \u00c9mile Jean-Baptiste Litre (wikipedia.org)",
    "points": 77,
    "submitter": "apollinaire",
    "submit_time": "2024-04-28T22:04:20",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=40192403",
    "comments": [
      "Ah, like 'Arkhan Land', rediscoverer of the Land Raider.(And of course, Jimmy Space).",
      "Adding this to my list of favorite April Fool's pranks -- my favorite being the recently featured islands of San Serriffe.",
      "lol, I love this. These gems are one of the reason I hang out on HN ;-)"
    ],
    "link": "https://en.wikipedia.org/wiki/Claude_%C3%89mile_Jean-Baptiste_Litre",
    "first_paragraph": "Claude \u00c9mile Jean-Baptiste Litre (1716-1778) is a fictional character created in 1978 by Kenneth Woolner of the University of Waterloo to justify the use of a capital L to denote litres.",
    "summary": "In a stunning revelation that has shocked precisely nobody but the kind of person who debates font choices on their blog, the esteemed Claude \u00c9mile Jean-Baptiste Litre is outed as a figment of academic boredom. Born from the womb of ennui in 1978 by a professor who clearly had run out of legitimate research grants, Litre's legacy has been to serve as a posthumous poster child for the metric system's most annoying unit. The comment section, an intellectual void rarely paralleled, is filled with self-congratulatory back-patting as readers compare this monumental academic zinger to the likes of mythical tech rediscoverers and fictional islands. It seems the bar for humor on Hacker News continues to be a limbo stick so low, even a quantum particle wouldn't bother to tunnel under it."
  },
  {
    "title": "Atomic nucleus excited with laser: A breakthrough after decades (tuwien.at)",
    "points": 337,
    "submitter": "geox",
    "submit_time": "2024-04-29T05:01:33",
    "num_comments": 152,
    "comments_url": "https://news.ycombinator.com/item?id=40194636",
    "comments": [
      "When you stop and look at QCD in the big picture, it's sort of shocking how little we know - like, really, really know - about the internal structure of the proton, or even the nucleon!It's the curse of \"probing\" with massive energies. No one's a hundred percent certain of whether they're detecting something that's actually there - like there there - or whether they're looking at by-product of enormous collision energies.Physicists are smart people! I could never do what they do. But there's a limit to certainty, and inside the proton especially there's unknown first principles at work. Bringing the precision of photons and lasers into this nucleon party is going to be huge. I can't wait!",
      "The measurement was already confirmed by a different group: https://arxiv.org/abs/2404.12311This is important since impurities in the crystals used lead to all kinds of fluorescence that could be mistaken for a signal from the Thorium ions. Now two groups have seen exactly the same signal in different Thorium-doped crystals which is very covincing that they have found the actual nuclear transition.",
      "I find it satisfying to see a researcher called THORsten SchUMm devoting his research to THORiUM.",
      "Nominative determinism :-)https://en.m.wikipedia.org/wiki/Nominative_determinism",
      "Frequency illusion :-Phttps://en.wikipedia.org/wiki/Frequency_illusion",
      "Lol that's a comic book name if I ever heard one. He's only one lab accident away from becoming a super hero/villain.",
      "And he's working with a dangerous radioactive isotope!",
      "We\u2019re so close! Send some spiders into the reaction chamber!",
      "But then I'm only a name change and a lab accident away.  And a name change isn't that hard...",
      "It doesn't work with name changes, unfortunately."
    ],
    "link": "https://www.tuwien.at/en/tu-wien/news/news-articles/news/lange-erhoffter-durchbruch-erstmals-atomkern-mit-laser-angeregt",
    "first_paragraph": "The \"thorium transition\", which physicists have been looking for for decades, has now been excited for the first time with lasers. This paves the way for revolutionary high precision technologies, including nuclear clocks.",
    "summary": "In a world thirsting for the next big pointless quantum leap, scientists at tuwien.at have finally managed to excite the ever-elusive \"thorium transition\" with lasers, inadvertently opening the Pandora's box to a future filled with nuclear clocks that nobody asked for. Meanwhile, in the comment sections, armchair physicists are having a meltdown over the existential crisis of not really knowing what's inside a proton - because, apparently, smashing particles together in a high-energy game of subatomic pinball leaves too much to the imagination. Amidst this intellectual frenzy, one commenter's acute observation that a scientist named Thorsten Schumm is researching Thorium has everyone's nominative determinism alarm ringing off the hook, because, of course, what we all need post-discovery is a superhero origin story to distract us from the impending doom of misunderstanding advanced physics. So, strap in for a future where time is even more confusingly precise, and where every lab accident is a missed opportunity for the birth of a villain with an aptly thematic name."
  },
  {
    "title": "How do satellites communicate with a GPS system? (2018) (allaboutcircuits.com)",
    "points": 5,
    "submitter": "reqo",
    "submit_time": "2024-04-28T15:34:59",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://www.allaboutcircuits.com/technical-articles/how-do-satellites-communicate-with-a-gps-system-a-look-at-the-gps-antenna/",
    "first_paragraph": "",
    "summary": "In an astonishing display of breaking news circa 1998, allaboutcircuits.com bravely reveals that satellites, through some form of space-age wizardry, manage to communicate with GPS systems. Commenters, in a delightful parade of ignorance, suddenly fancy themselves as heirs to Einstein, dissecting complex aerospace engineering concepts with the finesse of a gorilla performing brain surgery. One particularly enlightened soul wonders if this groundbreaking interaction between satellites and GPS might finally explain why his 2005 TomTom still thinks he's driving through a cornfield. Amidst the cacophony of self-proclaimed experts, the article stands as a monument to the shocking revelation that technology, indeed, communicates."
  },
  {
    "title": "Cheyenne Super Computer Auction (gsaauctions.gov)",
    "points": 81,
    "submitter": "zrules",
    "submit_time": "2024-04-29T12:10:47",
    "num_comments": 68,
    "comments_url": "https://news.ycombinator.com/item?id=40197277",
    "comments": [
      ">Components of the Cheyenne SupercomputerInstalled Configuration: SGI ICE\u2122 XA.E-Cells: 14 units weighing 1500 lbs. each.E-Racks: 28 units, all water-cooledNodes: 4,032 dual socket units configured as quad-node bladesProcessors: 8,064 units of E5-2697v4 (18-core, 2.3 GHz base frequency, Turbo up to 3.6GHz, 145W TDP)Total Cores: 145,152Memory: DDR4-2400 ECC single-rank, 64 GB per node, with 3 High Memory E-Cells having 128GB per node, totaling 313,344 GBTopology: EDR Enhanced HypercubeIB Switches: 224 unitsMoving this system necessitates the engagement of a professional moving company. Please note the four (4) attached documents detailing the facility requirements and specifications will be provided. Due to their considerable weight, the racks require experienced movers equipped with proper Professional Protection Equipment (PPE) to ensure safe handling. The purchaser assumes responsibility for transferring the racks from the facility onto trucks using their equipment.",
      "Given that the individual nodes are just x86_64 Xeons and run linux... it would be interesting to part it out for sale as individual, but functional, nodes to people.  There are a lot of people would like to have a ~2016 era watercooled 1U server from a supercomputer that was once near the top of the Top500 just to show to people.Get little commemorative plaques for each one and sell for $200 each or so.edit:\nit seems each motehrboard is a dual CPU board and so there are 4032 nodes, but the nodes are in blades that likely need their rack for power.  But I think individual cabinets would be cool to own.There are 144 nodes per cabinet... so 28 cabinets.  \nI'd pay a fair amount just to own a cabinet to stick in my garage if I was near there.",
      "The individual servers are not watercooled. The compute racks are air-cooled; the adjacent cooling racks then exchange that heat using the building's chilled water. It's the rack as a whole that is watercooled. If you extract a single node, you won't get any of that. As the other commenters also point out, these are blades; you can't run an individual node by itself.",
      "These are blades, so there is probably some kind of container chassis required to run them.Using them as desktop PCs would likely be a challenge.",
      "I can find a bunch of the E5-2697v4 CPUs on eBay in the $30-40 range.I wonder if there is a market for the SGI hardware.",
      "So getting 8,064 of them for $3,085 - 38 cents per CPU - is great value for money!",
      "this is basically \"free grand piano\" - not so free once you hire the movers and tuners",
      "Dump 8,064 old processors on eBay and you'll probably introduce some downwards price pressure.",
      "Does it come with a portable nuclear reactor to power it?",
      ">...totaling 313,344 GBCan you imagine the RAMDisk? Yes, you can. Especially in 20 years when it will be the norm. And also the Windows version that will require half of it in order to run /s"
    ],
    "link": "https://gsaauctions.gov/auctions/preview/282996",
    "first_paragraph": "",
    "summary": "Welcome to the grand garage sale of \"lightly used\" supercomputing equipment, where the defining line between \"enthusiast\" and \"certifiable\" gets blurrier by the minute. Watch in amazement as tech hoarders dream of turning a 1500 lb paperweight into their next DIY project, because nothing says \"practical\" like needing an industrial crane to install your latest eBay find into your suburban garage. Comments quickly devolve from hopeful speculation to the grim realization that owning a piece of the Cheyenne Supercomputer is less \"cool conversation starter\" and more \"how do I power this thing without annexing the local power station?\" Meanwhile, one bright spark is already calculating the eBay value of 8,064 processors, blissfully ignoring the economic concept of supply and demand. Who knew that the line between groundbreaking technology and \u201cI guess it\u2019s a quirky coffee table now\u201d could be so entertainingly thin?"
  },
  {
    "title": "Pdf.tocgen (krasjet.com)",
    "points": 124,
    "submitter": "nbernard",
    "submit_time": "2024-04-28T08:51:18",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=40187072",
    "comments": [
      "Recently I found the getToc function in PyMuPdf was too slow. I told them about it in their discord, and a day later they had fixed it. Now it only takes a couple of milliseconds. I'm using it for my project pdftomp3. Pdf.tocgen looks useful too, but I'm not sure if I can use it because of the licencse?",
      "Of course you can use it.What you can't do is deny others the same freedoms the license grants to you.",
      "There does appear to be some licensing awkwardness here. The license is nominally GPLv3, but it says it is based on AGPLv3 projects. It also appears to misidentify (it may have been correct at the time) PyMuPDF as GPLv3 when that appears to actually be AGPLv3. My assumption is that using this would require complying with AGPLv3?There's the additional oddity that a portion of the repository (the recipes directory) is licensed under CC-BY-NC-SA, and so the repository is not fully open source. This is particularly confusing, however, as the functional content of the recipes directory appears to be mostly records of direct observations of parameter choices in external documents and tools, and so doesn't seem like it would be copyrightable at all, at least in the US.",
      "Interested to know what is pdftomp3?",
      "You can upload a PDF and convert the chapters into MP3s (either original text or simplified text). But for PDFs without a table of contents, you can only convert single pages.",
      "- \"That is, you shouldn\u2019t expect it to work with scanned PDFs\"It's surprisingly easy to extend this type of workflow to scanned pdfs (as opposed to software-generated, text-containing ones). tesseract(1) makes short work of ToC pages with --psm set to 6 (an OCR setting that tends to collapse convoluted text layouts into a regular, software-parseable output).It should also be straightforward, but I don't know of an out-of-the-box solution, to automate that example of extracting \"text that looks like a header\"\u2013based on page layout/relative positioning, or font weight.  (I'm working on an adjacent problem, an automatic re-layout of raster documents to squeeze out whitespace and make them slightly nicer on small e-ink devices. Text islands are trivial to identify. I don't know how to quantify font weight, or things like that. I'm \"wasting\" a lot of time diving into lots of mathematics rabbit holes, but I don't know in advance which ones will be productive or not).",
      "I've found EasyOCR to work much better at pulling text out of irregular or unknown images. Requires more resources than tesseract but gets much better results in my projects.",
      "tesseract is fine for basic use cases, but it fails when the image is tilted (and thus the text isn't laid out horizontally), which can happen several times with scanned books. Compared to how well the Google OCR engine works, tesseract should be much better than it is.I wonder how difficult it is to develop a better OCR engine than tesseract.",
      "Tesseract is last gen. Multimodal is SOTA, and can handle even heavily distorted or destroyed text.",
      "Am I overlooking something, or is automating page rotation no more work than just a 2d FFT?"
    ],
    "link": "https://krasjet.com/voice/pdf.tocgen/",
    "first_paragraph": "",
    "summary": "In the latest installment of \"Hacker News Discovers Licensing,\" a brave soul ventures into the wild lands of software licensing hell to figure out if turning PDFs into sad robot audiobooks infringes on someone's grand idea of intellectual property. Meanwhile, the peanut gallery dives headfirst into a thrilling discussion about the snail's pace of PyMuPDF's getToc function, which\u2014hold your applause\u2014was fixed in a day, reducing its run time to \"only a couple of milliseconds.\" This groundbreaking achievement enables our protagonist to potentially revolutionize the commute by converting PDF manuals into ear-bleeding MP3 files, all while navigating the legal minefield that is GPLv3 and AGPLv3 without a map. But wait, there's more! Join the sideline experts as they sermonize on the holy grail of OCR technology, where the magical world of \"just use tesseract\" meets the gritty reality of rotated text and image-based PDFs. Truly, we live in the age of innovation, where the only thing more abundant than licensing confusion is the unwarranted confidence in OCR solutions. \ud83d\udcda\ud83c\udfa7\ud83d\udc69\u200d\u2696\ufe0f"
  }
]