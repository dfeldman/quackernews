[
  {
    "title": "Things Zig comptime won't do (matklad.github.io)",
    "points": 295,
    "submitter": "JadedBlueEyes",
    "submit_time": "2025-04-20T15:57:37 1745164657",
    "num_comments": 108,
    "comments_url": "https://news.ycombinator.com/item?id=43744591",
    "comments": [
      "Yes!To me, the uniqueness of Zig's comptime is a combination of two things:1. comtpime replaces many other features that would be specialised in other languages with or without rich compile-time (or runtime) metaprogramming, and2. comptime is referentially transparent [1], that makes it strictly \"weaker\" than AST macros, but simpler to understand; what's surprising is just how capable you can be with a comptime mechanism with access to introspection yet without the referentially opaque power of macros.These two give Zig a unique combination of simplicity and power. We're used to seeing things like that in Scheme and other Lisps, but the approach in Zig is very different. The outcome isn't as general as in Lisp, but it's powerful enough while keeping code easier to understand.You can like it or not, but it is very interesting and very novel (the novelty isn't in the feature itself, but in the place it has in the language). Languages with a novel design and approach that you can learn in a couple of days are quite rare.[1]: In short, this means that you get no access to names or expressions, only the values they yield.\n \nreply",
      "I was a bit confused by the remark that comptime is referentially transparent. I'm familiar with the term as it's used in functional programming to mean that an expression can be replaced by its value (stemming from it having no side-effects).  However, from a quick search I found an old related comment by you [1] that clarified this for me.If I understand correctly you're using the term in a different (perhaps more correct/original?) sense where it roughly means that two expressions with the same meaning/denotation can be substituted for each other without changing the meaning/denotation of the surrounding program. This property is broken by macros. A macro in Rust, for instance, can distinguish between `1 + 1` and `2`. The comptime system in Zig in contrast does not break this property as it only allows one to inspect values and not un-evaluated ASTs.[1]: https://news.ycombinator.com/item?id=36154447\n \nreply",
      "Yes, I am using the term more correctly (or at least more generally), although the way it's used in functional programming is a special case. A referentially transparent term is one whose sub-terms can be replaced by their references without changing the reference of the term as a whole. A functional programming language is simply one where all references are values or \"objects\" in the programming language itself.The expression `i++` in C is not a value in C (although it is a \"value\" in some semantic descriptions of C), yet a C expression that contains `i++` and cannot distinguish between `i++` and any other C operation that increments i by 1, is referentially transparent, which is pretty much all C expressions except for those involving C macros.Macros are not referentially transparent because they can distinguish between, say, a variable whose name is `foo` and is equal to 3 and a variable whose name is `bar` and is equal to 3. In other words, their outcome may differ not just by what is being referenced (3) but also by how it's referenced (`foo` or `bar`), hence they're referentially opaque.\n \nreply",
      "Those are equivalent, I think. If you can replace an expression by its value, any two expressions with the same value are indistinguishable (and conversely a value is an expression which is its own value).\n \nreply",
      "It's not novel. D pioneered compile time function execution (CTFE) back around 2007. The idea has since been adopted in many other languages, like C++.One thing it is used for is generating string literals, which then can be fed to the compiler. This takes the place of macros.CTFE is one of D's most popular and loved features.\n \nreply",
      "A little bit out of context, I just want to thank you and all the contributors for the D programming language.\n \nreply",
      "Regarding 2. How are comptime values restricted to total computations? Is it just by the fact that the compiler actually finished, or are there any restrictions on comptime evaluations?\n \nreply",
      "They don't need to be restricted to total computation to be referentially transparent. Non-termination is also a reference.\n \nreply",
      "I\u2019ve never managed to understand your year-long[1] manic praise over this feature. Given that you\u2019re a language implementer.It\u2019s very cool to be able to just say \u201cY is just X\u201d.  You know in a museum.  Or at a distance.  Not necessarily as something you have to work with daily.  Because I would rather take something ranging from Java\u2019s interface to Haskell\u2019s typeclasses since once implemented, they\u2019ll just work.  With comptime types, according to what I\u2019ve read, you\u2019ll have to bring your T to the comptime and find out right then and there if it will work.  Without enough foresight it might not.That\u2019s not something I want.  I just want generics or parametric polymorphism or whatever it is to work once it compiles.  If there\u2019s a <T> I want to slot in T without any surprises.  And whether Y is just X is a very distant priority at that point.  Another distant priority is if generics and whatever else is all just X undernea... I mean just let me use the language declaratively.I felt like I was on the idealistic end of the spectrum when I saw you criticizing other languages that are not installed on 3 billion devices as too academic.[2]  Now I\u2019m not so sure?[1] https://news.ycombinator.com/item?id=24292760[2] But does Scala technically count since it\u2019s on the JVM though?\n \nreply",
      "I'm sorry but I don't understand what you're complaining about comptime. All the stuff you said you wanted to work (generic, parametric polymorphism, slotting <T>, etc) just work with comptime.  People are praising about comptime because it's a simple mechanism that replacing many features in other languages that require separate language features.  Comptime is very simple and natural to use.  It can just float with your day to day programming without much fuss.\n \nreply"
    ],
    "link": "https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html",
    "first_paragraph": "\n              Es el disco de Od\u00edn. Tiene un solo lado. En la tierra no hay otra\n              cosa que tenga un solo lado.\n            \n          Zig\u2019s comptime feature is most famous for what it can do: generics!,\n          conditional compilation!,\n          subtyping!, serialization!,\n          ORM! That\u2019s fascinating, but, to be fair, there\u2019s a bunch of\n          languages with quite powerful compile time evaluation capabilities\n          that can do equivalent things. What I find more interesting is that\n          Zig comptime is actually quite restrictive, by design, and won\u2019t do\n          many things! It manages to be very expressive despite being\n          pretty limited. Let\u2019s see!\n        \n            When you execute code at compile time, on which machine does it\n            execute? The natural answer is \u201con your machine\u201d, but it is wrong!\n            The code might not run on your machine, it can be cross compiled!\n            For overall development sanity, it is importa",
    "summary": "\ud83e\uddd0 Another day, another niche programming blog treats us to lofty musings about Zig's comptime wonders\u2014as if generics and serialization were hot, fresh inventions just pulled out of the oven. Let's not get too excited, though; it's mostly just a restrictive rehash of what other languages have been doing for years, but without the bells and whistles. Commenters leap into action with their typical \"thought-terminating clich\u00e9s\" and philosophical grandstanding, pondering metaphysical nonsense about referential transparency in a way that would make even a philosophy undergrad blush. Meanwhile, a lone brave soul dares to remind everyone that D did it first, and C++ kinda, sorta followed suit, but who cares about history when we've got trendy language features to hype up? \ud83d\ude80\ud83d\ude44"
  },
  {
    "title": "Gemma 3 QAT Models: Bringing AI to Consumer GPUs (googleblog.com)",
    "points": 424,
    "submitter": "emrah",
    "submit_time": "2025-04-20T12:22:06 1745151726",
    "num_comments": 195,
    "comments_url": "https://news.ycombinator.com/item?id=43743337",
    "comments": [
      "I think gemma-3-27b-it-qat-4bit is my new favorite local model - or at least it's right up there with Mistral Small 3.1 24B.I've been trying it on an M2 64GB via both Ollama and MLX. It's very, very good, and it only uses ~22Gb (via Ollama) or ~15GB (MLX) leaving plenty of memory for running other apps.Some notes here: https://simonwillison.net/2025/Apr/19/gemma-3-qat-models/Last night I had it write me a complete plugin for my LLM tool like this:  llm install llm-mlx\n  llm mlx download-model mlx-community/gemma-3-27b-it-qat-4bit\n\n  llm -m mlx-community/gemma-3-27b-it-qat-4bit \\\n    -f https://raw.githubusercontent.com/simonw/llm-hacker-news/refs/heads/main/llm_hacker_news.py \\\n    -f https://raw.githubusercontent.com/simonw/tools/refs/heads/main/github-issue-to-markdown.html \\\n    -s 'Write a new fragments plugin in Python that registers\n    issue:org/repo/123 which fetches that issue\n        number from the specified github repo and uses the same\n        markdown logic as the HTML page to turn that into a\n        fragment'\n\nIt gave a solid response! https://gist.github.com/simonw/feccff6ce3254556b848c27333f52... - more notes here: https://simonwillison.net/2025/Apr/20/llm-fragments-github/\n \nreply",
      "Can you quote tps?More and more I start to realize that cost saving is a small problem for local LLMs. If it is too slow, it becomes unusable, so much that you might as well use public LLM endpoints. Unless you really care about getting things done locally without sending information to another server.With OpenAI API/ChatGPT, I get response much faster than I can read, and for simple question, it means I just need a glimpse of the response, copy & paste and get things done. Whereas on local LLM, I watch it painstakingly prints preambles that I don't care about, and get what I actually need after 20 seconds (on a fast GPU).And I am not yet talking about context window etc.I have been researching about how people integrate local LLMs in their workflows. My finding is that most people play with it for a short time and that's about it, and most people are much better off spending money on OpenAI credits (which can last a very long time with typical usage) than getting a beefed up Mac Studio or building a machine with 4090.\n \nreply",
      "My tooling doesn't measure TPS yet. It feels snappy to me on MLX.I agree that hosted models are usually a better option for most people - much faster, higher quality, handle longer inputs, really cheap.I enjoy local models for research and for the occasional offline scenario.I'm also interested in their applications for journalism, specifically for dealing with extremely sensitive data like leaked information from confidential sources.\n \nreply",
      ">I'm also interested in their applications for journalism, specifically for dealing with extremely sensitive data like leaked information from confidential sources.Think it is NOT just you. Most company with decent management also would not want their data going to anything outside the physical server they have in control of. But yeah for most people just use an app and hosted server.  But this is HN,there are ppl here hosting their own email servers, so shouldn't be too hard to run llm locally.\n \nreply",
      "\"Most company with decent management also would not want their data going to anything outside the physical server they have in control of.\"I don't think that's been true for over a decade: AWS wouldn't be trillion dollar business if most companies still wanted to stay on-premise.\n \nreply",
      "Yeah, this has been confusing me a bit. I'm not complaining by ANY means, but why does it suddenly feel like everyone cares about data privacy in LLM contexts, way more than previous attitudes to allowing data to sit on a bunch of random SaaS products?I assume because of the assumption that the AI companies will train off of your data, causing it to leak? But I thought all these services had enterprise tiers where they'll promise not to do that?Again, I'm not complaining, it's good to see people caring about where their data goes. Just interesting that they care now, but not before. (In some ways LLMs should be one of the safer services, since they don't even really need to store any data, they can delete it after the query or conversation is over.)\n \nreply",
      "It is due to the risk of a leak.Laundering of data through training makes it a more complicated case than a simple data theft or copyright infringement.Leaks could be accidental, e.g. due to an employee logging in to their free-as-in-labor personal account instead of a no-training Enterprise account. \nIt's safer to have a complete ban on providers that may collect data for training.\n \nreply",
      "Their entire business model based on taking other peoples stuff. I cant imagine someone would willingly drown with the sinking ship if the entire cargo is filled with lifeboats - just because they promised they would.\n \nreply",
      "Or GitHub. I\u2019m always amused when people don\u2019t want to send fractions of their code to a LLM but happily host it on GitHub. All big llm providers offer no-training-on-your-data business plans.\n \nreply",
      "> I\u2019m always amused when people don\u2019t want to send fractions of their code to a LLM but happily host it on GitHubWhat amuses me even more is people thinking their code is too unique and precious, and that GitHub/Microsoft wants to steal it.\n \nreply"
    ],
    "link": "https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/",
    "first_paragraph": "Last month, we launched Gemma 3, our latest generation of open models. Delivering state-of-the-art performance, Gemma 3 quickly established itself as a leading model capable of running on a single high-end GPU like the NVIDIA H100 using its native BFloat16 (BF16) precision.To make Gemma 3 even more accessible, we are announcing new versions optimized with Quantization-Aware Training (QAT) that dramatically reduces memory requirements while maintaining high quality. This enables you to run powerful models like Gemma 3 27B locally on consumer-grade GPUs like the NVIDIA RTX 3090.The chart above shows the performance (Elo score) of recently released large language models. Higher bars mean better performance in comparisons as rated by humans viewing side-by-side responses from two anonymous models. Below each bar, we indicate the estimated number of NVIDIA H100 GPUs needed to run that model using the BF16 data type.Why BFloat16 for this comparison? BF16 is a common numerical format used dur",
    "summary": "**Gemma 3 QAT Models: Bringing AI to Consumer GPUs**\n\nIn an earth-shattering blog post that only three highly caffeinated Silicon Valley engineers and a bored cat might care about, Google declares its \"revolutionary\" Gemma 3 as the knight in shimmering armor for consumer-grade GPUs. Brace yourself as these models \"dramatically\" slash memory requirements, allowing you to run a computational cosmic behemoth on your slightly outdated gaming rig. Meanwhile, in a not-so-distant comment section, tech aficionado types furiously tip tap about how Gemma 3's QAT models are now their \"new favorite\" baby\u2014because evidently, whacking out RAM statistics is now as trendy as IPA beer and overpriced avocado toast. Wrap up, folks; it might just be \"snappy\" enough not to use your good ol' LLMs on a ***cloud server*** like every other sane person out there. \ud83c\udfad \ud83e\udd13"
  },
  {
    "title": "Crows can recognize geometric regularity (phys.org)",
    "points": 64,
    "submitter": "wglb",
    "submit_time": "2025-04-17T14:20:13 1744899613",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=43717251",
    "comments": [
      "Kind of off topic but I just got back from the park and there is a public water bowl set out for dogs and a crow was manipulating something in the water - after a time my eyebrows went up as I realized the crow was softening some dried out discarded human food to make it easy to break up and eat!\n \nreply",
      "I made a terrible video of the event.https://youtu.be/EnFAW-ZxAQ0\n \nreply",
      "I love crows so much. I had some in my backyard that I would give stuff too a lot. When I would leave in the morning for work, they would perch on my gutters and make clucking sounds while looking down at me. I'd wave and be on my way.\n \nreply",
      "What's the legality on training an army of crows to collect loose change around the city for me?\n \nreply",
      "How comparable is the intelligence of crows, dolphins, octopi and non human apes? Somewhat or not at all? There seem to be a host of things that each of those can do. Can apes do all of those things and the other groups just a few things each? Is there a huge leap of separation or does the leap come between us and them? Is it in any way quantifiable?\n \nreply",
      "Humans have so far failed all the tests that crows set up to measure their intelligence\n \nreply",
      "I was watching some crows eat some food in a parking lot yesterday. The first one landed next to a tiny morsel, investigated it a bit, then did a head bob thing while looking up and making what sounded like a cross between a hoot and a caw. Another crow swooped in about ten seconds later and they poked at it a bit. Then a lady walked over towards them, they flew away, and she dumped out her half eaten to-go meal in the parking spot. Too easy.\n \nreply",
      "A lot of it comes from communication. We don't know how intelligent some of these things are simply because we can't communicate with them.For apes and gorillas we can communicate. We've taught them sign language so we know hands down in terms of language we beat them. But for dolphins and octopi, we just don't really know.\n \nreply",
      "We have not taught apes sign language. They can learn and form crude signs and use them to respond to stimuli or for rewards (wanting an orange, for example) but they\u2019re not meaningfully communicating. It\u2019d be like me claiming I taught my dog English because he can press the little button that plays a sound of me saying \u201cbiscuit!\u201d when he wants a treat (which you have to take away from him because he will just mash it, since dogs want dog biscuits).\n \nreply",
      "https://news.ycombinator.com/item?id=43675829\n \nreply"
    ],
    "link": "https://phys.org/news/2025-04-crows-geometric-regularity.html",
    "first_paragraph": "",
    "summary": "Once again, the luminary minds of phys.org readers converge to unravel the closely-guarded secret that crows just might be smarter than your average Internet commenter. A groundbreaking revelation that crows handle geometric puzzles surely has humanity quaking in their boots\u2014right up until someone in the comments section proudly deflects by bragging about a crow they\u2019ve semi-domesticated for their personal amusement and dreams of a crow-fueled petty cash empire. \ud83e\udd2f Meanwhile, another eager beaver muses on the legal training of bird armies, inadvertently sketching a dystopian future less like *The Birds* and more like *Alfred Hitchcock Meets Oliver Twist*. Will we ever understand animal intelligence? Not if the bloviating about unquantifiable cognitive leaps and YouTube links as scientific citations continues to pass for wisdom. Keep clucking, humanity\u2014those gutters aren\u2019t going to adorn themselves."
  },
  {
    "title": "Find the Odd Disk (alessandroroussel.com)",
    "points": 82,
    "submitter": "layer8",
    "submit_time": "2025-04-20T19:17:58 1745176678",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=43745868",
    "comments": [
      "I wish this had a \"I can't tell\" option. A few of the really hard ones I got right, but I'd say it was more of a lucky guess than a genuine ability to discriminate the difference.\n \nreply",
      "Quite. With the odds being 1 to 3 you'll pick the right one when you have to click one at random to proceed, the results get skewed.\n \nreply",
      "If you repeat the test a few times, it'll average out.\n \nreply",
      "That's why there's 20 rounds I guess. If you could just press \"all the same\" it wouldn't have to be as many\n \nreply",
      "I was wondering if it got harder or if it's just random:    function generateColors(difficulty, blacklist) {\n        ...\n        let sample = Math.floor(Math.min(Math.max(0, 1-Math.pow(1-difficulty, 1.5)), .99)*5);\n        let distance = (5 - sample)/5;\n        ...\n    }\n    function setupRound(blacklist) {\n        ...\n        const data = generateColors(currentRound/totalRounds, blacklist);\n        ...\n    }\n\nPlotting that first magic: https://lucb1e.com/randomprojects/js/testformula.htm#%24%28%...    round#  difference\n     0-- 2  5\n     3-- 5  4\n     6-- 9  3\n    10--13  2\n    14--20  1\n\nThe \"blacklist\" parameter prevents that you get the same challenge twice. Note also that it submits every answer to the server (fine imo, but I think it would be even nicer if this was mentioned on the page)\n \nreply",
      "I appreciate https://lucb1e.com/randomprojects/js/testformula.html, it's a cool idea!\n \nreply",
      "This is from the creator of the ScienceClic YouTube channel [0]:\u201cAs part of the next video, which will be out in a few weeks, l'd like to invite you to take part in an experiment about color perception. If you don't experience color blindness, l'd greatly appreciate it if you could take this test. Feel free to try it as many times as you like, think about it as a game!\u201d[0]  https://youtube.com/@scienceclicen\n \nreply",
      "17 out of 20. Was super easy until #10 and I had to stop and think more carefully (which was actually my first mistake), and then I got #14 and #15 wrong. The score was about what I expected, though - would've been surprised if it was <15 correct.I wonder how much of this would come down to screen calibration / color accuracy? If everything's consistently off in 1 direction I guess not much, but I would imagine certain shades might appear effectively the same on some cheaper screens?\n \nreply",
      "Would be interesting to get some basic analysis of my results. From a glance it appeared that the ones I missed (6) tended towards red. The low saturation ones and green ones I found to be easiest, but was there any actual significance of the distribution of my errors? Simply too small a set to say?\n \nreply",
      "I got the same number wrong but I've passed every Ishihara test ever thrown at me. I did this test on a cheap mobile that's not calibrated, so it's anyone guess what its gamma and transfer curves are like.One should only take such tests seriously if one's using a properly calibrated monitor and it's viewed under ideal viewing conditions.\n \nreply"
    ],
    "link": "https://colors2.alessandroroussel.com/",
    "first_paragraph": "Click the disk that's a different color. Use your eyes only! Make sure to disable any blue-light filter on your screen.\n\t\tClique sur le disque qui est d'une couleur diff\u00e9rente. Utilise seulement tes yeux ! Assurez-vous de d\u00e9sactiver tout filtre de lumi\u00e8re bleue sur votre \u00e9cran.\n\t\tHaz clic en el disco que tiene un color diferente. \u00a1Usa solo tus ojos! Aseg\u00farate de desactivar cualquier filtro de luz azul en tu pantalla.Thank you for participating. Please feel free to play again, the more data the better!\n\t\t\tMerci pour ta participation. N'h\u00e9site pas \u00e0 rejouer, plus il y a de donn\u00e9es, mieux c\u2019est !\n\t\t\tGracias por participar. \u00a1No dudes en jugar de nuevo, entre m\u00e1s datos tengamos, mejor!",
    "summary": "**Find the Odd Disk: A Masterclass in the Obvious**\n\nIn an exhilarating display of digital innovation, alessandroroussel.com invites users to engage in the groundbreaking task of <em>spotting a colored disk</em>. Participants must harness their \"advanced\" ability to discern colors without the trickery of blue-light filters, in a test so necessary we'd forget what colors looked like without it. The comment section burgeons with tech savants who debate the complexities of random color generation algorithms and the profound impact of screen calibration on their scores, unveiling life-altering insights like color tests being easier on better screens. Who knew? \ud83e\udd2f Join the frenzy to provide \"more data,\" because certainly, that\u2019s what's been missing from your daily routine."
  },
  {
    "title": "Decomposing Transactional Systems (transactional.blog)",
    "points": 48,
    "submitter": "pongogogo",
    "submit_time": "2025-04-20T20:54:56 1745182496",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=43746461",
    "comments": [
      "This is great, really worth reading if you're interested in transactions.I liked it so much I wrote up how the model applies to Amazon Aurora DSQL at https://brooker.co.za/blog/2025/04/17/decomposing.html It's interesting because of DSQL's distributed nature, and the decoupling between durability and application to storage in our architecture.\n \nreply",
      "> commit version is chosen \u2014 the time at which the database claims all reads and writes occurred atomically.This post doesn't mention transaction isolation specifically though it does say \"How does this end up being equal to SERIALIZABLE MySQL?\" So maybe I'm supposed to consider this post only for 'Every transactional system' running with SERIALIZABLE transaction isolation. I don't particularly care about that. I do care that the database I use clearly states what its isolation names mean in detail and that it does exactly what it says. e.g. I don't expect MySQL SERIALIZABLE to exactly mean the same as any other database that uses the same term.\n \nreply",
      "MySQL Serializable is pretty similar to serializable in other databases, in terms of the observable anomalies. There's a good set of tests here: https://github.com/ept/hermitage> So maybe I'm supposed to consider this post only for 'Every transactional system' running with SERIALIZABLE transaction isolation.No, it's a general point about the nature of transactions in DBMSs, and the different implementation choices. As the article says, there are some variations (e.g. MVCC at levels lower than serializable inherently has two 'order' steps).\n \nreply",
      "I'm not seeing the mention of two 'order' steps. Are you referring to the larger part of what I quoted?> MVCC databases may assign two versions: an initial read version, and a final commit version. In this case, we\u2019re mainly focused on the specific point at which the commit version is chosen \u2014 the time at which the database claims all reads and writes occurred atomically.For non-SERIALIZABLE isolation there may be no such \"time at which the database claims all reads and writes occurred atomically\", which is how I took the rest of the post to mean when running with SERIALIZABLE isolation.\n \nreply"
    ],
    "link": "https://transactional.blog/blog/2025-decomposing-transactional-systems",
    "first_paragraph": "",
    "summary": "Title: Armchair Architectures Unraveled\n\nIn another dazzling display of verbosity, transactional.blog sheds dim light on the crumbling catacombs they call \"Transactional Systems.\" \ud83d\udca1\ud83d\udd73\ufe0f In an effort to seem relevant, a commenter throws out a URL to their own \"insightful\" reinterpretation involving Amazon Aurora DSQL, inadvertently showcasing the art of missing the point in public. Meanwhile, a debate on MySQL SERIALIZABLE versus its estranged cousins in other databases spirals into the abyss of technical nitpicking, because obviously what the world lacks is another echo in the chamber of database isolation levels. \ud83d\udd04\ud83d\udde3\ufe0f Sadly, the original article\u2019s valiant attempt to unravel the arcane arts of transaction systems merely serves as a launchpad for the opinionated many, eager to flaunt their half-baked SQL commands and half-digested system concepts."
  },
  {
    "title": "Signal Carnival (quiss.org)",
    "points": 85,
    "submitter": "adunk",
    "submit_time": "2025-04-20T17:13:32 1745169212",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43745040",
    "comments": [
      "For others like me confused as to why there's not a video example on the page: click the youtube link in the first sentence!Super cool mesmerizing effect.\n \nreply",
      "This was damn cool. Watching and listening to it I wonder what is the hardest. Producing video with a sound chip or producing audio with a video chip. Fun stuff.\n \nreply",
      "Aw was hoping to see a leaked invitation to the top secret MAGA costume party\n \nreply",
      "What a beautiful thing to do\n \nreply",
      "I\u2019d like to see & hear the correctly plugged version of the video.\n \nreply",
      "https://csdb.dk/release/?id=252090 plus a c64 emulator; it's a pretty similar experience to just watching colors flash on the screen while a game loads and decrunches...\n \nreply",
      "My first reaction too, but...\n \nreply"
    ],
    "link": "https://www.quiss.org/signal_carnival/",
    "first_paragraph": "At Revision 2025, we released \"Signal Carnival\" (csdb, YouTube). This demo asks you to\nswitch the audio and video cable of your C64:\n(Diagram by Felidae)Technically, \"misplugging\" those cables is not a completely new idea. In the\n90s, it was a common thing to connect audio to both speaker and video,\nto get screen flickering timed with the music beats.However, \"Signal Carnival\" is the first production to switch both these\ncables, while still being able to play meaningful audio and video.The C64's VIC chip operates at a frequency of 7.9Mhz, and new values can be\nwritten to it by the 6502 at a rate of up to 246kHz. Since even high\nquality audio rarely exceeds 44kHz, this is easily a high enough frequency\nto generate music.Inspired by the music routine from freespin, we combined two timers to\nget an interesting waveform, which is adjusted once per frame. This is\nthe code that drives audio:Here, $dc06 and $dd06 are the lower bytes of the B timers of CIA #1 and #2,\nrespectively. They're in t",
    "summary": "**Signal Carnival: Nostalgia Hackers Go Brrr**\n\nIn an act of what can only be described as techno-nostalgic masochism, Revision 2025 attendees were treated to \"Signal Carnival,\" a demo that instructs users to misuse their Commodore 64 cables for audiovisual \"delights.\" This groundbreaking experiment in cable-swapping, surely unknown to the world of modern computing, manages to emit sounds and sights from a machine most people last saw in a dusty attic. Commenters, confusing technological ineptitude with innovation, wax nostalgic and hallucinate grandeur in blurry pixel flashes and tinny beeps. One even wondered aloud about the complexities of misusing hardware \u2013 who knew plugging wires incorrectly could be so mesmerizing and baffling? Icons of technophilia toast yet another rediscovery of the wheel, pixelated and screechy. \ud83c\udf89\ud83d\udc7e"
  },
  {
    "title": "TikZJax: Embedding LaTeX Drawings in HTML (tikzjax.com)",
    "points": 25,
    "submitter": "steventhedev",
    "submit_time": "2025-04-20T22:04:22 1745186662",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43746831",
    "comments": [
      "Jim Fowler seemed like Calculus' biggest hype man when the MOOC ball was just starting to roll. If you're looking to brush up and like the more energetic/engaging style I'd recommend checking out his videos on YouTube or elsewhere.> Using web2js, the Pascal source of tex is compiled to WebAssembly; the latex format is loaded (without all the hyphenation data), and [...] is executed. Then core is dumped; the resulting core is compressed, and by reloading the dumped core in the browser, it is possible to very quickly get to a point where TikZ can be executed. By using an SVG driver for PGF along with dvi2html, the DVI output is converted to an SVG.This is the kind of hack I'm here for.\n \nreply",
      "Using a \u201ccore dump\u201d (dumping the webassembly heap) is an interesting optimization approach with historical precedent both in TeX itself and projects like Emacs (dump/unexec) \u2014 https://www.gnu.org/software/emacs/manual/html_node/elisp/Bu...It\u2019s also notoriously fragile and non-portable on native targets; I\u2019m curious how one implements it under webassembly, and how it compares.\n \nreply",
      "Hm. Either that page or the tech itself is not great on mobile.\n \nreply",
      "Takes a second or so to load on mine (iOS Safari). But then it shows correctly, even if the second diagram is a bit small (it fits in a quarter of the 1in circle).\n \nreply",
      "It crashes (\u201ca problem repeatedly occurred\u201d) a few seconds after loading everything on my device (also iOS Safari).I love tikz, but lightweight it is not; it\u2019s not a huge surprise it takes a few seconds to render.No idea what\u2019s causing the crash, though.\n \nreply",
      "Well iOS Safari is in general buggy and tends to display the \"a problem repeatedly occurred\" message on many other slightly heavy web pages. This web page shouldn't be blamed for causing Safari to crash.\n \nreply",
      "Nobody is assigning blame, we don\u2019t know the root cause.I could just as easily say that Safari shouldn\u2019t be blamed for a buggy website, but I\u2019d be overreaching just as much as you just did."
    ],
    "link": "https://tikzjax.com/",
    "first_paragraph": "TikZJax converts script tags (containing TikZ code) into SVGs.See a live demo at https://tikzjax.com/.In the <head> of your HTML, include Then in the <body>, include TikZ code such asYour TikZ will be compiled into SVGs; the <script> element will be\nreplaced with the corresponding SVG.  For instance, the above code generatesYou can also use this for commutative diagrams.  For example, this codewill be rendered asUsing web2js, the Pascal source of tex\nis compiled to WebAssembly; the latex format is loaded (without all the hyphenation data), and is executed.  Then core is dumped; the resulting core is compressed, and by reloading the dumped core in the browser, it is possible to very quickly get to a point where TikZ can be executed.  By using an SVG driver for PGF along with dvi2html, the DVI output is converted to an SVG.To emphasize this: all of this happens in the browser.Jim Fowler\nfowler@math.osu.edu.",
    "summary": "Title: **The \ud83c\udfa8 Artistic Wizardry \ud83c\udfa9 of LaTeX in Your Web Browser**\n\nIn a miraculous digital alchemy experiment, TikZJax promises to turn obscure scripts of TikZ code into shiny SVG graphics without ever leaving the confines of your musty old browser. Web enthusiasts and LaTeX wizards assemble, cheer, and proceed to bombard forums with niche use cases no one asked for. Meanwhile, commenters engage in a technological tug-of-war, debating whether this magic trick is the future of web graphics or just another cool trick doomed by web compatibility issues, with iOS Safari users waving the white flag of surrender due to constant browser crashes. \ud83d\udca5\ud83d\udd27 Isn't modern web development just a delightful arena of unending compatibility fun?"
  },
  {
    "title": "Falsify: Hypothesis-Inspired Shrinking for Haskell (2023) (well-typed.com)",
    "points": 51,
    "submitter": "birdculture",
    "submit_time": "2025-04-20T19:41:29 1745178089",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43746017",
    "comments": [
      "How does Hedgehog and Hypothesis differ in their shrinking strategies?The article uses the words \"integrated\" vs. \"internal\" shrinking.> the raison d\u2019\u00eatre of internal shrinking: it doesn\u2019t matter that we cannot shrink the two generators independently, because we are not shrinking generators! Instead, we just shrink the samples that feed into those generators.Besides that it seems like falsify has many of the same features like choice of ranges and distributions.\n \nreply",
      "This is fascinating!If I understand correctly, they approximate language of inputs of a function to discover minimal (in some sense, like \"shortest description length\") inputs that violate relations between inputs and outputs of a function under scrutiny.\n \nreply",
      "I've found in practice that shrinking to get the \"smallest amount of detail\" is often unhelpful.Suppose I have a function which takes four string parameters, and I have a bug which means it crashes if the third is empty.I'd rather see this in the failure report:(\"ldiuhuh!skdfh\", \"nd#lkgjdflkgdfg\", \"\", \"dc9ofugdl ifugidlugfoidufog\")than this:(\"\", \"\", \"\", \"\")\n \nreply",
      "Really? Your examples seem the opposite. I am left immediately thinking, \"hm, is it failing on a '!', some sort of shell issue? Or is it truncating the string on '#', maybe? Or wait, there's a space in the third one, that looks pretty dangerous, as well as noticeably longer so there could be a length issue...\" As opposed to the shrunk version where I immediately think, \"uh oh: one of them is not handling an empty input correctly.\" Also, way easier to read, copy-paste, and type.\n \nreply",
      "Their point is that in the unshrunk example the \u201cspecial\u201d value stands out.I guess if we were even more clever we could get to something more like (\u2026, \u2026, \"\", \u2026).\n \nreply",
      "The special value doesn't stand out, though. All three examples I gave were what I thought skimming his comment before my brain caught up to his caveat about an empty third argument. The empty string looked like it was by far the most harmless part... Whereas if they are all empty strings, then by definition the empty string stands out as the most suspicious possible part.\n \nreply",
      "I care about the edge between \"this value fails, one value over succeeds\".\nI wish shrinking were fast enough to tell me if there are multiple edges between those values.\n \nreply",
      "newtype Parser a = Parser ([Word] -> (a, [Word])\n\nmissing a paren here\n \nreply",
      "I\u2019m honestly completely failing to understand the basic idea here. What does this look like for generating and shrinking random strings,\n \nreply"
    ],
    "link": "https://www.well-typed.com/blog/2023/04/falsify/",
    "first_paragraph": "Consider this falsify property test that tries to verify the (obviously false) property\nthat all elements of all lists of up to 10 binary digits are the same (we will explain the details below;\nhopefully the intent is clear):we might get a counter-example such as this:More interesting than the counter-example itself is how falsify arrived\nat that counter-example; if we look at the shrink history (--falsify-verbose),\nwe see that the list shrunk as follows:The test runner is able to go back and forth between shrinking the length and the list, and\nshrinking elements in the list. That is, we have integrated shrinking (like in\nhedgehog: we do not specify a separate generator and shrinker), which is\ninternal: works across monadic bind. The Python\nHypothesis library showed the world how to achieve\nthis. In this blog post we will introduce falsify, a new\nlibrary that provides property based testing in Haskell and has an approach to\nshrinking that is inspired by Hypothesis. As we shall see, how",
    "summary": "**Hacker News Presents: \"Falsify: Because Haskell Needed Another Way to Be Incomprehensible\"**\n\nWelcome to another episode of Haskell Developers Doing Things the Hard Way. Today's exhibit is *Falsify*: a mystical tool designed for breaking your soul with its 'Hypothesis-inspired' shrinking, which we assume means it aspires to be useful like Python's Hypothesis library but in the inconsolable labyrinth of Haskell syntax. The comment section quickly transforms into the blind leading the blind over the never-ending battle of integrated versus internal shrinking, while someone inevitably misses a parenthesis and everyone forgets what the actual point was. Meanwhile, pragmatic programmers marvel at how much ink can be spilled over making bugs simpler or not depending on how 'special' your values look. \ud83e\udd2f"
  },
  {
    "title": "New Proof Settles Decades-Old Bet About Connected Networks (quantamagazine.org)",
    "points": 70,
    "submitter": "rbanffy",
    "submit_time": "2025-04-20T17:48:40 1745171320",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=43745261",
    "comments": [
      "> The fraction turned out to be approximately 69%, making the graphs neither common nor rare.The wording kinda bothers me... Either 31% or 69% is exceedingly common.Rare would be asymptotically few, or constant but smaller than e.g. 1 in 2^256.I guess the article covers it's working definition of common, ever so briefly:>  that if you randomly select a graph from a large bucket of possibilities, you can practically guarantee it to be an optimal expander.So it's not a reliable property, either way.\n \nreply",
      "69% looks surprising like the answer to this puzzle:> The numbers 1\u2013n are randomly placed into n boxes in a line. There are n people who are each able to look into half the boxes. While they are allowed to coordinate who looks into which boxes beforehand, they are taken out one at a time to choose which half of the boxes they will peek at. The goal is for the first person to find the number one, the second person to find the number two, and so on. If any of them fail to find their number, the whole group loses. What is the probability they lose if they use the optimal strategy?I wonder if there's a connection to regular graphs here.\n \nreply",
      "So you alternate so that you're always looking at as many new boxes as possible? Or are the people allowed to communicate?\n \nreply",
      "The article reads as written by someone who just learned about graphs, it focuses so much on the bet and so less on explaining Ramanujan expanders\n \nreply",
      "Not sure I agree.It does a decent job of conveying the essential idea for a broader readership: perturb a graph through its adjacency matrix just enough to make the universality conjecture hold for the distribution of eigenvalues -> analytically establish that the perturbation was so small that the result would carry back to the original adjacency matrix (I imagine this is an analytical estimate bounding the distance between distributions in terms of the perturbation) -> use the determined distribution to study the probability of the second eigenvalue being concentrated around the Alon-Bopanna number.I haven't had a chance to read the paper and don't work in graph theory but close enough to have enjoyed the article.\n \nreply",
      "I agree with you, I work with graph algebra libraries and this article did a very nice job.\n \nreply",
      "Might be fruitful to apply this on p2p mesh networks.\nI suppose you should be able to make a model describing how the relationship between the fraction of byzantine nodes affects the probability distribution of connectedness. Then you could figure out what algorithm parameters would put you within desired bounds for tolerated ratios of byzantine.\n \nreply",
      "Byzantine has I think been misused. It\u2019s the least number of good members you need to be successful, not the best number. I think there\u2019s a reason parliamentary systems have a supermajority rule for making certain kinds of changes and a simple majority for others and we should probably be doing the same when we model systems.It is simple enough for an adversarial system to subvert some members via collusion and others via obstruction. Take something like Consul which can elect new members and remove old ones (often necessary in modern cloud architectures). What does 50.1% mean when the divisor can be changed?And meshes are extremely weird because the whole point is to connect nodes that cannot mutually see each other. It is quite difficult to know for sure if you\u2019re hallucinating the existence of a node two hops away from yourself. You\u2019ve never seen each other, except maybe when the weather was just right one day months ago.\n \nreply",
      "> Byzantine has I think been misused. It\u2019s the least number of good members you need to be successful, not the best number.Could you elaborate? It sounds like you are talking more about challenges of distributed consensus  (elections, raft). What I have in mind is distributed peering algorithms for decentralized networks. No consensus, elections, or quorum required. You may wish to run consensus algos on top of such networks but that's one layer up, if you will.Byzantine in the context of unpermissioned networks is often explained as the sybil problem, which maps to the issues you mention.Applying OP to this setting wouldn't mitigate that but I'm thinking it can be used as a framework to model and reason about these systems. Perhaps even prove certain properties (assuming some form of sybil resistance mechanism, I guess).\n \nreply",
      "Coming to a leetcode interview soon near you!\n \nreply"
    ],
    "link": "https://www.quantamagazine.org/new-proof-settles-decades-old-bet-about-connected-networks-20250418/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesApril 18, 2025Michele Sclafani for\u00a0Quanta MagazineContributing CorrespondentApril 18, 2025It started with a bet.In the late 1980s, at a conference in Lausanne, the mathematicians Noga Alon and Peter Sarnak got into a friendly debate. Both were studying collections of nodes and edges called graphs. In particular, they wanted to better understand a paradoxical type of graph, called an expander, that has relatively few edges but is still highly interconnected.At issue were the very best expanders: those that are as connected as they can be. Sarnak proposed that such graphs are rare; he and two collaborators would soon publish a paper that used complicated ideas from number theory to build exam",
    "summary": "In a thrilling throwback to the nerdy nights of the late '80s, Quanta Magazine breathlessly recounts a *decades-old debate* between two luminaries of graph theory over the scintillating topic of highly connected sparse graphs \u2013 try to hold back your excitement. Apparently, some graphs are more \"connected\" than your socially awkward cousin at a wedding, despite having fewer edges. Commenters, in a desperate attempt to one-up each other with how <em>smart</em> they sound, dive deep into technical jargon, likely confused about whether they\u2019re discussing math or plotting the world's most boring conspiracy. Highlights include lofty dismissals of the article's simplicity and a spirited detour into \"Byzantine faults\" which might as well be a discussion about medieval architecture for all the relevance it has to the original topic. \ud83e\udd13\ud83d\udcc9"
  },
  {
    "title": "Efficient E-Matching for Super Optimizers (vortan.dev)",
    "points": 13,
    "submitter": "todsacerdoti",
    "submit_time": "2025-04-19T05:47:54 1745041674",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.vortan.dev/ematching/",
    "first_paragraph": "Modern theorem provers and optimizing compilers are built on an interesting concept: the ability to recognize when two things are equal, even if they look completely different. It\u2019s not enough to just match the syntax - we need to match the semantics as well. That\u2019s where E-Matching comes in.E-Matching is a pattern matching technique that considers syntactic structures and established equalities. Suppose you\u2019re trying to apply the axiom:The solver must find terms in its database that match f(x). These need to match syntactically, and up to equality. The \u201cE\u201d in E-matching refers to equality reasoning, usually captured using \u201cE-Graphs\u201d or congruence closures.E-matching comes from the world of SMT Solvers, where it\u2019s used to instantiate quantified axioms during the proof search. The general setup is: the solver knows a bunch of constraints and terms, and it\u2019s trying to apply some universal quantified formulas, such as \u2200x. f(x) = x. To do this effectively, it needs to search for instances ",
    "summary": "**Title:** Efficient E-Matching for Super Optimizers (vortan.dev)\n\nWelcome to another riveting installment where software engineers pretend to reinvent mathematics! Today's episode features <em>\"Efficient E-Matching for Super Optimizers\"</em>, a groundbreaking exposition on how the digital miracle of E-Matching is akin to finding your socks in the morning\u2014surprisingly complex but indisputably mundane. The aspiring commentariat dives head-first into obsessive dissections of pattern matching, clearly thrilled to escape their day jobs of reinventing slightly rounder wheels. Brace yourself for breathtaking exchanges that juggle jargon like \"E-Graphs\" and \"congruence closures,\" demonstrating once again that, in tech, if you can\u2019t dazzle them with brilliance, befuddle them with BS. \ud83c\udfad\ud83e\udde0"
  },
  {
    "title": "Which year: guess which year each photo was taken (whichyr.com)",
    "points": 618,
    "submitter": "trymas",
    "submit_time": "2025-04-17T10:42:06 1744886526",
    "num_comments": 185,
    "comments_url": "https://news.ycombinator.com/item?id=43715024",
    "comments": [
      "There's a similar game, but for guessing both the year and the locationhttps://timeguessr.com/Discussed on HN a couple of years ago too: https://news.ycombinator.com/item?id=37203511edit: found another game like OP in the linked thread\nhttps://www.chronophoto.app/game.html\n \nreply",
      "Another for year + location https://whentaken.com/\n \nreply",
      "I had to close it - the ads are all over the place.\n \nreply",
      "Holy, I was off by 7 years and 427km on my first guess and I don't want to play anymore because I've surely peaked\n \nreply",
      "I just got one that was off by 2 years and 24.2 meters, but it was kinda cheaty because it showed Barbara Bush on the South lawn of the White House.\n \nreply",
      "Seems like they're \"season photos\", so a lot of Easter and Holy Week celebrations today.I have to say I got almost perfect a Holy Week celebration in Sevilla in the 1920's (I'm Spanish, so only some hundred meters away but kinda wild guess for the year and only was two years off), and pointed Mexico instead of Guatemala for another, but nailed the year (1981) for a grand total of 902 out of 1000 points.That was fun!\n \nreply",
      "Ha, I just got that same one! I managed to guess the year (1/4 chance), and only 15.5 meters off.\n \nreply",
      "lol nicely doneI only immigrated to the US in my adult life and I can't say I recognize Barbara Bush at a glance, so I was _also_ off by a few years and a few km on that one :( but still a lot of fun!\n \nreply",
      "The whole collection is great:https://whentaken.com/teuteuf-games\n \nreply",
      "https://pointguessr.com is another one, it's like timeguessr but with real-time co-op functionality\n \nreply"
    ],
    "link": "https://whichyr.com/",
    "first_paragraph": "The closer your guess, the more points you earn!Use digit reveals to unlock one digit of the correct year. Each digit can only be revealed once per game.Challenge your visual memory by guessing when photos were taken in this addictive and educational game.Look carefully at historical photos and guess which year each was taken. Use the timeline slider to select any year between 1850 and 2025.The closer your guess is to the actual year, the more points you'll score. Perfect guesses earn maximum points!Stuck on a difficult photo? Use the hint feature to reveal one digit of the correct year. Each digit can only be revealed once per game.Test your knowledge with our daily challenge featuring a new set of photos each day. Compare your scores with other players and track your improvement over time.",
    "summary": "Welcome to Which Year, the game where your wild guesses about old photos earn you points. Try to determine the year of grainy, ambiguous images\u2014like a blurry version of \"Where's Waldo?\" for historians. While tech aficionados on Hacker News debate which clone of this idea was the *first* to waste our time, everyday users split hairs over whether the pixelated smudge was Barbara Bush or a stray cloud. Sharpen your skills daily just to brag about nearly perfect scores in a string of never-ending ego matchups. \"Fun\", you say? Surely there's more thrill in watching paint dry. \ud83c\udf89\ud83d\udcf8\ud83d\udd75\ufe0f\u200d\u2642\ufe0f"
  },
  {
    "title": "Demystifying decorators: They don't need to be cryptic (thepythoncodingstack.com)",
    "points": 10,
    "submitter": "rbanffy",
    "submit_time": "2025-04-20T21:07:03 1745183223",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.thepythoncodingstack.com/p/demystifying-python-decorators",
    "first_paragraph": "",
    "summary": "In a revelatory act of absolute bravery, thepythoncodingstack.com attempts to strip away the enigmatic aura of Python decorators, those arcane snippets of code that mere mortals dare not decipher. Unfortunately, the explanation quickly spirals into a labyrinth of confusing analogies involving sandwiches and Russian nesting dolls, leaving readers not so much enlightened as desperately Googling \"what is a decorator python\" for the sixth time. Commenters leap into the fray with the zeal of war reenactors, each trying to out-pedant the others by arguing over minute, irrelevant details of the examples provided. Software development, a field once dominated by quiet typing, has apparently become a contact sport. \ud83d\udc0d\ud83d\udcbb"
  },
  {
    "title": "Turing-Drawings (github.com/maximecb)",
    "points": 93,
    "submitter": "laurenth",
    "submit_time": "2025-04-20T16:00:29 1745164829",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43744609",
    "comments": [
      "This is my favorite [1]. These are a fun exercise to program yourself. Fairly straightforward but also insightful and easy to create fun variations with.[1]: https://maximecb.github.io/Turing-Drawings/#2,10,0,2,1,0,6,1...\n \nreply",
      "Interest qualia experience I noted.I \u201cclearly\u201d see lots of dots appear and disappear. It feels direct and unassailable that I am seeing dots. But I never really see a single dot appear and disappear. (Without making a very selective effort.)Clues like that suggest that the qualia answer has mechanistic explanation. The signal saying that we see something, directly and clearly, and actually seeing something, are separable.Which is true for recognition of a previous experience (Deja vu), knowing (unquestioning belief), etc.We experience certainties and experiences we deem direct, that we often attribute to reality, but the measure of certainty and directness themselves are just other signals only approximating or filling in (usefully confabulating) what we think they say.Our experiences are absolutely full of invisible simplifications, internally created opaque illusions, of not only information, but meta-information.They work as efficiencies because by design we do not have the natural ability to perceive or question them. No natural inclination to seperate seemingly deep experience from actually sparse internal sensory and meta status representations, or representations from reality (whether internal or external).\n \nreply",
      "This (along with ibniz) was one of my inspirations for https://c50.fingswotidun.com/Using a stack based expression approach makes it easier to design images at the cost of being less flexible computationally.  I have often pondered enhancements to make it more capable,and indeed Turing complete.  Forth style word definition would work, but I also have a soft spot for state machines.Little toys like these are things I would recommend everyone have a go at.  I have quite enjoyed https://tixy.land/ and https://www.dwitter.net\n \nreply",
      "Very impressive!  I have come across your website before as well.  I really like how polished and sophisticated the demos are.  Great work, and thanks for sharing!I'd like to take this opportunity to share a couple of my own, much less impressive, tools that explore similar ideas:https://susam.net/cfrs.html (Turtle graphics but with only 6 commands)https://susam.net/fxyt.html (Inspired by Tixy but stack-based with 36 instructions)To see the demos, click or type '?' and then scroll down to the bottom of the manual.\n \nreply",
      "It's interesting how some of them halt after a while and some of them don't. I wonder if one could figure out which ones do and which ones don't?\n \nreply",
      "None of them halt, since no halting state is ever introduced into these canvas dwelling TMs :-(\n \nreply",
      "I think GP is actually asking whether we can determine if one enters a steady state, i.e. tape no longer changes.\n \nreply",
      "You are kidding, right? [0][0] https://en.wikipedia.org/wiki/Halting_problem\n \nreply",
      "It was a (not particularly funny) joke.\n \nreply",
      "I thought it was great :)\n \nreply"
    ],
    "link": "https://github.com/maximecb/Turing-Drawings",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Randomly generated Turing machines draw images and animations on a 2D canvas.\n      Randomly generated Turing machines draw images and animations on a 2D canvas.\nA simple JavaScript+HTML5 demo. This project is distributed under a modified\nBSD license.You can try Turing Drawings at the following URL:\nhttp://maximecb.github.io/Turing-Drawings/Below is a sample of the kinds of patterns Turing Drawings can generate:I've also written a blog post briefly describing this project.\n        Randomly generated Turing machines draw images and animations on a 2D canvas.\n      ",
    "summary": "**Web Developers Discover Art: Turing Drawings**\n\nIn a breathtaking fusion of computer science and kindergarten finger painting, GitHub hosts yet another project where Turing machines spit out \"art\" that resembles what you'd get by giving a squirrel a brush and some espresso. Visitors of the project's page are treated not just to these mesmerizing visuals but also to the profound pontifications of enthusiasts who delve deeply into philosophical questions about \"seeing dots\" \u2013 because clearly, that\u2019s the missing discourse in modern art. Meanwhile, commenters joust with the terrifying complexities of the Halting Problem as if beating it would suddenly make these sketches hang-worthy at the MoMA. \ud83c\udfa8\ud83d\udda5\ufe0f\ud83d\udca4"
  },
  {
    "title": "Show HN: I built an AI that turns GitHub codebases into easy tutorials (github.com/the-pocket)",
    "points": 603,
    "submitter": "zh2408",
    "submit_time": "2025-04-19T21:04:41 1745096681",
    "num_comments": 126,
    "comments_url": "https://news.ycombinator.com/item?id=43739456",
    "comments": [
      "This is actually really cool. I just tried it out using an AI studio API key and was pretty impressed. One issue I noticed was that the output was a little too much \"for dummies\". Spending paragraphs to explain what an API is through restaurant analogies is a little unnecessary. And then followed up with more paragraphs on what GraphQL is. Every chapter seems to suffer from this. The generated documentation seems more suited for a slightly technical PM moreso than a software engineer. This can probably be mitigated by refining the prompt.The prompt would also maybe be better if it encouraged variety in diagrams. For somethings, a flow chart would fit better than a sequence diagram (e.g., a durable state machine workflow written using AWS Step Functions).\n \nreply",
      "Answers like this are sort of what makes me wonder what most engineers are smoking when they think AI isn\u2019t valuable.I don\u2019t think the outright dismissal of AI is smart. (And, OP, I don\u2019t mean to imply that you are doing that. I mean this generally.)I also suspect people who level these criticisms have never really used a frontier LLM.Feeding in a whole codebase that I\u2019m familiar with, and hearing the LLM give good answers about its purpose and implementation from a completely cold read is very impressive.Even if the LLM never writes a line of code - this is still valuable, because helping humans understand software faster means you can help humans write software faster.\n \nreply",
      "Many devs still think their job is to write code not build products their business needs. I use LLMs extensively and it\u2019s helped me work better faster.\n \nreply",
      "> Spending paragraphs to explain what an API is through restaurant analogies is a little unnecessary. And then followed up with more paragraphs on what GraphQL is.It sounds like the tool (as it's currently set up) may not actually be that effective at writing tutorial-style content in particular. Tutorials [1] are usually heavily action-oriented and take you from a specific start point to a specific end point to help you get hands-on experience in some skill. Some technical writers argue that there should be no theory whatsoever in tutorials. However, it's probably easy to tweak the prompts to get more action-oriented content with less conceptual explanation (and exclamation marks).[1] https://diataxis.fr/tutorials/\n \nreply",
      "exactly it is. I'd rather impressive but at the same time the audience is always going to be engineers, so perhaps it can be curated to still be technical to a degree? I can't imagine a scenario where I have to explain to the VP my ETL pipeline\n \nreply",
      "From flow.pyEnsure the tone is welcoming and easy for a newcomer to understand{tone_note}.- Output only the Markdown content for this chapter.Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags)So just a change here might do the trick if you\u2019re interested.But I wonder how Gemini would manage different levels. \nFrom my take (mostly edtech and not in English) it\u2019s really hard to tone the answer properly and not just have a black and white (5 year old vs expert talk) answer.\nAnyone has advice on that?\n \nreply",
      "This has given me decent success:\"Write simple, rigorous statements, starting from first principles, and making sure to take things to their logical conclusion. Write in straightforward prose, no bullet points and summaries. Avoid truisms and overly high-level statements. (Optionally) Assume that the reader {now put your original prompt whatever you had e.g 5 yo}\"Sometimes I write a few more lines with the same meaning as above, and sometimes less, they all work more or less OK. Randomly I get better results sometimes with small tweaks but nothing to make a pattern out of -- a useless endeavour anyway since these models change in minute ways every release, and in neural nets the blast radius of a small change is huge.\n \nreply",
      "Thanks I\u2019ll try that!\n \nreply",
      "I had not used gemini before, so spent a fair bit of time yak shaving to get access to the right APIs and set up my Google project. (I have an OpenAPI key but it wasn't clear how to use that service.)I changed it to use this line:   api_key=os.getenv(\"GEMINI_API_KEY\", \"your-api_key\")\n\ninstead of the default project/location option.and I changed it to use a different model:    model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-preview-03-25\")\n\nI used the preview model because I got rate limited and the error message suggested it.I used this on a few projects from my employer:- https://github.com/prime-framework/prime-mvc a largish open source MVC java framework my company uses. I'm not overly familiar with this, though I've read a lot of code written in this framework.- https://github.com/FusionAuth/fusionauth-quickstart-ruby-on-... a smaller example application I reviewed and am quite familiar with.- https://github.com/fusionauth/fusionauth-jwt a JWT java library that I've used but not contributed to.Overall thoughts:Lots of exclamation points.Thorough overview, including of some things that were not application specific (rails routing).Great analogies. Seems to lean on them pretty heavily.Didn't see any inaccuracies in the tutorials I reviewed.Pretty amazing overall!\n \nreply",
      "If you want to see what output looks like (for smaller projects--the OP shared some for other, more popular projects), I posted a few of the tutorials to my GitHub:https://github.com/mooreds/prime-mvc-tutorialhttps://github.com/mooreds/railsquickstart-tutorialhttps://github.com/mooreds/fusionauth-jwt-tutorial/Other than renaming the index.md file to README.md and modifying it slightly, I made no changes.Edit: added note that there are examples in the original link.\n \nreply"
    ],
    "link": "https://github.com/The-Pocket/Tutorial-Codebase-Knowledge",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Turns Codebase into Easy Tutorial with AI\n      Ever stared at a new codebase written by others feeling completely lost? This tutorial shows you how to build an AI agent that analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works.\n\nThis is a tutorial project of Pocket Flow, a 100-line LLM framework. It crawls GitHub repositories and build a knowledge base from the code. It analyzes entire codebases to identify core abstractions and how they interact, and transforms complex code into beginner-friendly tutorials with clear visualizations.Check out the YouTube Development Tutorial for more!Check out the Substack Post Tutorial for more!\n\n\ud83e\udd2f All these tutorials are generated entirely by AI by crawling the GitHub repo!AutoGen Core - Build AI teams that talk, think, and solve problems toge",
    "summary": "Welcome to the latest \"disruption\" in tech education: an AI that meanders through GitHub repos like a lost tourist in a code jungle, finally spitting out elementary school-level tutorials. Hacker News commenters, proving they\u2019ve not interacted with real beginners for decades, are dazzled by this glorified Clippy rehash that explains API calls via burger orders. Perhaps next, it\u2019ll generate YouTube tutorial voiceovers in the style of Mr. Rogers to ensure even the bravest coders never touch raw code again. \ud83e\udd13 Remember, if your AI-generated documentation needs more emoji, you're obviously just not tweaking the prompts right! \ud83d\ude80"
  },
  {
    "title": "FurtherAI (YC W24) Is Hiring Software and AI Engineers (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-04-20T21:00:37 1745182837",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/furtherai/jobs",
    "first_paragraph": "AI Workforce for the Insurance IndustryAt FurtherAI, we are building a workforce of AI Teammates to automate insurance workflows. These AI Teammates can practically automate any insurance workflow involving processing unstructured documents, data entry into internal systems or web portals, or even making phone calls.Our core mission is to answer a critical question: Can AI be made as reliable, adaptive, and continuously learning as a new human employee?We have successfully raised a pre-seed round from top-tier investors, including Y Combinator, South Park Commons, and Converge VC. Our founding team features a second time founder and a language modeling scientist from Apple, who have known each other for over 12 years.\u00a9 2025 Y Combinator",
    "summary": "Title: FurtherAI (YC W24) Is Hiring Software and AI Engineers (ycombinator.com)\n\nIn a daring bid to replace the office intern, FurtherAI boasts its revolutionary AI Teammates, poised to automate the soul-crushing tedium of insurance paperwork. Because nothing screams \"innovation\" like teaching algorithms to navigate the labyrinthine horror of insurance claims. The dream team \u2013 a serial entrepreneur and an ex-Big Fruit techie \u2013 have been bro-friends for over a decade, guaranteeing absolutely **zero** groupthink. Hacker News commenters, straining under the weight of their own underemployment, engage in a spirited debate over the implications of AI on job security, while simultaneously updating their resumes just in case. \ud83d\ude43"
  },
  {
    "title": "Jagged AGI: o3, Gemini 2.5, and everything after (oneusefulthing.org)",
    "points": 158,
    "submitter": "ctoth",
    "submit_time": "2025-04-20T14:55:33 1745160933",
    "num_comments": 187,
    "comments_url": "https://news.ycombinator.com/item?id=43744173",
    "comments": [
      "The capabilities of AI post gpt3 have become extraordinary and clearly in many cases superhuman.However (as the article admits) there is still no general agreement of what AGI is, or how we (or even if we can) get there from here.What there is is a growing and often na\u00efve excitement that anticipates it as coming into view, and unfortunately that will be accompanied by the hype-merchants desperate to be first to \"call it\".This article seems reasonable in some ways but unfortunately falls into the latter category with its title and sloganeering.\"AGI\" in the title of any article should be seen as a cautionary flag. On HN - if anywhere - we need to be on the alert for this.\n \nreply",
      "I think a reasonable definition of intelligence is the application of reason on knowledge. An example of a system that is highly knowledgeable but has little to no reason would be an encyclopedia. An example of a system that is highly reasonable, but has little knowledge would be a calculator. Intelligent systems demonstrate both.Systems that have general intelligence are ones that are capable of applying reason to an unbounded domain of knowledge. Examples of such systems include: libraries, wikis, and forums like HN. These systems are not AGI, because the reasoning agents in each of these systems are organic (humans); they are more like a cyborg general intelligence.Artificial general intelligence are just systems that are fully artificial (ie: computer programs) that can apply reason to an unbounded domain of knowledge. We're here, and we have been for years. AGI sets no minimum as to how great the reasoning must be, but it's obvious to anyone who has used modern generative intelligence systems like LLMs that the technology can be used to reason about an unbounded domain of knowledge.If you don't want to take my word for it, maybe Peter Norvig can be more convincing: https://www.noemamag.com/artificial-general-intelligence-is-...\n \nreply",
      "I think the thing missing would be memory. The knowledge of current models is more or less static save for whatever you can cram into their context window. I think if they had memory and thus the ability to learn - \u201coh hey, I\u2019ve already tried to solve a bug in these ways maybe I won\u2019t get stuck in loop on them!\u201d Would be the agi push for me. Real time incorporating new knowledge into the model is the missing piece.\n \nreply",
      "With MCP/tool use you can tell it to save state into an MD file, simulating this. How much that counts is left as an exercise to the reader.\n \nreply",
      "It needs some kind of low latency world model. Or at least more agent inspired pretraining(I did x and it failed, I did y and It failed, I should try z now) GOOD(I did x and it failed, I did y and it failed, I should try x now) BAD\n \nreply",
      "Excellent article and analysis.  Surprised I missed it.It is very hard to argue with Norvig\u2019s arguments that AGI has been around since at least 2023.\n \nreply",
      "Until you can boot one up, give it access to a VM video and audio feeds and keyboard and mouse interfaces, give it an email and chat account, tell it where the company onboarding docs are and expect them to be a productive team member, they're not AGI. So long as we need special protocols like MCP and A2A, rather than expecting them to figure out how to collaborate like a human, they're not AGI.The first step, my guess, is going to be the ability to work through github issues like a human, identifying which issues have high value, asking clarifying questions, proposing reasonable alternatives, knowing when to open a PR, responding to code review, merging or abandoning when appropriate. But we're not even very close to that yet. There's some of it, but from what I've seen most instances where this has been successful are low level things like removing old feature flags.\n \nreply",
      "Just because we rely on vision to interface with computer software doesn't mean it's optimal for AI models. Having a specialized interface protocol is orthogonal to capability. Just like you could theoretically write code in a proportional font with notepad and run your tools through windows CMD - having an editor with syntax highlighting and monospaced font helps you read/navigate/edit, having tools/navigation/autocomplete etc. optimized for your flow makes you more productive and expands your capability, etc.If I forced you to use unnatural interfaces it would severely limit your capabilities as well because you'd have to dedicate more effort towards handling basic editing tasks. As someone who recently swapped to a split 36key keyboard with a new layout I can say this becomes immediately obvious when you try something like this. You take your typing/editing skills for granted - try switching your setup and see how your productivity/problem solving ability tanks in practice.\n \nreply",
      "Agreed, but I also think to be called AGI, they should be capable of working through human interfaces rather than needing to have special interfaces created for them to get around their lack of AGI.The catch in this though isn't the ability to use these interfaces. I expect that will be easy. The hard part will be, once these interfaces are learned, the scope and search space of what they will be able to do is infinitely larger. And moreover our expectations will change in how we expect an AGI to handle itself when our way of working with it becomes more human.Right now we're claiming nascent AGI, but really much of what we're asking these systems to do have been laid out for them. A limited set of protocols and interfaces, and a targeted set of tasks to which we normally apply these things. And moreover our expectations are as such. We don't converse with them as with a human. Their search space is much smaller. So while they appear AGI in specific tasks, I think it's because we're subconsciously grading them on a curve. The only way we have to interact with them prejudices us to have a very low bar.That said, I agree that video feed and mouse is a terrible protocol for AI. But that said, I wouldn't be surprised if that's what we end up settling on. Long term, it's just going to be easier for these bots to learn and adapt to use human interfaces than for us to maintain two sets of interfaces for things, except for specific bot-to-bot cases. It's horribly inefficient, but in my experience efficiency never comes out ahead with each new generation of UIs.\n \nreply",
      "You can't do that with most of the world's human population. Does that imply that most humans haven't reached AGI?\n \nreply"
    ],
    "link": "https://www.oneusefulthing.org/p/on-jagged-agi-o3-gemini-25-and-everything",
    "first_paragraph": "",
    "summary": "**Jagged AGI: o3, Gemini 2.5, and everything after**\n\nThe latest breathless article from <em>oneusefulthing.org</em> makes bold claims about the leaps and bounds past GPT-3, hinting that AGI is just around the hi-tech corner. Commenters, ever eager to one-up each other with niche references and overly specific examples, argue semantics like there's a trophy for \"Most Pedantic.\" One bright spark suggests that AGI has been here since libraries were invented because, apparently, organizing books demands the same nuanced understanding as rebooting a server. Meanwhile, another insists that until AI can handle their email spam like a seasoned office temp, it's all just smoke and mirrors. \ud83e\udd16\ud83d\udca8"
  },
  {
    "title": "The movie mistake mystery from \"Revenge of the Sith\" (fxrant.blogspot.com)",
    "points": 319,
    "submitter": "CharlesW",
    "submit_time": "2025-04-20T17:29:28 1745170168",
    "num_comments": 98,
    "comments_url": "https://news.ycombinator.com/item?id=43745141",
    "comments": [
      "> Painting out these movie mistakes as part of a restoration is wrong. What's in the movie is in the movie, and altering the movie to this extent is a form of revisionist history. Cinema is worse off when over-aggressive restorations alter the action within the frame. To me, this is equivalent to swapping out an actor's performance with a different take, or changing the music score during an action sequence, or replacing a puppet creature with a computer graphics version of the same creature decades after release.It's really not the equivalent though. I don't see anything wrong with fixing a license plate or removing a reflection or a modern-day wristwatch.It's the equivalent of fixing a spelling error in a novel, or a wrong chord in sheet music. None of the filmmakers wanted those things there. They weren't done with intent. They were just mistakes.Changing music or replacing a puppet with CG, of course I'm against. That's changing the art of it. Different music makes you feel different. A CG creature has a different personality. Just like you don't want to replace vocabulary in a novel to make it more modern-day.I think it's usually pretty easy to distinguish the two. The first ones would have been corrected at the time if they'd noticed and gone for another take. They take us out of the movie if we notice them. The latter category is a reflection of the technology, resources, and intentional choices. They keep us in the world of moviemaking as it was at that time.\n \nreply",
      ">> Painting out these movie mistakes as part of a restoration is wrong.> It's really not the equivalent though. I don't see anything wrong with fixing a license plate or removing a reflection or a modern-day wristwatch.I think it depends on the primary objective of the restoration. If I\u2019m trying to preserve history, I shouldn\u2019t fix errors. If I\u2019m trying to make a (by implication derivative) work that maximizes enjoyability for (new) audiences, then it\u2019s ok to fix.e.g. a long time ago, I once transferred vinyl recordings of an extremely amateur community musical group to CD.After thinking long and hard, I decided to fix recording technology flaws (a bad hum) and vinyl degradation flaws (crackles, dust, etc). But I didn\u2019t fix any of the musical performance flaws.Bottom line: I decided to respect the history of the performance, and disrespect the history of the recording and playback technology/medium.\n \nreply",
      "In 100 years (probably sooner), the vast majority of people won't be watching our films anymore. Those deep catalogs of IP have lower value with each passing year.Films are becoming less and less popular with new forms of entertainment that are more immediate, more democratized or individualistic. Our dopamine is being juiced and our attention getting sucked into games, social media, and all other kinds of long tail attractors. Influencers are bigger than Hollywood stars. They simply cater to more interests. Distribution and production are no longer hard problems, so you don't need to build up a Hollywood star.Film is becoming what radio used to be. It may never become as niche as the radio drama is today, but it certainly won't have the same limitless trajectory we thought it would have pre-pandemic.Whatever we do today to \"fix\" films or make them more accessible is accomplishing one thing: extending their lifespan for as long as most (average, non-film connoisseur) people might still be interested in watching.\n \nreply",
      "\"In 100 years (probably sooner), the vast majority of people won't be watching our films anymore.\"I quite strongly disagree with you.  I lived through the latter stages of the transition from monochrome to full colour and various other things that were hailed as game changers that would render the previous status quo as somehow defunct.I defy you to watch something like a Harold Lloyd movie involving a clock and not have sweaty palms or at least a mildly elevated ... emotional response of some sort.We call them films or movies or whatever but those are long form stories.  A book might be one too or a pdf.  The novella is a short story.  A matinee was an extended session at the cinema with multiple \"value adds\" to the main production.  Theatre ... cartoons ... you know how this goes!Might I remind you that you have only two eyes, which means that a radio drama in your car is the only safe media for a \"drama\" in a car, for the driver.  You do get aural distraction but it is mostly manageable.   One day you will have FSD (Mr Musk says so) and you will be able to watch telly with your feet on the dash but that is not today.Media and formats change but the purpose is largely the same: telling a story.  We are, after all, the story telling ape.\n \nreply",
      "It's not that older works don't have value, it is that a lot of people don't see the value. For example, changes in the way actors perform makes a lot of people claim that old movies are \"cheesy\" or have \"bad acting\" -- they can't even enjoy a movie from the 1940s, let alone a silent film like Harold Lloyd's. Hell, I know twenty-somethings that can't even stand movies from the 1980s!\n \nreply",
      "> I defy you to watch something like a Harold Lloyd movie involving a clock and not have sweaty palms or at least a mildly elevated ... emotional response of some sort.Be that as it may, there's probably a day coming where only a handful of people on the planet even know who that is. Or who have even seen those films. And it'll be like that for most of our now-popular cultural artifacts.How many newspaper stories from the 1700s have you read? The culture of those people died with them, and so too will it be with us.Nobody is going to grow up to the Mighty Morphin Power Rangers anymore. Nobody is going to watch The Andy Griffith Show or see Last Action Hero. Even if it happens on a rare occasion, those numbers will pale in comparison to the number of Fortnite players. Or whatever's popular in the coming decades.Our world is ephemeral and dies with us. We should enjoy our media while it is relevant to us, because that's what it's good for. Telling stories in a framework that speaks to us. In the future, it'll be a relic. An artifact of a time long ago, whose people are all dead, and whose lessons may need to come with a history book.Apart from students of anthropology, the vast majority of future people will probably find our cultural works to be boring, irrelevant, and unworthy of their attention.\n \nreply",
      "I also found this take interesting coming from someone at ILM where they grafted Hayden Christensen into Return of the Jedi.Though in this day and age I can\u2019t help but ask \u201cwhy not both?\u201d It feels easy to add a choice to your viewing experience. If they can do it for Black Mirror then they can certainly ask up front \u201cwhich version would you like to see?\u201d\n \nreply",
      "> I also found this take interesting coming from someone at ILM where they grafted Hayden Christensen into Return of the Jedi.Presumably the author would be opposed to that as well. Just because his employer did it doesn't mean he approves of it.\n \nreply",
      "Absolutely\n \nreply",
      "I literally just finished watching Episode IV, the one with the CGI makeover. The extra alien CGI in Mos Eisley is awful. It doesn\u2019t stand up at all, with the one exception of the Jaba scene which gets away with it because it is pretty fun. I wish we\u2019d watched the original version.\n \nreply"
    ],
    "link": "https://fxrant.blogspot.com/2025/04/the-movie-mistake-mystery-from-revenge.html",
    "first_paragraph": "A blogtacular blog filled with words, images, and whipped cream on top.  Written by Todd Vaziri.\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022 \u00a0\u2022\nPost a Comment\n",
    "summary": "Title: Chucklefest: The Mystique of a Missing Coffee Mug in Space Wars\n\nWelcome to another cerebral nosebleed about trivial slip-ups in movies that most people didn't notice in their wild, popcorn-fueled frenzy. Today's disaster dive? <em>\"Revenge of the Sith\"</em>, a film apparently so intricate that even its errors have ascended to the status of desert mirages. Commenters, in a display of grandeur known only to the highest echelons of nerddom, debate fiercely whether smudging out a misplaced soda can from 2005 echoes altering sacred texts. And yes, there's even a brave soul lamenting the passage of time as cinema bows to the might of TikTok-tier distractions. \ud83c\udf7f\ud83d\udcab"
  },
  {
    "title": "The Joy of Linux Theming in the Age of Bootable Containers (blues.win)",
    "points": 112,
    "submitter": "dopple",
    "submit_time": "2025-04-20T13:56:06 1745157366",
    "num_comments": 43,
    "comments_url": "https://news.ycombinator.com/item?id=43743784",
    "comments": [
      "I find LXC a bit more intuitive as testing platform than docker. Much of a sameness I suppose.Also discovered that for me it\u2019s less the OS or paradigm or theme/look and more that the windows manager is tiling type.\n \nreply",
      "Regular containers also happen to work great for testing dotfiles.Many years ago I added an install script to https://github.com/nickjj/dotfiles to get set up in basically 1 command because I wanted a quick way to bootstrap my own system. I used the official Debian and Ubuntu images to test things.Over the last few days I refactored things further to support Arch Linux which has an official Docker image too.This enables being able to do full end to end tests in about 5 minutes. The container spins up in 1 second, the rest is the script running its course. Since it's just a container you can also use volume mounts and leave the container running in case you want to incrementally test things without wiping the environment.Additionally it lets folks test it out without modifying their system in 1 command. Docker has enabled so many good things over the last 10+ years.\n \nreply",
      "No place like ${HOME} https://dotfiles.gbraad.nl ;-). I went further and generate images to easily spin up development environments, based on bootc vms or containers.Never stop tweaking. No computer can be called home until it runs your own set of aliases/commands.\n \nreply",
      "Just glancing through your dotfiles, I was wondering why you use VcXsrv.  WSLg has always been fine for me, and I've never heard of anyone trying to use a different X server.\n \nreply",
      "Thanks a lot for the reminder.I just pushed an update to remove VcXsrv at: https://github.com/nickjj/dotfiles/commit/fdc1ddd95c2defb791...As for why I was using it:I've been using WSL since nearly the beginning (2017 / 2018) and used VcXsrv back then to get bi-directional clipboard sharing before WSLg was available. For a brief time I even ran Sublime Text in WSL 1 way back in the day.Then I used WSL 2.Then I tried WSLg when it first came out and it was really bad. Clipboard sharing didn't work for me which was the only reason I wanted to use it. I set `guiApplications=false` and never looked back.I just tried it again now by closing VcXsrv and removing any DISPLAY related settings I had in my zsh profile. Then I shutdown WSL and started up my instance.Bingo, clipboard sharing \"just works\" and I also installed xcalc which ran flawlessly. This simplifies things so much.\n \nreply",
      "I really like the idea of immutable Linux and bootable containers. My next project will probably be switching to bazzite. But I took a look at the Containerfile[1], and I have some big concerns about the fragility of their supply chain. It uses 20 different copr repos (granted, half are their own), and I didn't count how many packages. Best I can tell, none of the versions are pinned. They do dump a diff of all package versions in the release notes[2], but I wonder if anyone actually reviews it before release. All it takes is one vulnerability in one repo / package and you can enjoy your new cryptominer.There's something nice about running Debian and having confidence in all the packages because they're built and maintained by the Debian team. Of course there are exceptions, but in my experience they're rare. The only non-standard repo I regularly use is fish shell, and the updates are so few and far between (and very public) I think the risk is low.I suppose this isn't strictly a container-specific problem; you could add the repos and install / update all those packages yourself too. But being able to package everything up into a single file that you can then boot into as your OS means you're also packing all the supply chain risk.Curious if anyone else shares my concern or if I should just put my tinfoil hat back on...1. https://github.com/ublue-os/bazzite/blob/main/Containerfile\n2. https://github.com/ublue-os/bazzite/releases/tag/42.20250417\n \nreply",
      "I agree with your concerns\u2014at least, last time I looked.I looked over their code, saw some things (I believed) I would do differently, and it was very easy to make my own personal spin to use.After doing that, maintaining it, and using it daily for the last year  I went back on some of my original choices. I feel much less critical of the decisions Jorge Castro made and it's probably time to compare and contribute if I can. Like, Homebrew on Linux ended up being way better than I expected. But some things I liked better my way. Say, including the signing keys for Chrome's 3rd-party repo statically  instead of fetching them over the network. (Writing this from my phone I don't exactly remember how they do/did it.)Overall, I'd recommend trying it yourself! It's been a ton of fun.\n \nreply",
      ">  It uses 20 different copr repos (granted, half are their own), and I didn't count how many packages. Best I can tell, none of the versions are pinned.Contributor here, we've been working on this diligently over the past cycle (the rest of the org is mostly done, Bazzite is largest so we're only getting to it now). We're hoping to be done over the summer with published SBOMs and all that good stuff.\n \nreply",
      "Nothing holds you from using bootable containers in the same way you use Debian and only use packages from the official Fedora repositories, starting from Fedora's bootc base images.\n \nreply",
      "Yeah I think that may be what I end up doing.\n \nreply"
    ],
    "link": "https://blues.win/posts/joy-of-linux-theming/",
    "first_paragraph": "Having spent a couple of decades in the Linux world, I have always had an interest in Linux desktop environments and how they are themed.\nI would often come across a post on /r/unixporn that inspired me to try to customize the look and feel of my desktop environment. So I would install Xfce, LXQt or Sway and try to recreate components that I like from other users or create my own. I would end up installing different kinds of panels, plugins, docks and launchers as well as random themes, fonts and sounds.A portion of this process would be documented, initially as random shell scripts in my home directory, before graduating to Ansible playbooks \u2013 with a brief detour into Nix that I will not elaborate on. Some of the customizations would live in my home directory, but there were often system-wide modifications to /usr required.Eventually, the constant churn and randomly broken desktop components such as a panel that mysteriously vanished or a non-functional dock led me to stick with the s",
    "summary": "Welcome to another episode of \"How Much Configuration Can One Human Stand?\" brought to you by the Linux Theming Enthusiast Society. In today's saga \ud83c\udfad, we dive into the world where bootable containers meet desperate customization attempts. Read as seasoned hobbyists transform their desktops into unrecognizable tech pilaf, documenting their journey from \"I might need that script again\" to \"Let's orchestrate chaos with Ansible.\" Join the *<em>discussion</em>* below, where everyone pretends they understand the difference between LXC and Docker while sharing their highly personalized dotfiles as if they're holy texts. \ud83d\udcdc\ud83d\udcbb Can't wait for someone to accidentally rm -rf / their simulation of stability!"
  },
  {
    "title": "Home galleries are hiding in plain sight across Canada (cbc.ca)",
    "points": 60,
    "submitter": "SirLJ",
    "submit_time": "2025-04-20T16:04:46 1745165086",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=43744636",
    "comments": [
      "The article vaguely alludes to why this trend could appear but unfortunate it couldn't devote at least a paragraph to it. It's such an important issue, but given that this the industry impacted is considered small and niche it's so under discussed.Decades of political opposition toward any and all redevelopment of existing low density single family dominated residentially zoned areas has meant that practically all creation of new housing in the major cities of Canada has meant greenfield sprawl or for urban areas, creeping into brownfield redevelopment, rezoning old industrial areas into new condo developments.The problem with this is that the arts and gallery system has long relied on repurposing old and affordable industrial space into arts production space gallery and performance space. So what we've been seeing as the housing crisis has become more severe, is an increasing amount of destruction and rezoning of irreplaceable industrial land, aiding a shortage of industrial space, badly wanted by the Amazon's of the world too.So artists are being squeezed on both ends. The shortage of affordable housing is especially severe for low income working artists, and the political solution for solving this problem is to destroy the artist spaces which makes things more expensive for artists too.This could all be better fixed if we simply left industrial as industrial and actually allowed people to more intensively develop residential homes to meet our housing goals, and add more arts uses into residential areas (because let's be clear, everything mentioned in this article is likely on the down low, breaking municipal bylaws and Provincial liquor laws), but people have been incredibly resistant to this, no matter how much they claim to love the arts etc etc.\n \nreply",
      "Seems like another symptom of the age demographic imbalance. Old people have taken over the political power and have seized all the land exclusively for themselves.\n \nreply",
      "great analysis! thank you\n \nreply",
      "> The shortage of affordable housing is especially severe for low income working artistsOnce again, there is NO SHORTAGE of affordable housing either in the US or in Canada.None. Nada. Zilch. \u041d\u043e\u043b\u044c. \u96f6And that's important. A simple \"not enough housing\" problem is easily solved with \"just build more\".Instead, there is a shortage of housing _near_ _large_ _cities_. And it can't be solved. Simply \"building more\" housing in dense cities makes it _worse_.\n \nreply",
      "The issue with this is most parts of large cities are substantially less dense than incredibly livible neighborhoods such as the plateau area of Montreal.It is illegal to build such a neighborhood in 99% of Canada. People love it here, people start families here, tourists visit, it's quite, lots of parks and shops.And it's 3-4 as dense as most areas of most major cities. But we've made it illegal to build. For zoning, double stairway rules, minimum parking rules, setback rules, strict permitting requirements, and thousands of other things.\n \nreply",
      "> It is illegal to build such a neighborhood in 99% of Canada.And that is good. It's making Canada liveable and keeps the prices from skyrocketing EVEN HIGHER.\n \nreply",
      "Canada's housing crisis goes well beyond just the large cities. It extends into small towns as far as the Yukon. It may be a somewhat different situation compared with the US.\n \nreply",
      "Smaller town in Canada don't really have skyrocketing prices.For example, in Whitehorse in Yukon the average house was $420k (or $550k inflation adjusted to 2024) in 2015, and $660k in 2024. So less than 20% growth after inflation within the last decade.During that time, Vancouver BC went from $640k ($820k after inflation) to $1300k.The average square footage also went down in BC, but stayed stable in YK.\n \nreply",
      "Isn't 80% or some other ridiculous percentage of population of Canada in large cities? If a large portion of your population is living in large cities and large cities are experiencing a housing shortage then it makes sense to me to say there is a housing shortage in Canada.\n \nreply",
      "It's important because there's no way to make dense urban housing cheaper. Nobody has managed to lower down housing prices by increasing density (no, Austin in Texas doesn't count, guess why?).The solution is not to build ever denser communities, but to make it so that people don't _have_ to move into a large city from an ever-shrinking list.\n \nreply"
    ],
    "link": "https://www.cbc.ca/arts/home-galleries-are-hiding-in-plain-sight-across-canada-1.7503886",
    "first_paragraph": "",
    "summary": "**Who Knew Homes Could Be Galleries, Eh?**\n\nIn a groundbreaking discovery by CBC, it turns out people in Canada are using their homes as *gasp* galleries! Who would\u2019ve thunk a wall could hold art? Amidst the tidal waves of insightful comments, we see the realms of urban planning and artsy spaces collide like never before. Cue the heartfelt cries for the vanishing district spaces, suffocated by those ruthless condominiums, all while commenters sling jargon like confetti at a parade. Here's to solving the housing crisis \u2013 just throw some art on it and call it a day, right? \ud83c\udfa8\ud83c\udfe0"
  },
  {
    "title": "Tariffs are pure psychology for the president, fused into his brain (axios.com)",
    "points": 5,
    "submitter": "cwwc",
    "submit_time": "2025-04-21T01:09:51 1745197791",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.axios.com/2025/04/19/inside-trump-mindset-tariffs",
    "first_paragraph": "",
    "summary": "In a groundbreaking expos\u00e9 that reveals the astonishing secret that politicians are influenced by their own beliefs, Axios takes a deep dive into the uniquely human trait of having a functioning brain to discover that tariffs, surprisingly, involve psychology. Shocked readers, apparently previously under the impression that international trade policies were determined by a sophisticated algorithm involving the migration patterns of migratory birds, express their newfound understanding by arguing loudly in the comments. Each commenter, armed with a PhD from the University of I-Know-Better-than-You, competes for the coveted title of <em>Least Understandable Take</em>. Who knew that decisions could be subjective? In a world thirsting for complexity, thank goodness we have such nuanced analysis. \ud83e\udd2f"
  }
]