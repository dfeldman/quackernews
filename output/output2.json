[
  {
    "title": "DARPA wants to bypass the thermal middleman in nuclear power systems (ans.org)",
    "points": 103,
    "submitter": "bilsbie",
    "submit_time": "2024-08-09T21:12:34",
    "num_comments": 63,
    "comments_url": "https://news.ycombinator.com/item?id=41205439",
    "comments": [
      "Nuclear batteries with beta emitters driving some kind of semiconductor have been around for a while, but they're very low power.[1]\"... Betavolt's team of scientists developed a unique single-crystal diamond semiconductor that is only 10 microns thick, placing a 2-micron-thick nickel-63 sheet between two diamond semiconductor converters to convert the decay energy of the radioactive source into electric current to form an independent unit.\"\"... 100 microwatts, a voltage of 3V, and a volume of 15 X 15 X 5 cubic millimeters ...\"3-4 orders below the power requirements for a phone. An AirTag-type intermittent device, though...So, can those be scaled up? Are all those little beta-emitters in coin cell form factor going to be a problem?  Nickel-63 has a half life of 100 years, so they'll be active for a while. Not dangerous unless broken up and ingested, but need to be kept out of the food chain.[1] https://www-betavolt-tech.translate.goog/359485-359485_64506...\n \nreply",
      "Not dangerous unless broken up and\n    ingested, but need to be kept out\n    of the food chain.\n\nNo worse than a NiCad in that respect. Probably better, if anything, since it's so much easier to detect and track.\n \nreply",
      "Without going into too many specifics, for a period during my career I was involved with an organization whose responsibility it is to track nuclear materials and keep them under safe surveillance\u2014in fact my job had the words 'surveillance engineer' in its title.I say that only bring to your attention how difficult this would be in practice. Putting safety aside for a moment, in most countries the regulatory restrictions are enormous because they are signatories to the NPT\u2014Treaty on the Non-Proliferation of Nuclear Weapons which tightly bind them to how they use and handle  nuclear materials. This involves, use, tracking, short and long-term disposal thereof not to mention how to keep radioactive materials away from bad actors/those who've ill intent.With safety, there are so many issues involved that I can hardly even mention them here. Just as an illustration, the once lack of regulations covering the manufacture and use of luminous radium paint turned out to be a disaster.I've thought about this a great deal, whenever the batteries in my flashlight die I wish I had some nuclear powered ones and evey time I'm brought back to reality when I think how difficult it would be to implement in practice.\n \nreply",
      "I meant precisely what I said. Tracking NiCads to the standards of the NPT would presumably be impossible. If your standard is \"keep it out of the food supply\", though, I guarantee that there's a little bit of NiCad in that hot dog in your fridge, and there'd be much less NiCad in your hot dog if they tripped a radiation detector before they hit the incinerator or the landfill.\n \nreply",
      "I've always said that I'm more afraid of heavy metal contamination then nuclear material contamination.There are no hand held devices which will tell you if the soil you're vaguely near has heavy metals in it, nor how much.\n \nreply",
      "There are no hand held devices which will tell you if the soil you're vaguely near has heavy metals in it, nor how much.Well, technically, there are, but now you're back in the ionizing-radiation business: https://www.youtube.com/watch?v=KdfHVcU8U7U\n \nreply",
      "And set off all the NBC detectors on the highways near major cities.\n \nreply",
      "Well yes but only if destroyed. Then wouldn\u2019t you call that a feature? When in their enclosures there should be no radiation.\n \nreply",
      "No way could all the alerts be tracked.\n \nreply",
      "Yeah, you bring up another angle to look at it from: even if the alerts didn't go off when the batteries are being used/stored properly, it would make it easy to create a lot of noise/false positives as part of an actual attack (were those batteries to be readily available).\n \nreply"
    ],
    "link": "https://www.ans.org/news/article-6276/darpa-wants-to-bypass-the-thermal-middleman-in-nuclear-power-systems/",
    "first_paragraph": "A message from Curtiss-WrightA Clean Energy Powerhouse: The Digital I&C Systems Modernizing NuclearLearn MoreNuclear power already has an energy density advantage over other sources of thermal electricity generation. But what if nuclear generation didn\u2019t require a steam turbine? What if the radiation from a reactor was less a problem to be managed and more a source of energy? And what if an energy conversion technology could scale to fit nuclear power systems ranging from miniature batteries to the grid? The Defense Advanced Research Projects Agency (DARPA) Defense Sciences Office (DSO) is asking these types of questions in a request for information on High Power Direct Energy Conversion from Nuclear Power Systems, released August 1.DARPA is interested in ideas to take alpha, beta, gamma, and neutron radiation from \u201cany type of reactor,\u201d including fission and fusion, and from nuclear processes and radioisotope decay, and directly convert those radioactive emissions to electricity to me",
    "summary": "<b>DARPA's Newest Brainwave: Nuclear Energy But With Lasers Instead of Steam</b> \ud83d\ude80\ud83d\udd0b DARPA, the governmental agency famed for throwing darts at a board to decide on their next research escapade, is now fantasizing about bypassing the steam turbine in nuclear reactors. Instead of following tried-and-true engineering practices, why not extract power directly from relentless radiation, because clearly, managing high-energy particles is child's play! Meanwhile, in the comments section, an assortment of armchair physicists is eagerly theorizing about scaling nickel-63 into the next iPhone battery, blissfully undisturbed by trifling concerns like practical application or radiation safety. \ud83e\udd37\u200d\u2642\ufe0f\ud83d\udca5 Could this be the critical mismatch between superhero science and it's the real-world application? Only time, and perhaps a few minor nuclear incidents, will tell."
  },
  {
    "title": "Urchin Software Corp: The unlikely origin story of Google Analytics, 1996\u20132005 (2016) (urchin.biz)",
    "points": 78,
    "submitter": "cpeterso",
    "submit_time": "2024-08-09T20:32:11",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=41205176",
    "comments": [
      "> The UTM, or Urchin Traffic Monitor, was an early method for augmenting Apache (or IIS, etc.) log files with cookies, such that unique visitors could be established.Of course, \u201cutm\u201d lives on today as a standard prefix for link tracking parameters, even outside of google analytics\n \nreply",
      "I'm very surprised that they signed up whole ISPs and got away with it.I wasn't able to process the daily logs of the tracking pixel of the site I was operating on Urchin 5 within 24 hours. I had a beefy server with a 16 drive raid array but the log processing never used multiple cores.By that time Google had aquired them, support didn't exist because they wanted you to switch to Google Analytics and it was basically 30k down the drain.\n \nreply",
      "I can't really explain how cool Urchin was. I think it was one of the first truly analytical software I used \u2014 not just from a web traffic point of view, but it was a site that used and presented data. The graphs, the maps, even the two-column UI was stuff a lot of web developers copied and riffed on back then. It really opened my mind up to a lot of things: design, software as a service (and installable software), even acquisitions in general. Strangely had a large impact on my future career.\n \nreply",
      "That picture of their first computer, a sparcstation, brought back memories.Our first computer at Cygnus was a sparcstation that was actually a \u201cprototype\u201d (probably PVT since it had all the housing etc) for the first sparcstation.  I think Andy B had given it to John.  It worked fine and lasted for years.When the author said they\u2019d started the company on a 10K investment I knew they were my kind of people.\n \nreply",
      "I loved this retell. Brings back the old days, especially the Google and Yahoo parties of the late 2000's. Those were crazy. They literally carried one of my employees out of a basement rave who had passed out from drinking too much.\n \nreply",
      "> Our first tradeshow ever, circa 1997.Pardon my one nitpick: the computer in the photo (PowerMac1,1) was introduced 1999-01-05.\n \nreply",
      "What about defunct startup shirts? Like, I\u2019d wear that Urchin shirt.\n \nreply",
      "That sounds like a potential business idea even. Although realistically I guess eBay and Craigslist already has that covered :p\n \nreply",
      "(2016)(In case anyone else is surprised by the line in the article that the \u201c10th anniversary of the acquisition has recently passed.\u201d)\n \nreply",
      "Worked at a hosting company in those years.. urchin was the GOAT. we ran it for every customer, and relied on it heavily for billing our customers. It was crazy fast and elegant. Nothing came close\n \nreply"
    ],
    "link": "https://urchin.biz/urchin-software-corp-89a1f5292999",
    "first_paragraph": "",
    "summary": "Welcome to the riveting world of <em>Urchin Software Corp</em>, where nostalgic tech bros reminisce over primitive analytics tools like they're discussing the lost city of Atlantis. A commenter waxes poetic about <i>\"real\" support</i> during the good old days, crying into the void about how Google's acquisition of Urchin was the \"30k down the drain\" event of their life \ud83c\udfbb. Meanwhile, another is absolutely floored that people used single-core servers to process mountains of data and lived to tell the tale. God forbid they ever encounter an actual floppy disk.  Who needs innovation and progress when you've got old startup T-shirts to remember the 'epic' Yahoo! parties by? Buckle up, it\u2019s a bumpy ride down memory lane! \ud83d\ude80"
  },
  {
    "title": "Show HN: LLM-aided OCR \u2013 Correcting Tesseract OCR errors with LLMs (github.com/dicklesworthstone)",
    "points": 290,
    "submitter": "eigenvalue",
    "submit_time": "2024-08-09T16:28:39",
    "num_comments": 126,
    "comments_url": "https://news.ycombinator.com/item?id=41203306",
    "comments": [
      "Fantastic work is emerging in this field, and with the new release of the schnell model of the flux series we will have the downstream captioning datasets we need to produce a new SOTA vision model, which has been the last straggler in the various open llm augmentations. Most vision models are still based on ancient CLIP/BLIP captioning and even with something like LLAVA or the remarkable phi-llava, we are still held back by the pretained vision components which have been needing love for some months now.Tessy and LLM is a good pipe, it's likely what produced SCHNELL and will soon be the reverse of this configuration, used for testing and checking while the LLM does the bulk of transcription via vision modality adaption. The fun part of that is that multi lingual models will be able to read and translate, opening up new work for scholars searching through digitized works. Already I have had success in this area with no development at all, after we get our next SOTA vision models I am expecting a massive jump in quality. I expect english vision model adapters to show up using LLAVA architecture first, this may put some other latin script languages into the readable category depending on the adapted model, but we could see a leapfrog of scripts becoming readable all at once. LLAVA-PHI3 already seems to be able to transcribe tiny pieces of hebrew with relative consistency. It also has horrible hallucinations, so there is very much an unknown limiting factor here currently. I was planning some segmentation experiments but schnell knocked that out of my hands like a bar of soap in a prison shower, I will be waiting for a distilled captioning sota to come before I re-evaluate this area.Exciting times!\n \nreply",
      "Is LLaVA-Phi better than Phi Vision?edit: I think parent just doesn't know about Phi Vision, it appears to be a better model\n \nreply",
      "In my experience, this works well but doesn't scale to all kinds of documents.\nFor scientific papers; it can't render formulas. meta's nougat is the best model to do that.\nFor invoices and records; donut works better.\nBoth these models will fail in some cases so you end up running LLM to fix the issues.\nEven with that LLM won't be able to do tables and charts justice, as the details were lost during OCR process (bold/italic/other nuances). I feel these might also be \"classical\" methods.\nI have found vision models to be much better as they have the original document/image. Having prompts which are clear helps but still you won't get 100% results as they tend to venture off on their paths.\nI believe that can be fixed using fine tuning but no good vision model provides fine tuning for images.\nGoogle Gemini seems to have the feature but I haven't tried it.\nFew shots prompting helps keep the LLM from hallucinating, prompt injection and helps adhering to the format requested.\n \nreply",
      "Maybe a pipeline like:1. Segment document: Identify which part of the document is text, what is an image, what is a formula, what is a table, etc...2. For text, do OCR + LLM. You can use LLMs to calculate the expectation of the predicted text, and if it is super off, try using ViT or something to OCR.3. For tables, you can get a ViT/CNN to identify the cells to recover positional information, and then OCR + LLM for recovering the contents of cells4. For formulas (and formulas in tables), just use a ViT/CNN.5. For images, you can get a captioning ViT/CNN to caption the photo, if that's desired.\n \nreply",
      "I don't see how you make LLM improve tables where most of the time table is single word or single value that doesn't have continuous context like a sentence.\n \nreply",
      "IMHO, the LLM correction is most relevant/useful in the edge cases rather than the modal ones, so I totally agree.\n \nreply",
      "I agree that vision models that actually have access to the image are a more sound approach than using OCR and trying to fix it up. It may be more expensive though, and depending on what you're trying to do it may be good enough.What I want to do is reading handwritten documents from the 18th century, and I feel like the multistep approach hits a hard ceiling there. Transkribus is multistep, but the line detecion model is just terrible. Things that should be easy, such as printed schemas, utterly confuse it. You simply need to be smart about context to a much higher degree than you need in OCR of typewriter-written text.\n \nreply",
      "I also think it\u2019s probably more effective. Every time hand-crafted tools are better than AI but then the model becomes bigger and AI wins. Think hand crafted image classification to full model or hand crafted language translation to full model.In this case, the model can already do the OCR and becomes an order of magnitude cheaper per year.\n \nreply",
      "We've been trying to solve this with https://vlm.run: the idea is to combine the character level accuracy of an OCR pipeline (like Tesseract) with the flexibility of a VLM. OCR pipelines struggle with non-trivial text layouts and don't have any notion of document structure, which means there needs to be another layer on top to actually extract text content to the right place. At the other end of the spectrum, VLMs (like GPT4o) tend to perform poorly on things like dense tables (either hallucinating or giving up entirely) and complex forms, in addition to being much slower/more expensive. Part of the fix is to allow a 'manager' VLM to dispatch to OCR on dense, simple documents, while running charts, graphs etc. through the more expensive VLM pipeline.\n \nreply",
      "It's not OSS, but I've had good experiences with using MathPix's API for OCR for formulas\n \nreply"
    ],
    "link": "https://github.com/Dicklesworthstone/llm_aided_ocr",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Enhance Tesseract OCR output for scanned PDFs by applying Large Language Model (LLM) corrections.\n      The LLM-Aided OCR Project is an advanced system designed to significantly enhance the quality of Optical Character Recognition (OCR) output. By leveraging cutting-edge natural language processing techniques and large language models (LLMs), this project transforms raw OCR text into highly accurate, well-formatted, and readable documents.To see what the LLM-Aided OCR Project can do, check out these example outputs:PDF to Image ConversionOCR ProcessingChunk CreationError Correction and FormattingDuplicate Content RemovalHeader and Page Number Suppression (Optional)Flexible LLM SupportLocal LLM HandlingAPI-based LLM HandlingAsynchronous ProcessingToken EstimationDynamic Token AdjustmentThe project uses a .env file for easy configurat",
    "summary": "**HN throws a house party for Optical Character Recognition and invites every LLM in town.** A groundbreaking GitHub project promises to fix the eyesight of Tesseract with the brainpower of Large Language Models, because apparently, it takes a village to read a PDF correctly these days. Commenters, in a feverish display of acronyms and model-name-dropping, juggle terms like SCHNELL, LLAVA, and donuts \ud83c\udf69, revealing their unbridled optimism that soon, software will read all of humanity's paperwork so we don't have to. Meanwhile, real problems like tables, formulas, and ancient handwriting remain as undecipherable as an alien radio signal, leaving users to debate whether we actually need software to be smart or just less blurry."
  },
  {
    "title": "Show HN: Attaching to a virtual GPU over TCP (thundercompute.com)",
    "points": 155,
    "submitter": "bmodel",
    "submit_time": "2024-08-09T16:50:41",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=41203475",
    "comments": [
      "Ah this is quite interesting! I had a usecase where I needed a GPU-over-IP but only for transcoding videos. I had a not-so-powerful AMD GPU in my homelab server that somehow kept crashing the kernel any time I tried to encode videos with it and also an NVIDIA RTX 3080 in a gaming machine.So I wrote https://github.com/steelbrain/ffmpeg-over-ip and had the server running in the windows machine and the client in the media server (could be plex, emby, jellyfin etc) and it worked flawlessly.\n \nreply",
      "Have you done a Show HN yet? If not, please consider doing so!https://gist.github.com/tzmartin/88abb7ef63e41e27c2ec9a5ce5d...https://news.ycombinator.com/showhn.htmlhttps://news.ycombinator.com/item?id=22336638\n \nreply",
      "This is more or less what I was hoping for when I saw the submission title. Was disappointed to see that the submission wasn't actually a useful generic tool but instead a paid cloud service. Of course the real content is in the comments.As an aside, are there any uses for GPU-over-network other than video encoding? The increased latency seems like it would prohibit anything machine learning related or graphics intensive.\n \nreply",
      "There is a GPU-over-network software called Juice [1]. I've used it on AWS for running CPU-intensive workloads that also happen to need some GPU without needing to use a huge GPU instance. I was able to use a small GPU instance, which had just 4 CPU cores, and stream its GPU to one with 128 CPU cores.I found Juice to work decently for graphical applications too (e.g., games, CAD software). Latency was about what you'd expect for video encode + decode + network: 5-20ms on a LAN if I recall correctly.[1] - https://github.com/Juice-Labs/Juice-Labs\n \nreply",
      "Some computation tasks can tolerate the latency if they\u2019re written with enough overlap and can keep enough of the data resident, but they usually need more performant networking than this. See older efforts like rcuda for remote cuda over infiniband as an example. It\u2019s not ideal, but sometimes worth it. Usually the win is in taking a multi-GPU app and giving it 16 or 32 of them rather than a single remote GPU though.\n \nreply",
      "Interesting. Do you know if your tool supports conversions resulting in multiple files, such as HLS and its myriad of timeslice files?\n \nreply",
      "I'm confused, if this operates at the CPU/GPU boundary doesn't it create a massive I/O bottleneck for any dataset that doesn't fit into VRAM? I'm probably misunderstanding how it works but if it intercepts GPU i/o then it must stream your entire dataset on every epoch to a remote machine, which sounds wasteful, probably I'm not getting this right.\n \nreply",
      "That understanding of the system is correct. To make it practical we've implemented a bunch of optimizations to minimize I/O cost. You can see how it performs on inference with BERT here: https://youtu.be/qsOBFQZtsFM?t=69.The overheads are larger for training compared to inference, and we are implementing more optimizations to approach native performance.\n \nreply",
      "> to approach native performance.The same way one \"approaches the sun\" when they take the stairs?\n \nreply",
      "Aah ok thanks, that was my basic misunderstanding, my mind just jumped straight to my current training needs but for inference it makes a lot of sense. Thanks for the clarification.\n \nreply"
    ],
    "link": "https://www.thundercompute.com/",
    "first_paragraph": "Scale your usage up or down instantly, without limits, while only being billed for what you useSwitch GPUs instantly with a single command without leaving your instanceRun your existing code on Thunder Compute without changes or configSave money by developing on your CPU. When you want to scale, access a cluster of GPUs on-demand.Switch to the GPUs you need, when you need themNever worry about config, quotas, or reservations againWith Thunder Compute, never pay for idle GPUs. Give developers direct access to GPUs, with the freedom to scale quickly.GPUs at other cloud providers are utilized on-average 15% of the time, while 85% of what you pay goes to waste. With long-term reservations you have to guess how many GPUs you will need, resulting in shortages and overpayment.A cluster of high-performance GPUs at your fingertips without ever having to talk to ITShrink your cloud budget by eliminating idle GPU timeUse the Thunder Compute CLI to run your existing GPU code without any setupBehin",
    "summary": "**n-gate.com HN Digest: Virtual GPU Black Magic and TCP Pixie Dust**\n\nIn the latest attempt to make buzzwords pay dividends, Thunder Compute boldly promises the alchemy of scaling GPU usage without the pesky need for corporeal hardware. Commenters, accomplished wizards in their own home labs, dive deep into the spellbook of networked pixels and inadvertently confirm that, just maybe, they didn't quite grasp the incantations\u2014or, hilariously, the pricing models. One brave soul questioned the utility of washing your GPU cycles down the latency drain, hinting that their \"disappointment\" was less about the technology and more about missing out on another cool toy. Meanwhile, another wonders aloud whether their ill-fated quest for encoding bliss might be solved, only to be educated on the magical realm of I/O bottlenecks, where dreams go to buffer eternally. <em>\"Approaching native performance,\"</em> they claim, as skeptically as one approaches a fire-breathing dragon with a leaky water pistol. \ud83d\udc09\ud83d\udca7\ud83d\udcbb"
  },
  {
    "title": "Grace Hopper, Nvidia's Halfway APU (chipsandcheese.com)",
    "points": 18,
    "submitter": "PaulHoule",
    "submit_time": "2024-08-09T22:52:13",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://chipsandcheese.com/2024/07/31/grace-hopper-nvidias-halfway-apu/",
    "first_paragraph": "",
    "summary": "Nvidia decides to half-ass a new APU named after Grace Hopper, because associating half-baked tech with historical figures is how we honor legacies now. Over at ChipsAndCheese.com, enthusiasts debate whether a glorified graphics calculator can indeed resurrect Ada Lovelace to fix the driver issues. Meanwhile, the comment section becomes a tragic battleground where confused would-be tech gurus throw around terms like \"tensor cores\" and \"proprietary spaghetti\" \u2013 blissfully unaware that their Fortnite rigs won\u2019t run any faster. \ud83d\ude02"
  },
  {
    "title": "GCVR (YC W22) Is Hiring Lead Animation Engineer (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-10T01:01:02",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/gym-class-by-irl-studios/jobs/7UKmLED-lead-animation-engineer",
    "first_paragraph": "Play basketball in virtual realityGym Class is the best rated experience on Meta Quest, and #1 most played sports game in VR. Millions have downloaded GC to step on the VR basketball court (+ now football, baseball, and more) and make new friends, compete in simulation gameplay, and even break a sweat. Join us on our mission to connect the world by simulating it!WHAT YOU'LL DOAs Lead Animation Engineer, you'll drive our animation gameplay and systems roadmap, and balance shipping product with applied research at the intersection of character motion AI, spatial computing, and social gaming. Your work is pivotal towards facilitating infinite, personalized social experiences for the Gym Class community. Your scope will first focus on:In this role, you'll collaborate with our co-founders, gameplay & ai engineers, artists, and designers. You'll ship great work directly to a large and eager community, while also building the foundations for a fast-moving Animation Engineering squad.QUALIFICA",
    "summary": "<b>Wanted: Lead Animation Engineer to Perfect Pretend Sports</b>\n\nIn an act of heartwarming technological redundancy, GCVR (<em>Y Combinator's</em> pride and joy) is on the hunt for a Lead Animation Engineer to simulate sports no one actually needs to sweat for. If reimagining the exhausting act of physical sports into button presses and headset adjustments sounds like your dream job, congratulations on finding your tribe! Commenters are evenly split between lamenting their lost youth sprawled across real basketball courts and breathlessly speculating just how real unreal football can feel. Join GCVR to create games that promise to connect the world, one virtual dunk at a time. \u2728\ud83c\udfc0"
  },
  {
    "title": ".INTERNAL is now reserved for private-use applications (icann.org)",
    "points": 159,
    "submitter": "joncfoo",
    "submit_time": "2024-08-09T16:36:50",
    "num_comments": 83,
    "comments_url": "https://news.ycombinator.com/item?id=41203368",
    "comments": [
      "My biggest frustration with .internal is that it requires a private certificate authority. Lots of organizations struggle to fully set up trust for the private CA on all internal systems. When you add BYOD or contractor systems, it's a mess.Using a publicly valid domain offers a number of benefits, like being able to use a free public CA like Lets Encrypt. Every machine will trust your internal certificates out of the box, so there is minimal toil.Last year I built getlocalcert [1] as a free way to automate this approach. It allows you to register a subdomain, publish TXT records for ACME DNS certificate validation, and use your own internal DNS server for all private use.[1] https://www.getlocalcert.net/\n \nreply",
      "Do you mean to say that your biggest frustration with HTTPS on .internal is that it requires a private certificate authority? Because I'm running plain HTTP to .internal sites and it works fine.\n \nreply",
      "There's some every packet shall be encrypted, even in minimal private VPCs lore going on. I'm blaming PCI-DSS.\n \nreply",
      "> Lots of organizations struggle to fully set up trust for the private CA on all internal systems.Made worse by the fact phone OSes have made it very difficult to install CAs.\n \nreply",
      "And in on some platforms and configurations, impossible.Same with the .dev domain\n \nreply",
      ".dev isn\u2019t a TLD for internal use though, do you have the same problem when you use .test?\n \nreply",
      "Oh neat, thanks for sharing this idea\n \nreply",
      "I'm pretty sure that if letsencrypt localhost certs work, they'll work fine with .internal too?\n \nreply",
      "let\u2019s encrypt does not support certain for localhost.\n \nreply",
      "Are there any good reasons to use a TLD like .internal for private-use applications, rather than just a regular gTLD like .com?It's nice that this is available, but if I was building a new system today that was internal, I'd use a regular domain name as the root. There are a number of reasons, and one of them is that it's incredibly nice to have the flexibility to make a name visible on the Internet, even if it is completely private and internal.You might want private names to be reachable that way if you're following a zero-trust security model, for example; and even if you aren't, it's helpful to have that flexibility in the future. It's undesirable for changes like these to require re-naming a system.Using names that can't be resolved from the Internet feels like all downside. I think I'd be skeptical even if I was pretty sure that a given system would not ever need to be resolved from the Internet. [Edit:] Instead, you can use a domain name that you own publicly, like `example.com`, but only ever publish records for the domain on your private network, while retaining the option to publish them publicly later.When I was leading Amazon's strategy for cloud-native AWS usage internally, we decided on an approach for DNS that used a .com domain as the root of everything for this reason, even for services that are only reachable from private networks. These services also employed regular public TLS certificates too (by default), for simplicity's sake. If a service needs to be reachable from a new network, or from the Internet, then it doesn't require any changes to naming or certificates, nor any messing about with CA certs on the client side. The security team was forward-thinking and was comfortable with this, though it does have tradeoffs, namely that the presence of names in CT logs can reveal information.\n \nreply"
    ],
    "link": "https://www.icann.org/en/board-activities-and-meetings/materials/approved-resolutions-special-meeting-of-the-icann-board-29-07-2024-en#section2.a",
    "first_paragraph": "Board Activities and MeetingsWhereas, on 1 March 2019, the Board took action on each of the 35 recommendations issued within the Competition, Consumer Trust, and Consumer Choice (CCT) Review Team Final Report dated 8 September 2018, as specified within the scorecard titled \"Final CCT Recommendations: Board Action (1 March 2019)\". The Board resolved to place 17 CCT recommendations into pending status (in whole or in part), and committed to take further action on these recommendations subsequent to the completion of intermediate steps identified in the scorecard.\nWhereas, on 22 October 2020, the Board resolved to take action on 11 of the 17 CCT pending recommendations, as specified within the scorecard titled \"Competition, Consumer Trust, Consumer Choice Review Team (CCT-RT) Pending Recommendations: Board Action on 11 Recommendations\".\nWhereas, on 10 September 2023, the Board took action on two CCT pending recommendations as specified within the scorecard titled \"Board Action/Rationale o",
    "summary": "**ICANN Discovers Time Machine, Still Can\u2019t Decide: .INTERNAL Edition**\nICANN, in an unsurprising display of indecision, has now decided (maybe?) that the .internal domain is perfect for those who hate choices and love bureaucratic limbo. Comments on this groundbreaking development reach new heights of pedantry, as readers bicker over certification authorities like divorced parents arguing over Thanksgiving dinner. One bright spark decides to reinvent the wheel by pushing his site, getlocalcert.net, ensuring that nobody can accuse him of not trying to profit from chaos. Meanwhile, another commenter debates protocol encryption like it\u2019s the new flat earth theory. Riveting. \ud83c\udfad \ud83c\udf7f"
  },
  {
    "title": "Hacking a Virtual Power Plant (rya.nc)",
    "points": 63,
    "submitter": "bo0tzz",
    "submit_time": "2024-08-08T18:52:57",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=41194929",
    "comments": [
      "> A major selling point for me was that they have a local network API which can be used to monitor and control everything without relying on their cloud services.That would be a MAJOR selling point for me, too! Especially because most of the companies that do this for residential are getting a huge portion of the profits using YOUR batteries, rather than getting that profit yourself. Unfortunately, where I live, that is mostly because energy utilities are hostile to anyone except large middlemen installers and their VPPs. It would be simple for them to also allow individuals to sign up for dispatch if they were half-competent. I can't find any companies providing battery systems plus solar that are able to be fully local in my area of the USA, either, even if I wanted to try and go \"off grid\" using massively overbuilt batteries linked to solar.\n \nreply",
      "> the legitimate ones I\u2019d initially generated still workedThis spooks me. I take this to mean either:- They are still using the compromised key for validation, meaning if you have access to any old token, you can still mutate that, maybe needing to play around with the issuing times- They built an allowlist of all permitted tokens, and check that list first. In which case, might as well use random session ids instead of JWTs, and at the same point where the allowlist is being checked, mutate the request to inject a JWT that the backend can use.Also, kind of curious why the switch to RSA4096 instead of elliptic curves, since they are generally faster / smaller.\n \nreply",
      "I think very few customers had ever generated API keys, and as best I can tell they made an allowlist for them.One of my suggestions to them was to switch to elliptic curve, but I imagine RSA 4096 \"just worked\".I suspect they'll rework it later now that it's not \"on fire\".\n \nreply",
      "Ah that makes sense. For sufficiently small values of N, a hardcoded allowlist isn't a problem.You're probably right that RSA 4096 \"just worked\", and some library in their stack doesn't have elliptic curve support. And again, if N is small, the verification performance doesn't matter that much.Nice find and writeup!\n \nreply",
      "My guess is they are still accepting keys signed with the old 512 key but are currently generating new tokens with a 4096 key.\n \nreply",
      "The Ars Technica article may also be of interest - it has a statement from the company.https://arstechnica.com/security/2024/08/home-energy-system-...\n \nreply",
      "I do wonder how you got in contact with them?\nNo security.txt or other obvious ways to report a security issue.\n \nreply",
      "In the footnotes:> Someone once asked me, as commentary on my ability to figure out email addresses, \u201cAre you a Hacker or in sales?\u201dJust a bit of OSINT and educated guessing.\n \nreply",
      "It's pretty easy to get in contact with someone most of the time if you search for relevant roles on google with site:linkedin.com, or if you want to reach execs, on their business pages. I usually include several combinations of first and last names @company.com so like I will email to JSmith@company.com and JohnSmith@company.com and others. This is useful for escalating customer service complaints if you're just hitting a brick wall with the low level customer service staff.\n \nreply",
      "Yeah, this is pretty much what I did, except the demo account had an employee's email address in the details so I knew the format.\n \nreply"
    ],
    "link": "https://rya.nc/vpp-hack.html",
    "first_paragraph": "rya.ncRyan Castellucci\u2019s blogI recently had solar panels and a battery storage system from GivEnergy installed at my house. A major selling point for me was that they have a local network API which can be used to monitor and control everything without relying on their cloud services. My plan is to set up Home Assistant and integrate it with that, but in the meantime, I decided to let it talk to the cloud. I set up some scheduled charging, then started experimenting with the API.The next evening, I had control over a virtual power plant comprised of tens of thousands of grid connected batteries.When I went to generate an API token, I was pleasantly surprised to find options for expiration time and fine-grained control over permissions. I clicked \u201cgenerate\u201d, and got:From the eyJ prefix, I recognized a base64 encoded JSON object. Upon closer inspection, it turned out to be two JSON objects and some binary data, separated by periods:More precisely, a JSON Web Token (JWT) signed with an RSA",
    "summary": "**Hacking the Sun: One Solar Panel at a Time**\n\nIn an online world where even your toaster probably mines Bitcoin, *Ryan* dissects how his new solar panels could, accidentally, also mine for... vulnerabilities? Good ol' Ryan, installs some solar and is immediately bamboozled into hacking a virtual power plant unintentionally \ud83d\udd75\ufe0f\u200d\u2642\ufe0f. Meanwhile, the comment section turns into a bizarre hybrid of a self-help tech forum and a low-key hacker meetup, with individuals either mourning their own much less hackable solar setups or providing unintentional comedy by recursively explaining how easy it is to talk to any exec, as long as you've mastered the LinkedIn-Google-email trifecta. \ud83d\udce7\ud83d\ude02 If only securing APIs was as simple as cold-emailing the CEO."
  },
  {
    "title": "OTranscribe: A free and open tool for transcribing audio interviews (otranscribe.com)",
    "points": 386,
    "submitter": "zerojames",
    "submit_time": "2024-08-09T07:31:15",
    "num_comments": 91,
    "comments_url": "https://news.ycombinator.com/item?id=41199567",
    "comments": [
      "I needed to do this this week (transcribe an interview with multiple speakers) and used https://github.com/MahmoudAshraf97/whisper-diarizationWorked excellent.It generates both a file that just contains a line per uninterrupted speaker speech prefixed with the speaker number, as well as a file with timestamps which I believe would be used as subtitles.\n \nreply",
      "I have had very good results using Spectropic [1], a hosted Whisper Diarization API service as a platform. I found it cheap and way easier and faster than setting up and using whisper-diarization on my M1. Audiogest [2] is a web service built upon Spectropic, I have not yet used it.disclaimer : I am not affiliated in any way, just a happy customer! I had some nice mail exchanges after bug reports with the (I believe solo-)developer behind these tools.---[1] https://spectropic.ai/[2] https://audiogest.app/\n \nreply",
      "Thanks for the shout-out and kind words!Thomas here, maker of Spectropic and Audiogest. I am indeed focused on building a simple and reliable Whisper + diarization API. Also working on providing fine-tuned versions of Whisper of non-English languages through the API.Feel free to reach out to me if anyone is interested in this!\n \nreply",
      "Hi! Any plans to support streaming transcription with diarization?\n \nreply",
      "Great looking API. Are you able to, or do you have plans, for there to be automatic speaker identification based on labeled samples of their voices? It would be great to basically have a library of known speakers that are auto matched when transcribing\n \nreply",
      "Thanks! That is something I might offer in the future and is definitely possible with a library like pyannote. Would be really cool to add for sure.I am also experimenting with post-processing transcripts with LLMs to infer speaker names from a transcript. It works pretty decent already but it's still a bit expensive. I have this feature available under the 'enhanced' model if you want to check it out: https://docs.spectropic.ai/models/transcribe/enhanced\n \nreply",
      "The problem with using OpenAI whisper is that its too slow on CPU only machines. Whisper.CPP is blazing fast compared to Whisper and I wish people build better diarization on top of that.\n \nreply",
      "Another advantage of Whisper.CPP is that it can use cublas to accelerate models too large for your GPU memory; I can run the medium and large models with cublas on my 1050, but only the small if I use the pure GPU mode.\n \nreply",
      "What's OpenAI Whisper vs whisper.cpp? Do you mean whisper-diarization uses the API?\n \nreply",
      "https://github.com/openai/whispervshttps://github.com/ggerganov/whisper.cppThey are two inference engines for running the whisper ASR model, each with their own API AFAIK.\n \nreply"
    ],
    "link": "https://otranscribe.com/",
    "first_paragraph": "A free web app to take the pain out of transcribing recorded interviews.As featured on\n Follow @oTranscribe\n\nCreated by\nElliot Bentley.\n                A project of the MuckRock Foundation.\n            \n\nPrivacy policy\nEnter your transcript here...\u00a0\u00a0Protips:\u00a0- Ctrl+I adds italic formatting and Ctrl+B adds bold formatting.- Press ESC to play/pause, and Ctrl+J to insert the current timestamp.\n\n",
    "summary": "In a world drowning in ***audio files***, Elliot Bentley throws out yet another ***lifeline*** to the teeming masses of journalists with OTranscribe, a \"revolutionary\" tool that surely no one has ever thought of before \ud83d\ude44. Meanwhile, the comments section quickly devolves into a tech flex-off, as users scramble to one-up each other with alternative transcription setups that are faster, cheaper, and \"not affiliated, just a super happy customer.\" The humble brag is strong with this one, as users toss around APIs like confetti at a parade, while discreetly dropping in bug report sagas that totally happened. Bonus points for the developer popping in to valiantly promise the moon and definitely not scout for free beta testers."
  },
  {
    "title": "Jerk (ivanish.ca)",
    "points": 176,
    "submitter": "surprisetalk",
    "submit_time": "2024-08-04T17:10:23",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41154895",
    "comments": [
      "I enjoyed reading that!> I haven\u2019t seen anyone do a shrinking game.While I'm also not aware of a shrinking game, there are more than a few[1] growing games. Some aspects of the mechanic are the same - \"what was dangerous before is not a problem now and vice versa\"; it's just the plot that changes.So: in a shrinking game a small enemy will become a large threat and a narrow passage will become a large escape route - and in a growing game a large enemy will turn into a small threat and a previously large passage will become inaccessible.[1]: E.g. https://en.wikipedia.org/wiki/Katamari_Damacy\n \nreply",
      "There was a game called Specter Spelunker Shrinks that required arbitrary shrinking and growing. It's more of a prototype than a full game, but I could imagine this exact mechanic working in a more fleshed out setting.A cool thing about the game is that time also scales with size. So the challenge provided by moving objects also scales with the character's size (sometimes beneficially, sometimes not).https://www.youtube.com/watch?v=NZ65SWBb1Kg\n \nreply",
      "It's not exactly either a shrinking or growing game but maybe a bit of both: https://en.wikipedia.org/wiki/Superliminal.(Ostensibly, in a growing game you get bigger as the game progresses and vice versa for a shrinking game; this lets one play with perspective to make things bigger or smaller as needed.)\n \nreply",
      "A wonderful example of fictional use of scale is Greg Egan's sci-fi novel \"Scale\" https://a.co/d/8t2LLwk .It also touches on the real thorny issues of metabolism, heat transfer, and density, aside from the obvious geometry and ergonomic concerns.\n \nreply",
      "Reminds me of a proposal I've seen floating around for a Metroidvania game where you lose abilities as you progress (never gaining them back.)IIRC the idea is that it would still be a game with progressive exploration (if not necessarily Metroidvania-style exploration), as terrain-navigation-gated area design would be combined with more-mainstream keys-and-locks area design \u2014 so while you're losing your abilities that allow you to \"sneak in through the back door\" of areas, you're gaining key items/flipping switches/making friends that enable you to \"come in through the front door\" instead.Also, as with a \"shrinking game\", you'd see things you're currently \"too powerful\" to enter and need to come back when you're \"weaker.\" In this case, because some abilities you start with would be too powerful \u2014 trading that power off for having too little precision/finesse \u2014 and so would be preventing you from entering some areas because every time you try to do so, the ability kicks in and pushes you past the gap you were trying to fit into (or whatever.) It's only once you lose the ability that you gain the precision required to aim for the gap.\n \nreply",
      "> Metroidvania game where you lose abilities as you progressThis is brilliant. It solves the inherent tension of metroidvanias wanting both ever-increasing power and ever-increasing challenge. Rather than unlocking new areas, each time you lose abilities, it changes your relationship to existing areas. But the knowledge you gain \u2014 room layouts, enemy placement, hazards, short cuts, etc \u2014 becomes way more helpful when you can't just fly (perhaps literally) across the map.\n \nreply",
      "This idea is really interesting to me but it seems like it would be hard to capture a similar kind of fun as progressing in a traditional metroidvania. That there is a trade-off between player movement abilities and map shortcuts is interesting enough to warrant a purchase (big fan of the genre, personally) but \"100%\" saves and the like could possibly be un-fun to make.Although, I'm already imagining a sequence where you trade away your last movement ability and walk down some long-ish path to a place where you get all or most of them back (or different ones!). That could be fun and rewarding but it would definitely have to be done right.\n \nreply",
      "Seems very unlikely to work. Most metroidvania mechanics are not \"useful\", they're just \"required\" like barrier breakers or hazard resistances. Players don't actually care about having the latter. Taking away the ability open orange doors isn't really different from introducing new purple doors that you can't open.The tech that is useful, like dashes and double jumps, is really nice to have but not especially interesting to lose imo. If you \"need\" the dash then it's just a key. If you don't need the dash then losing it is just less satisfying movement.\n \nreply",
      "Baba is You does this by giving you a bunch of tools to solve a level, then taking away one of them (or neutering it) and challenging you to solve it again as an Extra.It's fun because it makes you realize that you already had the power for a more a different solution, but didn't notice.\n \nreply",
      "My boss and a long time good friend developed one[1] with his previous indie studio. Originally VR but playable with mouse and keyboard too, player can shrink both themselves and their enemies.(Not trying to advertise for sales as the company went under years ago)[1]https://store.steampowered.com/app/754850/The_Spy_Who_Shrunk...\n \nreply"
    ],
    "link": "https://ivanish.ca/jerk/",
    "first_paragraph": "If you\u2019d like, you can listen to an audio version of this page. It includes all the contextually relevant bits of music, and a few ad libs not included in the text. But it doesn\u2019t include the links and images, naturally.In 2010 I quit my job to make indie games.My childhood friend Sterling had just finished studying comp sci at university, and I\u2019d talked him out of going straight into the industry. Big teams, no autonomy, no vision.He was a graphics programming savant, deeply specialized, and I could do all the other stuff. Together, we were T-shaped. We had a shared appreciation for weird, innovative games. And we each had some savings.This was before Steam Greenlight. Before Indie Game: The Movie. The paint was still drying on the App Store. The iPad was new and nobody knew what to do with it. Maybe this big portable screen would be a great canvas for games. Hell, any day now, Flash should come to iOS. But if it doesn\u2019t, Unity seems promising, if a little restrictive.It feels like in",
    "summary": "In a dazzling display of auditory pomposity, someone decided that reading wasn't enough and blessed the world with what no one asked for: audio with ad libs and tunes for a blog post about quitting a job to \ud83c\udfae**make indie games**. Tragically, this daring escape from the dreary 'big team' life comes years before hipster docs made game devs into misunderstood geniuses. Comment sections turn into a nerd Coliseum, debating the revolutionary differences between shrinking and growing in games\u2014a spectacle surely as engaging as watching paint dry. Intellectuals further massage their brains by comparing these high-stake game mechanics to Greg Egan's sci-fi novel because obviously, that's what's missing from their game night discourse. \ud83d\ude44"
  },
  {
    "title": "Qlot: Common Lisp Library Manager (github.com/fukamachi)",
    "points": 46,
    "submitter": "ducktective",
    "submit_time": "2024-08-06T05:04:04",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=41167921",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/fukamachi/qlot",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A project-local library installer for Common Lisp\n      \nQlot (pronounced ky\u00fc-'l\u00e4t, like culotte) is a project-local library installer using Quicklisp facility. This aims to be like Bundler of Ruby or Carton of Perl.We have Quicklisp, the central library registry. It made installation of libraries damn easy.However, what only you can specify is the month of distribution. Which means you have to use all libraries of the same moment and you cannot use a newer/older version of a library for your project.\"local-projects/\" or ASDF configurations may be a solution to this problem, but there are a couple of problems.They are not project-local. If you have multiple projects that use different versions of the same library, it would be a problem.They are difficult to fix the version or to update them. If your project needs to work on other th",
    "summary": "**Qlot: Lisp's Latest Leash**\n\nIn a stunning display of missing the point, the Lisp community gifts us with ***Qlot***\u2014*the* groundbreaking tool that promises to manage the chaos inflicted by Quicklisp's own efficacy. But don't worry, if handling numerous library versions across projects sounds like solving a Rubik's cube blindfolded on a roller coaster, it's because it probably is! Reassuringly, the ambitious souls commenting have graciously offered their undying devotion to untangle this mess, eagerly congratulating each other while blissfully ignoring that this is a problem that was solved a decade ago by every other programming environment. \ud83c\udf89\ud83d\ude44 Surely, this is the peak of software innovation!"
  },
  {
    "title": "Voice is a garden: Margaret Watts Hughes's Victorian sound visualizations (themarginalian.org)",
    "points": 33,
    "submitter": "benbreen",
    "submit_time": "2024-08-09T19:21:21",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41204622",
    "comments": [
      "Somewhat related: resonance figures formed by a vibrating plate, https://americanhistory.si.edu/science/chladni.htm\n \nreply",
      "That site is dated. Even the video won't play because it uses flash\n \nreply",
      "In fairness they do have a YouTube link below the video itself so that there exists another way to watch it.\n \nreply",
      "Cymatics is my spirit frequency!While I know a fair-enough-surface-level stuff about Cymatics, I really seek the day where I can focus on it more in depth.What I really love about Cymatic patterns that represent certain frequencies is that they have been seen throughout history in architectural reliefs and carvings.such as the famous rosslyn chapel:https://www.tokenrock.com/cymatics/rosslyn-chapel-cubes/---In a famous, meta-physical series of Tomes \"Life and teaching of the masters of the far east\" - there is a great part about the team of Baird T Spalding addressing a cliffside with a group of monks of some sort. There is a large flat stone that the group assembles on, then the monks form a half circle around the stone, and facing the cliffside, they chant and have some sort of horns which they blow a certain tonal frequency that bounces from the cliff face into the stone they are on, and the frequency resonates to levitate the stone up the cliff to hte temple they are seeking.(read it in 1992, so deep pull from memory)https://www.youtube.com/watch?v=588pfqtMtz0There are the biblical versions on Trumpet and Walls falling - and other sound-based stories of yore.We doing sonic surgery etc...And as Tesla famously says 3/69/ - if you want to understand the universe, understand frequency.===Movement and MeasureTime expressed through frequencyAll is vibration\n \nreply"
    ],
    "link": "https://www.themarginalian.org/2024/08/08/margaret-watts-hughes-voice-figures/",
    "first_paragraph": "",
    "summary": "In a *stunning* display of Victorian whimsy, the marginals at themarginalian.org take us through the mystical garden of voice visualizations by Margaret Watts Hughes. Here, the harmony of pseudo-science meets art, depicted in hauntingly blurry diagrams that could also pass for your nephew's preschool art project. Meanwhile, the enlightened commentariat dives deep into the fractal void, debating the merits of outdated technologies and the forgotten magic of sound levitating stones. To those aimlessly mulling over cymatics as a 'spirit frequency', rest assured, this profound pseudoscience has been represented in architecture and, transcendentally, in YouTube comments auguring the fall of Jericho. Truly, if the Victorians had the internet, they too would be leveraging 'sound vibes' to hoist their stones up cliff faces. Whatever that means. \ud83c\udfb6\ud83c\udf00\ud83e\udd37\u200d\u2642\ufe0f"
  },
  {
    "title": "Show HN: Personal Interactive Cantonese Dictionary (ctang.art)",
    "points": 19,
    "submitter": "ctangy",
    "submit_time": "2024-08-09T21:33:18",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://nomenclature.ctang.art",
    "first_paragraph": "",
    "summary": "In an audacious display of technological overkill, a Hacker News user unveils a \"Personal Interactive Cantonese Dictionary\" for all twelve people in Silicon Valley who think Cantonese is their shortcut to an Alibaba job. Readers break from their usual skirmishes over JavaScript frameworks to argue about tone contours, while someone inevitably suggests it should have been built on a blockchain. Meanwhile, every commenter with a \u201cfounder\u201d in their bio pitches why incorporating AI will solve the apparent crisis of not pushing enough cutting-edge tech into a dictionary. No one asks if it comes in dark mode. \ud83d\udcda\ud83d\ude80"
  },
  {
    "title": "VisiCalc \u2013 The Early Days (2003) (benlo.com)",
    "points": 52,
    "submitter": "hggh",
    "submit_time": "2024-08-06T14:25:35",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=41171142",
    "comments": [
      "xxx"
    ],
    "link": "https://benlo.com/visicalc/",
    "first_paragraph": "\n\r\nOn April 8, 2003, Dan Bricklin, Bob Frankston, and Mitch Kapor gathered with Charles Simonyi to\r\nspeak about the Origins and Impact of VisiCalc.\r\nThis got me thinking about those early days and my own first encounters with the program.\r\nBy the summer of 1978,\r\nMicrochess was available on all of the popular personal computers of the day, the S-100 bus machines,\r\nthe TRS-80 and the Commodore PET. Micro-Ware had a booth at PC-78 in Atlantic City and the cassettes were\r\nflying off the table. Carl Helmers, editor of Byte Magazine, came by and introduced me to someone I should meet\r\nbecause \u201cwe have a lot in common\u201d. This turned out to be Dan Fylstra, at the time, an MBA student at Harvard, a writer for Byte,\r\nand a 6800 programmer. I didn't remember at the time, but later I discovered that Dan had been the third person to order Microchess for the Kim-1 when it was announced in 1976.\r\nDan and I had lunch together and he told me about Personal Software, his small company distributing progr",
    "summary": "**The Vintage Silicon Youth Club Reunion**\nIn a self-congratulatory haze, a gaggle of tech veterans, led by the illustrious Dan Bricklin, gathers to wax nostalgic about the glory days of VisiCalc, the modest forefather of Excel. The scene is a poignant flashback to 1978, where software was swapped at trade shows like bootleg tapes and big dreams were coded on tiny RAMs. Commenters trip over themselves in digital adulation, reminiscing the days when a 3.5-inch floppy was actually something to marvel at, and not just a coffee coaster. In this corner of the internet, grey beards sparkle with the static cling of bygone binary battles, each keystroke a tribute to the good ol' days of command line interfaces and printed manuals. \u2728\ud83d\udc74\ud83d\udcbe\ud83c\udf89"
  },
  {
    "title": "Infinite Proofs: The Effects of Mathematics on David Foster Wallace (2012) (lareviewofbooks.org)",
    "points": 58,
    "submitter": "lordleft",
    "submit_time": "2024-08-09T16:55:14",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=41203509",
    "comments": [
      "Here is the book on Infinity DFW penned: https://www.amazon.com/Everything-More-Compact-History-Infin...\n \nreply",
      "I'm surprised by the mathematician's critiques of this book. I have a PhD in math and I read this book about 10 years ago now. I loved it. I'm sure there are some inaccuracies, but he gets the overall story correct. There's enough math in the book to be engaging for someone mathematically trained. There's also a lot more history than if you read a math book that just has proofs. And the book is entertaining in the way that DFW's books usually are. As a former mathematician I highly recommend.\n \nreply",
      "I agree. The inaccuracies listed are like, once every few pages the author makes a statement like \"P implies Q\" without mentioning some minor condition, like, \"only if you assume the axiom of choice\". Yes, this is annoying for mathematicians, but there's just a fundamental compromise that has to be made. If you spell out every single detail then you will create a book that is not engaging enough to read straight through.I think you will really enjoy this book if, like me, you:1. Enjoy David Foster Wallace's literary style2. Have a good mathematical understanding of set theoryUnfortunately, the intersection of these two conditions might make for a very small target audience!\n \nreply",
      "I want to note for the HN crowd that the book is in the \"just technical enough to inform yet not scare off the layman, but not technical enough for the practitioner\" nonfiction subgenre. Critically, there are a number of finer details that DFW gets wrong; if you're mathematically inclined and intend to read this, I suggest pairing it with a printed copy of Prabhakar Ragde's errata document hosted by the DFW fansite The Howling Fantods ([1]).[1] https://www.thehowlingfantods.com/dfw/images/enmerrata.pdf\n \nreply",
      "This. He tries to do a few epsilon delta proofs and completely gets the concept wrong. I\u2019m surprised an editor did not stop this.If he can\u2019t understand a limit it really puts a question mark on whether it\u2019s worth reading his insight into the subject.\n \nreply",
      "This should be read in parallel with the review by Michael Harris in the AMS Notices: \"A Sometimes Funny Book Supposedly about Infinity\" https://www.ams.org/notices/200406/rev-harris.pdfAs a DFW lover whose day job is as a mathematician... that book's a clunker.\n \nreply",
      "I feel less-bad about not having finished it now.I was doing fine until formulas started showing up more than very-occasionally. I\u2019m basically dyslexic when equations enter the picture.\n \nreply",
      "Michael Harris is a great mathematician (number theory, let's go!!), but that review to me is pretty rambling and doesn't point out many inaccuracies in DFW's book, but takes issue with DFW's approach and style or writing. I did just skim the review and am a DFW fanboy, but Harris seems to have issues with books about infinity and math for lay people, which is fine, that's driving his opinions here.I'd imagine there will also be a gap between what mathematician's think of novelists writing and what novelists think of real math. So there's that too.\n \nreply",
      "Such a great book. Do you know if he there is any other DFW math-ish writings to be found? All my searches thus far have turned up naught.\n \nreply",
      "I'm not aware of any, but maybe somebody else is.  A more general question is are there any other DFW-ish math-ish writings to be found?  Against The Day (Pynchon) is not really math-ish, but it does have a good bit of math (more than GR at least), and a DFW fan would probably like it.  Stella Maris (McCarthy) is perhaps neither DFW-ish nor math-ish, but it is Serious Fiction centered on a mathematician and is probably the best work of fiction to feature Alexander Grothendieck.  I have heard that Solenoid (Cartarescu) has some math in it, though I fear it's still sitting on my shelf.  Every Arc Bends Its Radian (due in a couple months from Sergio De La Pava) has a math-ish title but I doubt it will actually contain much math.Michael Harris -- a mathematician who wrote a review of the DFW Infinity book -- has a book called \"Mathematics Without Apologies\", which I liked, though it's non-fiction.  There is also \"Birth of a Theorem\" by Fields medalist Cedric Villani which is an interesting read -- not fiction, but it is experimental in many respects and I would say worth a read.\n \nreply"
    ],
    "link": "https://lareviewofbooks.org/article/infinite-proofs-the-effects-of-mathematics-on-david-foster-wallace/",
    "first_paragraph": "",
    "summary": "Today on HackerNoon, the literary-yet-mathematically-amateurish congregation brings forth their critical sagas about David Foster Wallace's flirtation with the infinite. One \"surprised\" PhD claims inaccuracies are just spice in the math-history stew, recommending the novel with oddly specific conditions that might appeal to precisely three readers and a stray cat. Another DIY mathematician advises coupling the reading with a hefty errata, just in case you mistake DFW\u2019s artistic epsilon for an actual mathematical proof. \ud83d\udcda\u2716\ufe0f\ud83e\udd26 Meanwhile, a fellow tries to argue Michael Harris's critique didn't poke enough at factual errors; I guess skimming qualifies as deep analysis in these high intellectual echelons!"
  },
  {
    "title": "Using the Moon as an Echo [video] (youtube.com)",
    "points": 45,
    "submitter": "tws",
    "submit_time": "2024-08-08T06:47:26",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=41188769",
    "comments": [
      "My understanding is that before the development of observation satellites in the 1960s, radio echos from the Moon were used to detect nuclear explosions (testing or military use) from the far side of the Earth.  Even if direct observations were not possible / were limited, so long as the Moon was observable both over the test site and a US-friendly (not necessarily US-based) receiving station, such signals could be detected.The Arecibo\u2019s site origins trace back to the 1950s, when Cornell University proposed its construction to the Department of Defense\u2019s Advanced Research Projects Agency (ARPA), which is today known as the Defense Advanced Research Projects Agency (DARPA). A desire to better understand the composition of the ionosphere and how it might impact objects passing through, including ballistic missile reentry vehicles carrying nuclear warheads, was a key reason for its construction.  At the time, ARPA was in charge of a broad ballistic missile defense program known as Project Defender. It was believed that nuclear warheads would produce a distinct signature when reentering the atmosphere, making it possible to distinguish them from decoys, so long as that signature could be quickly identified and categorized. The plan was to use the Arecibo Telescope to help gather general, but still valuable information about the ionosphere in support of this effort.<https://www.twz.com/37898/collapsed-arecibo-radio-telescope-...>\n \nreply",
      "What a useful tool that saves me time. My weekend project this week was literally just writing a canon for modular synth that used a moon bounce for the second voice. I figured I could compose it with a delay set to 2.7sec, and then play with the clock via Maths for a performance- but their plugin sounds like this is more complex and interesting.Lunar bounce canons could become an entire form arising from that 2.7sec constraint.\n \nreply",
      "Oh hey I have that same ham radio! Kenwood TS-2000.  I guess I just need a giant radio telescope...\n \nreply",
      "A cross polarized Yagi can do EME bounce as well. You need an az/el rotator to track the moon though.\n \nreply",
      "I was explaining ham radio to my kids last night (something neither teenager had ever heard of), and ran across https://en.wikipedia.org/wiki/Earth\u2013Moon\u2013Earth_communication.\n \nreply",
      "> radio telescopeFrom the title I thought it might be about using lasers against lunar retroreflectors, passive arrangements of precise mirrors that (usually) reflect light back to its source.https://en.wikipedia.org/wiki/List_of_retroreflectors_on_the...\n \nreply",
      "This is really cool. A small niche of ham radio operators perform EME (Earth Moon Earth) contacts all the time\n \nreply",
      "There are also digital modes that use radio scatter and reflections (meteorites, airplanes,....) to communicate \"further\" than line-of-sight allows.It's a quite well know \"ham radio thing\", although it's mostly \"empty\", and few hams actually use those modes.\n \nreply"
    ],
    "link": "https://www.youtube.com/watch?v=yxBC6XzuMPs",
    "first_paragraph": "",
    "summary": "<b>Using the Moon as an Echo: Another Day, Another Youtube Video</b>\n\nIn the latest testament to humanity's endless quest to make every celestial object about us, a Youtube video details the illustrious history of using the Moon to eavesdrop on Earth's own nuclear secrets. Commenters, in a remarkable display of missing the point, quickly transform the discussion into a humblebrag contest about who owns the nerdiest radio equipment and the most arcane knowledge about moon bounces. One visionary even dreams up a weekend project involving modular synths and lunar delays, blissfully unaware of his audience's collective eye-roll. Meanwhile, the rest of us ponder whether reflecting radio waves off the Moon was just a 1960s version of playing with really, really expensive mirrors."
  },
  {
    "title": "What the hell is a luminiferous theremin? (extkits.co.uk)",
    "points": 27,
    "submitter": "edent",
    "submit_time": "2024-08-09T18:49:45",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=41204378",
    "comments": [
      "Sounds fun, but meaningless.As with the wide array of VR instruments (that also use highly accurate position tracking as inputs), what it really boils down to is:If you are procedurally generating audio (ie. using a synth) on device then what you have is categorically inferior to using any number of existing synths and virtual synths along with a midi input device (such as the genki wave).Don\u2019t embed the audio generator.Just make it a midi input device.It\u2019s not a thermin. Same way a microphone that emulates a flute when you blow it is not a flute.It\u2019s a synth that can only take one kind of input.Novel input devices are fun, and I like that; people like exploring different ways of expressing themselves in performances\u2026 but I\u2019m a bit skeptical about this.Seems like it\u2019s lost what was special about the theremin.\n \nreply",
      "What part # is used for the TOF sensors? The PDF schematic shows nothing...\n \nreply",
      "there are many https://www.adafruit.com/product/3317 those work really well\n \nreply",
      "... but why the hell would you listen to one?This last question is unfortunately never answered.The theremin is a good example of we made it \"because we can\" instead of \"because we should\".Of course, glad to be proven wrong.\n \nreply",
      "It certainly is a bit of a novelty, but there are a few Theremin-featuring pieces that I find pleasing, a classic example is the Theremin and piano arrangement of Saint-Saens' The Swan.Here is Clara Rockmore performing it (I think this is a video of the recording on The Art of the Theremin) https://www.youtube.com/watch?v=XdFSU8sn3mo .  There's a very nice arrangement of La vie en rose from the same sessions.She developed a lot of the Theremin techniques and - despite some argument about how practical vs. performative they are - is likely the best Theremin player ever.  She was previously trained as violinist and an answer to why you would _play_ a theremin: her tendinitis killed her violin career but could keep playing music with the force-less theremin.There are also a surprising number of well-known pop (etc.) songs that include a theremin somewhere. Not my list, but I see most of the ones I'm aware of here: https://open.spotify.com/playlist/0p2SpBZ3xDEjemAVlwmWeF?si=...\n \nreply",
      "> There are also a surprising number of well-known pop (etc.) songs that include a theremin somewhere.There are definitely some strong uses of theremin in there, but after sampling a few I also found some that I don\u2019t think are theremin at all, which makes me wonder if that\u2019s a common theme - whether a good chunk of that list isn\u2019t theremin, but other instruments.For example, Wonderboy (Tenacious D) seems to be using a synth with portamento (aka glide), which is pretty common and does sound a little like theremin while the pitch is sliding, but ultimately gives a very different effect, mostly because when it reaches the target note, it holds the note strongly. Theremin\u2019s effect tends to be disorienting because you can\u2019t hold an exact note, so it\u2019s either sliding around or people use a lot of vibrato.Another one I confirmed is not Theremin is Lovely Head by Goldfrapp. Again, this vaguely sounds like it could be a theremin, but there are plenty of signs it\u2019s either heavily processed and edited, or just not a theremin. My first guess was a guitar under heavy filtering or via MIDI, but then I found this: \u201c What is often mistaken for a theremin synth in the song is, in fact, Alison's vocals manipulated through a Korg MS-20 synthesiser.\u201d https://en.wikipedia.org/wiki/Lovely_Head\n \nreply",
      "Good Vibrations used something else:https://en.wikipedia.org/wiki/Electro-Theremin\n \nreply",
      "I can't believe nobody has mentioned the original soundtrack of Marvel's \"Loki\"[0], but that's where I learned the actual name of the instrument. (With more details in the director's interviews eg. [1])[0]: https://youtu.be/watch?v=183tEhupiSQ[1]: https://www.indiewire.com/features/general/loki-theremin-nat...\n \nreply",
      "I thought The Day The Earth Stood Still used it to good effect[1], even if the Theramin=UFO has since become cliche.1: https://www.youtube.com/watch?v=utuDgREIS7Q\n \nreply",
      "Neutral Milk Hotel uses the theramin to great effect in their music. https://www.youtube.com/watch?v=1FeD16vu_qQ\n \nreply"
    ],
    "link": "https://extkits.co.uk/what-the-hell-is-a-luminiferous-theremin/",
    "first_paragraph": "The Theremin was created by Leon Theremin (Lev Sergeyevich Termen) around October 1920 after noticing that a high frequency oscillators would change their frequency when a hand (or other object) was brought close to them. He added a circuit to make this change audible as a change in pitch of audio tone.After demonstrating his ability to play notes and music on this device that he initially called the etherphone but over time became to be known as a Theremin.Theremin music is heard usually as an accompaniment in many 1960\u2019s and 1970 songs, but it\u2019s most popular use was probably in horror or Science fiction films. The Theremin  had two ariels each with its own high frequency oscillator, one (usually the ariel sticking up) controls the pitch and the other (usually out to the side) controls the volume. The two ariels are set at 90 degrees to each other to reduce the possibility that the signal from one could affect the other. The Theremin works as the proximity of the users hand changes th",
    "summary": "Title: What the hell is a luminiferous theremin?\n\nIn yet another twist of musical arcane, the world discovers the <em>luminiferous theremin</em>, because regular old theremins were not sci-fi horror enough. Extkits.co.uk takes us through a historical haunt of Leon Theremin's accidental discovery, proving that any high school science fair project can in fact, become a cult musical instrument\u2014as long as it makes noises suitable for the background of a B-grade 1960's sci-fi thriller. Commenters quickly leap into a spiral of technical jargon and YouTube links to demonstrate that absolutely nobody needs a theremin, but everyone has an obscure track where it is \"used to great effect\". A debate erupts over whether a synthesizer mimicking a theremin is still a theremin, and important life-changing discussions such as the part number for TOF sensors on a machine no one will build ensue. In the high-fidelity echo chamber of the internet, it appears everyone missed the memo that inventing things for the sake of it went out of style with the steam-powered toaster. \ud83d\ude80\ud83c\udfb6"
  },
  {
    "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models (github.com/facebookresearch)",
    "points": 19,
    "submitter": "MrTrvp",
    "submit_time": "2024-08-09T19:55:42",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41204881",
    "comments": [
      "I tried a few samples, I feel that its quality is not as good as stable fast 3d.\n \nreply",
      "I'm guessing this paper is more about \"It's neat that this works at all.\" rather than trying to improve on the state of the art.\n \nreply",
      "Interesting that they compare to totally different models than Stable Fast 3D\u2019s paper. I\u2019m not clear on relative size of VFusion3D vs Stable Fast 3D, but I do think the training idea is good and novel \u2014 getting relatively good quality out of movies is a much easier ask in terms of collecting training data than getting 3D model renderings.\n \nreply",
      "https://arxiv.org/abs/2403.12034\n \nreply"
    ],
    "link": "https://github.com/facebookresearch/vfusion3d",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        [ECCV 2024] Code for VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models\n      Project page, Paper link, HF DemoVFusion3D is a large, feed-forward 3D generative model trained with a small amount of 3D data and a large volume of synthetic multi-view data. It is the first work exploring scalable 3D generative/reconstruction models as a step towards a 3D foundation.VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models\nJunlin Han, Filippos Kokkinos, Philip Torr\nGenAI, Meta and TVG, University of Oxford\nEuropean Conference on Computer Vision (ECCV), 2024We provide a simple installation script that, by default, sets up a conda environment with Python 3.8.19, PyTorch 2.3, and CUDA 12.1. Similar package versions should also work.Run the inference script to get 3D assets.You may specify which",
    "summary": "Meta's new \ud83e\udd16 creation, VFusion3D, boldly promises to redefine the realms of 3D asset generation using a cocktail of scraped video data and wishful thinking. Commenters optimistically scratch their heads, wondering whether the stumbling quality is part of the design. One eager techie suggests this paper is less about \"improvement\" and more a playful \"it works, I guess?\" dance in academia. Meanwhile, real comparisons to other models risk being as elusive as the apparent logic behind this trainwreck of AI development. \ud83c\udfad\ud83d\udcc9"
  },
  {
    "title": "A heck of a wild bug chase (georgemauer.net)",
    "points": 42,
    "submitter": "togakangaroo",
    "submit_time": "2024-08-08T17:17:19",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=41193922",
    "comments": [
      "xxx"
    ],
    "link": "https://georgemauer.net/2024/08/01/a-heck-of-a-wild-bug-chase.html",
    "first_paragraph": "I just tracked down a bug with more twists, turns, and interconnecting weirdness than I\u2019ve seen in years - maybe ever. I feel like I\u2019m going to be telling the story of Olympic-level Bug-Chase-2024 for years, and I feel compelled to write about it. To be clear, the following is a streamlined retelling. I\u2019m omitting the twists, turns and dead-ends that abounded.Also, now is a good time to mention that I just coincidentally got laid off in a \u201cReduction in Force\u201d, want to hire me?!First, some context. (Commence handwaving) Some time ago, our tech team received a request to create a way of showcasing graduates of our workforce training program on a job board. Because this was scheduled for only a pilot and we were already committed to a lot of other work, I recommended a relatively simple NextJs application thrown up on Vercel and secured by Auth0. As we were outside of our AWS infrastructure and this was meant to be just for a small-scale pilot, I didn\u2019t want to deal with databases. Again,",
    "summary": "Title: The Epic Olympics of Bug Squashing\n\nIn a spectacular display of self-aggrandizement wrapped in the trappings of a job plea, a coder lands on the Olympic podium of delusion by proclaiming their pest control experience as the \"Olympic-level Bug-Chase-2024.\" \ud83e\udd47\ud83d\udc1c After getting axed (\"I just coincidentally got laid off\"), our hero seamlessly pivots from debugging to humblebragging about cobbling together a \"simple\" NextJs app as if they single-handedly dismantled the Gordian knot. The comment section, a dumpster fire of sympathy, misplaced awe, and techie one-upmanship, devolves into an echo chamber where the unemployable meet to swap war stories about their \"most epic bug chase.\" Just remember, when in doubt, overstate your hardship and understate your solutions for maximum LinkedIn karma. \ud83c\udfad\ud83d\udcbb"
  },
  {
    "title": "Show HN: Nous \u2013 Open-Source Agent Framework with Autonomous, SWE Agents, WebUI (github.com/trafficguard)",
    "points": 82,
    "submitter": "campers",
    "submit_time": "2024-08-09T14:16:09",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=41202064",
    "comments": [
      "If this isn't by Nous Research, may want to consider renaming (https://x.com/NousResearch, https://nousresearch.com/)\n \nreply",
      "I can confirm this is not a projected related to Nous Research in any way, just an unfortunate naming collision\n \nreply",
      "Nous is the french word for \"us\". haven't heard of NousResearch\n \nreply",
      "but they explain it is from the greek nous which fits better for ai\n \nreply",
      "And if it is is by Nous Research we there definitely needs to be clearer branding as this is very confusing.If OP is not Nous Research (which I suspect to be the case) then a name change is a must as they're already a fairly well established company in the LLM space (surprised OP isn't aware of the name collision already). It's a bit similar to creating a new library with the \"Smiling Face with Open Hands emoji\"[0] as your logo0. https://emojipedia.org/hugging-face\n \nreply",
      "When I first picked the name, after a chat with Claude, I hadn't come across Nous Research back then, and they didn't show up Googling for just nous.I see a bit of reuse of words in other various llm related projects.Langchain/langfuse/langflowLlama/ollama/llamaindexso I hadn't been too worried about it when became aware of them.That's what Show HN is for, getting feedback, and a name changed now would be easy before I post it around more.\n \nreply",
      "Never heard of you\n \nreply",
      "I'm not affiliated with Nous Research in anyway, but do work in the LLM space and at least in this community it's a fairly well known org. Since this project also is in that space I was just adding support for parent's observation.\n \nreply",
      "This looks fantastic! I've been using aider and had my own scripts to automate some things with it, but this looks next level and beyond.I wanted to try this out (specifically the web UI), so I configured the env file, adjusted the docker compose file, ran `docker compose up` and it \"just works\".It would be great if there was a basic agent example or two pre-configured, so you can set this up and instantly get a better sense of how everything works from a more hands-on perspective.\n \nreply",
      "This looks too good. I have a B2B AI product, the features that exist in Nous easily outclass anything I could make in a reasonable timeline.Maybe I should rewrite my app using Nous...\n \nreply"
    ],
    "link": "https://github.com/TrafficGuard/nous",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        TypeScript AI agent platform with Autonomous agents, Software developer agents, AI code review agents and more\n      \n\n\nThe open-source TypeScript platform for autonomous AI agents and LLM based workflows \nHome |  Setup | Observability | Function calling |\nAutonomous AI Agent | AI Software Engineer | AI Code reviews |\nTools/Integrations | RoadmapThe Nous Story | Features | UI Examples | Code examples | ContributingNous started from a simple goal: to harness AI's potential to enhance real-world productivity, born in DevOps and Platform Engineering space. We envisioned a tool that could:At TrafficGuard we process billions of events a month for our global clients, increasing their ad spend ROI by protecting against bots and other forms of invalid traffic.\nOur SaaS on GCP comprises projects developed in TypeScript, Python, GoogleSQL, PH",
    "summary": "Welcome to the latest circus in HackerNews town: \"Nous \u2013 Open-Source Agent Framework blah blah with stuff and things\". It's another day and another <em>open-source TypeScript</em> platform claiming to revolutionize productivity by adding bloatware agents that can supposedly do everything from AI code reviews to optimizing your coffee breaks. Meanwhile, in the comment section, geniuses are bickering about name collisions like there's a shortage of words in English. One bright spark <em>fantastically</em> realizes \u201cNous\u201d means \u201cus\u201d in French after a deep dive on Google, while another suggests a name change to avoid confusion with a totally unrelated company. Meanwhile, someone probably named Claude is regretting all their life choices as they consider renaming their project to avoid internet shade. Look out for their next update, hopefully under a name not used by seventeen other projects. \ud83e\udd21\ud83c\udfaa"
  }
]