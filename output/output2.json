[
  {
    "title": "Webb captures iconic Horsehead Nebula in unprecedented detail (esa.int)",
    "points": 684,
    "submitter": "rbanffy",
    "submit_time": "2024-04-29T15:31:49",
    "num_comments": 205,
    "comments_url": "https://news.ycombinator.com/item?id=40199624",
    "comments": [
      "Absolutely incredible.For a little bit of context for how impressive this is, here's my take on it with a consumer grade 8\" Newtonian telescope from my backyard: https://www.astrobin.com/full/w4tjwt/0/",
      "Your picture is itself quite impressive. Do you mind sharing more about the equipment and process it takes to capture something like that?Edit: Oh, you can click through the image and see technical details. Very cool.",
      "You already noticed the technical card [1], but I can describe some of the details that go into this for those unfamiliar with the items on it.1. The scope they used is roughly equivalent to shooting with an 800mm telephoto lens. But the fact that it's 8\" wide means it can let in a lot of light.2. The camera [2] is a cooled monochrome camera. Sensor heat is a major source of noise, so the idea is to cool the sensor to -10deg (C) to reduce that noise. Shooting in mono allows you shoot each color channel separately, with filters that correspond to the precise wavelengths of light that are dominant in the object you're shooting and ideally minimize wavelengths present in light pollution or the moon. Monochrome also allows you to make use of the full sensor rather than splitting the light up between each channel. These cameras also have other favorable low-light noise properties, like large pixels and deep wells.3. The mount is an EQ6-R pro (same mount I use!) and this is effectively a tripod that rotates counter to the Earth's spin. Without this, stars would look like curved streaks across the image. Combined with other aspects of the setup, the mount can also point the camera to a specific spot in the sky and keep the object in frame very precisely.4. The set of filters they used are interesting! Typically, people shoot with RGB (for things like galaxies that use the full spectrum of visible light) or HSO (very narrow slices of the red, yellow, and blue parts of the visible spectrum, better for nebulas composed of gas emitting and reflecting light at specific wavelengths). The image was shot with a combination: a 3nm H-Alpha filter captures that red dusty nebulosity in the image and, for a target like the horsehead nebula, has a really high signal-to-noise ratio. The RGB filters were presumably for the star colors and to incorporate the blue from Alnitak into the image. The processing here was really tasteful in my opinion. It says this was shot from a Bortle-7 location, so that ultra narrow 3nm filter is cutting out a significant amount of light pollution. These are impressive results for such a bright location.5. They most likely used a secondary camera whose sole purpose is to guide the mount and keep it pointed at the target object. The basic idea is try to put the center of some small star into some pixel. If during a frame that star moves a pixel to the right, it'll send an instruction to the mount to compensate and put it back to its original pixel. The guide camera isn't on the technical card, but they're using PHD2 software for guiding which basically necessitates that. The guide camera could have its own scope, or be integrated into the main scope by stealing a little bit of the light using a prism.6. Lastly, it looks like most of the editing was done using Pixinsight. This allows each filter to be assigned to various color channels, alignment and averaging of the 93 exposures shot over 10 hours across 3 nights, subtraction of the sensor noise pattern using dark frames, removal of dust/scratches/imperfections from flat frames, and whatever other edits to reduce gradients/noise and color calibration that went into creating the final image.[1] https://www.astrobin.com/w4tjwt/0/[2] https://astronomy-imaging-camera.com/product/asi294mm-pro/",
      "Thanks!  I hadn't gotten to writing this out, but you've pretty much nailed it.> They most likely used a secondary camera whose sole purpose is to guide the mount and keep it pointed at the target object.I did use a guide camera with an off-axis guider, I'm not sure why it wasn't in the equipment list.  I've added it.> The RGB filters were presumably for the star colors and to incorporate the blue from Alnitak into the image.This is primarily an RGB image, so the RGB filters were used for more than the star colors.  This is a proper true color image.  I could get away with doing that from my location because this target is so bright.  The HA filter was used as a luminance/detail layer.  That gave me a bunch of detail that my local light pollution would hide, and let me pick up on that really wispy stuff in the upper right :)> The processing here was really tasteful in my opinion.Awe shucks, thanks :blush:",
      "One of my favorite comments ever on HN. I\u2019m big into photography and yet learned something on nearly every bullet. Thank you!",
      "Well, if you think photography is too easy you could try taking up astrophotograhy :)",
      "Amazing shot! Lots of good stuff, really liked this full moon shot https://www.astrobin.com/w0lzn5/B/ - the color!",
      "Yours is a superb image, too. Very impressive indeed. Kudos!",
      "I mean I don't know if I'm more impressed by their level of detail from a $10 billion telescope or your level of detail from a consumer-grade telescope!",
      "Thanks, but it if you look closely you'll see that the Webb image has almost an image worth of detail within each pixel of my image."
    ],
    "link": "https://www.esa.int/Science_Exploration/Space_Science/Webb/Webb_captures_iconic_Horsehead_Nebula_in_unprecedented_detail",
    "first_paragraph": "The NASA/ESA/CSA James Webb Space Telescope has captured the sharpest infrared images to date of one of the most distinctive objects in our skies, the Horsehead Nebula. These observations show a part of the iconic nebula in a whole new light, capturing its complexity with unprecedented spatial resolution.",
    "summary": "In an awe-inspiring display of humankind's dedication to staring at brightly colored gas thousands of light-years away, the James Webb Space Telescope (costing more than the GDP of some small countries) snaps a high-definition shot of the Horsehead Nebula, because apparently, the universe wasn't HD enough. On the ground, amateur astronomers with their backyard setups and monochrome sensors are doing their darnedest to keep up, delving into exhaustive detail about their equipment setups and how they, too, can capture less impressive, yet charmingly ambitious photos of space dust. The comments section quickly becomes an astrophotography forum, as enthusiasts compare notes on the best ways to spend thousands of dollars and countless hours to capture their own fuzzy versions of celestial wonders, all while marveling at how their efforts pale next to Webb\u2019s billion-dollar snapshots. It's a reminder that, no matter how advanced our technology becomes, there will always be someone in the comments section ready to say, \"Nice pic, but check out mine!\""
  },
  {
    "title": "Show HN: 3D Framework for the Web. Built on Svelte and Three.js (threlte.xyz)",
    "points": 37,
    "submitter": "spxneo",
    "submit_time": "2024-04-29T23:38:02",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=40205509",
    "comments": [
      "I have been using react-three-fiber and react-three-drei - the react version of this project. The examples are over whelming and wonderfulhttps://docs.pmnd.rs/react-three-fiber/getting-started/examp...One of my favorites is the image gallery. I modified this so that clicking on an image take you into another room (gallery).https://github.com/pmndrs/drei is a collection of examples and helpers.Most impressive to me is the one using a GLTF model, video textures on text, reflections and more. A standalone version ishttps://bfplr.csb.app/But even more impressive is the sandbox showing the not-very-many-lines-of-code athttps://codesandbox.io/p/sandbox/ground-reflections-and-vide...A lot goes into putting a layer on top of threejs and I have run a fair number of head scratchers. But still the potential is huge. Using threejs has completely changed the way I look at website development. So if you are a svelte person I would definitely look into this. [edit for grammar]",
      "Looks promising. It would be nice if the XR components exposed more configuration options. For example I don't see a way to use the transient pointer of the apple headset nor the depth buffer of the Quest 3.[0] https://webkit.org/blog/15162/introducing-natural-input-for-...[1] https://www.w3.org/TR/webxr-depth-sensing-1/",
      "Ooh love both Svelte and Threlte. I made a 3D chess board set in a park environment integrated with lichess and everything was remarkably easy.I think a lot of UI could benefit from use of 3D.",
      "I for one would love to see what that looks like, even just an example use of these libs c:",
      "My portfolio site was made with it https://perprogramming.com/There isn\u2019t much content though.",
      "https://www.reddit.com/r/sveltejs/comments/11y8flg/i_made_a_...https://github.com/ando818/chess2Codes probably bit messy but you can see the rendering of the board and pieces here.\nhttps://github.com/ando818/chess2/blob/main/src/routes/Board...I found some 3D model files that came with the board and pieces in one gltf file, which threlte makes easy to extract out the nodes. Svelte reactivity also makes it super easy to move/position/scale/animate things, its just plain JS!I was going to make a \"Chess lounge\" in the next iteration where you could walk around and watch other people playing in a cafe, but my interests moved on.",
      "Nice, the documentation seems a bit lightweight at first glance. It would be good if it went to show how the home page animation was created and how interactivity could make this framework useful.Also, how accessible is this content from the perspective of a screen reader?",
      "Dang, I appreciate how smooth the scrolling is! I feel like so often when a webpage has scroll-based animations they stutter and lag. I didn't realize how nice the effect could be when it actually ran at my monitor refresh rate.",
      "I love Threlte! My company uses it and it is remarkably clean & clear. Fantastic OSS project",
      "Very nice, I think I'll find use for it."
    ],
    "link": "https://threlte.xyz/",
    "first_paragraph": "Amr",
    "summary": "In today's episode of Hacker News Theatre, we're treated to a dazzling display of developers excitedly chattering about the newest 3D framework lovechild of Svelte and Three.js: Threlte. Watch in awe as our protagonist, Amr, navigates the treacherous waters of web development jargon, armed with nothing but an eclectic collection of hyperlinks and a burning passion for unnecessarily complex portfolio websites. Commenters, in a plot twist no one saw coming, throw around buzzwords like \"XR components\" and \"GLTF model\" as if they're vying for the Nobel Prize in Obscurity. Amidst declarations of love for Threlte and daydreams about 3D chess lounges, one can't help but marvel at the sheer determination to make the web unreadable by both humans and screen readers alike. Who knew web development could be both a spectator sport and a comedy show?"
  },
  {
    "title": "I made a new backplane for my consumer NAS (codedbearder.com)",
    "points": 266,
    "submitter": "granra",
    "submit_time": "2024-04-29T15:56:56",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=40199967",
    "comments": [
      "> This was my first time soldering DFN packages and they\u2019re tiny! Because I don\u2019t have a microscope or anything to visually inspect my soldering and these components were mostly just handling power anyway I just checked for shorts between power and ground and tried to take close-ups using my phone to see if I could spot any shorts. The technique that ended up working well for me to solder on these DFN packages without a stencil is to put what can only be described as way too much solder paste on the pads and then pushing the IC in the pool of solder, when the solder melts from the heat of my hot air station the IC will float on top and then I push down on it with tweezers and all excess solder will squeeze out and bead up, hopefully landing somewhere on the solder mask, where I can pick it up later once it\u2019s solid.This is positively an insane methodology for soldering. Fun, but insane.I actually can imagine that it works, strangely enough. But I don't think you'd get much consistent reliability with this. More like it'd work often enough that its good for one-off projects.",
      "It worked for me 8 times :)Thankfully the bottom pad on DFN packages reach the edge on 2 sides and I would think this technique wouldn't work with QFN.",
      "To save others from looking up DFN / QFN:DFN -> Dual-flat no-leads packageQFN -> Quad-flat no-leads packagehttps://en.wikipedia.org/wiki/Flat_no-leads_package",
      "The important note is:1. Longer leads of older packaging have higher parasitic inductance, parasitic capacitance, and parasitic resistance. Which hurts many electronic designs.2. The thermal pad on DFN / QFNs allows for direct-heat transfer off of the chip-package (while DIP, SOIC, and other older designs do NOT allow for this direct-heat transfer off). This improves thermals of hot parts, as you can use the whole PCB as a heatsink now, instead of having to add a heatsink elsewhere.3. DFN and QFN are physically much smaller than traditional DIP, SOIC or TQFP packages. So people who use these smaller chip packages can make even smaller circuit boards, or alternatively, a more complex design fit in smaller spaces.-------QFN and DFN are very popular parts for modern professional level electronics. But do note that their small size and tiny leads make them harder to deal with at the hobbyist level. But its possible with enough flux, solder paste, and hot-air rework to work with these parts.I would recommend that beginner electronics who are just getting into custom PCB design work with larger parts (like SOIC packages) instead. Bigger is often easier. Move to these tiny DFN or QFN parts only after you've gained confidence in your surface-mount skills.",
      "QFN is (very, very slowly) opening up to the idea of \"wettable flanks\", which is increasing the reliability of QFN soldering anyway... which is increasing the opportunity of \"side-soldering\" QFN packages.https://www.ti.com/document-viewer/lit/html/SSZTBN6Here's to hoping for more manual soldering techniques to work into the future! We all know that these parts are getting extremely small and harder to work with, but its still clearly possible to make devices.But yeah, QFNs, if this wettable-flank trend continues, could be quite reliably soldered and inspected from the sides soon. The needs of the high-reliability automotive industry to create an inspection methodology outside of X-Rays is driving this change.------------Glad to hear that your technique worked in any case. I wouldn't have tried that, but maybe I'll try it once next time I have a hot-air gun out and a DFN part to try it on.EDIT: Note that Microchip apparently has some DFNs that also have wettable-flanks: https://www.microchip.com/en-us/about/media-center/blog/2022...",
      "That's very interesting, I've always found QFN to be annoying to work with",
      "Perfectly valid method, IMO. I always put some vias in the thermal pad and expose the copper on the other side. That way the vias don't get plugged with solder mask and wick away excess solder. It also lets you heat the thermal pad with an iron from the other side for real low-budget SMD work.Squishing the chip down is fine, you just have to be careful to align it with the pads properly. I don't think I've ever seen a solder bridge on a QFN other than a big ball on the outside edge. Modern solder alloys and solder masks are really good at preventing bridges. The solder really doesn't want to stay between the package body and the solder mask, it will try very hard to only stick to metal.",
      "Perhaps I should better explain why I dislike it.A lot of parts (ex: Crystals) are extremely heat-sensitive. So even just a few dozen seconds extra of soldering could damage your crystals (ex: internal solder will reflow and break the vacuum seal).So *ANY* action which requires you to \"carefully squeeze down\" the solder paste out means you're going in there with tweezers, and possibly leaving the board in 200C+ soldering temperatures a bit longer than usual.-----------IE: Its not that this technique \"won't work\" in isolation, its that you won't get \"consistent and reliable results\", especially when we consider the nearby parts (ex: a possible XTAL) that could be damaged while you're messing around with the hot-air gun.The standard technique of reflow soldering is to use a solder-paste stencil to deposit the \"right\" amount of solder  paste onto your board, then use a reflow oven to carefully heat the board (and hold the heat) at the right temperatures, so that nothing gets damaged.Once we start relying upon \"Well, keep the board heated a bit longer so that my tweezers can do the work\" is when you start overheating chips and otherwise creating failed parts.> Modern solder alloys and solder masks are really good at preventing bridges. The solder really doesn't want to stay between the package body and the solder mask, it will try very hard to only stick to metal.This also matches my experience. The surface tension of solder (be it lead based, or lead-free, or low-temperature solder paste) is far greater than most people expect.You can strongly rely upon \"surface tension\" to magically do the right thing on your boards, as long as you've got good quality soldermasks.In fact, a big problem with reflow soldering is \"tombstoning\" (ex: surface tension of lead is so strong, it pulls your components OUT of position). There's a LOT of surface tension at play here. Its great in most cases, but keep an eye out for those tombstones!Everything that could go wrong in my experience is fixed with rework hot-air + generation applications of flux + solder wick + soldering iron + tweezers. And rather easily mind you, I know that didn't sound very easy but... the right tools make any situation fixable. Its just knowing how to use all those tools in the right cases.",
      "I extend the pads out from underneath the chip. Then it's easily hand-solderable if needed.",
      "I'm not sure why \"too much\" solder paste is needed to make this work. I've done something similar on quite a few occasions, but using regular solder. Just put a slightly generous amount on each pad, but not enough to create any shorts. Add flux (ideally, something rosin-based that will last longer in the heat of the next step). Heat up the whole footprint with the hot air gun, and gently place the IC on top.This method saves blasting the IC with hot air while heating up the whole lot."
    ],
    "link": "https://codedbearder.com/posts/f3-backplane/",
    "first_paragraph": "",
    "summary": "In an astounding revelation that threatens the very fabric of electronics manufacturing, a brave hobbyist wielding nothing more than a surplus of solder paste and a hot air station has single-handedly revolutionized how to haphazardly adhere tiny computer chips to a circuit board. Critics marvel at the audacity to use a technique akin to squishing bugs with a sledgehammer - applying *just* enough solder paste to recreate the La Brea tar pits, then gently nudging a DFN package into its doom with the finesse of a tweezers-wielding titan. The commentariat, buoyed by their collective expertise sourced from the deepest crevices of Wikipedia, spills forth advice ranging from the blatantly obvious to the esoterically arcane, ensuring that even the most novice of solder jockeys can confidently ruin their electronics with the reckless abandon of a seasoned professional. Meanwhile, the specter of 'wettable flanks' looms large, promising a future where even the grossly over-applied solder can find its way home, heralding an era of slightly less terrible DIY electronics."
  },
  {
    "title": "You can't just assume UTF-8 (csvbase.com)",
    "points": 60,
    "submitter": "calpaterson",
    "submit_time": "2024-04-29T06:11:17",
    "num_comments": 169,
    "comments_url": "https://news.ycombinator.com/item?id=40195009",
    "comments": [
      "How about assume utf-8, and if someone has some binary file they'd rather a program interpret as some other format, they turn it into utf-8 using a standalone program first. Instead of burning this guess-what-bytes-they-might-like nonsense into all the software.We don't go \"oh that input that's supposed to be json? It looks like a malformed csv file, let's silently have a go at fixing that up for you\". Or at least we shouldn't, some software probably does.",
      "I doubt you can handle UTF-8 properly with that attitude.The problems is, there is one very popular OS which is very hard to enforce UTF-8 everywhere, Microsoft Windows.It's very hard to ensure all the software stack you are depending on it use Unicode version of Win32 API. Actually the native character encoding in Windows is UTF-16 so you can't just assume UTF-8. If you're writing low level code, you have to convert UTF-8 to UTF-16 and back. Even if you don't you have to ensure all the low level code you are depending on it do the same for you.Oh and don't forget about the Unicode Normalizations. There is no THE UTF-8. There are bunch of UTF-8s with different Unicode normalizations. Apple macOS use NFD while other mostly use NFC.These are Just some examples. When people living in ASCII world casually said \"I just assume UTF-8\", in reality, you still assume it's ASCII.",
      "I\u2019ve been sharing it multiple times but I love it: WTF-16 spec https://simonsapin.github.io/wtf-8/#ill-formed-utf-16",
      "Agreed. Continuing to support other encodings is like insisting that cars should continue to have cassette tape players.It\u2019s much easier to tell the people with old cassette tapes to rip them, rather than try to put a tape player in every car.",
      "> It\u2019s much easier to tell the people with old cassette tapes to rip themI assume you mean \"rip them\", as in transcode to a different format?In that case, you need a tool that takes the old input format(s) and convert them to the new format.For text files, you'd need a tool that takes the old text files with various encodings and converts them to UTF-8.Isn't the point of the article to describe how an engineer would create such a tool?",
      "> Isn't the point of the article to describe how an engineer would create such a tool?Honestly, no, because the tool that it's suggesting how to write isn't one that will even come close to doing a good job.If you want to write such a tool, the first thing you need to do is to understand what the correct answer is. And to do that, you need to sample your input set to figure out what the correct answer should be for several inputs where it matters. There's unfortunately no easy way to avoid that work; universal charset detection isn't really a thing that works all that well.",
      "I agree 100% on the technical issues.But the point of the article is not the same thing as how well it achieved its goals.",
      ">universal charset detection isn't really a thing that works all that well.This seems like something LLMs would be good at. A mundane use of them, but I bet they'd be really good at determining that the input has the wrong encoding. Then the program would iterate through encodings, from most probable to least, and select the one that the LLM likes the most. Granted, this means your tool will be 1GB or more. But hey, thems the breaks.",
      "Yeah, that could be an interesting use of LLMs. It could at least tell you which languages might be present in the input text.In the 1980s, we had a version of 7-bit ASCII in Sweden where the three extra Swedish vowels \"\u00e5\u00e4\u00f6\" were represented by \"{}|\".So what might look like regular US 7-bit ASCII should be interpreted as the Swedish version if the text is in Swedish with \"{}|\" where \"\u00e5\u00e4\u00f6\" normally goes.",
      "A tool whose purpose is to transcode should be asking the user to select explicit input and output formats, not guessing."
    ],
    "link": "https://csvbase.com/blog/9",
    "first_paragraph": "csvbase is a simple web database.\n          Please join the committee to keep csv evil.",
    "summary": "In a world tragically undersupplied with real problems, <em>csvbase.com</em> steps up to insist that you can't just *assume* UTF-8, sparking a ferocious debate among those who believe in the healing powers of re-encoding and the dark arts of character set detection. Commenters, armed with the ancient wisdom of \"just convert everything to UTF-8 lol\" and haunted by the spectre of Windows' UTF-16, engage in a ritualistic dance around the bonfire of compatibility issues and Unicode Normalizations. One brave soul suggests machine learning might save us from this encoding hell, presumably by training an AI so powerful it can tell the difference between legacy text files and modernist UTF-8 art. Meanwhile, someone remembers the Swedish version of ASCII, bringing nostalgia for a simpler, more Nordic time when \"\u00e5\u00e4\u00f6\" were not exotic characters but the equivalent of wearing a horned helmet in a string of text."
  },
  {
    "title": "93% of paint splatters are valid Perl programs (2019) (mcmillen.dev)",
    "points": 297,
    "submitter": "ellieh",
    "submit_time": "2024-04-29T11:30:51",
    "num_comments": 72,
    "comments_url": "https://news.ycombinator.com/item?id=40197013",
    "comments": [
      "Jokes aside, isn't it wrong that OCR software still always produces textual result from images wich are not text? More than a decade ago I OCRed an old book, and I remember how annoying it was to deal with all the garbage text produced from small pictures, smudges, and dirt. It looks like there's not much progress done since in the field",
      "Concatenative languages [1] have the property that every token sequence is a valid program.For languages using single bits as tokens, every bit sequence is a valid program. One such language is Chris Barker's zot [2].Inspired by zot, I defined a concatenative version of Binary Lambda Calculus that shares the same property [3].[1] https://en.wikipedia.org/wiki/Concatenative_programming_lang...[2] https://en.wikipedia.org/wiki/Iota_and_Jot#Zot[3] https://cstheory.stackexchange.com/questions/32309/concatena...",
      "> Concatenative languages [1] have the property that every token sequence is a valid program.I don't think this is correct? Concatenative languages have the property that if a and b are both valid programs, that the program a || b is valid (where || means \"concatenate\"). But that property doesn't imply that every sequence of tokens is valid.For example, in Cat,  [1 2\n\nis not grammatically valid.",
      "You're right. I should have qualified it as \"some concatenative languages\".",
      "> making Jot a natural G\u00f6del numbering of all algorithms.This sounds very cool. I wish I understood both Jot and that sentence.",
      "A G\u00f6del numbering is simply a mapping to integers (that is easily decoded).\nIf your programs are arbitrary binary strings, then you're basically already done, since bitstrings are in 1-1 correspondence with integers:    empty  0  1  00  01  10  11  000  001  ...\n        0  1  2   3   4   5   6    7    8  ...",
      "I enjoyed footnote 5:\u2075 This feature does enable a neat quine: the Perl program \u201cIllegal division by zero at /tmp/quine.pl line 1.\u201d, when saved in the appropriate location, outputs \u201cIllegal division by zero at /tmp/quine.pl line 1.\u201d The reason for this behavior is left as an exercise for the reader.",
      "I wrote a blog post to explain it at https://dotat.at/@/2019-04-04-a-curious-perl-quine.htmlAnd also a superficially-related but actually rather different Python quine:  File \"quine.py\", line 1\n   File \"quine.py\", line 1\n   ^\n IndentationError: unexpected indent",
      "Can you help out a reader who does not know any Perl?I tried it in the REPL and found that \"Illegal division\" can't locate method \"illegal\" in package \"division\", so presumably that gets ignored, same with method \"by\" in package \"zero\", and that \"at /tmp\" is the simplest version of the string that produces the error message, which apparently is more severe than the missing package warnings and terminates the program?I'd guess the / is the operator for division, and the \"tmp\" is getting initialized as a variable and coerced into an integer? But \"/tmp\" doesn't do it, and \"/tmp/\" does something with regex, so I'm not sure why the parser would split it there.",
      "The footnote is originally referenced here:    Figure 6 represents the string \u201cgggijgziifiiffif\u201d, which by pure\n    coincidence happens to accurately represent the authors\u2019 verbal reaction\n    upon learning that \u201cunquoted strings\u201d were a feature intentionally\n    included in the Perl language.\u2075\n\nSo, the hint is that this has to do with the \"unquoted strings\" feature (aka \"bare words\"[1]).See the sibling comment about the actual parse \u2014 \"at\" and \"tmp\" are seen as strings.The strings get coerced into numbers due to being used with the numeric \"/\" operator (that's normal Perl behavor).\nSince the strings can't be parsed as numbers, they become \"0\".\nSo, you get division by 0.[1] https://perlmaven.com/barewords-in-perl"
    ],
    "link": "https://www.mcmillen.dev/sigbovik/",
    "first_paragraph": "Posted 2019-04-01.",
    "summary": "In a dazzling display of mock technical profundity posted on April Fools' Day, a blogger achieves the peak of programmer humor by revealing that 93% of paint splatters are valid Perl programs. As one might expect, the digital peanut gallery leaps in with commentary ranging from surprisingly earnest concerns about OCR technology's inability to discriminate between Renaissance art and spam emails, to self-congratulatory musings on the nature of concatenative languages and G\u00f6del numbering. One brave soul even tries to decipher the arcane mysteries of a Perl quine that mirrors the absurdity of its own existence, much like the commenter trying to parse basic humor. Meanwhile, in the echo chambers of internet forums, readers nod vigorously, misunderstanding every joke but pretending to laugh, lest they seem as confused as a Perl interpreter staring down a Jackson Pollock painting."
  },
  {
    "title": "Show HN: I made a privacy friendly and simple app to track my menstruation (play.google.com)",
    "points": 176,
    "submitter": "stormqueen",
    "submit_time": "2024-04-29T15:35:04",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=40199670",
    "comments": [
      "If you want to avoid Google, F-Droid also has options: https://search.f-droid.org/?q=period",
      "Other options that are open source include:drip.\nhttps://dripapp.org/Bluemoon\nhttps://gitlab.com/ngrob/bluemoon-android\nhttps://www.nilsgrob.ch/work/bluemoonPeriodical\nhttps://arnowelzel.de/en/projects/periodicalLog28\nhttps://github.com/wildeyedskies/log28",
      "My wife is a big fan of Euki which is privacy focused and developed by a non-profit.https://eukiapp.org/",
      "Sorry to hijack a bit here, but have any good links to open source baby tracking apps that allow sharing between partners? The popular options out there on the app stores are a privacy dumpster fire.",
      "I\u2019d recommend FindMy, but it only works for iBabies.",
      "tell me about it. And I can never Green text bubbles from my droid baby.",
      "I used this for our newborn recently and was very impressed with how flawlessly the partner sharing worked - https://mangobaby.app It is not open source. But didn't look suspicious like the other apps on the store. The privacy labels on the iOS App Store seemed acceptable too (for us).",
      "No experience but Alternativeto.net might help.The sharing part makes that a little less common.",
      "To help drive home the privacy angle, you might consider open-sourcing it and also making it available on F-Droid. Also, why does it say \"in-app purchases\"? If this is a simple calendar I don't see why any purchasable extras should be necessary.(While many in the HN crowd lack the requisite equipment to make use of your project, we do have girlfriends, wives, sisters, and daughters who might benefit from a privacy-respecting app like this.)",
      "In-app purchases: You can buy another theme if you want. There is no need to do that - the whole functionality is available anyway. I thought it would be nice to at least have the opportunity to make some money from it...I haven't thought about open-sourcing it... Maybe I'll think about it..."
    ],
    "link": "https://play.google.com/store/apps/details?id=earlyowlsoftware.justme.justme&hl=en_US",
    "first_paragraph": "Play Pass",
    "summary": "In a world where the volume of data mined from your personal life directly correlates to how much of a human being you're still considered, some Hacker News hero decides to launch an app to track menstruation cycles without inviting Google to the party. Cue the collective gasp of the HN crowd, who immediately sees this as an opportunity to debate the merits of various privacy respecting, open-source options because, apparently, tracking menstruation is just another coding challenge to optimize. Meanwhile, the comments section turns into a bizarre bazaar of recommendations for baby tracking apps as if these digital newborns need more surveillance than the NSA provides. \"But hey,\" says one commenter, \"why not slap in-app purchases on this simple calendar app, because nothing says 'privacy respecting' quite like monetizing it.\" This entire discussion proves that if you thought understanding human biology was hard, wait until you try understanding tech bro logic. \ud83d\ude44"
  },
  {
    "title": "Cheyenne Super Computer Auction (gsaauctions.gov)",
    "points": 179,
    "submitter": "zrules",
    "submit_time": "2024-04-29T12:10:47",
    "num_comments": 125,
    "comments_url": "https://news.ycombinator.com/item?id=40197277",
    "comments": [
      "I once bought a far larger supercomputer. It was 1/8 (roughly) of ASCI Blue Mountain. 72 racks. Commissioned in 1998 as #1 or #2 on the TOP500, officially decommissioned in 2004, purchased my 1/8 for $7k in ~2005.Moving 72 racks was NOT easy. After paying substantial storage fees, I rented a 1500sf warehouse after selling off a few of them and they filled it up. Took a while to get 220V/30A service in there to run just one of them for testing purposes. Installing IRIX was 10x worse than any other OS. Imagine 8 CD's and you had to put them each in 2x during the process. Luckily somebody listed a set on eBay. SGI was either already defunct or just very unfriendly to second hand owners like myself.The racks ran SGI Origin 2000s with CRAYlink interlinks. Sold 'em off 1-8 at a time, mainly to render farms. Toy Story had been made on similar hardware. The original NFL broadcasts with that magic yellow first down line were synthesized with similar hardware. One customer did the opening credits for a movie with one of my units.I remember still having half of them around when Bitcoin first came out. It never occurred to me to try to mine with them, though I suspect if I'd been able to provide sufficient electrical service for the remainder, Satoshi and I would've been neck-and-neck for number of bitcoins in our respective wallets.The whole exercise was probably worthwhile. I learned a lot, even if it does feel like seven lifetimes ago.",
      "Wow, that's ridiculous. I bought two racks of Origin2000 with a friend in high school and that was enough logistic overhead for me! I can't imagine 72 racks!!Installing IRIX doesn't require CDs; it's much, much easier done over the network. Back in the day it required some gymnastics to set up with a non-IRIX host, now Reanimator and LOVE exist to make IRIX net install easy. There are huge SGI-fan forums still active with a wealth of hardware and software knowledge - SGIUG and SGInet managed to take over from nekochan when it went defunct a few years ago.I have two Origin 350s with 1Ghz R16ks (the last and fastest of the SGI big-MIPS CPUs) which I shoehorned V12 graphics into for a sort of rack-Tezro. I boot them up every so often to mess with video editing stuff - Smoke/Flame/Fire/Inferno and the old IRIX builds of Final Cut.I think that by the time Bitcoin came out, Origin2000s would have been pretty majorly outgunned for Bitcoin mining or any kind of compute task. They were interesting machines but weren't even particularly fast compared to their contemporaries; the places they differentiated were big OpenGL hardware (InfiniteReality) with a lot of texture memory (for large-scale rendering and visualization) and single-system-image multiprocessor computing (NUMAlink), neither which would help for coin mining.",
      ">Components of the Cheyenne SupercomputerInstalled Configuration: SGI ICE\u2122 XA.E-Cells: 14 units weighing 1500 lbs. each.E-Racks: 28 units, all water-cooledNodes: 4,032 dual socket units configured as quad-node bladesProcessors: 8,064 units of E5-2697v4 (18-core, 2.3 GHz base frequency, Turbo up to 3.6GHz, 145W TDP)Total Cores: 145,152Memory: DDR4-2400 ECC single-rank, 64 GB per node, with 3 High Memory E-Cells having 128GB per node, totaling 313,344 GBTopology: EDR Enhanced HypercubeIB Switches: 224 unitsMoving this system necessitates the engagement of a professional moving company. Please note the four (4) attached documents detailing the facility requirements and specifications will be provided. Due to their considerable weight, the racks require experienced movers equipped with proper Professional Protection Equipment (PPE) to ensure safe handling. The purchaser assumes responsibility for transferring the racks from the facility onto trucks using their equipment.",
      "Given that the individual nodes are just x86_64 Xeons and run linux... it would be interesting to part it out for sale as individual, but functional, nodes to people.  There are a lot of people would like to have a ~2016 era watercooled 1U server from a supercomputer that was once near the top of the Top500 just to show to people.Get little commemorative plaques for each one and sell for $200 each or so.edit:\nit seems each motehrboard is a dual CPU board and so there are 4032 nodes, but the nodes are in blades that likely need their rack for power.  But I think individual cabinets would be cool to own.There are 144 nodes per cabinet... so 28 cabinets.  \nI'd pay a fair amount just to own a cabinet to stick in my garage if I was near there.",
      "The individual servers are not watercooled. The compute racks are air-cooled; the adjacent cooling racks then exchange that heat using the building's chilled water. It's the rack as a whole that is watercooled. If you extract a single node, you won't get any of that. As the other commenters also point out, these are blades; you can't run an individual node by itself.",
      "These are blades, so there is probably some kind of container chassis required to run them.Using them as desktop PCs would likely be a challenge.",
      "I don't think there's that big of a market for obsolete server pieces as nostalgia...But you could probably make a decent profit on just the CPUs alone parted out, even with the moving/handling costs.",
      "Going off one listing for a E5-2697v4, $50 with free shipping, 386 already sold.If you figure after the double-dipping of eBay/Paypal and then shipping fees, that's ~$30 profit per CPU.8024 x 30 = 241,920 USD.  Not too shabby for what's got to be some weeks/months of work.  You could probably assume that they can sell or scrap the rest of it for a bit more as well, minus the fees for storage and moving company.",
      "Is it not \"Personal\" protective equipment?https://en.wikipedia.org/wiki/Personal_protective_equipment",
      "You need PPE to protect your profession of moving stuff."
    ],
    "link": "https://gsaauctions.gov/auctions/preview/282996",
    "first_paragraph": "",
    "summary": "In a breathtaking display of e-waste bravado, enthusiasts of the Cheyenne Supercomputer Auction at gsaauctions.gov wax nostalgic over purchasing slabs of tech history that require small power plants to operate and enough square footage to make a real estate agent giddy. One proud purchaser reminisces about the logistical nightmare of housing 72 racks from a bygone era, while barely suppressing the regret of not turning them into Bitcoin mines. Others chime in with corrections about IRIX installations and affectionate tales of their own outdated tech behemoths, as if competing in the \"Obsolete Tech Hoarders Olympics.\" Amidst this, a pedantic debate emerges over the correct acronym for protective gear needed to move these relics, proving once again that in the nerd realm, no detail is too trivial to contest. \ud83e\udd13"
  },
  {
    "title": "Green Software Foundation's Software Carbon Intensity Spec Becomes ISO Standard (greensoftware.foundation)",
    "points": 18,
    "submitter": "wbeckler",
    "submit_time": "2024-04-29T21:20:52",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=40204269",
    "comments": [
      "How about starting with getting rid of all the bloated web-based crap...I'm strongly against inefficient software for different reasons, but if stuff like this can have an effect beyond the virtue-signaling bureaucracy it's likely to become instead, maybe it's a good thing.",
      "Surely there is in argument to be made that applications targeting a non-native but cross-platform runtime, like webapps, save more power than the devices running them waste. Cross-platform applications mean broader compatibility, so maybe less e-waste? Or perhaps a knock-on effect, like web apps running in browsers are trivially modifyable by extentions like uBO, which skip executing expensive tracking and advertising code that would be very difficult to stip out of a native binary. As a further knock-on, blocking those ads will make the user less likely to buy and have shipped random garbage, which reduces pollution!",
      "Ridiculous.",
      "Why do we need such a thing? Operating cost is proportional to energy, carbon emissions are also proportional to energy. Making your information systems cost less also makes them emit less carbon. The information industry has a clear and direct economic incentive to reduce carbon intensity, and all large operators do catalog and reduce GHG emissions.",
      "> Why do we need such a thing?For starters, growing numbers of companies are required to report on their own emissions, which includes software - and therefore having agreed standards on how to measure this would seem a good way forward to me?> carbon emissions are also proportional to energyReducing energy usage reduces carbon emissions, but they are not proportional. Highly dependent on the grid composition, time of day you draw the energy, and any behind-the-meter infrastructure.> and all large operators do catalog and reduce GHG emissions.To take AWS as an example, they still do not report scope 3 emissions for customers (due 'early 2024') - without which, these 'catalogued' numbers are essentially meaningless given how understated they are.",
      "...companies are required to report on their own emissions, which includes software...Just be careful not to double-count hardware and software emissions.",
      "There is a lack of political will to do the screamingly obvious thing and put a price on carbon emissions.",
      "because the cheapest energy is not necessarily the cleanest",
      "That may be, but the awkward method used by this initiative says that PPAs don't count, so if you literally spin off a subsidiary to build a solar array for your new datacenter, and use a PPA to buy all of its output, that doesn't matter and you are supposed to use the regional grid carbon intensity. It is hard to imagine why anyone would adopt that method.",
      "A friend of mine joked a few years ago that one should invent \"green computing\" to fleece unsuspecting Fortune 500s and NGOs.Well..."
    ],
    "link": "https://greensoftware.foundation/articles/sci-specification-achieves-iso-standard-status/",
    "first_paragraph": "",
    "summary": "In a world where software bloat is the real carbon footprint, the Green Software Foundation proudly announces that hugging trees can now be certified through their revolutionary Software Carbon Intensity Specification, now an ISO standard. Tech enthusiasts and armchair environmentalists collide in a comment section more bloated than the web apps they love to hate, debating whether the answer to the climate crisis is simply blocking ads or maybe\u2014just maybe\u2014rewriting the entire internet in Rust. Amidst warnings of virtue-signaling bureaucracy and the shock revelation that yes, inefficient software might actually be bad for the environment, someone points out that maybe, just maybe, making software that doesn\u2019t suck could also save the polar bears. But why strive for efficiency or embrace the economic incentives of reducing carbon emissions when you can participate in endless debates about the real impact of carbon accounting methodologies on AWS's unpublished scope 3 emissions? After all, what's a little existential crisis among friends?"
  },
  {
    "title": "FC8 \u2013 Faster 68K Decompression (2016) (bigmessowires.com)",
    "points": 40,
    "submitter": "electricant",
    "submit_time": "2024-04-28T15:58:26",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=40189526",
    "comments": [
      "In \u201cfaster than memcpy\u201d series we have also Blosch for modern CPUshttps://www.blosc.org/pages/blosc-in-depth/I have never been able to use Blosch myself. But it sounds really interesting, outperforming RAM. Not sure what are the applications - columnar data processing, Parquet files, etc?",
      "Why doesn't BLOSC have a little chart comparing itself to LZ4, Zstd, etc? Kinda like this:https://stackoverflow.com/questions/37614410/comparison-betw...Because it seems like such a trivial chart to make.",
      "It gets used a fair amount in the weather data space. Forecasting and climate reanalysis grids are typically large (gigabytes) N-dimensional arrays of float32 values and Blosc provides enough tunable knobs that it's fairly easy to find a combination that performs acceptably without writing a bunch of custom handling code to keep track of which underlying compression schemes and settings were used. Additionally, it supports byte- and bit-shuffling filters which can really help boost the compressibility of certain data sets.",
      "I'm working on a game for Amiga (another 68k-based platform) and settled on ZX0 to decompress assets on the fly: https://github.com/einar-saukas/ZX0I was originally using LZ4, but I switched to ZX0 after learning that it can do in-place decompression, which means I don't have to allocate separate memory for the compressed data. I'm very happy with the compression ratio, and decompression of large assets (~48kb) only takes a few frames on a 7MHz 68000.Also of note is LZ4W, included in Sega Genesis Dev Kit (and discussed in the comments section of OP's article), a variant of LZ4 that only uses word-aligned operations. That makes it much faster on the 68000, which can struggle to efficiently handle 8-bit data. More info here: https://github.com/Stephane-D/SGDK/blob/master/bin/lz4w.txt",
      "Nice! When you say \u201con the fly\u201d are you literally decompressing assets from disk during gameplay? Can you do asynchronous IO on the Amiga?"
    ],
    "link": "https://www.bigmessowires.com/2016/05/06/fc8-faster-68k-decompression/",
    "first_paragraph": "",
    "summary": "In a thrilling display of \"I-knew-about-this-technology-before-it-was-cool,\" the hive mind at FC8 flutters about with the kind of zeal typically reserved for debates over the best Star Trek captain. One commenter, perhaps lost on their way to a more relevant discussion, waxes philosophical about Blosc\u2014the unsung hero of compressing weather data for those who absolutely *must* have their terabytes squeezed tighter than their grip on reality. Meanwhile, an Amiga enthusiast emerges from the depths of nostalgia, keen to share their groundbreaking discovery of ZX0's in-place decompression for asset management during those high-octane, 7MHz gaming sessions. The collective response is a robust echo of \"Nice!\" and a flurry of self-congratulatory pats on the back for being able to discuss arcane compression algorithms with the casual ease of ordering a latte at Starbucks."
  },
  {
    "title": "I Built an Ld_preload Worm (lcamtuf.substack.com)",
    "points": 24,
    "submitter": "zdw",
    "submit_time": "2024-04-29T20:23:32",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://lcamtuf.substack.com/p/that-time-i-built-an-ld_preload-worm",
    "first_paragraph": "",
    "summary": "In a stunning display of what happens when someone actually reads the manual, a blogger triumphantly recounts how they've harnessed the arcane powers of `ld_preload` to create a worm that's about as welcome as a skunk at a garden party. The tech glitterati, in their infinite wisdom, swarm the comments section like moths to a flame, offering a delightful mix of backhanded compliments and \"well, actually\" critiques that serve as a stark reminder of why we can't have nice things. As the post zigzags between technical bravado and an almost confessional tone, it becomes clear that the real epidemic isn't insecure software, but an unstoppable urge to broadcast every clever hack, no matter how mischievous it gets. Meanwhile, the commenters, ever eager to display their own superiority, manage to both miss the point and illustrate it perfectly, proving once again that the only thing tech enthusiasts love more than innovation is telling someone, anyone, that they're doing it wrong."
  },
  {
    "title": "Project Habbakuk: Britain\u2019s ice\u00a0\u201cbergship\u201d aircraft carrier project (2017) (99percentinvisible.org)",
    "points": 111,
    "submitter": "not_a_boat",
    "submit_time": "2024-04-29T15:35:31",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=40199679",
    "comments": [
      "The outcome of this project is an illustration of a rule of thumb from materials science:  many solid materials begin to lose their mechanical properties at about half their absolute melting point.  This is why (for example) ordinary steel should not be used above about 550 C; there's too much creep.",
      "I thought the \"failed\" result of the project was more due to economic factors, and the reduced need due to other actions in the war meaning a waterborne carrier was less useful. Not some issue with the mechanical properties of pykrete. If the tested properties were already past that 1/2 absolute temperature point and considered acceptable, it doesn't really matter what the behavior would have been at less than that.Did I read it wrong?",
      "As I understand it, they found out they needed to refrigerate the ice to keep it from creeping too much, and this increased the cost too much for the idea to work.",
      "Also what looks like insane scope creep.\"The full-sized ship would also need to have a range of 7,000 miles, support heavy bombers and be torpedo-proof. It was to be over a mile in length, weigh as much as 2.2 million tons and require as many as 26 electric motors to move and steer across the ocean.\"That's a crazy target growing out of \"cheap to produce aircraft carrier\"",
      "Yeah it\u2019s like a mobile midway pacific war at that point!Sailing midway all the way!",
      "Can you expand on this comment in the context of ice/water? It implies ice changes behavior at about 140K, but that isnt close to a phase change boundary, so what would you expect to be seeing here?",
      "The glass transition temperature of amorphous ice is approximately 140 K.",
      "At a guess, movement like that seen in glaciers and ice sheets?",
      "That's right -- plastic deformation under stress.",
      "This is that how jet fuel melted steel beams?"
    ],
    "link": "https://99percentinvisible.org/article/project-habbakuk-britains-secret-ice-bergship-aircraft-carrier-project/",
    "first_paragraph": "",
    "summary": "In an ambitious crossover episode between *Ice Road Truckers* and *World War II*, the brilliance of British engineering gives us Project Habbakuk: a testament to what happens when you ignore literally every practical consideration in favor of making a really *big* ice cube. Comments section warriors, armed with their honorary degrees from the University of Google, enthusiastically dive into the physics of why this frosty fiasco melted faster than their attention spans. Between arguing about the melting point of steel beams and the economic viability of essentially turning an iceberg into an aircraft carrier, it's clear that everyone missed the memo on practical wartime solutions. And yes, Jerry, we\u2019re all very impressed that you can use the word \"pykrete\" in a sentence, but that still doesn\u2019t make your idea for an ice cream truck/tank hybrid any more viable."
  },
  {
    "title": "Exploit.education (exploit.education)",
    "points": 12,
    "submitter": "udev4096",
    "submit_time": "2024-04-29T06:41:24",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://exploit.education",
    "first_paragraph": "exploit.education provides a variety of resources that can be used to learn\nabout vulnerability analysis, exploit development, software debugging, binary\nanalysis, and general cyber security issues.",
    "summary": "In the latest attempt to save the cyber world from its own demise, exploit.education gallantly offers a tutorial buffet for aspiring hackers and professional cybersecurity masochists alike. The site arrogantly assumes anyone can ascend from script kiddie to cyber deity with just a few clicks and a heavy dose of suffering. Meanwhile, the comment section quickly devolves into a digital Lord of the Flies, with \"experts\" viciously one-upping each other's understanding of stack overflows, as if the prize was anything more than fleeting internet smugness. Clearly, the real vulnerability being exploited here is the human ego."
  },
  {
    "title": "FireChat was a tool for revolution, then disappeared (fromjason.xyz)",
    "points": 147,
    "submitter": "evah",
    "submit_time": "2024-04-29T21:47:46",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=40204516",
    "comments": [
      "Hi there, developer of Newnode here, a successor of FireChat founded by two of the same people (https://www.newnode.com/). We now provide both, a VPN and a Messenger, with purpose to help people evade censorship and enable device-to-device connectivity. You can find the source code at https://github.com/clostra/newnode",
      "I downloaded your app just now from App Store.When I get to the phone number step, it briefly shows a captcha screen but then transitions to the phone number screen.When I enter my phone number. Country code +47. I don\u2019t get any sms at all.When I switch to the sms app to see if an sms arrived (it didn\u2019t), and I switch back to your app, the counter on the screen that is counting down to allow resending code resets to 00:59 although it was at like 00:30 when I switched away from the app.When after waiting for another full minute and occasionally touching the screen to prevent it from locking I am presented with the following options when clicking \u201cI didn\u2019t get a code\u201d:- Contact NewNode Support- Resend code- Call me instead- CancelI tried resend code. No code arrives still.Great, now I have to wait another full minute with your app in focus before I can try another option.After waiting another full minute, I click \u201ccall me instead\u201d. No call comes.",
      "I'm just curious, why not build ontop of another app like Signal?[0] My understanding is that there's nothing stopping anyone from using the same app and creating their own server and nodes. My understanding is that you can even hook into multiple nodes with a custom fork of the app. Wouldn't this give a big advantage of not requiring people to have a whole new app and you can work synergistically with a company with similar/compatable goals?The thing I see is that if you really want to make a huge P2P network, you need a reason to have the app installed for reasons other than P2P. The problem I've always seen with FireChat was that I'd never get anyone to talk to me and then when there was an emergency no one would be able to download. So we need to have the features built into something with more normal day-to-day utility.[0] https://community.signalusers.org/t/signal-airdrop/37402",
      "I think the obvious question is: can you shed any light or provide context on why the service was shut down?And if not, it seems unrealistic to expect people to adopt / trust your alternative.",
      "Is it open source? Will it one day mysteriously disappear from the internet too?",
      "Is the new product funded by In-q-tel as well?",
      "Can you go into any additional detail about why firechat shut down?",
      "Well now we can put the speculation to rest, exactly how many CIA agents visited to shut down your radical operation??",
      "The other option given the surprisingly one-sided list of protest movements they brag about: how many CIA agents were involved in its genesis?",
      "> Then, one day in February 2020, as COVID-19 swept the globe, access to FireChat was completely cut off without explanation.If it could be shut off from one place like that, it doesn't sound very \"decentralized\".  Anyway, are there significant obstacles to re-implementation?Someone above mentioned an alternative that uses LoRa.  That's nice but it sounds like the attraction of Firechat was that it used ordinary phones that everyone already has.  LoRa by comparison is special hardware that is already a bit suspicious.If you're willing to use special purpose radios and live with low bandwidth text communication, you can do quite a bit better than LoRa, such as with JS8CALL and HF radios.  But, a sad \"theorem\" tells us that any communications medium will be beaten into carrying video...."
    ],
    "link": "https://www.fromjason.xyz/p/notebook/firechat-was-a-tool-for-revolution-then-it-disappeared/",
    "first_paragraph": "For years, FireChat helped people circumvent their internet gatekeepers\u2014 the authoritarian governments and spineless corporations that control our every move through a network of proprietary data centers and deep-sea cables.",
    "summary": "In a shocking turn of events that nobody could have predicted, FireChat, the once hailed savior of free speech and nightmare of \"authoritarian governments and spineless corporations,\" has slid into the digital abyss, leaving behind a wake of disillusioned would-be revolutionaries. Meanwhile, in the comment section, hopefuls cling to the wreckage by discussing its successor, Newnode, with the fervor of a ham radio operator during the apocalypse. One commentator, after a thrilling saga of failed SMS verification attempts, questions the existential reasoning behind not leeching off Signal's user base, ignoring the fact that convincing people to install another \"revolutionary\" app is as hard as teaching a cat quantum physics. Another braves the wilds of open-source lore to ask if this digital phoenix rising from FireChat's ashes is also destined to vanish into the CIA's Bermuda Triangle, while enthusiasts debate the merits of switching to radio waves as if preparing for the world's most underwhelming spy movie."
  },
  {
    "title": "Memary: Open-Source Longterm Memory for Autonomous Agents (github.com/kingjulio8238)",
    "points": 179,
    "submitter": "james_chu",
    "submit_time": "2024-04-29T11:12:51",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=40196879",
    "comments": [
      "This seems like its overloading the term knowledge graph from its origins. Rather than having information and facts encoded into the graph, this appears to be a sort of similarity search over complete responses. It's blog style \"related content\" links to documents rather than encoded facts.Searching through their sources, it looks like the problem came from Neo4j's blog post misclassifying \"knowledge augmentation\" from a Microsoft research paper with \"knowledge graph\" (because of course they had to add \"graph\" to the title).This approach is fine, and probably useful but its not a knowledge graph in the sense that its structure isn't encoding anything about why or how different entities are actually related. A concrete example in a knowledge graph you might have an entity \"Joe\" and a separate entity \"Paris\". Joe is currently located in Paris so would have a typed edge between the two entities of something like \"LocatedAt\".I didn't dive into the code but what I inferred from the description and referenced literature, it is instead storing complete responses as \"entities\" and simply doing RAG style similarity searches to other nodes. It's a graph structured search index for sure but not a knowledge graph by the standard definitions.",
      "Exactly. Glad to see this. I do think knowledge graphs are important to AI assistants and agents though and someone needs to build a knowledge graph solution for that space.The idea of actual entities and relationships defined like triples with some schema and appropriately resolved and linked can be useful for querying and building up the right context. It may even be time to start bringing back some ideas from the schema.org back the day to standardize across agents/assistants what entities and actions are represented in data fed to them.",
      "Yeah, one of the specific things I'd love to do is collaboratively bulking up WikiData more. It's missing a ton of low hanging fruit that people using an ML augmented tool could really make some good progress on, similar to ML assisted OpenStreetMapping work",
      "Yeah precisely. Knowledge graphs are simple to think about but as soon as you look into them you realize all the complexity is in the creation of a meaningful ontology and loading data into that ontology. I actually think LLMs can be massively useful for building up the ontology but probably not in the creation of the ontology itself (far too ambiguous and large/conceptual task for them right now).",
      "How do we build ontology using LLMs? Will the building blocks be like the different parts of a brain?\nP.S I am assuming that by \"creation of ontology itself\" means creation of AGI.",
      "Ontologies are just defining what certain category, words, and entity types mean. Commonly used in NLP for data representation (\u201cfacts\u201d/triples/etc.) in knowledge graphs and other places where the definition of an ontology helps provide structure.This doesn\u2019t have anything to do with AGI or brains. They are typically created or tuned by humans and then models fit/match/resolve entities to match the ontology.",
      "@dbish nailed it, but I can give you a bit more concrete example. Continuing off the light example I started off with. An ontology for knowing what city a person currently is in. We have two classes of entities, a person, and a city. There is a single relationship type \"LocatedAt\" you can add and remove edges to indicate where a person is and you can construct some verification rules such as \"a person can only be in one city at time\".To have an LLM construct a knowledge graph of where someone is (and I know this example is incredibly privacy invasive but its a simple concrete example not representative). Imagine giving an LLM access to all of your text messages. You can imagine giving it a prompt along the lines of \"identify who is being discussed in this series of messages, if someone indicates where they are physically located report that as well\" (you'd want to try harder than that, keeping it simple).You could get an output that says something like `{\"John Adams\": \"Philadelphia, PA, US\"}`. If either the left or right side are missing create them. Then remove any LocatedAt edges for the left side and add one between these two entities. You have a simple knowledge graph.Seems easy enough, but try to ask slightly harder questions... When did we know John Adams was in Philadelpha? Have they been there before? Where were they before Philadelphia? The ontology I just developed isn't capable of representing that data. You can of course solve these problems and there are common ontological patterns for representing it.The point is, you kind of need to know the kind of questions you want to ask about your data when you're building your ontology and you're always going to miss something. Usually you find out the unknown questions you want to ask of the data only after you've already built your system and started asking it questions. It's the follow-ups that kill you.There has been a lot of work on totally unstructured ontologies as well, but you're moving the hard problem elsewhere not solving it. Instead of having high quality data you can't answer every question with, you have arbitrary associations that may mean the same thing and thus any query you make is likely _missing_ relevant data and thus inaccurate.Huge headache to go down, but honestly I think it is a worthwhile one. Previously if you changed your ontology to answer a new question, a human would have to go through and manually and painstakingly update your data to the new system. This is boring, tedious, easy-to-get-wrong-due-to-inattention kind of work. It's not complex, its not hard, its very easy to double check but it does require an understanding of language. LLMs are VERY capable of doing this kind of work, and likely more accurately.",
      "Yes! My position on KGs largely flipped post GPT-3. Before KGs were mostly a niche thing given the cost vs rewards, and now they're an everyone thing.- The effort needed for KGs has higher general potential value because RAG is useful- The KG ontology quality-at-scale problem is now solvable by LLMs automating index-time data extraction, ontology design, & integrationThis is an area we're actively looking at: Self-determining ontologies for optimizing RAG, such as during evolving events, news, logs, emails, customer conversations. We have active projects across areas here already like for emergency response, cyber security, news mining, etc. If folks are interested here, we're definitely looking for design partners with challenging problems on it. (And, looking to hire a principal cybersecurity researcher/engineer on it, and later in our other areas too!)",
      "Are graph databases really relevant during retrieval? Does anyone use them to augment a vector store as a candidate source?",
      "We're using in a large engineering use-case, 100s of millions of objects - and it almost doubled the nDCG@10 score vs pure vector search"
    ],
    "link": "https://github.com/kingjulio8238/memary",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In an ambitious attempt to redefine confusion, a blog post under the guise of \"Memary: Open-Source Longterm Memory for Autonomous Agents\" manages to muddle the definition of a knowledge graph so thoroughly that one wonders if it's actually a satirical piece on the state of AI research. The comments quickly transform into a battleground where semantics go to die, as enthusiasts and skeptics alike dig trenches around the proper use of \"knowledge graph\" versus a glorified similarity search machine. One commenter, in a heroic feat of missing the point, embarks on a quest to resuscitate schema.org from its peaceful slumber to standardize... something. Meanwhile, another eager soul fantasizes about LLMs solving the enigma of ontology creation, blissfully ignoring their propensity for generating creative nonsense. Amidst this existential discourse, a job listing for a cybersecurity engineer appears like a beacon of hope, or perhaps a cry for help, reminding us that regardless of whether we're building AI to save the world or just to argue about semantics, someone still needs to keep the lights on."
  },
  {
    "title": "Show HN: Kaytu \u2013 Optimizing cloud costs using actual usage data (github.com/kaytu-io)",
    "points": 72,
    "submitter": "acx1729",
    "submit_time": "2024-04-29T15:57:03",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=40199972",
    "comments": [
      "What is the server component? Why is there a server component? What data does this send to the server?",
      "lol. that\u2019s your first comment for  cloud run software? who paid you?",
      "No one's paid me, seems like a neat tool. I just prefer to know what data is being sent to servers I don't control.",
      "It\u2019s a valid question. Trusting your business resources to a tool is not a decision you make lightly.",
      "I appreciate the skepticism, however :)We will release the open source server-side - promise :)With three developers and the lookup involving 1M+ possible entries, we are trying to find an effective method.",
      "Hi, Deadbunny. The server component is being open-sourced, and we will release it in 3 weeks.https://github.com/kaytu-io/kaytu/blob/main/pkg/api/wastage/... has the data (it's just sizing info).It's the sizing metadata that is being sent.",
      "How do you compare against Vantage, Dashdive, Infracost, Usage, etc.?",
      "Always looking to conserve cloud spend, I'll give this a shot.Heads up- there's an error on your landing page's Calendly link. \"This Calendly URL is not valid.\"",
      "Cool, Any chance on GCP integration?",
      "hi dddw, we would love to. We are looking for someone who has $100K+ spend monthly on VMs and BigQuery on GCP. If you have or know someone, we can bang this out quickly."
    ],
    "link": "https://github.com/kaytu-io/kaytu",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In the latest HN circus show, a brave soul introduces \"Kaytu,\" a cloud penny-pincher's dream, promising to slash those pesky bills by actually looking at what you use. Responses from the crowd range from paranoid tech bros questioning the existence of a server component (\"What sorcery requires a server?\") to would-be customers tripping over each other's ethical standards while drooling over potential savings. Amidst this, the creators play a tantalizing game of open-source peek-a-boo, promising all the transparency... eventually. In the grand tradition of bikeshedding, the most critical issue identified is a broken Calendly link. Because, clearly, the ability to schedule a call is the linchpin of cloud cost optimization."
  },
  {
    "title": "Husband and wife outed as GRU spies aiding bombings and poisonings across Europe (theins.ru)",
    "points": 549,
    "submitter": "dralley",
    "submit_time": "2024-04-29T15:00:15",
    "num_comments": 331,
    "comments_url": "https://news.ycombinator.com/item?id=40199193",
    "comments": [
      "> In both cases, she used sophisticated tradecraft that sought to leave no trace in databases accessible to European authorities. For instance, she booked her trips and bought her plane tickets using her Czech passport, registering only that nationality with the Greek airline. But upon crossing the Russian border, \u0160apo\u0161nikova used her secret Russian passport, thus bypassing the need to obtain a Russian visa issued to her as a Czech citizen and eliding the digital footprint associated with the relevant application.Wouldn't the Greek airline (and Greek exit immigration control) check for a valid Russian visa for a Czech passport? They had to put the exit stamp on _something_?Or did she use the Russian passport, but then wouldn't that get scanned into the system?",
      "The airline only needs to be physically shown a passport which will get the person into the destination: they don\u2019t record it. So you book with the Czech passport, go through border control with the Czech passport, then show the airline staff the Russian passport.",
      "At least some of them certainly do record it. I have multiple passports and have had difficulties several times with an airline that has one of my passports listed, that doesn't show the right of entry to the other country and which I was not intending to use for that flight. Moreover, if you are travelling on a codeshare flight, for instance, all airlines involved will record your travel documents, but only the airline operating the flight will be able to make updates to them.edit: they certainly do not anywhere rely on eyeballing a piece of paper and letting some random staff member say \"yup looks legit\".",
      "Is there a conflict between (a) wanting to exit a country using the passport you entered it on*, and (b) wanting to enter a flight using the passport you will exit on?* don't want to be recorded as an overstayer",
      "At the airports I've done this, they are separate checkpoints. e.g departing Australia for the US:\n1. check in with the airline using your US passport. If it is the return leg of a flight from the US, you probably put in your Australian passport details when booking so you may need to go through an agent.\n2. go through Australian border control checkpoint, and show your Australian passport\n3. board the plane showing your US passport if asked for ID.",
      "That's a definite maybe!Some countries make the airline need to know in advance whether to let you even board. Say the US with the electronic visaless authorization. You gotta give the airline your ESTA. Canada wants to know as well. So you need to have your PR card or Canadian passport on file or the electronic authorization.But that doesn't stop you from entering Europe on a European passport. You can have your say Canadian passport on file and fly out on that. At the destination you show your European passport (smaller line ups and basically you are just waived through). You never show the Canadian passport in the EU on arrival. You have the Canadian one on file and show that when leaving so they let you board.  Back in Canada you use your Canadian passport to enter.This way you never get any visa stamps and you \"fly through passport control\" on either end.So yeah, even if this is maybe used by \"sleeper agents\" it's also just normal for dual citizens.",
      "To the extent that the airlines care, anyone who cannot enter the country is deported at the airline's expense. So where it matters they do care.This is not really new. Back when steamship was the most common way of traveling between continents, the ocean lines gave lessons to the third-class passengers on what to say to US immigration officials so that they wouldn't have to pay for their lodging and return if they got denied entry. (At the time first-class passengers were not screened this way, so they didn't bother teaching them.)",
      "That makes a lot of sense! Incentives!Also a nice fun fact about steamships I did not know. Love learning little tidbits like that!",
      "I may be wrong, but every time you do a check-in you are giving your information and they record it, and you have to show the same document at boarding time.This doesn't mean that it must be the same document that you used to enter or exit the country though, although depending on the destination the airline may require proof that you can enter the destination country, like a visa or a passport, because having a passenger refused entry may be an hassle for them.You can use a document to exit the departure country, another for the airline (with the caveat above) and another one for the destination, even with different names on them.",
      "You would should both to the airline staff. \"This is my ID I used, and this is my ID which allows me entry so you(the airline) won't have to bring me all the way back to my arrival point.\" Airline staff will not record this second form of ID: it's only to show the staff for a moment."
    ],
    "link": "https://theins.ru/en/politics/271205",
    "first_paragraph": "",
    "summary": "Welcome to another episode of \"Spies R Us: Bumbling Through Borders,\" where the thrilling world of espionage meets the thrilling incompetence of internet commentators. Our star couple, the \u0160apo\u0161nikovas, charmed their way across Europe with a handful of passports and a dream, dodging digital footprints with the finesse of drunk ballerinas. Meanwhile, our intrepid commenters, armed with their vast (non-)experience in international travel, spy games, and Google searches, dissect their methods with the precision of a toddler performing surgery with a spoon. In one corner, we have amateur travel agents debating the nuances of passport presentations and airline policies with the fervor of sports fans arguing about a game they didn't watch. In the other, history buffs offering unsolicited trivia about steamship travel as if it's the missing piece of the puzzle. Together, they form the perfect storm of the blind leading the blind through the complex world of international espionage, leaving no stone unturned, no fact checked, and no stereotype about dual citizens unexplored."
  },
  {
    "title": "Haunting Sounds from the Largest Living Thing (sciencealert.com)",
    "points": 14,
    "submitter": "neverenginelabs",
    "submit_time": "2024-04-29T05:44:43",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=40194868",
    "comments": [
      "Ugh, what a load of LLM. Here's the real link:https://www.ecosystemsound.com/beneath-the-tree",
      "I was hoping this would be about Oregon\u2019s \u201chumongous fungus\u201d, which would seem to be larger than this in every metric (3.5 square miles vs 1.5, 7,500+ tons vs ~6,000). Unfortunately it makes for a far less impressive tourist attraction than it sounds \u2013 especially when you drive 8 hours to get there just for it to be covered in a foot of snow! (still a beautiful trip do not regret in any way, a must for anyone in PNW).https://www.sciencealert.com/haunting-sounds-made-from-the-w...",
      "I was hoping this would be about Oregon\u2019s \u201chumongous fungus\u201d, which would seem to be larger than this in every metric (3.5 square miles vs 0.15; 7,500+ tons vs ~6,000). Unfortunately it makes for a far less impressive tourist attraction than it sounds \u2013 especially when you drive 8 hours to get there just for it to be covered in a foot of snow! (still a beautiful trip I do not regret in any way, a must for anyone in PNW).https://thatoregonlife.com/2020/01/largest-organism-humongou..."
    ],
    "link": "https://www.sciencealert.com/haunting-sounds-made-from-the-worlds-largest-living-thing-recorded",
    "first_paragraph": "",
    "summary": "In the latest echo chamber reverberation, ScienceAlert decides that eerie noises from nature are *the* groundbreaking news we've been waiting for, supposedly due to the cacophony produced by \"the largest living thing.\" Cue the collective eyeroll of the internet as one commenter attempts to hijack the narrative with a link they swear is *actually* interesting, while another laments their missed opportunity to discuss something even bigger (and, predictably, more fungus among us). The real horror, however, isn't the haunting sounds or the fungal comparisons; it's the thought of driving 8 hours just to see your destination buried under snow. Nonetheless, the sheer dedication to see a sub-par tourist attraction in the Pacific Northwest has inadvertently become the most thrilling part of this saga. \ud83c\udf44\ud83c\udfb6"
  },
  {
    "title": "The Pen, Mightier (themillions.com)",
    "points": 3,
    "submitter": "scour",
    "submit_time": "2024-04-28T15:34:22",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://themillions.com/2011/02/the-pen-mightier.html",
    "first_paragraph": "",
    "summary": "In a stunning display of relevance, themillions.com publishes yet another think piece on why the pen isn't just mightier than the sword, but also apocalyptic if it runs out of ink. The prose meanders like a retiree in a garden, discovering the lost art of handwriting in between sips of tea and bouts of nostalgia. Commenters, in an astonishing feat of missing the point, wage a keyboard war about their preferred brand of pens, turning the comment section into an unholy battleground over ballpoint superiority. Truly, a testament to how the written word can ignite the fiercest of passions, especially when it's about absolutely nothing of consequence."
  },
  {
    "title": "Debugging Tech Journalism (asteriskmag.com)",
    "points": 24,
    "submitter": "ra7",
    "submit_time": "2024-04-29T18:05:18",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://asteriskmag.com/issues/06/debugging-tech-journalism",
    "first_paragraph": "A huge proportion of tech journalism is characterized by scandals, sensationalism, and shoddy research. Can we fix it?",
    "summary": "In a heroic revelation, <em>asteriskmag.com</em> blasts the doors wide open on tech journalism's worst-kept secret: it's sometimes sensationalist and occasionally low on the research. Fans of the groundbreaking \"water is wet\" and \"the sky is often blue\u201d studies will find themselves riveted. Meanwhile, in the sprawling metropolis of the comments section, a cohort of armchair experts and would-be Woodward and Bernsteins pledge to rise above the mire by continuing to do exactly what they've always done: nothing. Tech journalism, much like Schr\u00f6dinger's cat, is simultaneously dead and alive in the minds of those who can't stop reading about the next big scandal over their morning coffee."
  }
]