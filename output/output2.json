[
  {
    "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison (arxiv.org)",
    "points": 78,
    "submitter": "matt_d",
    "submit_time": "2024-09-06T22:09:14.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41470074",
    "comments": [
      "This paper is light on background so I\u2019ll offer some additional context:As early as the 90s it was observed that CPU speed (FLOPs) was improving faster than memory bandwidth. In 1995 William Wulf and Sally Mckee predicted this divergence would lead to a \u201cmemory wall\u201d, where most computations would be bottlenecked by data access rather than arithmetic operations.Over the past 20 years peak server hardware FLOPS has been scaling at 3x every 2 years, outpacing the growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and 1.4 times every 2 years, respectively.Thus for training and inference of LLMs, the performance bottleneck is increasingly shifting toward memory bandwidth. Particularly for autoregressive Transformer decoder models, it can be the dominant bottleneck.This is driving the need for new tech like Compute-in-memory (CIM), also known as processing-in-memory (PIM). Hardware in which operations are performed directly on the data in memory, rather than transferring data to CPU registers first. Thereby improving latency and power consumption, and possibly sidestepping the great \u201cmemory wall\u201d.Notably to compare ASIC and FPGA hardware across varying semiconductor process sizes, the paper uses a fitted polynomial to extrapolate to a common denominator of 16nm:> Based on the article by Aaron Stillmaker and B.Baas titled \u201dScaling equations for the accurate prediction of CMOS device performance from 180 nm to 7nm,\u201d we extrapolated the performance and the energy efficiency on a 16nm technology to make a fair comparisonBut extrapolation for CIM/PIM is not done because they claim:> As the in-memory accelerators the performance is not based only on the process technology, the extrapolation is performed only on the FPGA and ASIC accelerators where the process technology affects significantly the performance of the systems.Which strikes me as an odd claim at face value, but perhaps others here could offer further insight on that decision.Links below for further reading.https://arxiv.org/abs/2403.14123https://en.m.wikipedia.org/wiki/In-memory_processinghttp://vcl.ece.ucdavis.edu/pubs/2017.02.VLSIintegration.Tech...\n \nreply",
      "Curious if anyone is making AccelTran ASICs?\n \nreply",
      "I'm unfamiliar; in this context is \"in-memory\" specialized hardware that combines CPU+RAM?\n \nreply",
      "I'd expect it to be MAC hardware embedded on the DRAM die (or in the case of stacked HBM, possibly on the substrate die).To quote from an old article about such acceleration which sees 19x improvements over DRAM + GPU:   Since MAC operations consume the dominant part of most ML workload runtime,\n   we propose in-subarray multiplication coupled with intra-bank accumulation.\n   The multiplication operation is performed by performing AND operations and\n   addition in column-based fashion while only adding less than 1% area overhead.\n\nhttps://arxiv.org/pdf/2105.03736\n \nreply",
      "In-mem (generally) means no (re)loading of data from a storage device.\n \nreply",
      "Sure, but I don't think that makes sense here; when I run an LLM on CPU, I load to memory and run it, when I run on GPU I load the model into the GPU's memory and run it, and I don't have anything like that much money to burn but I imagine if I used an FPGA then I would load the model into its memory and then run it from there. So the fact that they're saying \"in-memory\" in contrast to ex. GPU makes me think that they're talking about something different here.\n \nreply",
      "In-memory sounds like the way to go not just in terms of performance, but in that it makes no sense to build an ASIC or program an FPGA for a model that will most likely be obsolete in a few months at best if you're lucky.\n \nreply",
      "https://arxiv.org/pdf/2402.09709\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2409.03384",
    "first_paragraph": "This week: the arXiv Accessibility ForumHelp | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "This week on arXiv, scholars blessed us with \"Hardware Acceleration of LLMs: A comprehensive survey and comparison,\" an article so in love with acronyms and niche tech that even seasoned engineers need a CliffsNotes version. The comment section turned into an impromptu lecture on in-memory processing, reminiscing about the '90s and the grand ole \"memory wall\" with the clarity of a foggy night. One brave soul ventured to ask about \"AccelTran ASICs\" only to spiral down a rabbit hole led by a commentariat thrilled to flex obscure hardware knowledge and avoid the original point. Meanwhile, another commenter is still scratching their head trying to align \"in-memory\" with something other than their next Jimmy Johns order. \ud83e\udd2f\ud83d\udd27"
  },
  {
    "title": "QtCS2024 Compile once. Run everywhere (qt.io)",
    "points": 22,
    "submitter": "mmphosis",
    "submit_time": "2024-09-06T23:55:12.000000Z",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=41470571",
    "comments": [
      "Made possible using Cosmopolitan Libc.Justine writes some pretty cool software.https://justine.lol/cosmopolitan/index.html\n \nreply",
      "That she does. Just don't Google her name + \"slavery\".\n \nreply",
      "When i googled, it was very difficult to find anything in its original context, just some quotes, that certainly sound nutty on their face but also are very short to the point it is unclear if she was quoted fairly.\n \nreply",
      "The page 8 (\"Building\") of the slides has the badger picture to the right. His right hand has some weird \"nails\". Another example of AI-generated image.\n \nreply",
      "Key point:> running with the vnc QPAThe demo they have running has no native display or input support; it just serves the interface over a socket via VNC.\n \nreply"
    ],
    "link": "https://wiki.qt.io/QtCS2024_Compile_once._Run_everywhere",
    "first_paragraph": "Compiling and deploying of C++ applications on Windows, Linux, macOS for x86_64 and arm64 can be challenging. By using Cosmopolitan Libc we could have an alternative.\nSlides at 2024.09.06/QtCS2024-CompileOnce-RunEverywhere.pdf \u00b7 GitLab\nCristian Adam\nCristian Adam, a member of the Qt Creator team, presented a talk on \"Compile Once, Run Everywhere\" using Cosmopolitan libc for C++ applications. \nKey points include:\n",
    "summary": "**QtCS2024: The Holy Grail of Laziness or A Compiler's Daydream?**\n\nToday in tech, Cristian Adam unveils his \"revolutionary\" idea of using Cosmopolitan Libc to *Compile Once, Run Everywhere*, because apparently, coding separately for different systems is <em>sooooo</em> 2023. The online crowd goes wild, seeing this as either the second coming of coding Jesus or a sneaky way to make developers even lazier. One comment enlightens us on Justine's *cool software*, leading to a bizarre detour into character assassination, out-of-context quotations, and AI-generated badgers with questionable nails. Remember kids, in the world of tech, it's not about the quality of your code, but how effectively you can shield your eyes from the flashy gimmicks and needless controversies!"
  },
  {
    "title": "Show HN: Wealthfolio: Private, open-source investment tracker (wealthfolio.app)",
    "points": 615,
    "submitter": "a-fadil",
    "submit_time": "2024-09-06T12:56:15.000000Z",
    "num_comments": 182,
    "comments_url": "https://news.ycombinator.com/item?id=41465735",
    "comments": [
      "I strongly encourage you to charge (lots) of money ASAP. I love the open source, offline, rent-free ethnos but also if you've built something truly valuable, charge money for it. Donations don't count as a viable monetization strategy.Even if the software is free and you're just offering $500/hour consulting as an add-on to the software, that helps me trust the project has sticking power.P.S. I think Tauri is such a cool framework and a delight to use. Rust's approach to platform-specific code is so much saner than anything I've tried previously.\n \nreply",
      "First question from reading through the landing page is about this part:> Import your statements from your broker or bank.Exactly what brokers/banks that are supported should be listed somewhere and linked here, as that's a \"make or break\" feature for a lot of people I bet. Not much point in replacing my homegrown \"Banks CSV export -> Data processing > Import into spreadsheet\" workflow unless I just replace that last step but the previous ones remain the same.\n \nreply",
      "As an avid, daily Quicken user, yes, seamless integration with financial institutions is my #1 requirement. I am not willing to manually navigate a dozen banks' broken UIs to find their \"download CSV\" option, hope it works, download a bunch of files to my computer, and then hope that they can be imported into my application--and then repeat every day when I update.I have in the past switched physical banks purely because their integration was either terrible or not working and I refused to go the \"download CSV\" route.Unfortunately some banks are starting to drop support for applications directly connecting to them, and moving to an unacceptable model where intermediaries like Intuit's servers have to do the communication and store your credentials. This has been getting noticeably shittier in the last couple of years.My #2 requirement (a close second) is that the application must be running on my local PC. I will never accept a cloud-based web-app or something I have to host on a VPS and access through some dinky HTML/JS UI.\n \nreply",
      "Interesting perspective because my #1 requirement is that no 3rd party gets financial login credentials at all.  I'm willing to do CSVs in order to not compromise on security, although the experience most certainly is bad.\n \nreply",
      "> I am not willing to manually navigate a dozen banks' broken UIs to find their \"download CSV\"> My #2 requirement (a close second) is that the application must be running on my local PC. I will never accept a cloud-based web-appYou're lucky you don't live in the EU since well then you are straight out of luck since the bank APIs are only available to commercial entities thus the software generally is in the cloud and costs money.\n \nreply",
      "Banks in Germany offer access to consumers via the HBCI standard.\nNot sure about the rest of the EU.\n \nreply",
      "Banks in Germany provide it because of EU regulation.https://www.digiteal.eu/open-banking-apis-all-you-need-to-kn...\n \nreply",
      "This sounds illegal and against what GDPR stands for.\n \nreply",
      "Why is accessing your own banking data through a standard against what GDPR stands for?  GDPR has a right to data portability.\n \nreply",
      "I miss interpreted. I thought someone else can gain access to consumer data.\n \nreply"
    ],
    "link": "https://wealthfolio.app",
    "first_paragraph": "",
    "summary": "**Title:** Wealthfolio: For Those Who Mistake Complexity for Security\n\n**Summary:**\nA fresh software gig, \"Wealthfolio,\" appears to crack the code of tracking investments privately, seemingly unaware that their target audience of spreadsheet fetishists won't pay for open-source bread if they've only tasted free circuses. Meanwhile, in the comment rodeo, an open-source aficionado breaks character, clamoring for the developers to charge \"a lot\" for their software unless they fancy subsisting on goodwill and fresh air. Elsewhere, a Quicken warrior battles the vile UIs of the banking world, weeping for the golden age of direct imports without ever getting their armor dirty with CSV files. Ah, the modern-day heroes who'd rather storm out of a bank than click 'download.' Truly, we live in a society. \ud83d\ude43"
  },
  {
    "title": "Show HN: Infinity \u2013 Realistic AI characters that can speak",
    "points": 240,
    "submitter": "lcolucci",
    "submit_time": "2024-09-06T16:47:04.000000Z",
    "num_comments": 176,
    "comments_url": "https://news.ycombinator.com/item?id=41467704",
    "comments": [
      "https://6ammc3n5zzf5ljnz.public.blob.vercel-storage.com/inf2...It\u2019s astounding that 2 sentences generated this. (I used text-to-image and the prompt for a space marine in power armour produced something amazing with no extra tweaks required).\n \nreply",
      "As soon as I saw the \"Gnome\" face option I gnew exactly what I gneeded to do: https://6ammc3n5zzf5ljnz.public.blob.vercel-storage.com/inf2...EDIT: looks like the model doesn't like Duke Nukem: https://6ammc3n5zzf5ljnz.public.blob.vercel-storage.com/inf2...Cropping out his pistol only made it worse lol: https://6ammc3n5zzf5ljnz.public.blob.vercel-storage.com/inf2...A different image works a little bit better, though: https://6ammc3n5zzf5ljnz.public.blob.vercel-storage.com/inf2...\n \nreply",
      "Fixed Duke Nukem: https://youtu.be/mcLrA6bGOjY\n \nreply",
      "Haha I almost wake up my kid with my sudden laugh!\n \nreply",
      "This is why we do what we do lol\n \nreply",
      "There is prior art here, e.g. Emo from alibaba research (https://humanaigc.github.io/emote-portrait-alive/), but this is impressive and also actually has a demo people can try, so that's awesome and great work!\n \nreply",
      "This is my favorite: https://6ammc3n5zzf5ljnz.public.blob.vercel-storage.com/inf2...\n \nreply",
      "FYI dang they kinda ripped off our product down to copying the UI (Hedra.com). Our model is about 12x faster and supports 4 minute long videos\u2026\n \nreply",
      "Love this one as well. It's a painting of Trithemius, a German monk, who actually said that\n \nreply",
      "Although I assume he didn't say it in British English ;-)\n \nreply"
    ],
    "link": "item?id=41467704",
    "first_paragraph": "",
    "summary": "**The Brave New World of AI That Barely Works**\n\nIn a stunning display of technological mediocrity, Hacker News showcases \"Infinity\" \u2014 the latest AI that can allegedly create realistic characters who speak, if your definition of \"realistic\" stretches to include zombie-eyed Gnomes and botched Duke Nukems. One user, flailing in the depths of AI-generated nonsense, fixes Duke by linking a YouTube video, presumably because the AI couldn\u2019t handle a pixelated 90s action hero. Amidst the self-congratulatory echo chamber, another user cries foul, claiming their product was ripped off, right down to the buggy UI. But don't worry, there\u2019s always a side-splitter browsing these geniuses at work \u2014 someone almost woke their kid up laughing. Oh, the hilarity of low expectations! \ud83e\udd16\ud83d\ude02"
  },
  {
    "title": "Show HN: Using SQL's Turing completeness to build Tetris (github.com/nuno-faria)",
    "points": 176,
    "submitter": "nffaria",
    "submit_time": "2024-09-04T12:28:20.000000Z",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41444855",
    "comments": [
      "It seems at first to be a toy or silly intellectual exercise, but after reading the whole thing it really feels like an example of how constraints can lead to creative solutions. Can't log to stdout in the recursive CTE's loop? Maybe `RAISE NOTICE` will work. Can't take user input from the query itself? What if we stored the input in a table locally and read from that instead with `dblink`?It's just a lot of fun, kudos for hacking this together, this is the sort of thing that makes me love software so much.\n \nreply",
      "Thanks for the kind words, it was exactly like you described. Many times I thought it would not be possible after hitting some of those walls, but luckily there was always a way around them.\n \nreply",
      "I once wrote a top-like tool in Oracle's sqlplus client, that is not designed for building self-refreshing terminal UI display apps. Just to see if I could do it, had to get creative too. Used pipelined PL/SQL functions with never-ending output stream and a sleep function within it and had to carefully match the sqlplus \"fetch array size\" with number of rows returned in a batch from the pipelined function. Called it MOATS - the Mother of All Tuning Scripts - and then someone took the idea further and built v2.0 with added colors and charts, etc:The v2.0 UI GIF is here: https://github.com/dbsid/moats_rac\n \nreply",
      "I will admit that in the past I've used `RAISE NOTICE` quite frequently for debugging difficult to navigate PL/pgSQL procedures.\n \nreply",
      "Cool project! I remember I had coded a Reinforcement Learning (RL) assignment long ago back in college with just SQL (I was familiar with Oracle back then, so that's what I had used). The course instructor was amused, more so when he saw how loops were implemented: I had a \"loop\" table with a sequence of N numbers in a column, and used to join with it to \"loop\" N times!\n \nreply",
      "This is hilarious and amazing.  But moreso than most such cool hack projects, it has a great writeup.  The author really did a great job walking through how it worked.  Love it.\n \nreply",
      "This is great but even more impressive than the code is all the documentation and explanation of how it works. Well done!\n \nreply",
      "Wow this is amazing. Makes me realize how elementary my SQL skills are.\n \nreply",
      "This is really really cool. Very cool work and welcome to HN!\n \nreply",
      "This is awesome. I did linear regression in T-SQL once and it's a fun way to figure out what you can do with the language (eg - if you're unfamiliar with CTEs or cursors).I'll definitely be checking this out later. Thanks for the post!\n \nreply"
    ],
    "link": "https://github.com/nuno-faria/tetris-sql",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Using SQL's Turing Completeness to Build Tetris\n      A complete implementation of Tetris in a SQL query.(The initial delay is caused by Postgres optimizing the plan with JIT compilation. This can be disabled with SET jit = off.)Requirements:Installing everything with Docker:In one terminal, run the input.py script:If using Docker:If running Python locally:In another terminal, run the Tetris query:Switch to the input terminal to play the game.The code is structured into two components:While SQL is a powerful declarative language to query and modify data, it is not designed for general programming tasks. However, since the introduction of recursive Common Table Expressions (CTEs) in SQL:1999, SQL became a Turing complete language. Informally, this means that, in theory, we can implement \"any\" algorithm in it. Complex examples of recu",
    "summary": "**Today in Hacker News absurdity**: A brave soul demonstrates Turing completeness by torturing SQL into running <em>Tetris</em>. Because why improve software or solve real issues when you can spend hours making a database query drop blocks? Meanwhile, comments oscillate between incredulity and adoration, as Hacker News denizens typically starved for weekend plans now enshrine SQL Tetris next to their rediscovered sourdough starters. \"This is the sort of thing that makes me love software so much,\" cries one user, likely echoing from the depths of their parent's basement. SQL, welcome to the game dev playground \u2014 may the queries be ever in your favor! \ud83d\udcbe\ud83c\udfae"
  },
  {
    "title": "Mapping 20k ships that sank during WW II (arcgis.com)",
    "points": 255,
    "submitter": "ohjeez",
    "submit_time": "2024-09-03T15:38:11.000000Z",
    "num_comments": 124,
    "comments_url": "https://news.ycombinator.com/item?id=41436009",
    "comments": [
      "What was fascinating to me was that the first US commercial ship sunk by Germany was sunk on the south side of Australia! I found that by accident clicking around. Truly a world war, I suppose.https://en.m.wikipedia.org/wiki/MS_City_of_Rayville\n \nreply",
      "For context, the contemporary commercial merchant fleet is about 80,000 ships, roughly a third of which are bulk liquid carriers (a/k/a oil tankers).  As a percentage, that's actually down from the 1970s/80s when half of all commercial ships were tankers.  Most of the growth has been in container ships.Relevant to WWII, oil tanker losses by the US alone were staggering.  \"A total of 129 tankers were lost in American waters in the first five months of 1942.\" (<https://warfarehistorynetwork.com/article/operation-drumbeat...>).A consequence was the US government building the first long-distance oil pipelines, the \"Big Inch\" and \"Little Big Inch\" pipelines from east Texas to refineries on the Atlantic seaboard in New Jersey.  They remain in use.<https://en.wikipedia.org/wiki/Big_Inch>I've also realised that both whales and large-scale commercial shipping rely on similar circumstances:  the ability to on- and off-board cargo (or food) rapidly, widely-separated ports (or feeding grounds), and no significant predators (or war / piracy hazards).  Whales are a remarkably recent evolutionary development, with the large great whales dating back only about 5 million years.  Similarly, bulk shipping required not only global markets but cargos which could be handled in aggregate, whether liquids (as with petroleum), dry solids (mostly ores), or containerised miscellaneous cargo, the latter being premised on standardisation.  Canals, safe shipping routes, and quayside cargo handling capacity were also prerequisites.\n \nreply",
      "> oil tanker losses by the US alone were staggering. \"A total of 129 tankers were lost in American waters in the first five months of 1942.\"Of the U.S. services, the U.S. Merchant Marine had the highest casualty rate in WWII of 4%, followed by the U.S. Marines at 2%.Attacking the supply lines is an important strategy in war.https://www.nationalww2museum.org/war/articles/merchant-mari...http://www.usmm.org/ww2.html\n \nreply",
      "The US submarine warfare operation in the Pacific was also absolutely devastating to Japan, particularly as that country has virtually no indigenous mineral or energy resources.A large part of Japan's invasion of China (from whence it could reasonably readily ship resources) was China's own mineral supplies, particularly coal an iron.  For petroleum though the nearest viable source was Indonesia (then the Dutch East Indies), and the US sank much of what moved from there.This fact, as well as my parent comment above about U.S. east-coast oil shipping and pipeline construction are quite well covered in Daniel Yergin's book The Prize.  If you want an appreciation for just how much oil transformed the US and world, it is an absolutely excellent resource.  And that's from someone who's not partial to Yergin's oil-industry boosterism.\n \nreply",
      "> Of the U.S. services, the U.S. Merchant Marine had the highest casualty rate in WWII of 4%There was a Tom Hanks movie on Apple TV a couple years ago called \"Greyhound\", about a destroyer captain leading a convoy across the Atlantic in WWII.Just the very beginning, the banter about what they were about to encounter was pretty chilling.\n \nreply",
      "And the crazy thing is they were denied access to the GI Bill after WW2 and didn\u2019t get veteran status until 60 years later.\n \nreply",
      "Is the US Merchant Marine a \"US service\"?They aren't an officially recognized uniformed service [0] even though they do have uniforms, a paramilitary structure and (those which are US citizens) can be called for mandatory service [1].Not taking away from their very real service and sacrifice, it's just an interesting question what we mean by \"US service\".[0] https://en.wikipedia.org/wiki/Uniformed_services_of_the_Unit...[1] https://www.usmma.edu/admissions/service-obligation\n \nreply",
      "Is the US Merchant Marine a \"US service\"?They are (or have been) during wartime, attaching to the US Navy:During World War II the fleet was in effect nationalized; that is, the federal government controlled the cargo and the destinations, contracted with private companies to operate the ships, and put guns and Navy personnel, the Navy Armed Guard, on board.U.S. Department of Transportation Maritime Administration FAQ<https://web.archive.org/web/20150411091000/http://www.marad....>\n \nreply",
      "The National Memorial Arboretum in the UK has a memorial dedicated to the Merchant Navy.  When visiting the scale of it is thought provoking, each tree represents a lost UK ship.https://www.iwm.org.uk/memorials/item/memorial/13633> A wood of oak trees representing the 'convoy' of merchant and fishing vessels lost in conflicts of the 20th Century, resulting in the deaths of 46,000 crew. The 2,535 trees each represent a ship lost during WW2.There is also a memorial at Tower Hill to those with no known grave.https://www.cwgc.org/visit-us/find-cemeteries-memorials/ceme...\n \nreply",
      "My dad served in the Merchant Navy in WWII and narrowly escaped death more than once. I don't recall the details because I was too self absorbed as a young man to pay sufficient attention to his stories. Which astonishes and shames me now, nearly 40 years after his death.\n \nreply"
    ],
    "link": "https://storymaps.arcgis.com/stories/41d4bd6029044afbb1b9ad805a4731d8",
    "first_paragraph": "",
    "summary": "**World War II Shipwreck Extravaganza! Click Here to See How Nations Literally Threw Metal Into the Ocean!**\n\nWe get it, kids, WW II was baaaad, but did anyone know just how many toys were lost in the tub? Thanks to our favorite online arts and crafts tool, ArcGIS, we can now point and sob at every single spot where a ship decided to embrace the ocean floor. One inquisitive commenter discovers the geographical wonders of wars and somehow manages to turn it into a trivia fact that will surely impress no one at their next dinner party. Meanwhile, maritime enthusiasts casually discuss the logistics of whale feeding habits versus global ship-routing problems \ud83d\udc33\u2708\ufe0f\ud83d\udea2, because *apparently*, that's an urgent comparison we all needed clarified. Buckle up, history buffs\u2014this map won't just show you sunken ships; it'll sink your whole evening with excessively minute details!"
  },
  {
    "title": "Manipulating Large Language Models to Increase Product Visibility (arxiv.org)",
    "points": 11,
    "submitter": "bookofjoe",
    "submit_time": "2024-09-06T22:13:47.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arxiv.org/abs/2404.07981",
    "first_paragraph": "This week: the arXiv Accessibility ForumHelp | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "The digital prophets at arXiv have churned out yet another groundbreaking paper that will surely revolutionize the way we spam search engines and manipulate consumer perception. As intellectuals and basement-dwellers unite, the quest to <em>creatively</em> abuse large language models for increased product visibility is heralded as the next big leap in making sure your completely unnecessary product appears first on every unrelated search query. Meanwhile, the discerning commenters trip over themselves in a frenzied rush to miss the point entirely, debating the ethical nuances of AI in a scramble to display their superior intellect. Will they succeed in changing the world, or just inflate their digital egos? Only time, and the next refresh of arXiv's RSS feed, will tell."
  },
  {
    "title": "How does cosine similarity work? (tomhazledine.com)",
    "points": 89,
    "submitter": "tomhazledine",
    "submit_time": "2024-09-04T11:52:41.000000Z",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=41444590",
    "comments": [
      "I think the use of the term \"cosine\" here is needlessly confusing. It is the dot product of normalized vectors. Sure, when you do the maths, it gives out a cosine, but since we are not doing geometry here, so it isn't really helpful for a beginner to know that. Especially considering that these vectors have many dimensions and anything above 3D is super confusing when you think about it geometrically.Instead just try to think about what it is: the sum of term-by-term products of normalized vectors. A product is the soft version of a logic AND, and it makes intuitive sense that vectors A and B are similar if there are a lot of traits that are present in both A AND B (represented by the sum) relative to the total number of traits that A and B have (that's the normalization process).Forget about angles and geometry unless you are comfortable with N-dimensional space with N>>3. Most people aren't.\n \nreply",
      "> we are not doing geometry herewe absolutely are doing geometry here, given we're talking about metrics in a vector space \u2013 and this is trigonometry you learned by the first year of high school.\n \nreply",
      "I bet it's whether your primary background is programming or mathematics. From the latter, the cosine is very natural (scalar projection etc.) and it's lots of steps to get to your thing. I'd say this was intuitive for us post high-school because of that pedagogical background.\n \nreply",
      "Imagine 2 points in 3 dimensional space with a vector being the line from the origin to the point. So you have 2 vectors pointing going to the 2 points from the origin.If those points are really close together, then angle between the two vector lines is very small. Loosely speaking cosine is a way to quantize how close two lines with a shared origin is. If both lines are the same, the angle between them is 0, and the cosine of 0 is 1. If two lines are 90 degrees apart, their cosine is 0. If two lines are 180 degrees apart, their cosine is -1. \nSo cosine is a way to quantify the closeness of two lines which share to same originTo go back with 2 points in space that we started with, we can measure how close those 2 points are by taking the cosine of the lines going from origin to the two points. If they are close, the angle between them is small. If they are the exact same point, the angle between the lines is 0. That line is called a vectorCosine similarity measures how closes two vectors are in Euclidean space. That\u2019s we end up using it a lot. It\u2019s no the only way to measure closeness. There are many others\n \nreply",
      "Are all the points in question one unit distant from the origin?\n \nreply",
      "I vaguely remember some paper where they didn\u2019t even bother normalizing the vectors, because they expected zeros to be very close to zero, and anything else was considered a one.I have no idea if this a common optimization or if it was something very niche. It was for a heuristic matrix reordering strategy, so I think they were willing to accept some mistakes.\n \nreply",
      "yes, cosine similarity involves normalizing the points (by the L2 norm) and then dot product. In other words the points lie on the unit (hyper)sphere.\n \nreply",
      "Good explanation, can you also explain how a sentence ends up as a point next to another point where the sentences has similar meaning. What does it mean for two sentences to be similar?\n \nreply",
      "A good way to understand why cosine similarity is so common in NLP is to think in terms of a keyword search. A bag-of-words vector represents a document as a sparse vector of its word counts; counting the number of occurrences of some set of query words is the dot product of the query vector with the document vector; normalizing for length gives you cosine similarity. If you have word embedding vectors instead of discrete words, you can think of the same game, just now the \u201ccount\u201d of a word with another word is the similarity of the word embeddings instead of a 0/1. Finally, LLMs give sentence embeddings as weighted sums of contextual word vectors, so it\u2019s all just fuzzy word counting again.\n \nreply",
      "One thing I've wondered for a while: Is there a principled reason (e.g. explainable in terms of embedding training) why a vector's magnitude can be ignored within a pretrained embedding, such that cosine similarity is a good measure of semantic distance? Or is it just a computationally-inexpensive trick that works well in practice?For example, if I have a set of words and I want to consider their relative location on an axis between two anchor words (e.g. \"good\" and \"evil\"), it makes sense to me to project all the words onto the vector from \"good\" to \"evil.\" Would comparing each word's \"good\" and \"evil\" cosine similarity be equivalent, or even preferable? (I know there are questions about the interpretability of this kind of geometry.)\n \nreply"
    ],
    "link": "https://tomhazledine.com/cosine-similarity/",
    "first_paragraph": "Look up \"how to compare vectors\" and cosine similarity will be the most common (if not the only) approach you will see. I've been working with vectors a lot lately in the context of LLM embeddings, and being able to measure how similar any two embeddings are has become an important part of my workflow. But how does the cosine similarity process actually work?I've been relying on copy/pasting cosine similarity code without really understanding how it works. To give myself a deeper understanding, I want to answer the following questions:Before getting too deep, it's worth clarifying what we mean by \"vectors\". For my projects I'm using the terms \"embedding\" and \"vector\" interchangeably. To quote a previous post of mine:The general concept of \"embeddings\" is an offshoot of the Large Language Model (LLM) technology that makes tools like ChatGPT work. The basic idea is that you can take a piece of text (a blog post, for example) and turn it into a vector (an array of numbers). This vector is",
    "summary": "<h3>The Mystical Art of Cosine Similarity: A Guide for the Terminally Confused</h3>\n<p>In yet another groundbreaking exposition, tomhazledine.com takes us deep into the bewildering world of vectors, or <em>'embeddings'</em> if you prefer sounding smarter at cocktail parties. Here, we learn that 'cosine similarity' isn't just a fancy term you drop to impress a date but indeed involves angles and math, shocking legions of commenters who had blissfully avoided trigonometry since high school. One brave soul attempts to demystify the concept by reducing it to <i>'the sum of term-by-term products of normalized vectors'</i>, essentially describing every math problem ever but with more steps. Meanwhile, the rest of the peanut gallery battles over whether this is geometry or just a desperate cry for help from people who see vectors in their sleep.</p>"
  },
  {
    "title": "L\u00d6VR \u2013 A simple Lua framework for rapidly building VR experiences (github.com/bjornbytes)",
    "points": 93,
    "submitter": "siegers",
    "submit_time": "2024-09-04T14:27:47.000000Z",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=41446248",
    "comments": [
      "Somewhat related, I'm trying to convert the Learn OpenGL tutorials [0] to L\u00d6VR [1]. The main goal is to help me (and hopefully others) to understand the changes needed to convert the Open GL shaders to the L\u00d6VR version, as there are some differences.Not all tutorials are completed yet, this is still a work in progress.---[0]: https://learnopengl.com/[1]: https://github.com/wolf81/lovr-learnopengl\n \nreply",
      "Related:L\u00d6VR \u2013 An open source framework for rapidly building immersive 3D experiences - https://news.ycombinator.com/item?id=28081656 - Aug 2021 (122 comments)Show HN: L\u00d6VR \u2013 VR framework for Lua - https://news.ycombinator.com/item?id=15177549 - Sept 2017 (23 comments)\n \nreply",
      "I\u2019ll give it another shot because there\u2019s definitely some really good stuff including hot reload! \nLast I tried it, there was definitely some missing love (pun intended) in setting it up to compile from source for custom Quest stuff I was working on\u2026 when the compilation instructions start with \u201chere be dragons\u201d you know it\u2019s gonna be fun (https://lovr.org/docs/Compiling).\n \nreply",
      "Is anyone having success combining generative AI with building VR experiences? Now that would be wildly powerful but I\u2019m sure quite difficult\n \nreply",
      "Does this support Apple Vision Pro?\n \nreply",
      "Maybe not yet https://github.com/bjornbytes/lovr/issues/654\n \nreply",
      "From the faq: \"Apple Vision Pro is not known to work because it doesn't support OpenXR or Vulkan.\"\n \nreply",
      "It does support WebXR (featured flag in VisionOS 1, by default in 2) which LOVR looks like it was usinghttps://github.com/bjornbytes/lovr/issues/176#issuecomment-6...EDIT - WebXR target was removed from README in July 2023, with commit message \"Don't trick people into thinking WebXR is supported right now, and spell macOS better.\"\n \nreply",
      "\"Metamour\" would have also been acceptable\n \nreply",
      "Probably built on LOVE, the 2d engine, or at least a homeage\n \nreply"
    ],
    "link": "https://github.com/bjornbytes/lovr",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Lua Virtual Reality Framework\n      A simple Lua framework for rapidly building VR experiences.You can use L\u00d6VR to easily create VR experiences without much setup or programming experience.  The framework is tiny, fast, open source, and supports lots of different platforms and devices.\n\n\nHomepage | Documentation | FAQ\n\n\n\nIt's really easy to get started making things with L\u00d6VR.  Grab a copy of the executable from https://lovr.org/download,\nthen write a main.lua script and drag it onto the executable.  Here are some example projects to try:More examples are on the docs page.You can build L\u00d6VR from source using CMake.  Here are the steps using the command line:See the Compiling Guide for more info.MIT, see LICENSE for details.\n        Lua Virtual Reality Framework\n      ",
    "summary": "**Hacker News Discovers VR: A New Dimension of Incompetence**\n\nIn today's enthralling episode of \"Developers Discover VR and Pretend It's Easy,\" L\u00d6VR, a Lua framework that promises you can make VR experiences while being blissfully ignorant of programming, hits the spotlight. Watch excitedly as tech enthusiasts turn a straightforward drag-and-drop interface into the equivalent of defusing a bomb. Comment sections overflow with the kind of sheer optimism and confusion that makes you question if humanity really should be trusted with technology. Meanwhile, a brave soul attempts to port OpenGL tutorials, only to discover that L\u00d6VR is as supportive of Apple Vision Pro as a soup spoon is of eating cereal. \ud83c\udfad\ud83d\udd76\ufe0f"
  },
  {
    "title": "Will open science change chemistry? (chemistryworld.com)",
    "points": 35,
    "submitter": "daphnemichala",
    "submit_time": "2024-09-03T11:59:56.000000Z",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=41434062",
    "comments": [
      "I've been playing with Avogadro lately, and some of the libraries seem pretty advanced.You know it's legitimate science when Fortran libraries start compiling as part of the dependencies.https://avogadro.cc/\n \nreply",
      "This article conflates a lot of different \u201copen\u201d ideas. Open access journals, machine readable datasets, standardized data representation, open community engagement, etc. Each of which is challenging on its own, let alone hoping to tackle all at once.Chemistry has the blessing and the curse of being an older disciple. Fundamentals have not changed in decades. If you grab an organic chemistry text from the 70s, I guess you would be missing on some cutting edge reactions, and you would be using hilariously obsolete analytical techniques, but the synthesis is going to be the same.Which is to say, that I do not believe chemistry is particularly held back at this point. I think the author was angling for, \u201cWould it not be great if we had standardized, digitized chemistry reaction libraries so we could have an AlphaFold moment?\u201d Which sure, but someone is going to have to fund the effort of digitizing decades of chemistry knowledge.\n \nreply",
      "We already have standardized, digital chemistry reaction libraries (reaxys, scifinder). They cost money, but are heavily used by organic chemists.\n \nreply",
      "More the focus on open libraries. I am not aware of anything of significant size, but I have not been in the space for a while.\n \nreply",
      "I think it\u2019s okay if people take responsibility for their actions, to take risks known or unknown. It\u2019s okay to live in a world where we\u2019re not entirely insulated from negative consequences. Ive always been bummed by how inaccessible chemistry is.\n \nreply",
      "Back when I ran a maker-space in Seattle we interviewed this dude who had a full bio-safety rated lab in his garage. I wish to see more of that stuff!\n \nreply",
      "I'm a strong proponent of open data but I think it needs to go beyond science. \nCompanies should also be incentivized or even required for keeping designs, data, interfaces etc. open. Because often scientists are reliant on devices and data platforms that restrict ownership. So even if the scientist wants to keep his data open, a product or a service that the scientist uses for his research might prevent this.\n \nreply",
      "> Twenty years ago the debate surrounding open science focused on access to journals. By 2020 around 25% of all chemistry papers published were open access, and now most of the major publishers of chemistry journals offer some version of open access.I'm a big fan of open source and open access, but I'm not sure that access to journals is really a big problem for any working chemist. So if the question is whether it will \"change chemistry\", I would say it's unlikely.Why? This isn't an easy field and it's full of landmines, sometimes literally so. It's really only possible to work in the field if you're in a well-funded lab in a well-funded university. Not only that, but many of these chemicals are dangerous. You're just not supposed to be dorking around with them in your basement or garage.It's nice to imagine that somewhere out there is some poor, underfunded genius who doesn't have $100 or $200 to pay for a copy of some article, but I think from a practical perspective that's just not something holding back the field. And really, most of his or her neighbors would be happy if he/she wasn't working with dangerous chemicals in a garage.\n \nreply",
      "I\u2019m pretty sure the majority of chemistry teachers I\u2019ve had with real research or professional experience over the age of 50 were blind or deaf on at least one side. The remainder have a story about how they almost died.\n \nreply",
      "Yep, as much as I love the idea, I would not recommend that anyone approaches even some of the basic chemistry without someone being able to stop you in time.\n \nreply"
    ],
    "link": "https://www.chemistryworld.com/careers/will-open-science-change-chemistry/4020023.article",
    "first_paragraph": "",
    "summary": "**Will Open Science Transform Chemistry? Or Just Open a Can of Worms?**\n\nIn another groundbreaking revelation from Chemistry World, where nostalgia for the 70's still lingers like the scent of mothballs, eager commenters tackle the titanic idea of \"open science\" transforming chemistry. One reminds us that clinging to Fortran libraries is the true mark of scientific prowess, while another highlights our heroic journey from shady basement labs to possibly, one day, clicking through a digital library as seamlessly as a Netflix binge. Critics unite in the solemn fear that some poor rogue chemist might bootstrap the next Frankenstein from Open Access parts -- that is, if they can dodge the crippling subscription fees and evade blowing themselves up first. \ud83e\uddea\ud83d\udca5\ud83d\udcbb"
  },
  {
    "title": "2M users but no money in the bank (exercism.org)",
    "points": 441,
    "submitter": "leandot",
    "submit_time": "2024-09-06T07:16:38.000000Z",
    "num_comments": 267,
    "comments_url": "https://news.ycombinator.com/item?id=41463734",
    "comments": [
      "I don't know if iHiD will see this, but this is Lane, the founder of Boot.dev. One of our students shared this article with me, and it really sucks to see. Exercism has always been a go to recommendation of mine for students who need additional practice or to learn languages we don't yet teach.I know you mentioned you don't wanna do ads, which makes sense. But if you're getting 1200 sign ups a day thats worth a lot, and id be happy to chat and see if there's a sponsor opportunity thats uninvasive and that could at least keep you going.anyhow, you can reach me on Twitter @wagslane or via email: lane @ boot dev.either way, hope things turn around\n \nreply",
      "Exorcism? Seriously?\n \nreply",
      "This is one of the saddest posts I've read in a long time.I love exercism. It's beautiful, works in the browser and from CLI. The lessons are really well designed. It's the perfect coding school environment.So, I'm sad it isn't viable.But, I think I am really saddened by the comments here, second guessing all the decisions they made. If I were to second guess the people behind most of the comments, I would assume no one has ever run a business with payroll, or built something with this much reach. The comments ring really hollow for the most part and seem really callous given this person has sunk his life into making the world, and my life, a better place. I'm scared for the moment he is in life and sad there are not more positive comments, and even better, comments from people where they would actually help by making a donation.\n \nreply",
      "Thank you for being so kind and supportive. I really appreciate your comment.I'm ok. I'm hopeful for the future. I believe there's lots of amazing stuff we can do and that Exercism has a great future. But right now it's in a hard, low place. The last few months have been pretty tough emotionally, but I think getting to this place is one of acceptance. The only direction now is up :)\n \nreply",
      "Can you please just add a paid tier before shutting down? Just a $5/mo something that shows a badge on your account, or whatever. Call it something other than \"donate\", \"premium account\" or something, and see how many people pick it up. You \"only\" need 1500 people to do that to cover the costs, which is less than a thousandth of your user base.Even if it gives a few useful features to subscribers, that's better than shutting down.\n \nreply",
      "They have a paid tier, called insiders, although you can also \"pay\" through helping other users on the site: https://exercism.org/insiders\n \nreply",
      "Never knew they had this, signed up immediately :)\nExercism is worthy of 120$ a year and worth its weight in gold. It has helped me learn Rust, Swift and Kotlin and I'll be forever greatful!\n \nreply",
      "Ah, thanks, I looked for that but didn't find it (the \"donate\" button reminded me of a one-off). I'd recommend changing the link name to \"pricing\" and A/B testing the recommended donation amount being $5/mo.I don't know how much effort they've put into experimenting with pricing, but it'd be worth trying to increase donations, especially for such a well-designed site.\n \nreply",
      "We're not shutting down :)But yes, rather than shutting down, I'd probably just put the whole thing behind a paywall to try and keep it alive.\n \nreply",
      "Well yes, I mean \"maintenance mode\" or whatever it is. You've built a really valuable service, if you want some help with maybe reducing costs, I'd be happy to have a chat at some point, I have experience in running things on budget hardware.\n \nreply"
    ],
    "link": "https://exercism.org/blog/september-2024-restructure",
    "first_paragraph": "Upskill in 65+ languagesA different challenge each week in 2024Explore your Exercism journeyOffers & discounts from our partnersStreaming, walkthroughs & moreShort language overviewsGet inspired by people's storiesChat & hang with the communityDig deeper into topicsHow you can help ExercismSupport others as they learnEverything you need to helpMeet the people behind ExercismHelp support our missionLearn about our organisationExplore what we've achievedOur way of saying thank youHoodies, stickers & moreShare it around!Share it around and have others benefit too!Hi everyone!Last week we hit the huge milestone of two million users.\nWithin a few hours, we also hit 45 million exercise submissions.A day later, I paid the final payroll for me, Erik and Aron, and our bank account reduced down to the point we can't afford to pay another.I think this sums up Exercism's story pretty well.\nOver 1,200 people per day sign up to Exercism.\nTens of thousands solve exercises each day.\nBut we don't have ",
    "summary": "**Exercism Bankruptcy Bonanza**  \nHere\u2019s another feel-good trainwreck \ud83d\ude82\ud83d\udca5 for the internet to gawk at. Exercism, the digital dojo for ambitious code jockeys, joyfully reveals hitting **2M users** but simultaneously wallows in the agony of having less cash than a lemonade stand. iHiD, eternally optimistic captain of this sinking ship, sobs into his keyboard as he explains why even 45 **million** exercise submissions can\u2019t keep the lights on. Meanwhile, the comment section descends into a circus of \ud83d\udca9 unsolicited business advice, self-promoting saviors, and emotionally charged fans advocating for anything but the real answer: making actual money. God forbid we weave in a few non-invasive ads in between those precious lines of code! \ud83e\udd11"
  },
  {
    "title": "LSP: The Good, the Bad, and the Ugly (michaelpj.com)",
    "points": 144,
    "submitter": "bryjnar",
    "submit_time": "2024-09-03T23:17:23.000000Z",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=41440275",
    "comments": [
      "Rabbit hole warning (started by the article linked above):https://matklad.github.io/2022/04/25/why-lsp.htmlhttps://matklad.github.io/2023/10/12/lsp-could-have-been-bet...\n \nreply",
      "Andhttps://rust-analyzer.github.io/blog/2020/07/20/three-archit...Is a must-read if you are to build your own LSP server!\n \nreply",
      "That 2022 post is amazing, thanks for the link!\n \nreply",
      "RE: \"Not a truly open project.\"If LSP isn't truly open, then neither are most GNU projects.  It was very common for the first 15+ years of GNU's existence for the public development process of a project to be \"the single maintainer publishes a release archive whenever they feel like it\"It's a standard freely published and available for all to implement.  If that's not \"truly open\" then we have moved the goalposts way too far.\n \nreply",
      "I think \"truly open\" is not specific enough. Not being developed \"in the open\" is one thing, not having \"open governance\" is another thing.That said, I guess the problem here is that for standards it helps if well, you collaborate with the people for which the standard is meant to be used by, which is presumably a little hard if there's a huge asymmetric relationship when it comes to Microsoft's concerns vs the rest of the world's concerns.This is one of those cases where having a standards committee or consortium is the way to go. Committees have their problems, but I think it's only reasonable. If you think about it, doesn't it seem inevitable that eventually, big organizations that make editors would want a consortium of some sort to collaborate on protocols like this? LSP is really just the beginning, since there are plenty of things that editors would probably like to integrate deeper with, such as build systems.\n \nreply",
      "> This is one of those cases where having a standards committee or consortium is the way to go. Committees have their problems, but I think it's only reasonable.I think a committee is a reasonable backup plan, and can even be done without Dirk's approval (see e.g. WHATWG done without the W3C's approval).  If the LSP continues to be \"good enough\" then it seems unlikely.> If you think about it, doesn't it seem inevitable that eventually, big organizations that make editors would want a consortium of some sort to collaborate on protocols like this?Maybe?  Editors tend to be an afterthought for most companies; JetBrains is the only company I can think of that is big on the LSP and for whom the editor is a primary experience.\n \nreply",
      "Yes I certainly agree, there is not really much of an impetus for this to happen right away or anything. In the long term, though, I do suspect it is inevitable.> Maybe? Editors tend to be an afterthought for most companies; JetBrains is the only company I can think of that is big on the LSP and for whom the editor is a primary experience.I have some thoughts:- I think there will be more. At the very least, I suspect there is a reasonable chance Apple/XCode would eventually adopt LSP.- Realistically, there aren't that many browser engines either. There's really just two truly distinct browser engines, and really only one of them is the main product of the company that produces it. Arguably there are already more distinct text editor engines that have LSP clients built-in today: Neovim, VSCode/Monaco, Visual Studio, IntelliJ IDEA, Eclipse, Zed, and probably more I'm not thinking of.- I think that it is likely LSP clients will continue to appear in more places. In \"cloud compute\" UIs for things like serverless functions, inside of code forge's built-in editors, and so forth. It's not that it's necessarily that it's so easy to do it well, it's more that the value:effort ratio of doing it is pretty great, and every time someone develops a new high-quality LSP, it gets even better.\n \nreply",
      "Two things:1. (As another commenter mentioned) Most GNU projects are not standards that are expected to be adopted by a significant number of implementers, and used by a huge number of users.  Most GNU projects are totally fine having a few maintainers.2. I am a lot more comfortable with a GNU project being run by a single maintainer than a public specification being owned by a corporation, where changes to that specification are largely driven by that company's product choices and profit motive.And regardless, it seems a little weird to compare the GNU of the 80s and 90s to any public project today.  In GNU's first 15 years the internet was nascent (at best!), and the number of people who implemented, used, and cared about these sorts of things were orders of magnitude smaller than they are today.  Needs have changed.\n \nreply",
      "Most GNU projects are typically not standards/specifications but programs and libraries. That's a significant difference IMO.\n \nreply",
      "But that is not the standard for current GNU projects in large part because of all the easily avoidable friction. \"If it was good enough for Richard Stallman in 1987, it's good enough for Microsoft in 2024\" is just a dumb argument.Not to mention you're conflating apples with oranges, since a software standard is very different from an application. POSIX wasn't just one Bell Labs employee working by himself.From the article:> The LSP should be an open standard, like HTTP, with an open committee that represents the large community which is invested in LSP, and can offer their insight in how to evolve it.There is no goalpost moving here.\n \nreply"
    ],
    "link": "https://www.michaelpj.com/blog/2024/09/03/lsp-good-bad-ugly.html",
    "first_paragraph": "Sep 3, 2024For a few years now I have been working on the Haskell Language Server (HLS), and the lsp library for the LSP protocol and writing LSP servers. \nUnsurprisingly, I have developed some opinions about the design of the LSP!Recently I gave a talk about HLS and LSP at the Haskell Ecosystem Workshop at Zurihac 2024.\nOne slide featured a hastily-written table of \u201cLSP: the good, the bad, and the ugly\u201d.\nAs I gave the talk I realised that there was plenty to say on that topic, hence this post.Most of what I have to say is about the architecture or design of the protocol.\nI won\u2019t have much to say about the features that the protocol supports.\nOther people probably have a lot to say about that (e.g the folks working on languages that use heavy editor integration, like interactive theorem provers).\nMy perspective here is from my time implementing LSP servers, rather than my time using them.I will repeat this a few times, but I want to be very clear that LSP is great and I am very happy t",
    "summary": "**Haskell Hipster Hates His Handiwork**  \nThe Webcentric Wonderboy of Haskell, after half a decade lost in LSP labyrinths, decides it's time to share his \"revolutionary\" truisms on why designing open protocols is like making a salad in a blender\u2014messy, mixed, but somehow still predictable. Revelations include such earth-shattering insights as \"architecture matters,\" and \"protocols are hard,\" only rivaled by commenters falling over themselves to praise the chewed-over regurgitations of old blog posts like they\u2019ve discovered fire. Indeed, the heated debate about LSP's openness ends up sounding less like expert protocol critique and more like a GNU/Linux flame war, proving once again that tech debates are the gift that keeps on recompiling. \ud83d\udd04\ud83d\udcbb\ud83d\udd25"
  },
  {
    "title": "Tesorio Is Hiring a Senior GenAI Engineer and Django Engineer (100% Remote) (tesorio.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-09-06T21:02:21.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.tesorio.com/careers#job-openings",
    "first_paragraph": "We\u2019re a distributed team of data scientists, developers, and finance geeks who all love to make products that are easy to use and backed by cutting edge machine learning. We take a lot of pride in what we do, and we have fun while doing it.In 2023, we brought the Tesorio team together for the first time in Panama City. Our offsite was an absolute blast, filled with team-building, insightful workshops, and unforgettable memories. We can't wait to see where our next offsite will take us in 2024!\n                                    Nicole Thompson, Customer Success Manager\n                                \n                                    Nate Manchester, Regional Sales Director\n                                \n                                    Felipe Vieira, Senior Software Engineer\n                                \n\nDemo\nBook a Live Demo\n\nTry Now\nJump Into Our Virtual Sandbox",
    "summary": "<b>Another Day, Another <i>\"Disruptive\"</i> Startup Looking for Saviors:</b> Tesorio, the leader in buzzword bingo, is scouring the globe for a GenAI Engineer and a Django Engineer who can pretend to love data science as much as ping pong tournaments at company retreats. Experience in leveraging overhyped tech while crafting \"revolutionary\" finance tools preferred. Commenters, limber up those typing fingers to either blindly worship the groundbreaking team spirit or deride the hollow artificial intelligence claims. Either way, pack your virtual bags\u2014you\u2019re probably heading to Panama City in 2024 \ud83c\udf34 where insights and algorithms flow like cheap margaritas!"
  },
  {
    "title": "Minifying HTML for GPT-4o: Remove all the HTML tags (blancas.io)",
    "points": 69,
    "submitter": "edublancas",
    "submit_time": "2024-09-05T13:51:22.000000Z",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41456635",
    "comments": [
      "I don't think that Mercury Prize table is a representative example because each column has an obviously unique structure that the LLM can key in on: (year) (Single Artist/Album pair) (List of Artist/Album pairs) (image) (citation link)I think a much better test would be something like \"List of elements by atomic properties\" [1] that has a lot of adjacent numbers in a similar range and overlapping first/last column types. However, the danger with that table might be easy for the LLM to infer just from the element names since they're well known physical constants. The table of counties by population density might be less predictable [2] or list of largest cities [3]The test should be repeated with every available sorting function too, to see if that causes any new errors.[1] https://en.wikipedia.org/wiki/List_of_elements_by_atomic_pro...[2] https://en.wikipedia.org/wiki/List_of_countries_and_dependen...[3] https://en.wikipedia.org/wiki/List_of_largest_cities#List\n \nreply",
      "Additionally, using any Wiki page is misleading, as LLMs have seen their format many times during training, and can probably reproduce the original HTML from the stripped version fairly well.Instead, using some random, messy, scattered-with-spam site would be a much more realistic test environment.\n \nreply",
      "Good points. But I feel like even with the cities article it could still \u2018cheat\u2019 by recognising what the data is supposed to be and filling in the blanks. Does it even need to be real though? What about generating a fake article to use as a test so it can\u2019t possibly recognise the contents? You could even get GPT to generate it, just give it the \u2018Largest cities\u2019 HTML and tell it to output identical HTML but with all the names and statistics changed randomly.\n \nreply",
      "thanks a lot for the feedback! you're right, this is much better input data. I'll re-run the code with these tables!\n \nreply",
      "What I do is convert to markdown, that way you still get some semantic structure. Even built an Elixir library for this: https://github.com/agoodway/html2markdown\n \nreply",
      "Seems to be the most common method I've seen, it makes sense given how well LLMs understand markdown.\n \nreply",
      "I roughly came to the same conclusion a few months back and wrote a simple, containerized, open source general purpose scraper for use with GPT using Playwright in C# and TypeScript that's fairly easy to deploy and use with GPT function calling[0].  My observation was that using `document.body.innerText` was sufficient for GPT to \"understand\" the page and `document.body.innerText` preserves some whitespace in Firefox (and I think Chrome).I use more or less this code as a starting point for a variety of use cases and it seems to work just fine for my use cases (scraping and processing travel blogs which tend to have pretty consistent layouts/structures).Some variations can make this better by adding logic to look for the `main` content and ignore `nav` and `footer` (or variants thereof whether using semantic tags or CSS selectors) and taking only the `innerText` from the main container.[0] https://github.com/CharlieDigital/playwright-scrape-api\n \nreply",
      "ChatGPT is clearly trained on wikipedia, is there any concern about its knowledge from there polluting the responses? Seems like it would be better to try against data it didn't potentially already know.\n \nreply",
      "Yep - one good option is to use Wikipedia pages from the recent Olympics, which GPT has no knowledge of: https://github.com/openai/openai-cookbook/blob/457f4310700f9...\n \nreply",
      "I found that reducing html down to markdown using turndown or https://github.com/romansky/dom-to-semantic-markdown works well;if you want the AI to be able to select stuff, give it cheerio or jQuery access to navigate through the html document;if you need to give tags, classes, and ids to the llm, I use an html-to-pug converter like https://www.npmjs.com/package/html2pug which strips a lot of text and cuts costs. I don't think LLMs are particularly trained on pug content though so take this with a grain of salt\n \nreply"
    ],
    "link": "https://blancas.io/blog/html-minify-for-llm/",
    "first_paragraph": "",
    "summary": "### HTML Minification: The Comedy of Errors\n\nOnce again, the internet treats us to a thrilling debate on whether GPT-4o can differentiate between a Mercury Prize table and the periodic table without its HTML training wheels. As one enlightened commenter suggests, throwing the LLM into the chaotic depths of \"random, messy, scattered-with-spam\" sites might yield *true enlightenment*\ud83e\uddd9\u200d\u2642\ufe0f. Meanwhile, another genius proposes we cheat the system by feeding it a fake HTML page\u2014because, apparently, GPT-3 taught us nothing about fabricating data. Meanwhile, several scrapers, Markdown converters, and an unsolicited Elixir library recommendation later, we are no closer to solving if GPT can truly parse HTML, or if it's just well-versed in the art of Internet Clutter. Bravo \ud83d\udc4f, tech wizards. Bravo."
  },
  {
    "title": "Moondance: Experience the marvel that is night-blooming tobacco (theamericanscholar.org)",
    "points": 8,
    "submitter": "samclemens",
    "submit_time": "2024-09-03T19:08:28.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://theamericanscholar.org/moondance/",
    "first_paragraph": "",
    "summary": "The American Scholar, in yet another audaciously thrilling expos\u00e9, unveils the \"marvel\" of night-blooming tobacco\u2014a plant that does the unthinkable: it blooms at night. Gardening enthusiasts and insomniacs alike rejoice in the comments, one-upping each other with tales of nighttime flora, while subtly hinting their backyards might just be portals to Narnia. Debates flare as to whether the scent of moonlit nicotine can indeed cure existential dread or if it\u2019s just another suburban legend like privacy or affordable housing. \ud83c\udf1a\ud83d\udeac"
  },
  {
    "title": "Pulsar, micro creative coding playground (muffinman.io)",
    "points": 4,
    "submitter": "stankot",
    "submit_time": "2024-09-03T05:25:22.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://muffinman.io/pulsar/",
    "first_paragraph": "\n          Micro creative coding playground.\n          Try changing the code below or go through the tutorial.\nSource code\n",
    "summary": "On muffinman.io, apparently nobody told them that \"micro creative coding playground\" sounds more like a techy euphemism for \"my first JavaScript disaster\" than a legitimate coding resource. This pulsar of pretension invites aspiring coders to tweak pre-written scripts, ensuring a therapeutic escape into a world where, yes, changing a color can feel like hacking the Matrix. The comment section, a delightful cesspool of misplaced semicolons and existential dread, excels at celebrating minimal change as if it's groundbreaking innovation. \ud83d\ude80\ud83d\udcbb Finally, a safe space where \"Hello, World!\" impresses everyone."
  },
  {
    "title": "Oya, I've heard of mega-ROMs (leadedsolder.com)",
    "points": 64,
    "submitter": "zdw",
    "submit_time": "2024-09-03T16:50:26.000000Z",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41436734",
    "comments": [
      "> Normally, I would expect floating address pins to be completely random, but this was very consistent.I think \u201crandom\u201d assumes some kind of perfect conditions. There will be leakage current to ground, leakage current from Vcc, and the actual threshold between high and low for the input will be some particular value.Just a quick look at the data sheets for the SN54LS670 from TI, and I see that the output pin leakage current is rated at no more than 20 \u00b5A. That\u2019s worst-case, but if you apply Ohm\u2019s law and plug in 3.3 V, you get something on the order of 160 k\u03a9 output impedance. I have a limited understanding of electronics and I\u2019m sure I\u2019m missing something, but I\u2019d expect a lot of the output pins to settle on a reasonably stable voltage when they are turned off. Maybe natural variations in geometry or chemistry will push some pins higher and other pins lower.You can run an entire CPU core with less than 20 \u00b5A current these days, just to give it a sense of scale.\n \nreply",
      "The detail and knowledge shared in these three paragraphs are why I love Hacker News.\n \nreply",
      "> [About buying a used cartridge] In retrospect, after all this adventure, that doesn\u2019t seem like much, but \u201cspending fifty bucks instead of learning something\u201d is not the name of this blog.Absolutely love this quote.\n \nreply",
      "Very interesting post.  I played some MSX recently and there\u2019s some good games there.  I\u2019m sure I\u2019m going to end up with an MSX2 some day.I recently started making some SEGA master system cartridges.  Your only option there is to destroy a Monopoly or After Burner.  :-|\n \nreply",
      ">  Your only option there is to destroy a Monopoly or After Burner.Not sure what you mean. krikzz sells a Master Everdrive if you want something suitable for multiple games. If you want a single game cartridge for a homebrew release, raphnet has you covered.The one exception is if you have a Japanese Master System (or Mark III). These have a different cartridge connector that's shared with the SG-1000 and SC-3000 and flash carts are harder to find. You may be able to use a western SMS flash cart with a passive adapter though\n \nreply",
      "The Maze of Galious is such an underrated game in terms of IP.I had the pleasure of attending MSXGOTO40 last december.Great explanation what megarom actually means, I never knew\n \nreply"
    ],
    "link": "https://www.leadedsolder.com/2024/09/03/msx-megarom-oyanami.html",
    "first_paragraph": "Konami made some of the best games for the MSX computer platform. Unfortunately, the market being what it is, those cartridges have become expensive collectibles rather than cheap games, which takes all the fun out of it. Before we can build a bootleg cart, we need to understand Konami\u2019s unique mapper.My goal for this project was to make a functional cartridge that could play Konami\u2019s Knightmare II: The Maze of Galious. Although I think the game is interesting and ambitious, this was mostly because blog friend famiclone loved the game, and I owed him an MSX in exchange for the lovely ColecoVision and busted 48k Spectrum he sent me.For a couple years now, I\u2019ve wanted to play The Maze of Galious on an MSX. Galious is a side-scrolling action-adventure game with some long gameplay, multiple characters, and some fiendish puzzles. Blog friend famiclone has been really interested in it, and I wanted to see what all the fuss is about, too.At first, I looked at the used market to see if I could",
    "summary": "### Digital Dumpster Diving for Dummies\n\nAt <em>leadedsolder.com</em>, the tech-savvy nostalgia nerd embarks on a quixotic quest to reverse-engineer Konami cartridges so he can relive the heady days of MSX gaming without paying eBay's extortionate nostalgia tax. What could be a heartwarming tale of digital archeology swiftly turns into a master class in techno-jargon that alienates all but the most devout circuit-heads. Meanwhile, the comment section predictably transforms into a humblebrag battleground, where readers flex their trivial knowledge on obscure tech specs and the moral intricacies of game piracy. Because why pay fifty bucks for a game when you could spend countless hours and brain cells building a contraption to play it for free? \ud83e\udd13\ud83d\udcbe\ud83d\udd27"
  },
  {
    "title": "Effects of Gen AI on High Skilled Work: Experiments with Software Developers (ssrn.com)",
    "points": 139,
    "submitter": "Anon84",
    "submit_time": "2024-09-06T11:21:57.000000Z",
    "num_comments": 240,
    "comments_url": "https://news.ycombinator.com/item?id=41465081",
    "comments": [
      "I sometimes wonder about whether the decline in IT worker quality is down to companies trying to force more and more roles onto each worker to reduce headcount.Developers, Operations, and Security used to be dedicated roles.Then we made DevOps and some businesses took that to mean they only needed 2/3 of the headcount, rather than integrating those teams.Then we made DevSecOps, and some businesses took that to mean they only needed 1/3 the original roles, and that devs could just also be their operations and appsec team.That's not a knock on shift-left and integrated operations models; those are often good ideas. It's just the logical outcome of those models when execs think they can get a bigger bonus by cutting costs by cutting headcounts.Now you have new devs coming into insanely complex n-microservice environments, being asked to learn the existing codebase, being asked to learn their 5-tool CI/CD pipelines (and that ain't being taught in school), being asked to learn to be DBAs, and also to keep up a steady code release cycle.Is anyone really surprised they are using ChatGPT to keep up?This is going to keep happening until IT companies stop cutting headcounts to make line go up (instead of good business strategy).\n \nreply",
      "One person can do the work of 3 and regularly does in startups.I think that what the MBAs miss is this phenomena of overconstraint.  Once you have separate the generic role of \"developer\" into \"developer, operations, and security\", you've likely specified all sorts of details about how those roles need to be done.  When you combine them back into DevSecOps, all the details remain, and you have one person doing 3x the work instead of one person doing the work 3x more efficiently.  To effectively go backwards, you have to relax constraints, and let that one person exercise their judgment about how to do the job.A corollary is that org size can never decrease, only increase.  As more employees are hired, jobs become increasingly specialized.  Getting rid of them means that that job function is simply not performed, because at that level of specialization, the other employees cannot simply adjust their job descriptions to take on the new responsibilities.  You have to throw away the old org and start again with a new, small org, which is why the whole private equity / venture capital / startup ecosystem exists.  This is also Gall's Law exists:https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law\n \nreply",
      "I think there is another bit to this which is cargo cult tendencies. Basically DevOps is a great idea under certain circumstances and works well for specific people in that role. Realistically if you take a top talent engineer they can likely step into any of the three roles or even some others and be successful. Having the synergy of one person being responsible for the boundary between two roles then makes your ROI on that person and that role skyrocket.And then you evangelize this approach and every other company wants to follow suit but they don\u2019t really have top talent in management or engineering or both (every company claims to hire top talent which obviously cannot be true). So they make a poor copy of what the best organizations were doing and obviously it doesn\u2019t go well. And the thing is that they\u2019ve done it before. With Agile and with waterfall before that, etc. There is no methodology (organizational, programming, testing, etc.) that can make excellence out of mediocrity.\n \nreply",
      "They're realizing that 10x (+) developers exist, but think they can hire them at 1x developer salaries.Btw, they key skill you're leaving out, is to understand the business your company is in.If you can couple even moderate developer ability with a good understanding of business objectives, you may stay relevant even while some of the pure developers are replaced by AI.\n \nreply",
      "This 100%. It's rare to find anyone that wants to learn a complex biz domain.\n \nreply",
      "It's because it's often a suboptimal career move from an individual perspective. By branding yourself as a \"FinTech Developer\" or whatever instead of just a \"Developer\" you're narrowing your possible job options for questionable benefit in terms of TC and skills growth. This isn't always the case and of course if you spend 20 years in one domain maybe you can brand yourself as an expert there and pull high $/hr consulting rates for fixing people's problems. Maybe. More likely IMO you end up stagnating.I went through this myself early in my career. I did ML at insurance companies and got branded as an insurance ML guy. Insurance companies don't pay that well and there are a limited number of them. After I got out of that lane and got some name-brand tech experience under my belt, job hunting was much easier and my options opened up. I make a lot more money. And I can always go back if I really want to.\n \nreply",
      "> I did ML at insurance companies and got branded as an insurance ML guy.If you're an \"ML insurance guy\" outside of the US, it may be quite lucrative compared to other developers. It's really only in the US (and maybe China) that pure developers are demanding $200k+ salaries.In most places in Europe, even $100k is considered a high salary, and if you can provide value directly to the business, it will add a lot to your potential compensation.And in particular, if your skills are more about the business than the tech/math of your domain, you probably want to leverage that, rather than trying to compete with 25-year-olds with a strong STEM background, but poor real-life experience.\n \nreply",
      "> In most places in Europe, even $100k is considered a high salaryI think it's worth adding here that US developers can have a much higher burden for things like health insurance.  My child broke his arm this year, and we hit our very high deductible.I would like to see numbers factoring in things like public transportation, health insurance, etc., because I personally feel like EU vs US developers are a lot closer in quality of life after all the deductions.\n \nreply",
      "I would also like to see those numbers. IMHO, the biggest difference comes from housing cost. For example, from my own back-of-the-envelope calculations, a \u00a3100k Oxbridge job affords a better lifestyle than a $180k NY job mostly because of housing.However, some US areas have competitively priced housing and jobs that would make the balance tilt in favor of America. In EU, affordable spots with lots of desirable local jobs are becoming increasingly rare. Perhaps Vienna, Wroc\u0142aw and a few other places in Central/Eastern EU.\n \nreply",
      "what are typical Oxbridge jobs? Are they really in Oxford and Cambridge? if yes, then ... NYC vs not-London UK seems like almost incomparable.\n \nreply"
    ],
    "link": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4945566",
    "first_paragraph": "",
    "summary": "<b>Corporate Alchemy: Turning One Programmer into Three!</b> In the latest technobabble echolalia to mercilessly assail the sanity of anyone with actual work experience, a scholarly rant disguised as research gloriously discovers that companies still think cloning their employees via fancy terms like \"DevSecOps\" counts as innovation. \ud83d\udcbc\ud83d\udd27 Meanwhile, the peanut gallery swaps tales of surviving under caffeinated duress in their code-laden trenches, morphing slowly but surely into jargon-juggling, business-strategizing, half-baked AI \"experts\" overnight. And through it all, the heroic commentators pat themselves on the back, reaffirming their shared delusion that squeezing three jobs into one deserves a medal\u2014or at least hazard pay. \ud83e\udd47\ud83e\udd16\ud83d\udcb8"
  },
  {
    "title": "Your Name in Landsat (nasa.gov)",
    "points": 176,
    "submitter": "warrenm",
    "submit_time": "2024-09-03T14:43:01.000000Z",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=41435444",
    "comments": [
      "snippet to view all the letters at once  <!DOCTYPE html>\n  <html lang=\"en\">\n  <head>\n      <meta charset=\"UTF-8\">\n      <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n      <title>Landsat Alphabet Viewer</title>\n      <style>\n          body { font-family: Arial, sans-serif; text-align: center; }\n          #imageContainer { display: flex; flex-wrap: wrap; justify-content: center; }\n          img { width: 100px; height: 100px; margin: 5px; object-fit: cover; }\n      </style>\n  </head>\n  <body>\n      <h1>Landsat Alphabet Viewer</h1>\n      <div id=\"imageContainer\"></div>\n      <script>\n          const letters = {\n              'A': 3, 'B': 1, 'C': 2, 'D': 1, 'E': 3, 'F': 1, 'G': 0, 'H': 1,\n              'I': 4, 'J': 2, 'K': 1, 'L': 3, 'M': 2, 'N': 2, 'O': 1, 'P': 1,\n              'Q': 1, 'R': 3, 'S': 2, 'T': 1, 'U': 2, 'V': 3, 'W': 1, 'X': 2,\n              'Y': 2, 'Z': 1\n          };\n          const container = document.getElementById('imageContainer');\n          const baseUrl = 'https://landsat.gsfc.nasa.gov/apps/YourNameInLandsat-main/public/images/';\n  \n          for (const [letter, maxNum] of Object.entries(letters)) {\n              for (let i = 0; i <= maxNum; i++) {\n                  const img = document.createElement('img');\n                  img.src = `${baseUrl}${letter.toLowerCase()}_${i}.jpg`;\n                  img.alt = `${letter}_${i}`;\n                  img.title = `${letter}_${i}`;\n                  container.appendChild(img);\n              }\n          }\n      </script>\n  </body>\n  </html>\n \nreply",
      "Your comment activates Reader Mode on Safari mobile automatically.\n \nreply",
      "It does? Mine seems to load this page just like any other comment section. I\u2019m using iOS 16.7.10, so perhaps Reader Mode activates automatically on higher versions. I\u2019m curious to test this a bit more but, sadly, my phone is out of warranty for non-security updates so I\u2019ll have believe that what you say is true without testing anything.\n \nreply",
      "File a bug report?\n \nreply",
      "I was showing this to my daughter a few days back - naming her, her dolls, and what not. Between Landsat and Sentinel satellites, there are over 77TB of free satellite data available for public use every month. Lot can be done while a lot of these data are wasted and not-so-usable. And we have fun things like lettering words with earth's landscape. Have fun.\n \nreply",
      "Reminiscent of the famous Butterfly Alphabet photographic artwork: https://mymodernmet.com/kjell-bloch-sandved-butterfly-alphab...\n \nreply",
      "Would be cool to be able to cycle through images for a specific letter in a word. So you can refine the look.\n \nreply",
      "Love this! But seems to be hugged to death.The images load extremely slowly. This can be confusing as you do not see any spinner or indication that they are loading. But they do come if you are patient.As you have some letters in the cache they render instantly so to the users it looks totally broken. Unless your linger on the page and the letters snaps in 20 seconds later (one by one).An empty place holder (so you can see the space) until the letter loads would be nice. Maybe even a \"Loading\" placeholder text for each letter. An animated (fake) spinner would be a nice visual cue. Then swap the image when it is loaded.\n \nreply",
      "This does have a profanity filter, but it's a bit rusty. Minutes of quality entertainment for 14-year-old boys of all ages.\n \nreply",
      "How did they locate these particular images? I would love to read something on that, or similar projects looking across satellite imagery for specific patterns.\n \nreply"
    ],
    "link": "https://landsat.gsfc.nasa.gov/apps/YourNameInLandsat-main/index.html",
    "first_paragraph": "Please enter letters from A-Z",
    "summary": "In an unparalleled showcase of combining technology with a kindergarten letter-teaching moment, NASA's \"Landsat Alphabet Viewer\" graces us by turning high-tech satellite imagery into a very slow-loading game of Scrabble\u2122 over the globe. Web enthusiasts and accidental visitors bombard the comments section with technical tangents\u2014from declaring Safari glitches as gospel to unwarranted tech support sessions that no one asked for. Meanwhile, an imaginative parent leverages this glacial tool to spell out the names of all their child's dolls, because what else would you use 77TB of satellite data for? Keep refreshing, your letter might just pop up before the next Ice Age. \ud83c\udf0d\ud83d\udd24\ud83d\udca4"
  },
  {
    "title": "Understanding the Layout Process in Qt Widgets (felipefarinon.com)",
    "points": 41,
    "submitter": "felipefar",
    "submit_time": "2024-09-05T18:44:01.000000Z",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41459373",
    "comments": [
      "I'm curious why you chose Qt Widgets for a relatively new application? Is desktop support that much better than QML?There's a bunch of tradeoffs for everyone who has to make that choice and it's always interesting to know why people choose the one that they do.\n \nreply",
      "Well, I don't want to bash on QML, since I really appreciate the efforts that the team puts in it. But I do think its current state is not the best when compared to Qt Widgets, and lament that there's  a split between the two toolkits inside the framework. The problems I had with QML are:1) The controls that Qt Quick 2 provides are oriented toward touch interfaces, and some are not even feature-complete. For example, QML's Flickable on desktop can be scrolled by clicking and moving the mouse, a behavior that is clearly an artifact from the touch implementation. QML's TextEdit doesn't support much that QTextEdit does, which was particularly important for implementing an app that offers advanced text editing. Ironically, even though Qt Quick is touch-centric, Qt has lots of bugs on mobile platforms, and has a history of presenting regressions on those platforms.2) Communication between QML and C++ is finicky. You have to use macros and Qt-specific constructs (Q_PROPERTY, signals, slots) to bridge both worlds. Qt Widgets doesn't need bridging in the first place, since it's C++ all the way down.3) Control customization is a pain. In Qt Widgets, we can create a class that inherits from a standard widget, and then we can customize it however we want while inhering the behavior from the base control. In QML, you have to resort to javascript for that, which has different tooling and ecosystem than C++. Besides, C++ programmers find javascript dynamic typing more error-prone than static typing.4) The latency of interfaces built with QML is higher than the ones built with Widgets. QML's rendering engine is lagging behind in the input latency mitigation front when compared to browsers, although they've been making efforts in this area.I don't think those problems are unsolvable, and historically Qt has evolved a lot, so I hope they eventually tackle these issues seriously.\n \nreply",
      "Thanks for the detailed response! I'm definitely feeling the majority of these paint points.> 2) Communication between QML and C++ is finicky. You have to use macros and Qt-specific constructs (Q_PROPERTY, signals, slots) to bridge both worlds. Qt Widgets doesn't need bridging in the first place, since it's C++ all the way down.This hurts so bad. I'm actually implementing in Rust so I've got double the pain and any Rust type is either a QVariant or *mut pointer but integrating with Serde to easily (de)serialize JS objects has mitigated some of the pain points.> 4) The latency of interfaces built with QML is higher than the ones built with Widgets. QML's rendering engine is lagging behind in the input latency mitigation front when compared to browsers, although they've been making efforts in this area.This one is surprising. I've had more problems with Widgets, especially when doing a lot of animations (even just scrolling big text views) on a 4K display on MacOS, but maybe I'm thinking graphics and not input lag. The software rasterizer/GPU pipeline seems to get overloaded on Mac (Great article on the rendering pipeline btw!)The big thing that sold me on QML over Widgets - other than the latter being the redheaded step child by this point - was implementing hot reloading. Having the entire UI completely refresh when changing the QML is definitely a nice coming from browser frontend, especially given Rust compile times.\n \nreply",
      "> I've had more problems with Widgets, especially when doing a lot of animations (even just scrolling big text views) on a 4K display on MacOS, but maybe I'm thinking graphics and not input lag.There's two things to consider when comparing rendering performance: throughput and latency. Throughput, or how much FPS the engine can sustain, is much better in QML since it's leveraging the GPU, but latency it's very platform-dependent. Mac is actually the one where QML does best in terms of latency (and by that I believe it approaches the latency of Qt Widgets), since it's synchronizing with the VBlank signal provided by CVDisplayLink. On Windows and Android the situation is worse.\n \nreply",
      "Sidenote: Have you seen the new TextEdit improvements in Qt 6.7? I'm curious if that bridges the gap that you had when you started working on your app. My app is also text editing heavy so I'm hoping it's a big improvement.\n \nreply",
      "I saw them, they are a very small step in what I believe is the right direction, since you can use a custom textDocument. Anyway what I think would be useful is to jailbreak the QML API. Make the QML C++ API publicly available. Let us derive from the controls, manipulate and customize them with C++, as the Qt team devs themselves do.\n \nreply",
      "I recommend checking out Qt design studio. Once you get the flow on there, it creates a nice clean separation between the UI and backend.While it may seem like Qt Quick is designed for touch primarily, it is not actually the case. Modern UI/UX design is a bit more abstract, and requires a bit more skill to get working.My recommendation is to be patient, and work with Qt Quick it will pay off in the end (e.g porting the app to Android, etc). Focus on the UI/UX completely separately from the backend. And once that is established, the models can be developed in C++.\n \nreply",
      "I can't speak for the author, but many systems programmers look at declarative/markup-based UI as a kind of black magic that you can't trust and that will get in your way sooner or later by not leaving a sane way for you to address some project requirement.We/they are often way more comfortable working with the kind of programmatic widget and graphics library that we/they might write, and whose behavior we can debug and trace with our usual tools, then somebody else's weird new declarative/markup syntax that obscures what's actually going on behind the scenes and frustrates our tooling and workflow.And this instinct traces back many decades, for as long as visual RAD tools and declarative UI syntaxes first started being introduced by big vendors.\n \nreply",
      "Amen. I've coding in Qt since v1.x and have never felt the need to switch to QML.\n \nreply",
      "QML wasn't (at least in the Qt5 days) a good fit for traditional desktop style applications. It was nice for more multimedia touch screen stuff, but HTML5 in an embedded browser like CEF or Electron was catching up fast and with much better third party library support.\n \nreply"
    ],
    "link": "https://felipefarinon.com/articles/qt-widgets-layout/",
    "first_paragraph": "5 Sep 2024I\u2019ve been mostly involved with systems programming during my career, and only eventually I had to deal with user-facing applications. But since starting my work on  Cahier roughly a year and a half ago, the balance has shifted towards user-facing software, so I\u2019ve resumed my studies on UX and UI. The UI framework I chose to develop Cahier with is Qt, more specifically the Qt Widgets toolkit, because of its good support for desktop applications. While the road hasn\u2019t been without bumps, I\u2019m satisfied with what I\u2019m achieving and have been writing about some of the core concepts behind the toolkit. To follow up on my Qt Widgets Rendering Pipeline article, I\u2019d like to explore this toolkit\u2019s take on layout handling.UI frameworks are code libraries that provide input management, UI widgets and rendering APIs for application developers. Along the lifecycle of a UI, these widgets have to be shown and arranged on the screen according to the intentions of the developers, and frameworks",
    "summary": "**Understanding The Layout Process in Qt Widgets: A Cry for Help**\n\nToday in tech, a systems programmer dives headfirst into the heart-pounding world of <em>Qt Widgets</em>, choosing it over the sultry temptations of QML for the \"good support\" - a phrase we presume is now synonymous with \"it just barely works on desktops.\" In this thrilling edition of \"Why I'm Stuck with What I've Got,\" our hero outlines the myriad of ways in which Qt Widgets is less about coding and more about coping. Commenters chime in with tales of woe and nostalgia, each reaffirming their commitment to the beleaguered framework like survivors at a support group meeting. One daring soul suggests patience with Qt Quick, which is a bit like advising someone to enjoy the music while their ship sinks. \ud83c\udfbb\ud83d\udea2"
  }
]