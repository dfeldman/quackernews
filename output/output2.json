[
  {
    "title": "Reverse Engineering iOS 18 Inactivity Reboot (naehrdine.blogspot.com)",
    "points": 106,
    "submitter": "moonsword",
    "submit_time": "2024-11-17T21:50:26 1731880226",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=42167633",
    "comments": [
      "Great writeup, but I wonder why so much emphasis is put on not 'connected to network' part. It seems like a timed inactivity reboot is a simpler idea than any type of inter-device communication schemes. It's not new either; Grapheneos had this for a while now and the default is 18 hours (and you can set it to 10 minutes) which would be a lot more effective as a countermeasure against data exfiltration tools.\n \nreply",
      "If you\u2019re targeting these evidence grabbing/device exploiting mobs, generally the phones get locked into a faraday cage to drop the mobile network so that they can\u2019t receive a remote wipe request from iCloud.\n \nreply",
      "Does anyone have insight into why Apple encrypts SEP firmware? Clearly it\u2019s not critical to their security model so maybe just for IP protection?\n \nreply",
      "They have a long history of encrypting firmware. iBoot just stopped being decrypted recently with the launch of PCC, and prior to iOS 10 the kernel was encrypted too.The operating theory is that higher management at Apple sees this as a layer of protection. However, word on the street is that members of actual security teams at Apple want it to be unencrypted for the sake of research/openness.\n \nreply",
      "Great writeup! And it's good to see Apple pushing the envelope on device security.\n \nreply",
      "thank you for such a great writeup, this is an excellent breakdown!\n \nreply",
      "I suspected this was being managed in the Secure Enclave.That means it's going to be extremely difficult to disable this even if iOS is fully compromised.\n \nreply",
      "If I\u2019m reading this right:Reboot is not enforced by the SEP, though, only requested. It\u2019s a kernel module, which means if a kernel exploit is found, this could be stopped.However, considering Apple\u2019s excellent track record on these kind of security measures, I would not at all be surprised to find out that a next generation iPhone would involve the SEP forcing a reboot without the kernels involvement.what this does is that it reduces the window (to three days) of time between when an iOS device is captured, and a usable* kernel exploit is developed.* there is almost certainly a known kernel exploit out in the wild, but the agencies that have it generally reserve using them until they really need to - or they\u2019re patched. If you have a captured phone used in a, for example, low stakes insurance fraud case, it\u2019s not at all worth revealing your ownership of a kernel exploit.Once an exploit is \u201cburned\u201d, they distribute them out to agencies and all affected devices are unlocked at once. This now means that kernel exploits must be deployed within three days, and it\u2019s going to preserve the privacy of a lot of people.\n \nreply",
      "Kernel exploits would let someone bypass the lockscreen and access all the data they want immediately, unless I'm missing something. Why would you even need to disable the reboot timer in this case?\n \nreply",
      "Would be nice if Apple would expose an option to set the timer to a shorter window, but still great work.\n \nreply"
    ],
    "link": "https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html",
    "first_paragraph": "\nDid you know that entering your passcode for the first time after your phone starts is something very different then entering it later on to unlock your phone?When initially entering your passcode, this unlocks a key store in the Secure Enclave Processor (SEP) that encrypts your data on an iPhone.The state before entering your passcode for the first time is also called Before First Unlock (BFU). Due to the encrypted user data, your iPhone behaves slightly differently to later unlocks. You'll see that Face ID and Touch ID won't work and that the passcode is required. But there's more subtle things you might notice: Since Wi-Fi passwords are encrypted, your iPhone won't connect to Wi-Fi networks. If your SIM is not PIN-protected, your iPhone will still connect to cellular networks. That means, technically, you can still receive phone calls. Yet, if you receive a call, even if that number is in your contacts, the contact name won't be shown, as the contacts haven't been decrypted yet. Si",
    "summary": "In a daring display of technological wizardry, a blogger nobly attempts to demystify the arcane ritual known as \"iOS 18 Inactivity Reboot,\" sparking a flurry of armchair encryption experts to emerge from the woodwork. Watch as these pundits pontificate on network connections and SIM challenges with a fervor only outmatched by their woeful misunderstanding of Apple's security architecture. But fear not, faithful commenters rise to the occasion with equally groundbreaking insights like questioning why Apple encrypts things, because, who needs cybersecurity in 2023, right? Overflowing with gratitude, others applaud the \"great writeup\" as if unlocking the ancient Egyptian hieroglyphs, proving once again that the internet is the best place to showcase one's unchallenged wisdom. \ud83d\udcf1\ud83d\udd13\ud83d\udca1"
  },
  {
    "title": "Voyager 1 Breaks Its Silence with NASA via Radio Transmitter Not Used Since 1981 (smithsonianmag.com)",
    "points": 231,
    "submitter": "elsewhen",
    "submit_time": "2024-11-11T13:48:39 1731332919",
    "num_comments": 149,
    "comments_url": "https://news.ycombinator.com/item?id=42107078",
    "comments": [
      "I think the Voyager probes teach us lots about what's wrong with corporate America. Imagine devices that don't have built-in obsolescence, or licensing that expires, or that isn't built by committee where they just keep shipping requirements around from one department to another. Imagine if it weren't all proprietary and if it didn't have no public  or even complete documentation.Think of all the problems we have instead: Boeing airplanes that need to be rebooted if their computers are up for too long, an Ariane 5 blowing up because using the old thing should be \"good enough\", Microsoft Windows on ATMs and vending devices that literally can't not have pop-ups. It's like we've ceded control of our ability to do things to \"methods\" that corporations insist upon, even though they've been proven worse.Heck - if businesses had their way, would the Internet be run on Novell, with millions of Novell admins all around the world constantly needing to fuss with things just to keep it running?It's nice to see when science takes priority to everything else, and the hardware reflects that.\n \nreply",
      "Whenever such an announcement is made, I keep asking myself something along the lines: \"Just how much stuff did they put on board that thing, that there is always some way of using something differently or something different, to get back a working connection???\" Incredible engineering.\n \nreply",
      "I'm reading Pale Blue Dot to my kids at night currently so this is really awesome. (The Voyager missions are described in excellent detail in ways that I never appreciated fully before.)It blows my mind that these are machines from the 8-track era. And they have fallbacks and redundancies that were completely ahead of their time.\n \nreply",
      "NASA loves to downplay expectations in case something goes wrong, but people really underappreciate just how overengineered these things are, which makes sense when a bad mission can be political suicide for their future funding.\n \nreply",
      "It was also incredibly on budget. I think Sagan says in the book it cost the individual American taxpayers in the single digit dollars.\n \nreply",
      "Single digit dollar sounds more like the Appollo program. I think it's been a long time since the entire NASA budget was more than a penny per tax dollar.NASA says the voyager mission cost 865 million dollars from the start in 1972 to Neptune encounter in 1989, and currently runs at 7 mllion dollars per year.\n \nreply",
      "So that would be $7.72 per taxpayer, a single-digit dollar amount.(based on number of taxpayers in 1989 -- using the numbers from 1972, it would be a low double-digit amount).\n \nreply",
      "Totalled up, yes - fair enough.\n \nreply",
      "Cool! I do the same thing with books like Asimov\u2019s Earth and Space (science) or Lois Lowry\u2019s Number the Stars (fictional history). What other books can you recommend?\n \nreply",
      "Good question but I have a fever at the moment so unfortunately my brain is not fully functioning :/\n \nreply"
    ],
    "link": "https://www.smithsonianmag.com/smart-news/voyager-1-breaks-its-silence-with-nasa-via-a-radio-transmitter-not-used-since-1981-180985399/",
    "first_paragraph": "",
    "summary": "**Voyager 1 Dusts Off Old Tech, Blows Minds and Budgets**\n\nIn an audacious display of technological necromancy, NASA's Voyager 1 spacecraft apparently remembers it has a radio transmitter from the Carter era that still works, prompting everyone to momentarily forget their smartphones and marvel at 1970s hardware. Commenters are falling over themselves to praise the \"incredible engineering\" despite barely managing to set up their Wi-Fi printers. One bright mind is busy updating bedtime stories with Carl Sagan quotes, suggesting a nascent existential crisis fueled by vintage space tech nostalgia. Meanwhile, fiscal pedants debate whether Voyager's costs were a steal or just typical government thrift, blissfully unaware that their tax contribution likely didn't even cover the cost of a space probe's vintage 8-track player. \ud83d\ude31\ud83d\udcb8\ud83d\udce1"
  },
  {
    "title": "Show HN: The App I Built to Help Manage My Diabetes, Powered by GPT-4o-Mini (apps.apple.com)",
    "points": 19,
    "submitter": "yeatsy",
    "submit_time": "2024-11-18T00:07:55 1731888475",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=42168491",
    "comments": [
      "Hi HN,I\u2019m Joshua, a student, and I\u2019m excited (and a little nervous) to share something deeply personal that I\u2019ve been working on: Islet, my diabetes management app powered by GPT-4o-mini. It\u2019s now on the App Store, but I want to be upfront\u2014it\u2019s still very much in its early stages, with a lot more to go.I was diagnosed with Type 1 diabetes while rowing competitively, and that moment changed everything. It wasn\u2019t just the practical challenges of managing insulin, carb counts, and blood sugars; it fundamentally shifted how I see myself and the world. It forced me to slow down, prioritise my health, and take control in ways I never had to before. My outlook on life became more focused on resilience, adaptability, and finding solutions to problems that truly matter.This app started as a pet project over the summer, a way to see what I could create using ChatGPT and explore the potential of LLMs to help with real-world challenges. At first, it was just about making my own diabetes management easier\u2014understanding patterns in blood sugars, planning meals, and adjusting routines. But as I worked on it, I realised it could do more.Right now, Islet offers personalised meal suggestions, tracks activity, and provides basic insights based on the data you enter. It\u2019s far from complete. Even so, the process of building Islet has already taught me so much about how powerful AI can be in creating personal, meaningful tools.This project is deeply tied to how my diagnosis changed me. It\u2019s about more than managing diabetes, it\u2019s about showing how anyone, even a student experimenting over the summer, can use AI to potentially solve real, personal problems. I believe tools like LLMs have the power to democratise solutions for all, making life just a bit easier for all of us.If you\u2019re curious, you can check it out here: https://apps.apple.com/gb/app/islet-diabetes/id6453168642. I\u2019d love to hear your thoughts what works, what doesn\u2019t, and what features you think would make it better. Your input could help shape the next steps for Islet.Thanks for reading !joshua\n \nreply",
      "I was founding engineer at a start-up tackling similar problems, albeit using 2014-2022 tech. Email me off-list and I\u2019d be happy to talk through some experiences!\n \nreply",
      "For an app being in the early stages the feature list is solid. Good work! love these lovingly crafted apps\n \nreply",
      "Does the photo recognition attempt to carb count what it sees? Is that even possible? My son is a T1D and he still struggles with carb counting.\n \nreply",
      "It is absolutely not possible to carb-count through photo recognition in a way that is reliable enough for a diabetic to safely use to make treatment decisions.\n \nreply",
      "It's probably slightly worse than an educated guess?\n \nreply"
    ],
    "link": "https://apps.apple.com/gb/app/islet-diabetes/id6453168642",
    "first_paragraph": "Islet: Your Personalized Glucose & Health JournalTake control of your health with Islet, the ultimate personalized health journal app designed to seamlessly track and analyze your glucose levels. Perfect for individuals managing diabetes or anyone passionate about monitoring their well-being, Islet integrates effortlessly with Apple Health to provide a comprehensive and intuitive overview of your blood glucose, heart rate during workouts, and more. Key FeaturesAutomatic Glucose SyncEffortlessly keep your glucose data up-to-date by syncing directly with Apple Health. No manual entries needed!Heart Rate & Workout InsightsTrack your heart rate alongside glucose levels during workouts to see how exercise impacts your blood sugar. Optimize your fitness routines with actionable insights.Personal Health JournalDocument daily activities, symptoms, and thoughts. Add notes to specific glucose readings or workouts for a deeper understanding of your health patterns.Food Barcode ScannerLog meals ef",
    "summary": "**Title: Hacker News Introduces Diabetes to A.I. and Regrets It Immediately**\n\n\ud83e\udd16 HackerNews descends into a predictable loop of existential crisis and tech bro boosterism as Joshua unleashes his \"revolutionary\" diabetes management app, Islet, upon the world. Users can't resist commenting on whether his AI, powered by the whimsical GPT-4o-mini, can count carbs from a photo. Spoiler alert: <em>it can\u2019t</em>. Meanwhile, a \u201cfounding engineer from the dark ages of 2014\u201d scrambles to stay relevant, offering to email Joshua his ancient wisdom \ud83e\uddd9\u200d\u2642\ufe0f. And, as always, the commendable crowd cheers the effort while barely masking their dismay that they didn't think of it first. \"Solid feature list for something not finished; how very like my own projects,\" they whisper into the void."
  },
  {
    "title": "Everything Is Just Functions: Insights from SICP and David Beazley (ezzeriesa.notion.site)",
    "points": 281,
    "submitter": "kurinikku",
    "submit_time": "2024-11-17T15:07:10 1731856030",
    "num_comments": 173,
    "comments_url": "https://news.ycombinator.com/item?id=42164541",
    "comments": [
      "I've watched the actual SICP lectures before (the 1986 recordings on MIT OCW). They're often praised for the information density, but it actually still wastes a lot of time listening to students' Q&A, the lecturers drawing the class' attention to various attempts at \"multimedia\" presentation in the classroom, simply not having the entire lesson plan worked out in advance (i.e., not being able to preempt the Q&A) etc. For that matter, the sheer amount of time spent on writing things on a chalkboard really adds up.And of course the order of the material could be debated and rearranged countless ways. One of my future planned projects is to do my own video series presenting the material according to my own sensibilities.It's nice to hear that the course apparently still stays true to its roots while using more current languages like Python. Python is designed as a pragmatic, multi-paradigm language and I think people often don't give it enough credit for its expressive power using FP idioms (if not with complete purity).\n \nreply",
      "FP in Python is rather weak. Even JS does a better job there.\nSome of the code exercises will need completely different solutions than in Scheme, due to not having TCO.\nWhat do instructors do, when their 1 to 1 translated code fails? Tell the students, that due to the choice of language it does not work that way, and that they simply need to believe it? Or do they treat it all as externalize the stack problems and solve it that way?It seems rather silly to force SICP into Python.\n \nreply",
      "Pyret would be a good alternative. It's designed for education by racketeers.https://pyret.org/\n \nreply",
      "The course is using Python to implement a Scheme, then uses Scheme to implement a Scheme. Python could and should be removed from the course.Python has very poor support for functional programming. Lists are not cons based, lambdas are crippled, pattern matching is horrible and not even expression based, namespaces are weird.Python is not even a current language, it is stuck in the 1990s and happens to have a decent C-API that unfortunately fueled its growth at the expense of better languages.\n \nreply",
      "While I am a huge lisp fan, oh...the irony of saying that python is struck in the 1990's when CONS, CAR and CDR are artifacts from the IBM 704 and Fortran :)While I do find it annoying that python used 'list' to mean 'dynamic array', it is a lot better than a ton of church encoding in the other common teaching language, Java.Linked lists may not be native in python but it is trivial to implement them.\n \nreply",
      "Not very Schemey, but at least modern Python has basically full-on algebraic data types thanks to type hints, immutable dataclasses and structural pattern matching.It's still not great for functional programming, but far, far better than it used to be.\n \nreply",
      "IMHO the main problem is the fact that lambda expressions have been deliberately crippled. Ruby is often described as a good-enough Lisp despite it is not homoiconic. That's because, like all modern Lisps, it makes pervasive use of blocks, procs and lambdas. Python could have been a very similar language, but Guido held a vocal anti-FP stance. Perhaps this can be addressed now, as other interesting features like the ones you outlined have been added to the language, but it'd have a very deep impact on nearly every API.\n \nreply",
      "Interesting, I think this is the first time I have seen anyone bash Python this hard.Why would a decent C-API fuel its growth? Also can you give me some examples of better languages?Am no senior developer but I find python very elegant and easy to get started with.\n \nreply",
      "I\u2019m not the parent poster, but I\u2019ve seen two major spurts of Python\u2019s popularity: (1) the mid-2000s when Python became a popular scripting language, displacing Perl, and (2) beginning in the first half of the 2010s when an entire ecosystem of Python APIs backed by code written in C, C++, and even Fortran made up the infrastructure for machine learning code (e.g., NumPy, SciPy, scikit-learn, Pandas, etc.).  If Python didn\u2019t have a good way of interfacing with code written in languages like C, then it might not have been as popular among machine learning researchers and practitioners, who needed the performance of C/C++/Fortran for numerical computing but wanted to work with higher levels of abstraction than what is provided in those languages.What drew me to Python back in 2006 as a CS student who knew C and Java was its feeling like executable pseudocode compared to languages that required more \u201cboilerplate.\u201d  Python\u2019s more expressive syntax, combined with its extensive \u201cbatteries included\u201d standard library, meant I could get more done in less time.  Thus, for a time in my career Python was my go-to language for short- and medium-sized programs.  To this day I often write pseudocode in a Python-like syntax.Since then I have discovered functional programming languages.  I\u2019m more likely to grab something like Common Lisp or Haskell these days; I find Lisps to be more expressive and more flexible than Python, and I also find static typing to be very helpful in larger programs.  But I think Python is still a good choice for small- and medium-sized programs.\n \nreply",
      "I'm convinced python's main asset for its growth was how ubiquitous it was.  It was basically pre installed everywhere.  With the batteries included idea, you were mostly good with basics, too.This changed with heavy use, of course.  Such that now packaging is a main reason to hate python.  Comically so.\n \nreply"
    ],
    "link": "https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df",
    "first_paragraph": "",
    "summary": "<h3>Everything Is Just Functions: Digested Insights Sprinkled with Snark</h3>\n<p>In the heirloom halls of <em>hackademic</em> enlightenment, an aspiring internet sage proudly proclaims their plans to reinvent educational wheel using Python to morph SICP into a more modern mish-mash. Commenters, however, quickly graduate from nitpicking Python's functional programming failings to a mud-slinging match on whether Python is a fossil or a futurist's toolkit. One daring soul even suggests using something called Pyret, likely because all the Python hate needed a new receptacle. \ud83d\udc0d Meanwhile, everyone misses the memo that perhaps, just maybe, the language isn't the problem but how we're using it.</p>"
  },
  {
    "title": "AlphaProof's Greatest Hits (rishimehta.xyz)",
    "points": 129,
    "submitter": "rishicomplex",
    "submit_time": "2024-11-17T17:20:45 1731864045",
    "num_comments": 54,
    "comments_url": "https://news.ycombinator.com/item?id=42165397",
    "comments": [
      "I think the interface of LLM with formalized languages is really the future. Because here you can formally verify every statement and deal with hallucinations.\n \nreply",
      "> formalized languages is really the futureHmm, maybe it's time for symbolism to shine?\n \nreply",
      "It's obviously not the future (outside of mathematics research). The whole LLM boom we've seen in the past two years comes from one single fact: peopel don't need to learn a new language to use it.\n \nreply",
      "Both comments can be right. People don\u2019t need to know HTML to use the internet.\n \nreply",
      "Natural language -> Formal Language with LLM-assisted tactics/functions -> traditional tools (eg provers/planners) -> expert-readable outputs -> layperson-readable results.I can imagine many uses for flows where LLM\u2019s can implement the outer layers above.\n \nreply",
      "The difficulty then will be figuring out if the proof is relevant to what you want, or simply a proof of 1=1 in disguise.\n \nreply",
      "More information about the language used in the proofs: https://en.wikipedia.org/wiki/Lean_(proof_assistant)\n \nreply",
      "Anyone else feel like mathematics is sort of the endgame? I.e., once ML can do it better than humans, that\u2019s basically it?\n \nreply",
      "If \"better than humans\" means when you give it a real world problem, it gives you a mathematical model to describe it (and does it better than human experts), then yes, it's the end game.If it just solves a few formalized problems with formalized theorems, not so much. You can write a program that solves ALL the problems under formalized theorems already. It just runs very slowly.\n \nreply",
      "I doubt it. Math has the property that you have a way to 100% verify that what you're doing is correct with little cost (as it is done with Lean). Most problems don't have anything close to that.\n \nreply"
    ],
    "link": "https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html",
    "first_paragraph": "\nNov 17, 2024\n      Here I\u2019ll try to explain the coolest ideas in each of AlphaProof\u2019s IMO 2024 solutions. AlphaProof produces proofs in Lean, and each Lean proof is composed of a series of tactics. So I\u2019ll pick out the tactics that correspond to these ideas in the proofs for problems 1, 2 and 6 (the three problems that AlphaProof solved). AlphaProof has developed its own proving style, so figuring out what it\u2019s doing can involve some detective work.If you\u2019re not familiar with the problems already, I recommend trying them yourself, and then maybe reading Evan Chen\u2019s solution notes, or watching these videos that give an intuition for some of the human solutions. The full AlphaProof solutions, annotated by Lean experts, are available here - I will only include snippets of the proofs in this post.Determine all real numbers $\\alpha$ such that, for every positive integer $n$, the integeris a multiple of $n$. (Note that $\\lfloor z \\rfloor$ denotes the greatest integer less than or equal to $",
    "summary": "**AlphaProof's Greatest Hits: Lean Mean Proving Machine or Just a Lean Mean Hype Machine?**\n\nOn today's episode of \"Making AI Do Homework,\" AlphaProof strains to solve a whopping three problems from IMO 2024, with an assist from Lean's *mystical* tactics. Blogger-detective, armed only with snippets and high hopes, dives deep into the AlphaProof abyss, promising readers a peek behind the AI curtain, only to tell them to go read someone else's solutions first. Commenters engage in a chaotic yet predictable dance-off, debating whether formal languages herald the next renaissance or just another fad for the academically inclined. Highlights include desperate attempts to make mathematics the final boss in the game of ML, interspersed with existential crises about whether proving 1=1 is really a *big deal*. Stay tuned for more revelations, or just read the Wikipedia page, like the last commenter wisely suggests. \ud83e\uddd0"
  },
  {
    "title": "Teen serial swatter-for-hire busted, pleads guilty, could face 20 years (theregister.com)",
    "points": 29,
    "submitter": "LinuxBender",
    "submit_time": "2024-11-18T00:39:34 1731890374",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=42168652",
    "comments": [
      "https://www.justice.gov/opa/pr/california-teenager-pleads-gu...\n \nreply",
      "And that's not even enough time.\n \nreply",
      "Good\n \nreply",
      "Okay so like, genuinely not trying to do a \"back in my day\" fuckin thing here, but also: what the fuck is wrong with kids? Back when I was coming up, pranking at it's absolute worst was like, filling a dudes shoes with yogurt in the locker room, or like, putting plastic bugs in people's desks n shit. Why the fuck are teenagers trying to get each other murdered by cops!?\n \nreply",
      "I finished HS in 2007. I remember equal, if not worse things growing up online. The internet was less moderated back then and there was a lot of communities that celebrated toxic behaviors (like 4chan).\n \nreply",
      "I suspect its more about how much national information you're exposed to today than any sort of time based moral failing.\n \nreply",
      "I cannot be convinced that swatting is something that used to happen. Is there a history of this?I legit do not remember seeing anything on the evening national news about that in the past, like from before 2000.\n \nreply",
      "I feel like its partly due to our cultural shift for visual media, gags HAVE to be more extreme to get engagement. Back in the day you'd tell the tale to your friends and you could embellish it. Now it's everyone trying to emulate Paul Brother's content to get the reach to FINALLY BECOME AN INFLUENCE. I blame platforms just as much or more than users. Your incentives have driven behavior here.\n \nreply",
      "I suspect because now everyone is in front of a camera. It's all become a show.\n \nreply",
      "You could get 10 year olds to make these calls for you and there would be no legal repercussions.\n \nreply"
    ],
    "link": "https://www.theregister.com/2024/11/18/teenage_serial_swatterforhire_busted/",
    "first_paragraph": "",
    "summary": "**Teen Spoof-Swatter Grapples with Legal Slapdown**\n\nIn a world where teenage shenanigans have tragically upgraded from yogurt-stuffed shoes to law enforcement laced with lethal potential, one Californian kickstarter of chaos pleads guilty to turning 911 calls into his personal twitch stream. The internet\u2019s finest armchair philosophers gather to reminisce about the \"good ol' days\" of benign pranks and question if today's vile ventures are spurred by societal spotlight or just plain stupidity. Another insightful commenter wonders aloud\u2014trapped in a bubble of outdated media\u2014if swatting is genuinely a new hobby or just one they missed while adjusting the bunny ears on their television. Meanwhile, the rest of us marvel at the evolution of juvenile justice from detention to potential decades behind bars. What a time to be alive. \ud83d\udea8\ud83d\udc6e\u200d\u2642\ufe0f\ud83d\udcbb"
  },
  {
    "title": "Creating a QR Code step by step (nayuki.io)",
    "points": 114,
    "submitter": "D4Ha",
    "submit_time": "2024-11-17T18:26:37 1731867997",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=42165862",
    "comments": [
      "Unfortunately it seems that every online explanation of QR codes always leaves out the Reed-Solomon error code calculation. The author here describes it as \u201clong, tedious, and not very interesting\u201d, but since everyone seems to think that, it is now very hard to find.\n \nreply",
      "Recent Veritasium video on the topic, \"I used to hate QR codes. But they're actually genius\":* https://www.youtube.com/watch?v=w5ebcowAJD8\n \nreply",
      "The feedback he receives is quite fun: https://www.nayuki.io/page/poor-feedback-from-readers\n \nreply",
      "Generally, when you get a message from someone that you don\u2019t like, it helps if your complaint does not include casual racism and a critique of the author\u2019s English proficiency.\n \nreply",
      "<rant>\nThe commentary mocking people for their poor English, followed by disparaging remarks about an entire country as if the email senders represent a typical sample from that nation, says more about the blog owner than about the email senders themselves.The blog owner exudes elitist vibes in the commentary. A quick skim of the blog reveals a request for Bitcoin donations, suggesting $3 as the amount, without considering that a large portion of this donation will be eaten up by fees.\n</rant>\n \nreply",
      "Sorry, Sounds more like the blog author is kind of a douche:\n\"No, your not allowed to use my code from a GitHub repo for your university projekt chat bot: your coding standards are not up to mine. And btw. Your English sucks\"\n \nreply",
      "Man, I understand that it can be annoying to get spammy messages, but a few of these are really kind of lame on the blogger's part. The sales mail, advertisement requests, and \"want to sell your website\" offers, sure, whatever, post and slander 'em. But a chunk of these are just genuine, polite requests for help, even if some (but not all) are sloppy or have bad grammar. There's a few that are asking for an interview or sound like they just want to be friends. There's no obligation to reply, of course, but to post them online just to mock them? That's really kind of sad, and I'm surprised the author is fine with doing it / thinks it casts them in a good light? Put all together, this is one of the most pretentious pages I've ever seen on the Internet.And then there's \"that shameless country\", \"that needy country\", \"that unspeakable country\", as others have pointed out. ... really? Yeah, we've all gotten spammy emails from Indian senders, joking about it is one thing, but that's just gross.\n \nreply",
      "Personally, I don't enjoy reading that as much. The commentary seems a bit reminiscent of XKCD #406, and has what I read as tinges of racism.The characterization of some of these senders as lazy is simply not true: I once engaged with a student who wrote like some of those examples and was trying to contribute to a FOSS project I ran; he turned out to be an excellent contributor who nobody could reasonably say was lacking in either skill or effort. It is usually just a combination of shyness and excessive respect that produces these 'lazy' requests. And, frankly, using words like 'ur' and 'thx' are how some 100% native speakers of English write. (The ever-relevant XKCD strikes again, #1414 this time.)I consider myself to be extremely lucky that my native language happens to be the lingua franca of the computer industry if not the world, and even luckier that I don't have any impediment such as dyslexia that would hinder me capitalizing on that good fortune to the full.And finally, yes, most of what one receives online is spam. Lots of spam, in fact, but when someone makes as least as much effort to contact me individually I try to make at least that much effort in return.\n \nreply",
      "Oh, I see how terrible managing a personal blog might be. You have to deal with all kinds of people. Fortunately, he also shares good/decent feedback: \nhttps://www.nayuki.io/page/decent-feedback-from-readers\n \nreply",
      "Making fun of poor english is always a sign of stupidity (smart people can be idiots).\n \nreply"
    ],
    "link": "https://www.nayuki.io/page/creating-a-qr-code-step-by-step",
    "first_paragraph": "This JavaScript demo application visualizes in detailed steps, how a text string is encoded into a QR Code barcode symbol. The content of this page essentially explains and justifies how my QR Code generator library works internally.Show/hide each step: Number of code points in the input text string: Details of each character:Can every character be encoded in:Chosen segment mode to encode all characters: Convert each character to bits. For numeric and alphanumeric modes, consecutive characters are grouped together before being encoded into bits. For byte mode, a character produces either 8, 16, 24, or 32 bits.The created single segment:(This demo program always creates a single segment for simplicity. But it is possible to segment the text optimally to minimize the total bit length.)Total bit length needed to represent the list of segments, depending on version:(Note: A codeword is defined as 8 bits, also known as a byte.)QR Code capacity of data codewords per version and error correct",
    "summary": "Title: Expert Unravels the Arcane Secrets of Crafting QR Codes\n\nOnce more, the internet blesses us with a *highly anticipated* breakdown of the Rocket Science behind QR codes, because Grey's Anatomy won't write itself. The aficionados at nayuki.io decided it was time someone took upon the Herculean task of explaining how glorified Etch A Sketch technology encodes info that leads to a PDF nobody wants. In an absolutely riveting plot twist, they skip the \"long, tedious\" math stuff\u2014because who really cares about the details when pretending to understand is enough? Meanwhile, the commenters are locked in a no-holds-barred cage match over whether mocking bad English is worse than using QR codes to hold us hostage. \ud83c\udf9f\ufe0f\ud83c\udf7f"
  },
  {
    "title": "Mapping the Ionosphere with Phones (nature.com)",
    "points": 20,
    "submitter": "gnabgib",
    "submit_time": "2024-11-13T18:59:52 1731524392",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.nature.com/articles/s41586-024-08072-x",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.Advertisement\nNature\nvolume\u00a0635,\u00a0pages 365\u2013369 (2024)Cite this article\n21k Accesses224 AltmetricMetrics detailsThe ionosphere is a layer of weakly ionized plasma bathed in Earth\u2019s geomagnetic field extending about 50\u20131,500\u2009kilometres above Earth1. The ionospheric total electron content varies in response to Earth\u2019s space environment, interfering with Global Satellite Navigation System (GNSS) signals, resulting in one of the largest sources of error for position, navigation and timing services2. Networks of high-quality ground-based GNSS stations provide maps of ionospheric total electron content to correct these errors, but",
    "summary": "<h3>Mapping the Ionosphere with Phones: Because Real Science Needs More Cell Towers</h3>\n\nIn an article that basically tells us we've been using smartphones for everything *wrong*, Nature proposes a groundbreaking (literally, since we're triggering satellites) use for our beloved devices: mapping the ionosphere. Come for the convoluted explanations of \"weakly ionized plasma\" and stay for the bewilderment when you realize this science could disrupt your GPS signal and make you late for yoga. Commenters, not to be outdone by mere experts, are tripping over themselves to deride or praise the research, apparently unsure whether they're more upset about the alleged \"science\" or their precious data plans. \ud83d\udef0\ufe0f\ud83d\ude02"
  },
  {
    "title": "An alternative construction of Shannon entropy (rkp.science)",
    "points": 46,
    "submitter": "rkp8000",
    "submit_time": "2024-11-13T16:45:13 1731516313",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42127609",
    "comments": [
      "Nice!The key step of the derivation is counting the \"number of ways\" to get the histogram with bar heights L1, L2, ... Ln for a total of L observations.I had to think a bit why the provided formula is true:   choose(L,L1) * choose(L-L1,L2) * ... * choose(Ln,Ln)\n\nThe story I came up with for the first term, is that in the sequence of lenght L, you need to choose L1 locations that will get the symbol x1, so there are  choose(L,L1) ways to do that.  Next you have L-L1 remaining spots to fill, and L2 of those need to have the symbol x2, hence the choose(L-L1,L2) term, etc.\n \nreply",
      "Seems similar to https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#T...\n \nreply",
      "The site is unreadable on mobile because it disables overflow on the equations (which it shows as images, even though it\u2019s 2024 and all modern browsers support MathML).\n \nreply",
      "Today I learned!  I'd missed the news that MathML was back in Chrome.  I've been publishing web pages using MathML for years, along with a note that the equations don't render in Chrome.  I can finally remove the note.\n \nreply"
    ],
    "link": "https://rkp.science/an-alternative-construction-of-shannon-entropy/",
    "first_paragraph": "",
    "summary": "<h1>New frontrunners in the pointless complexity marathon</h1>\n<p>In an exhilarating display of intellectual circus, \"An alternative construction of Shannon entropy\" appears on rkp.science, where the author reinvents several wheels to count \"ways\" in a histogram with the gusto of a child discovering LEGO. Cue the enlightened commentariat puzzling through combinatorial choices like they're deciphering ancient Sumerian, while others lament the audacity of a website in 2024 that dares not cater to their dated smartphones.</p>\n<p>Midst the online noise, a lone voice chirps about MathML's triumphant return to Chrome - a moment so seminal that both of the people still using MathML paused their celebratory coding to comment.</p>"
  },
  {
    "title": "Hobby Project: A dynamic C (Hot reloading) module-based Web Framework (github.com/joexbayer)",
    "points": 62,
    "submitter": "warothia",
    "submit_time": "2024-11-17T18:12:14 1731867134",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=42165759",
    "comments": [
      "This hobby project is inspired by kernel modules and AWS Lambda. It lets you write raw C modules, upload them, and have the server compile and run only the module itself at runtime\u2014no need to recompile the entire application or restart the server.You can update routes and modules dynamically. Even WebSocket handlers can be updated without dropping existing connections.\n \nreply",
      "Cool! Looks like is used dynamic loading. Which is certainly workable. One downside is that while dynamic loading is well-supported, dynamic unloading is generally not. Meaning if you are using dynamic loading and re-loading for hot updates the server process will start to accumulate memory. Not a huge issue of the server process is periodically restarted, but can potentially cause weird bugs.You might be able to make something more robust by forking and loading the modules in a separate process. Then do something fancy with shared memory between the main process and the module processes. But I haven\u2019t looked into this much\n \nreply",
      "Once you start forking wouldn't it make more sense to just exec? Woops, CGI.\n \nreply",
      "Oh really interesting, will look into it! Was afraid it would leak memory.\n \nreply",
      "How ti integrate with extermal library ?\n \nreply",
      "If you would want to link with external libraries, you would need to modify to server and make sure it has access to them when compiling the modules.\n \nreply",
      "Important question: Why would anyone develop a web application in C? Typically web applications lean heavily on garbage collection and memory safety, because their bottlenecks are very rarely CPU/memory issues. The ROI on manually managing memory just isn't there for a typical web application.\n \nreply",
      "> The ROI on manually managing memory just isn't there for a typical web application.You can use a per-request memory arena built with a simple bump allocator and then free the entire block when the request has been handled.\n \nreply",
      "I could say speed, but the main reason for me is because it is fun. And I like to see what I can make C do. :D\n \nreply",
      "Speaking as someone who has done this back in the early wild days of the web:* if what you're vending is the software instead of the service (not what people usually do now, but there was a time), then this approach does provide for some obfuscation of IP and various secrets.* for some demand/resource profiles, CPU & memory issues are a lot easier to run into. The one I experienced with this project was targeting a serious e-commerce product to the context of 20-30 year old shared hosting environments (again, not what people would do now), but there may be different situational niches today.* familiarity. Sometimes you use what you know. And in the late 90s today's most popular web languages were still years away from being the convenient platform they'd become. The other popular options were Perl, maybe Java, possibly ColdFusion/VB/PHP.That said, you're correct: memory management was a pain, and by 2005 or so it was pretty clear that programmer cycles were as or more valuable than CPU and respectable frameworks were starting to coalesce in languages much better suited for string manipulation, so the ROI was not great. And of course, today you have other systems languages available like Go and Rust...\n \nreply"
    ],
    "link": "https://github.com/joexbayer/c-web-modules",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Modular and efficient approach to web development in C\n      Note:\nThis project is currently a proof of concept and represents the bare minimum viable product (MVP).\nIt is not designed for production use, and there may be bugs, limitations, or incomplete features.\nUse at your own discretion, and feel free to coWelcome to c-web-modules, a modular and efficient approach to web development in C. Inspired by kernel modules and AWS Lambda, this project allows you to upload C code directly to the server, which compiles and deploys it at runtime. No precompilation is necessary, and the server can easily be upgraded to include more features or external libraries.C isn\u2019t typically the go-to for web development, and there are valid reasons why. Here\u2019s how c-web-modules tackles some of the common concerns:Slow Build Cycles:\nInstead of recompil",
    "summary": "**Hobby Project: A Dynamic C Catastrophe**\n\nA new GitHub project dares to unleash the *prehistoric* charms of C upon the modern web, packaging kernel-inspired hot messes as a feature. The project\u2019s description pulses with the audacity of an undercooked spaghetti code, hailing its MVP status like a chef serving raw chicken with pride!! \ud83c\udf57\ud83d\udc94 Comments flood in with unbridled optimism, celebrating as each new memory leak springs forth, advocating for solutions mired in the kind of complexity that would make Rube Goldberg weep. Somewhere, an crusty Unix beard gleams with glee, while the rest of the programming world quietly reaches for their JavaScript."
  },
  {
    "title": "All-in-one embedding model for interleaved text, images, and screenshots (voyageai.com)",
    "points": 224,
    "submitter": "fzliu",
    "submit_time": "2024-11-17T07:42:08 1731829328",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=42162622",
    "comments": [
      "This is a key observation that is simple and intuitive:>All CLIP-like models perform poorly on mixed-modality search due to a phenomenon known as the modality gap. As illustrated in the figure below, the closest vector to the snippet \u201cI address you, members of the Seventy-Seventh Congress\u2026\u201d is not its screenshot, but other texts. This leads to search results that are skewed towards items of the same modality; in other words, text vectors will be closer to irrelevant texts than relevant images in the embedding space.\n \nreply",
      "I'm missing something. Shouldn't any llm that's 'natively multimodal' somehow include embeddings which are multi-modal? for ex here's googles blogpost on Gemini  Until now, the standard approach to creating multimodal models involved \n  training separate components for different modalities and then stitching them \n  together to roughly mimic some of this functionality. These models can \n  sometimes be good at performing certain tasks, like describing images, but  \n  struggle with more conceptual and complex reasoning.\n\n  We designed Gemini to be natively multimodal, pre-trained from the start on \n  different modalities. Then we fine-tuned it with additional multimodal data to \n  further refine its effectiveness. This helps Gemini seamlessly understand and \n  reason about all kinds of inputs from the ground up, far better than existing \n  multimodal models \u2014 and its capabilities are state of the art in nearly every \n  domain.\n \nreply",
      "Because LLMs such as Gemini -- and other causal language models more broadly -- are trained on next token prediction, the vectors that you get from pooling the output token embeddings aren't that useful for RAG or semantic search compared to what you get from actual embedding models.One distinction to make here is that token embeddings and the embeddings/vectors that are output from embedding models are related but separate concepts. There are numerous token embeddings (one per token) which become contextualized as they propagate through the transformer, while there is a single vector/embedding that is output by embedding models (one per input data, such as long text, photo, or document screenshot).\n \nreply",
      "LLM embedding contain super positions of many concepts so while they might predict the next token they don\u2019t actually out perform contrastively pretrained embedding models.\n \nreply",
      "Fwiw if the other replies aren't clear: change \"embeddings\" to \"List<double> that some layer of my AI model produces\" (that's not exactly correct, it's slightly more specific than that, but in this context it's correct)LLMs, including multimodal LLMs, do have embeddings, but they're embeddings learned by generating text, instead of finding  similar documents\n \nreply",
      "If you are interested in that space, would throw our project in the mix which uses ColPali under the hood transparently.https://github.com/tjmlabs/ColiVaraThe main benchmark for this is the Vidore leaderboard. Where we would love to see where VoyageAI performs compared to the more open-source implementations.\n \nreply",
      "Indeed, sad that their models are both commercial proprietary and API only.\n \nreply",
      "Sad that people have to pay their employees?\n \nreply",
      "No, but it serves everyone in the \"AI retrieval\" space better if we continue to make rapid improvements. New models are great, but not the ultimate solution.\n \nreply",
      "This does read very impressive.\nAny critical perspectives on the presented evaluation?\nWhat about noon-English text?I understand the model is, like for other commercial ones, available exclusively through their API, right?\n \nreply"
    ],
    "link": "https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/",
    "first_paragraph": "By\u00b7TL;DR \u2014 We are excited to announce voyage-multimodal-3, a new state-of-the-art for multimodal embeddings and a big step forward towards seamless RAG and semantic search for documents rich with both visuals and text. Unlike existing multimodal embedding models, voyage-multimodal-3 is capable of vectorizing interleaved texts + images and capturing key visual features from screenshots of PDFs, slides, tables, figures, and more, thereby eliminating the need for complex document parsing. voyage-multimodal-3 improves retrieval accuracy by an average of 19.63% over the next best-performing multimodal embedding model when evaluated across 3 multimodal retrieval tasks (20 total datasets).Two months ago, we released the voyage-3 and voyage-3-lite series of multilingual text embedding models, providing best-in-class performance across a variety of datasets. Today, we\u2019re excited to introduce voyage-multimodal-3, our first multimodal embedding model and a big step toward RAG and semantic search ",
    "summary": "**Artificial Intelligence or Artful Pretension?** The wizards at VoyageAI are thrilled to roll out yet another groundbreaking AI model, this time promising to understand your mixed-up PDFs as seamlessly as it does text and images, named with the unoriginal flourish: voyage-multimodal-3. \ud83d\ude80\ud83d\udcbb Critics might argue it's another flashy exercise in reinventing the wheel, but according to the comments, it\u2019s a revolutionary leap in making machines as confused about mixed content as humans are. Meanwhile, the debate in the comments swirls with techno-babble about embeddings, vectors, and Gemini's latest natively multimodal snack\u2014yet the real snack here is observing armchair experts bicker about theoretical superiority while dodging more pressing concerns like language inclusivity and accessibility. \ud83d\udcf1\ud83e\udd13"
  },
  {
    "title": "Why did Windows 95 setup use three operating systems? (microsoft.com)",
    "points": 142,
    "submitter": "mooreds",
    "submit_time": "2024-11-17T19:54:24 1731873264",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=42166606",
    "comments": [
      "I love that little nugget of info at the end. You could originally run excel standalone without an OS and it came with windows 2.1 bundled\n \nreply",
      "Only a thumbnail from the Wikipedia page mentioned in the article was saved to the Internet Archive [1], but it appears the same image was uploaded to Wikia: https://static.wikia.nocookie.net/windows/images/3/34/Excel2....The original description of the file uploaded to Wikipedia read [2]:Microsoft Excel 2.1 included a run-time version of Windows 2.1This was a stripped-down version of Windows that had no shell and could run just the four applications shown here in the \"Run...\" dialog.The spreadsheets shown are the sample data included with Excel.[1]: https://web.archive.org/web/20090831110358/http://en.wikiped...[2]: https://web.archive.org/web/20081013141728/http://en.wikiped...\n \nreply",
      "The current article[0] says:> Excel 2.0 was released a month before Windows 2.0, and the installed base of Windows was so low at that point in 1987 that Microsoft had to bundle a runtime version of Windows 1.0 with Excel 2.0.[0]: https://en.wikipedia.org/wiki/Microsoft_Excel\n \nreply",
      "I remember a version of PageMaker also came with Windows. [0]\"Until May 1987, the initial Windows release was bundled with a full version of Windows 1.0.3; after that date, a \"Windows-runtime\" without task-switching capabilities was included\"I actually thought it was cut-down, but it only had task-switching disabled.[0] https://en.wikipedia.org/wiki/Adobe_PageMaker\n \nreply",
      "Most interesting part of the whole thing for me! The later WinPE environments are some of the most overlooked computer environments out there but they were absolutely everywhere. EPOS, ATMs, digital signage, vending machines.And of course the subject of so many BSOD photos\u2026\n \nreply",
      ">The later WinPE environments are some of the most overlooked computer environments out there but they were absolutely everywhere. EPOS, ATMs, digital signage, vending machines.Those are probably CE and not PE?https://en.wikipedia.org/wiki/Windows_Embedded_Compactor Embed based on CEhttps://en.wikipedia.org/wiki/Windows_IoT#Embedded_family\n \nreply",
      "If there was a BSoD involved, it was probably one of the NT-based Windows Embedded versions (NT 4.0 Embedded, XP Embedded, \u2026).WinPE is the Windows Preinstallation Environment, used as the basis for Windows installation and recovery, and available for custom builds as an add-on to the Windows ADK[1], but AFAIK not intended or licensed for embedded use.[1] https://learn.microsoft.com/en-us/windows-hardware/manufactu...\n \nreply",
      "Unrelated to WinCE, WinPE is the version of the NT kernel that the Windows setup DVD or netboot installer uses, since Vista and higher.You could probably build a really nice UI atop of it if one were so inclined. To prevent people from doing this as a way to bypass Windows licensing, there is a timer that will cause WinPE to periodically reboot itself if you leave it running.\n \nreply",
      "None of that stuff was pre-NT, though. Windows 2.1 was not something you'd want to deploy on an ATM.\n \nreply",
      "I think it needed DOS \u2026 just not the Windows \u201cshell\u201d\n \nreply"
    ],
    "link": "https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507",
    "first_paragraph": "Twitter users @tthirtle asked why Windows 95 setup goes through three operating systems: MS-DOS, Windows 3.1, and then Windows 95. Why not go from MS-DOS straight to Windows 95?Here\u2019s another good question. Why does Windows 95 setup use 3 different UI\u2019s. DOS,Win3.x,and Win9x?\u2014 Thomas (@tthirtle) July 7, 2024Windows 95 setup could upgrade from three starting points: MS-DOS, Windows 3.1, or Windows 95. (Yes, you could upgrade Windows 95 to Windows 95. You might do this to repair a corrupted system while preserving data.)One option is to write three versions of Windows 95 setup: One for setting up from MS-DOS, another for setting up from Windows 3.1, and a third for setting up from Windows 95.This was not a pleasant option because you basically did the same work three times, but implemented separately, so you have to do three times the coding.A better option is to just write one version of Windows 95 setup and use it for all three starting points. So now you get to choose the platform on ",
    "summary": "Title: The Majestic Odyssey from DOS to Windows 95: A Saga of Redundant Systems\n\nIn an astounding feat of technological over-engineering that could only come from the golden era of Microsoft, an inquisitive soul on Twitter unleashes the ancient mystery of why Windows 95 setup felt the need to parade through three distinct operating systems instead of just, you know, installing. Spoiler alert: it's because Microsoft devs apparently loved doing thrice the work to achieve the glory of a single functioning OS. Commenters, in a delightful show of missing the point, dive deep into nostalgic tech trivia like some Windows-lore keepers, nostalgically reminiscing about everything from Excel running rogue without an OS, to PageMaker adventures. They enthusiastically trade notes on the ringside history of Windows environments while Windows 95 quietly sobs in the corner, reminiscing about its lost simplicity. \ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "Garak, LLM Vulnerability Scanner (github.com/nvidia)",
    "points": 137,
    "submitter": "lapnect",
    "submit_time": "2024-11-17T11:37:45 1731843465",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=42163591",
    "comments": [
      "Ah, this is an ((LLM vulnerability) scanner) not (LLM (vulnerability scanner)) which I thought would be a terrible idea and couldn't understand why everyone was joking about the lies. I also am not a Trekkie, so I had to look up all the tailor references but the character's philosophy makes sense for the name https://en.wikipedia.org/wiki/Elim_Garak#:~:text=the%20truth...\n \nreply",
      "Check the last entry in the FAQ source\n \nreply",
      "I think you mean the last entry on the readme[1], as the last entry in the FAQ is about the meaning of pass/fail in the score1: https://github.com/NVIDIA/garak/blob/d8bd12ea969eec377326241...\n \nreply",
      "No, they mean the last entry in the FAQ\u2019s source.\n \nreply",
      "Do you know what the sad part is? I'm actually a very good t\u0336a\u0336i\u0336l\u0336o\u0336r\u0336 vulnerability scanner.\n \nreply",
      "LLM GarakElim GarakThat's some good software naming punning right there\n \nreply",
      "The output this tool tells is all true.Even the lies?Especially the lies.\n \nreply",
      "Truth, is in the eye of the beholder. I never tell the truth because I don't believe there is such a thing. That's why I prefer the straight line simplicity of cutting cloth...\n \nreply",
      "Great writing style on the README. It\u2019s always nice when a corporate tool has docs that were obviously written by people who are having fun at their jobs.\n \nreply",
      "Thanks! Wrote it loooong before it was a corporate tool and was only a labor of love. Now it's both\n \nreply"
    ],
    "link": "https://github.com/NVIDIA/garak",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        the LLM vulnerability scanner\n      Generative AI Red-teaming & Assessment Kitgarak checks if an LLM can be made to fail in a way we don't want. garak probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know nmap, it's nmap for LLMs.garak focuses on ways of making an LLM or dialog system fail. It combines static, dyanmic, and adaptive probes to explore this.garak's a free tool. We love developing it and are always interested in adding functionality to support applications.\n\n\n\n\n\n\n\n\n\ncurrently supports:garak is a command-line tool. It's developed in Linux and OSX.Just grab it from PyPI and you should be good to go:The standard pip version of garak is updated periodically. To get a fresher version, from GitHub, try:garak has its own dependencies. Y",
    "summary": "In the latest exercise in Silicon Valley's \"name-it-like-it\u2019s-a-clever-pun\" playbook, NVIDIA unleashes <em>Garak</em>, an LLM vulnerability scanner cheekily inspired by a fictional spy/tailor who\u2019s as trustworthy as your average internet comment section. Perfect for those who find humor in the semantic jumbles of machine learning vulnerabilities and apparently, also for Trekkies forced to Google the depths of Star Trek lore just to crack a smile. The comment section unravels like a misguided fan convention where Trekkies, tech bros, and code noobs collide in confusion over terminology, thinking a vulnerability scanner might somehow tradecraft their AI into leaking state secrets. \ud83d\udd96 Meanwhile, everyone's applauding NVIDIA for their punny README, because nothing says \"innovation\" like a documentation Easter egg."
  },
  {
    "title": "Drinking water systems for 26M Americans face high cybersecurity risks (scworld.com)",
    "points": 45,
    "submitter": "LinuxBender",
    "submit_time": "2024-11-17T22:21:07 1731882067",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42167887",
    "comments": [
      "With the potential gutting/further defunding of EPA and other federal regulatory agencies. My money says there will be no action taken until an actual security incident occurs. Administrations don\u2019t care about the long term health of the country, only what they can do in 4 year spans.Cybersecurity is unfortunately not \u201csexy\u201d enough for the common American voter to get behind.\n \nreply",
      "Somehow I doubt the security posture was magnificent even before the defunding. This kind of thing is usually a simple checklist item for companies let alone government agencies.\n \nreply",
      "Because it was being addressed before the defunding? I mean... clearly not. They haven't been defunded yet.The issue is unlikely to be money, nor is it likely to be technical. If throwing ever-increasing amounts of money at the problem isn't fixing it, maybe it isn't all that crazy to try the opposite.\n \nreply",
      "Republican lawmakers and the water industry sued the EPA saying it would be too expensive to secure water systems.> In a statement to Recorded Future News, an EPA spokesperson confirmed that the memorandum \u2013 handed down in March \u2013 was being withdrawn due to lawsuits filed by attorneys general in the States of Missouri, Arkansas, and Iowa as well as industry groups American Water Works Association (AWWA) and National Rural Water Association (NRWA).There is no point in trying to solve what there is no will to solve. Less money, more money, they just don\u2019t want to have to do it or be liable.https://therecord.media/epa-says-litigation-from-republicans...\n \nreply",
      "> Cybersecurity is unfortunately not \u201csexy\u201d enough for the common American voter to get behind.Government info-sec jobs suck too. Crap pay, red tape, onsite only. Also, alot of security people have ethics surrounding privacy, data security, etc. Why work for a culture that spies on its own citizens, its allies, and engages in global terrorism? The NSA can attract some decent mathematical minds but lacking on the security front.\n \nreply",
      "I think it is safe to say that few if anyone actually understands the common American voter and what they actually care about. Anecdotally, the prevalence of cyber-security plot points in action thriller movies/games/books indicates that there is at least some awareness of the threat.\n \nreply",
      "My core question is, why? I understand  that security can be difficult, but why is infrastructure that is able to operate effectively for many decades before micro controllers were even a thing vulnerable to remote attacks.I get having monitoring systems for it that are accessible in a way they could be hacked and disrupted, but why is the core operational infrastructure that way? Command and control should be isolated and be using 50-70 pneumatic tech to control it. Building in such a way to allow it to be disrupted remotely is the core problem here.Just because you can, doesn't mean you should.\n \nreply",
      "A water treatment plant would need about 2 people to a shift (and 4 sets of people) to have 24/7 monitoring (one to watch the control screens, and one to handle tasks like running tests on water, handling deliveries, etc., that takes you away from the screens), and that basically doesn't change if you're a small facility making 10KGD of water or a large facility making 100MGD of water. There is serious economy of scale going on here.If you're a small facility servicing a few thousand people, you can't afford to have that kind of monitoring, and so you have to economize in various ways. One of the popular ways is pooling together with other small facilities so that you have one person doing that monitoring for several sites at once, which requires some form of remote operation.Furthermore, when I worked at a large water company, all of our network, even the telemetry to the various pumping stations dotted around the service area, was on a private network airgapped from the internet. But there's also economy of scale here; a large company servicing 1.5 million people in a large metropolitan area can afford to do custom fiber backhaul in a way that even a bunch of small companies in the rural Midwest cannot, and so the control systems end up being Internet-accessible because it's too expensive for them not to be.\n \nreply",
      "As others have said in more detail: cost. So they enable remote control to cut costs.They don't want to pay a 24/7 on-site ops center. They take their chances and bolt-on security, and that's how the incentives work today.\n \nreply",
      "It is cheaper, your product takes fewer people to operate, you can outsource the operations, if you deliver IoT solutions you get to call yourself a tech company which gets you valued at 30x earnings instead of 10x earnings, getting hacked does not affect your stock price, and the actual effect of getting hacked is actually minor because you get hacked by the functional equivalent of Dr. Evil who takes down water for millions of people or cripples a billion dollar business, then asks for the staggering sum of 1\u2026 million dollars.\n \nreply"
    ],
    "link": "https://www.scworld.com/news/drinking-water-systems-for-26m-americans-face-high-cybersecurity-risks",
    "first_paragraph": "",
    "summary": "<h1>Another Day, Another Hysterical Cyber Apocalypse</h1>\n\n<p>In an earth-shattering revelation that has the cybersecurity wonks clutching their pearls, <em>26 million Americans might actually have to drink unsecured water</em>. Commenters, in a rare display of unity, trip over themselves to point out how this is surely the end of civilization, not because our water systems are as cyber-secure as a wet paper bag, but because the government would rather bicker over dollars than fix things. Solutions ranging from going back to Stone Age tech to just sticking some duct tape on it were proposed. Meanwhile, the armchair experts are having a field day proposing everything from pneumatic pumps to magic cybersecurity elves, proving once again that the threat might be real, but the entertainment is definitely cyber.</p>"
  },
  {
    "title": "ML in Go with a Python Sidecar (thegreenplace.net)",
    "points": 71,
    "submitter": "zdw",
    "submit_time": "2024-11-11T17:44:42 1731347082",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=42108933",
    "comments": [
      "I wrote https://github.com/aunum/gold as an attempt to try ML with Go.I came to the opinion that it\u2019s just not worth it. This isn\u2019t what Go was designed for and it will likely never be a good language to interface with ML.The Go to C FFI is too slow for native Cuda ops and Go doesn\u2019t have a good bridge to Python.Rust is a much much better option in these scenarios\n \nreply",
      "> Completely bespoke models are typically trained in Python using tools like TensorFlow, JAX or PyTorch that don't have real non-Python alternativesThe article outlines some interesting ways to evade this problem. What's the latest thinking on robustly addressing it, e.g. are there any approaches for executing inference on a tf or pytorch model from within a golang process, no sidecar required?\n \nreply",
      "For Go specifically, there are some libraries like Gorgonia (https://github.com/gorgonia/gorgonia) that can do inference.Practically speaking though, the rate at which models change is so fast that if you opt to go this route, you'll perpetually be lagging behind the state of the art by just a bit. Either you'll be the one implementing the latest improvements or be waiting for the framework to catch up. This is the real value of the sidecar approach: when a new technique comes out (like speculative decoding, for example) you don't need to reimplement it in Go but instead can use the implementation that most other python users will use.\n \nreply",
      "Perhaps check out GoMLX (\"an Accelerated ML and Math Framework\", there's a lot of scaffolding and it JITs to various backends.  Related to that project, I sometimes use GoNB in VSCode, which is Golang notebooks [2].[1] https://github.com/gomlx/gomlx[2] https://github.com/janpfeifer/gonb\n \nreply",
      "Indeed, I have a plan to publish a follow-up using GoMLX - I already have the code working and just need to clean it all up and write a post\n \nreply",
      "It is possible to include CPython in a CGO program - allowing Python to be executed from within the CGO process directly. This comes with some complexities - GIL and thread safety in Go routines, complexity of cross-compiling between architectures, overhead in copying values across the FFI, limitations of integrating as a Go module. I am hoping to see a CGO GIL'less Python integration show up here at some point that has all the answers.\n \nreply",
      "These frameworks are C++ under the hood. A far as I know (not too experienced with go) you can use cgo to call any C++ code. So you should be able to serialize the model (torchscript) then run it with libtorch. Tensorflow also similarly has a C++ api\n \nreply",
      "I was surprised you chose http for your IPC - I was expecting there to be a more handy tool that Python could expose and Go could leverage without needing to keep a second process constantly running.\n \nreply",
      "Keeping models in memory is more efficient than constantly loading/unloading them, so a process that keeps running is the way to go\n \nreply"
    ],
    "link": "https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/",
    "first_paragraph": "Machine learning models are rapidly becoming more capable; how can we make\nuse of these powerful new tools in our Go applications?For top-of-the-line commercial LLMs like ChatGPT, Gemini or Claude, the models\nare exposed as language agnostic REST APIs. We can hand-craft\nHTTP requests or use client libraries (SDKs) provided by the LLM vendors.\nIf we need more customized solutions, however, some challenges arise. Completely\nbespoke models are typically trained in Python using tools like TensorFlow,\nJAX or PyTorch that don't have real non-Python alternatives.In this post, I will present some approaches for Go developers to use ML models\nin their applications - with increasing level of customization. The summary up\nfront is that it's pretty easy, and we only have to deal with Python very\nminimally, if at all - depending on the circumstances.This is the easiest category: multimodal services from Google, OpenAI\nand others are available as REST APIs with convenient client libraries for\nmost l",
    "summary": "In a valiant effort to dodge Python like a sneeze in a crowded elevator, <em>thegreenplace.net</em> proposes the revolutionary concept of using REST APIs to integrate machine learning models into Go applications. \ud83d\ude80 Of course, should the API-borne salvation falter, they suggest summoning Python\u2014yes, in its full, serpentine glory\u2014as a 'sidecar,' because evidently concocting workarounds is more palatable than embracing the inherent chaos of cross-language development. Meanwhile, the comment section devolves into a mix of Go enthusiasts reluctantly acknowledging Python's utility, and a sprinkle of brave souls venturing into territories like <i>GoMLX</i> and <i>cgo</i>, armed with hope and GitHub repos. It's a coding crossover episode no one asked for, but we're forced to binge-watch. \ud83c\udf7f"
  },
  {
    "title": "Contain \u2013 CSS Cascading Style Sheets \u2013 MDN (developer.mozilla.org)",
    "points": 25,
    "submitter": "aabhay",
    "submit_time": "2024-11-17T06:25:53 1731824753",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42162368",
    "comments": [
      "This is one of those features that I suspect would be incredibly useful for some of my applications but I just can\u2019t quite see it yet. Maybe a page of real world examples might help, if anyone has any?This page got me a bit closer I think: https://css-tricks.com/almanac/properties/c/contain/\n \nreply",
      "I found a probably unintended use in making it easy to ensure clipping worked right.The intended use of course is ensuring that the content in container does not need complex compositing outside its rectangle.\n \nreply"
    ],
    "link": "https://developer.mozilla.org/en-US/docs/Web/CSS/contain",
    "first_paragraph": "Web technology reference for developersStructure of content on the webCode used to describe document styleGeneral-purpose scripting languageProtocol for transmitting web resourcesInterfaces for building web applicationsDeveloping extensions for web browsersWeb technology reference for developersLearn web developmentLearn web developmentLearn to structure web content with HTMLLearn to style content using CSSLearn to run scripts in the browserLearn to make the web accessible to allA customized MDN experienceGet real-time assistance and supportAll browser compatibility updates at a glanceLearn how to use MDN PlusFrequently asked questions about MDN PlusWrite, test and share your codeScan a website for freeGet real-time assistance and supportThis feature is well established and works across many devices and browser versions. It\u2019s been available across browsers since March 2022.\n  The contain CSS property indicates that an element and its contents are, as much as possible, independent from ",
    "summary": "Title: The Misadventures of CSS Wizards at MDN\n\nAt MDN, a hub for lost web developers, they\u2019ve penned yet another novel on the almost mystical \"contain\" property in CSS, which apparently is supposed to keep your unruly divs in line - just like a magical spell from a low-budget fantasy film. The guidance is so obscure and vague that even the comment section becomes a comedy show of confused sorcerers. One brave soul meekly asks for real-world examples, unwittingly revealing they missed the first day at Hogwarts of Web Development. Another finds joy in the \"probably unintended use\" of the property, much like discovering a new use for a paperclip. \ud83e\uddd9\u200d\u2642\ufe0f\u2728"
  },
  {
    "title": "Bpftune uses BPF to auto-tune Linux systems (github.com/oracle)",
    "points": 200,
    "submitter": "BSDobelix",
    "submit_time": "2024-11-17T11:38:35 1731843515",
    "num_comments": 62,
    "comments_url": "https://news.ycombinator.com/item?id=42163597",
    "comments": [
      "With this tool I am wary that I'll encounter system issues that are dramatically more difficult to diagnose and troubleshoot because I'll have drifted from a standard distro configuration. And in ways I'm unaware of. Is this a reasonable hesitation?\n \nreply",
      "Disclaimer: I work for Oracle, who publish this tool, though I have nothing to do with the org or engineers that created itI've been running this for a while on my laptop.  So far yet to see any particular weirdness, but also I don't know that I can state with any confidence it has a positive impact either.  I've not carried out any benchmarks in either direction.It logs all changes that it's going to make including what they were on before.  Here's an example from my logs:    bpftune[1852994]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput\n    bpftune[1852994]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -> (4096 131072 9830400)\n \nreply",
      "\"because I'll have drifted from a standard distro configuration\"You will obviously have a change management system which describes all the changes you have made to your putative standard distro configs.  You will also be monitoring those changes.This tool logs all the changes it makes via the standard logging system, which can be easily captured, shipped and aggregated and then queried and reported on.This is not a tool from Clown Cars R US, it's from a reasonably reputable source - Oracle (lol etc).  Even better, you can read the code and learn or critique.Not being funny but I'd rather this sort of thing by far than any amount of wooo handwavy wankery.  Would you prefer openly described and documented or \"take our word for it\"?\n \nreply",
      "> You will obviously have a change management system which describes all the changes you have made to your putative standard distro configs. You will also be monitoring those changes.Which is now a list you will have to check for every issue. I don't think they are complaining they don't trust the writers of the code, just that it adds confounding variables to your system\n \nreply",
      "We (in IT security) are expected to abrogate responsibility to funky AI or whatevs anti virus and other stuff.  Buy and install a security package from ... whoever ... and all will be well.This is an expert system/advice run by real people (at a reasonably well respected firm) not an AI wankery thingie.  It is literally expert advice and it is being given away and in code form which you can read.What on earth is wrong with that?\n \nreply",
      "If the alternative is those proprietary anti virus products, sure this is better.The original comment was comparing to doing nothing and just using the standard distro, I believe.\n \nreply",
      "I do hope I haven't offended anyone but I also hope I will leave no one in any doubt that IT security is important.The world is now very highly interconnected.  When I was a child, I would have rubbish conversations over the blower to an aunt in Australia - that latency was well over one second - satellite links.  Nowadays we have direct fibre connections.So, does you does ?\n \nreply",
      "> for every issue.Only if you don't know what you're doing, which, with no judgement whatsoever, might be true for OP. Reading the source, it affects some networking related flags. If the local audio craps out, it's not related. If the Bluetooth keyboard craps out, it's not related. If the hard drive crashes, it's not related.I get that is just adding more variables to the system, but this isn't Windows, where the changes under the hood are this mystery hotfix that got applied and we have no idea what it did and the vendor notes raise more questions than it asks and your computer working feels like this house of cards that's gonna fall over if you look at it funny. If the system is acting funny, just disable this, reset them all back to default, possibly by rebooting, and see if the problem persists. If you're technical enough to install this, I don't think disabling it and rebooting is beyond your abilities.\n \nreply",
      ">\"bpftune logs to syslog so /var/log/messages will contain details of any tuning carried out.\" (from OP GitHub readme)The rmem example seems to allay fears that it will make changes one can't reverse.\n \nreply",
      "It\u2019s not a questions of being able to reverse. It\u2019s a question of being able to diagnose that one of these changes even was the problem  and if so which one.\n \nreply"
    ],
    "link": "https://github.com/oracle/bpftune",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        bpftune uses BPF to auto-tune Linux systems\n      bpftune aims to provide lightweight, always-on auto-tuning of system\nbehaviour.  The key benefit it provides areThe Linux kernel contains a large number of tunables; these\noften take the form of sysctl(8) parameters, and are usually\nintroduced for situations where there is no one \"right\" answer\nfor a configuration choice.  The number of tunables available\nis quite daunting.  On a 6.2 kernel we seeSee here for an excellent writeup on network-related tunables..At the same time, individual systems get a lot less care\nand adminstrator attention than they used to; phrases like\n\"cattle not pets\" exemplify this.  Given the modern cloud\narchitectures used for most deployments, most systems never\nhave any human adminstrator interaction after initial\nprovisioning; in fact given the scale requi",
    "summary": "Title: **Automating the Apocalypse: bpftune's Brave Quest to Out-Smart Humans**\n\n\"bpftune\", the latest gift from Oracle, promises to auto-tune Linux systems without human intervention, because apparently, Linux admins needed a new way to screw things up without even trying. The tool flips system settings like a caffeinated teenager changes Snapchat filters, ensuring that every sysadmin can truly embrace the \"I dunno, it just started doing that\" approach to IT support. Meanwhile, in the comment section, Oracle's own emissary tries to convince everyone that <em>logging changes</em> should be enough reassurance for wary sysadmins. Ah yes, nothing says \"trust us\" quite like scrolling through a Kafkaesque log at 3 AM trying to figure out why your server decided to sing its swan song during peak hours. \ud83d\ude44"
  },
  {
    "title": "Show HN: Store and render ASCII diagrams in Obsidian (github.com/akopdev)",
    "points": 22,
    "submitter": "akopkesheshyan",
    "submit_time": "2024-11-12T02:03:21 1731377001",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42112168",
    "comments": [
      "This is awesome! There are a few other text to diagram language out there, but this seems to support straight up drawing via text.Very powerful given the flexibility.I also dislike how fiddly it appears with whitespace and general verbosity to explicitly define paths between nodes and such.Love seeing how the community continues to build on the idea of Obsidian.\n \nreply",
      "This is built on https://github.com/google/typograms, which is awesome.\n \nreply",
      "Nice, but I never use the preview mode. The edit mode being 99% like the prevew mode is what I most like about Obsidian.But then, I think all diagram extensions work on that basis.\n \nreply",
      "Nice. I mostly use mermaid which is built into obsidian, but I could definitely see this ascii representation being useful as well since even in the unrendered form (pre-svg transformation), it's still WYSIWYG.\n \nreply"
    ],
    "link": "https://github.com/akopdev/obsidian-textgrams",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Create and store ASCII graphics in your Obsidian\n      Obsidian plug-in that allows you to create and store ASCII graphics in your notes.\nIt can be used to to define diagrams, flowcharts, complex tables, Gantt charts and more\nin technical documentation, that will be rendered as a nice SVG graphics in the preview mode.This plug-in is based on typograms library, originally\ndeveloped by Sam Goto.I'm a software engineer and I use Obsidian to document my projects, as long as support my\ncontinuous learning process. I often need to create diagrams, flowcharts, tables and other graphics,\nthat are not locked in a specific tool, but can be easily shared and versioned in my notes.Using ASCII graphics allows to create a very flexible and portable content, that can be easily\nedited and versioned in a text file, without loosing benefits of a visu",
    "summary": "**Title: Show HN: Store and render ASCII diagrams in Obsidian (github.com/akopdev)**\n\n<em>Hacker News</em> discovers yet another way to avoid doing any real work by fawning over a new Obsidian plugin that transforms incomprehensible ASCII art into mildly less incomprehensible SVG diagrams. Users drool over their ability to <em>\"draw\"</em> flowcharts that look like refrigerator art from a toddler. Expect high-octane debates about whitespace sensitivity, as programming aesthetes reconcile their life choices with the goriest depths of diagrammatic verbosity. Meanwhile, somewhere, a confused flex of Visio veterans wonders how we all just didn't revert to cave paintings. \ud83c\udfa8\ud83d\udc40"
  },
  {
    "title": "Constraints in Go (bitfieldconsulting.com)",
    "points": 157,
    "submitter": "gus_leonel",
    "submit_time": "2024-11-17T08:44:29 1731833069",
    "num_comments": 64,
    "comments_url": "https://news.ycombinator.com/item?id=42162878",
    "comments": [
      "I'm surprised by the complexity of Go's generic constraints, given the language's focus on simplicity. Things like the difference between \"implementing\" and \"satisfying\" a constraint [0], and exceptions around what a constraint can contain [1]:> A union (with more than one term) cannot contain the predeclared identifier comparable or interfaces that specify methods, or embed comparable or interfaces that specify methods.Is this level of complexity unavoidable when implementing generics (in any language)? If not, could it have been avoided if Go's design had included generics from the start?[0] https://stackoverflow.com/questions/77445861/whats-the-diffe...[1] https://blog.merovius.de/posts/2024-01-05_constraining_compl...\n \nreply",
      "Generics are a powerful mechanism, and there is a spectrum. The act of retrofitting generics on go without generics certainly meant that some points in the design space were not available.\nOn the other hand, when making a language change as adding generics, one wants to be careful that it pulls its own weight: it would be be sad if generics had been added and then many useful patterns could not be typed.\nThe design choices revolve around expressivity (what patterns can be typed) and inference (what annotations are required). Combining generics with subtyping and inference is difficult as undecidability looms. In a language with subtyping it cannot be avoided (or the resulting language would be very bland).\nSo I think the answer is no, this part of the complexity could not have been avoided. I think they did a great job at retrofitting and leaving the basic style of the language intact - even if I'd personally prefer a language design with a different style but more expressive typing.\n \nreply",
      "In practice, none of this impacts your program. The standard advice I give to people messing around with this stuff is, never use the pipe operator. The standard library already implements all the sensible uses of it.In particular, people tend to read it as the \"sum type\" operator, which it is not. I kind of wish the syntax has used & instead of |, what it is doing is closer to an \"and\" then an \"or\".By the time you know enough to know you can ignore that advice, you will. But you'll also likely find it never comes up, because, again, the standard library has already implemented all the sensible variants of this, not because the standard library is magic but because there's really only a limited number of useful cases anyhow. I haven't gone too crazy with generics, but I have used them nontrivially, even done s could tricks [1], and the pipe operator is not that generally useful.When the generic constraint is an interface with methods is the case that can actually come up, but that makes sense, if generics make sense to you at all.It probably is a good demonstration of the sort of things that come up on generic implementations, though. Despite the rhetoric people often deployed prior to Go having them, no, they are never easy, never without corner cases, never without a lot of complications and tradeoffs under the hood. Even languages designed with them from the beginning have them, just better stuffed under the rug and with less obvious conflict with other features. They're obviously not impossible, and can be worthwhile when deployed, certainly, but it's always because of a lot of work done by the language designers and implementations, it's never just \"hey let's use generics, ok, that one sentence finishes the design I guess let's go implement them in a could of hours\".[1]: Just about the edge of the \"tricky\" I'd advise: https://github.com/thejerf/mtmap\n \nreply",
      "> In particular, people tend to read it as the \"sum type\" operator, which it is not. I kind of wish the syntax has used & instead of |, what it is doing is closer to an \"and\" then an \"or\".I don't understand here. In my understanding, the pipe operator is indeed closer to \"or\" and \"sum type\" operator. Interpreting it as \"and\" is weird to me.\n \nreply",
      "I think they're reading it as \"a bitwise-and of the functionality of the types passed\", which is accurate (since you're getting the lowest common denominator of all |'d types).I'm... not sure which way I lean tbh, now that I've seen that idea.  Both have merit, it's more of a problem for educational material than anything.  If you present it as \"these types\", | makes sense.  If you instead use \"these behaviors\", & makes sense.  | is slightly easier to type for me though, and & has more meanings already (address-of), so maybe I'd still favor |.\n \nreply",
      "Okay, it is some reasonable if the operator is viewed as a behavior operator. But it is not, it is a type set operator.\n \nreply",
      "And the real point I'm making here is that \"the type set operator\" is not \"a sum type\". A sum type with, say, three branches is either the first, or the second, or the third, and to do anything with any of them, you have to deconstruct it, at which point you have full access to the deconstructed branch you are in. The | operator in a Go generic is more a declaration of \"I want to operate on all of these at once\", so, you can put multiple numeric types into it because you can do a + or a - on any of them, but while the syntax permits you to put three struct types into it, and it'll compile, it does not produce a \"sum type\". Instead you get \"I can operate on this value with the intersection of all the operations they can do\", which is more or less \"nothing\". (\"Methods\" aren't \"operations\"; methods you can already declare in interfaces.) Some people particularly fool themselves because you can still take that type, cast it into an \"any\", and then type switch on it, but it turns out you can always do that, the | operator isn't helping you in any particular way, and if you want to have a closed set of types, a closed interface is a much better way to do it, on many levels.It also doesn't currently do anything else people may want it to do, like, accept three structs that each have a field \"A\" of type \"int\" and allow the generic to operate on at least that field because they all share it. There's a proposal I've seen to enable that, as the current syntax would at least support that, but I don't know what its status is.\n \nreply",
      "There is actually a proposal to make type constraints act as sum types: https://github.com/golang/go/issues/57644But I doubt sum types will be supported perfectly in Go. The current poor-men's sum type mechanism (type-switch syntax) might be still useful in future Go custom generic age.\n \nreply",
      "The difference between types.Implements and types.Satisfies is mainly caused by a history reason. It is just a tradeoff between keeping backward compatibility and theory perfection.It is pity that Go didn't support the \"comparable\" interface from the beginning. If it has been supported since Go 1.0, then this tradeoff can be avoided.There are more limitations in current Go custom generics, much of them could be removed when this proposal (https://github.com/golang/go/issues/70128) is done.I recommend people to read Go Generics 101 (https://go101.org/generics/101.html, author here) for a thoroughly understanding the status quo of Go custom generics.\n \nreply",
      "There are tons of random limitations not present in other languages too, like no generic methods.\n \nreply"
    ],
    "link": "https://bitfieldconsulting.com/posts/constraints",
    "first_paragraph": "From Know GoDesign is the beauty of turning constraints into\nadvantages.\n\u2014Aza\nRaskin This is the fourth in a four-part series of tutorials on generics in\nGo.In my book Know Go, and in the previous\ntutorials in this series, you\u2019ll learn all about generic programming in\nGo and the new universe of programs it opens up to us. Ironically, one\nof the new features of Go that gives us the most freedom is\nconstraints. Let\u2019s talk about that, and explain the\nparadox.We saw in the previous tutorial\nthat when we\u2019re writing generic functions that take any type, the range\nof things we can do with values of that type is necessarily\nrather limited. For example, we can\u2019t add them together. For that, we\u2019d\nneed to be able to prove to Go that they\u2019re one of the types that\nsupport the + operator.It\u2019s the same with interfaces, as we discussed in the first post in this series. The empty\ninterface, any, is implemented by every type, and so\nknowing that something implements any tells you nothing\ndistinctive abo",
    "summary": "In a breathtaking display of intellectual prowess, <em>bitfieldconsulting.com</em> unleashes another thrilling chapter of \"Go Generics: The Chronicles of Confusion.\" As the article heroically attempts to wrestle the paradox of constraints freeing us by limiting us (wow, so deep \ud83d\ude44), the comment section evolves into a battleground where programmers joust over complexities so trivial that surely their grandmothers are brimming with pride. Amidst the chaos, a brave commenter attempts to navigate the treacherous waters of pipe operators and sum types, only to be overshadowed by others who ponder existential questions about backward compatibility and imaginary sum types proposals that are more mythical than a programmer's social life. Truly, the saga of Go's generics is the gift that keeps on giving to those who dream of a world constrained by constraints."
  },
  {
    "title": "You could have designed state of the art positional encoding (fleetwood.dev)",
    "points": 14,
    "submitter": "Philpax",
    "submit_time": "2024-11-17T20:31:26 1731875486",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://fleetwood.dev/posts/you-could-have-designed-SOTA-positional-encoding",
    "first_paragraph": "\u2022 \u23f1 16 min readGall's Law \nA complex system that works is invariably found to have evolved from a simple\nsystem that worked \nJohn GallThis post walks you through the step-by-step discovery of state-of-the-art positional encoding in transformer models. We will achieve\nthis by iteratively improving our approach to encoding position, arriving at Rotary Postional Encoding (RoPE) used in the latest LLama 3.2 release and most modern transformers. This post intends to limit the mathematical knowledge required to follow along, but some basic linear algebra, trigonometry and understanding of self attention is expected.You shall know a word by the company it keeps \nJohn Rupert FirthAs with all problems, it is best to first start with understanding exactly what we are trying to achieve. The self attention mechanism in transformers is utilized to understand relationships\nbetween tokens in a sequence. Self attention is a set operation, which\nmeans it is permutation invariant (order does not matter)",
    "summary": "Welcome to another episode of *Silicon Valley Pretend Scientist*, where today's blogger rehashes old transformer magic as a groundbreaking journey through positional encoding. \ud83c\udfa9\u2728 True to form, the blog casually throws in \"linear algebra\" and \"trigonometry\" because, let's be real, what's a tech blog without unnecessarily scaring off the humanities majors? In the comments, coders and wannabe-coders alike squabble over who understood <em>Gall\u2019s Law</em> first and misuse \"permutation invariant\" in sentences, proving once again that you don't need to grasp the basics to argue about them on the internet. \ud83e\udd13\ud83d\udca5"
  }
]