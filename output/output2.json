[
  {
    "title": "AI PCs Aren't Good at AI: The CPU Beats the NPU (github.com/usefulsensors)",
    "points": 224,
    "submitter": "dbreunig",
    "submit_time": "2024-10-16T19:44:02.000000Z",
    "num_comments": 121,
    "comments_url": "https://news.ycombinator.com/item?id=41863061",
    "comments": [
      "I think the results show that just in general the compute is not used well. That the CPU took 8.4ms and GPU took 3.2ms shows a very small gap. I'd expect more like 10x - 20x difference here.\nI'd assume that the onnxruntime might be the issue. I think some hardware vendors just release the compute units without shipping proper support yet. Let's see how fast that will change.Also, people often mistake the reason for an NPU is \"speed\". That's not correct. The whole point of the NPU is rather to focus on low power consumption. To focus on speed you'd need to get rid of the memory bottleneck. Then you end up designing your own ASIC with it's own memory. The NPUs we see in most devices are part of the SoC around the CPU to offload AI computations.\nIt would be interesting to run this benchmark in a infinite loop for the three devices (CPU, NPU, GPU) and measure power consumption.\nI'd expect the NPU to be lowest and also best in terms of \"ops/watt\"\n \nreply",
      "> Also, people often mistake the reason for an NPU is \"speed\". That's not correct. The whole point of the NPU is rather to focus on low power consumption.I have a sneaking suspicion that the real real reason for an NPU is marketing. \"Oh look, NVDA is worth $3.3T - let's make sure we stick some AI stuff in our products too.\"\n \nreply",
      "I assume you're both right. I'm sure NPUs exist to fill a very real niche, but I'm also sure they're being shoehorned in everywhere regardless of product fit because \"AI big right now.\"\n \nreply",
      "Looking at it slightly differently: putting low-power NPUs into laptop and phone SoCs is how to get on the AI bandwagon in a way that NVIDIA cannot easily disrupt. There are plenty of systems where a NVIDIA discrete GPU cannot fit into the budget (of $ or Watts). So even if NPUs are still somewhat of a solution in search of a problem (aka a killer app or two), they're not necessarily a sign that these manufacturers are acting entirely without strategy.\n \nreply",
      "You forget \"Because Apple is doing it\", too.\n \nreply",
      "I think other ARM SoC vendors like Rockchip added NPUs before Apple, or at least around the same time.\n \nreply",
      "I was curious so looked it up. Apple's first chip with an NPU was the A11 bionic in Sept 2017. Rockchip's was the RK1808 in Sept 2019.\n \nreply",
      "Google TPU was introduced around same time as apple. Basically everybody knew it can be something around that time, just don't know exactly how",
      "Face ID was the first tent pole feature that ran on the NPU.\n \nreply",
      "> Also, people often mistake the reason for an NPU is \"speed\". That's not correct. The whole point of the NPU is rather to focus on low power consumption.It's also often about offload. Depending on the use case, the CPU and GPU may be busy with other tasks, so the NPU is free bandwidth that can be used without stealing from the others. Consider AI-powered photo filters: the GPU is probably busy rendering the preview, and the CPU is busy drawing UI and handling user inputs.\n \nreply"
    ],
    "link": "https://github.com/usefulsensors/qc_npu_benchmark",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Code sample showing how to run and benchmark models on Qualcomm's Window PCs\n      TL;DR - We see 1.3% of Qualcomm's NPU 45 Teraops/s claim when benchmarking Windows AI PCsMicrosoft now offers Surface tablets that run Windows on a Qualcomm Arm-based\nSoC. These are marketed as AI PCs, due to their ability to run machine learning\nmodels faster and more efficiently than other systems. We are fans of\nQualcomm's hardware, and its NPU in particular, so we've invested a lot of time\nand resources into porting our third-party app to this plaform.Unfortunately there  aren't many code examples or benchmarks available to\ndemonstrate how to achieve fast results as an external developer, so we've put\ntogether a small standalone project to show the performance we're seeing. It's\nsignificantly below what we'd hoped for, so we're publishing this ben",
    "summary": "**AI PCs Aren't Good at AI: The CPU Beats the NPU**\n\nIn an astonishing feat of underachievement, a new article on GitHub reveals a woeful gap between marketing dreams and harsh reality, as Qualcomm\u2019s AI PC NPU performs at a pathetic 1.3% of its advertised prowess. The developer community, in their infinite wisdom, showers us with insights that oscillate between pointing out the obvious and regurgitating AI buzzword bingo. One commenter brilliantly states that the NPU is not for speed but for low power, forever confusing purpose with performance, while another dives into a Silicon Valley conspiracy theory that the NPU is mainly a jazzed-up marketing gimmick to compete with NVIDIA and Apple. Meanwhile, several participants in the comments section embark on a nostalgic trip down memory lane, debating who slapped an NPU onto an SoC first, capturing perfectly the essence of missing the point. \ud83d\udca1\ud83e\udd16"
  },
  {
    "title": "Amazon reveals first color Kindle, new Kindle Scribe, and more (aboutamazon.com)",
    "points": 116,
    "submitter": "bookofjoe",
    "submit_time": "2024-10-16T13:52:55.000000Z",
    "num_comments": 223,
    "comments_url": "https://news.ycombinator.com/item?id=41859047",
    "comments": [
      "I have a pretty old paperwhite. When it started deleting books i had side-loaded via calibre, I decided to get a kobo. I have a libra color, and I have to say: price notwithstanding, it's a great device. I don't have a lot of experience with more recent devices, but compared to my 2nd gen paperwhite, it is _amazing_.Color is good enough to read comic books on it, the google drive integration means it's not too hard to get my CBR/CBZ files on directly. The annotation / notetaking featureas are nice (I haven't leaned into them yet, but they work well even on the small screen size), plus all the regular stuff with normal book reading. Also, since it's kobo/rakuten, the libby integration is better (search and select library books right from the device).The actual reading app is maybe 90% as good as reading on the kindle (or a more specialized reader like perfectviewer on android). There's some annoyingly fiddly features- font size is kind of weirdly variable, when going through CBR files there's no \"read next in the folder\" gesture nor is there a \"this is read/unread\" state in the google drive ui, so you always have to remember which book you are finishing when opening the next in the series.I tried out one of those boox readers with the android apps, which would be even better software-wise, but the boox hardware seems like garbage (for an N=1 at least). My display came with several rows of stuck pixels, and apparently it's a good thing that I ordered from amazon instead of the boox store, because the reviews indicate getting an RMA from boox directly is a pain.\n \nreply",
      "I stopped using Calibre because it turned out to be more annoying than simply emailing an epub to my designated kindle address. I've never had anything removed. Most of the content comes from libgen.\n \nreply",
      "I think Calibre is able to integrate with the \u201cSend to Kindle\u201d email interface as well, so you can have the best of both worlds :)\n \nreply",
      "> When it started deleting books i had side-loaded via calibreCould you expand on that? I load all my books via calibre but I also have the kindle set to aeroplane mode at all times. Does this happen when syncing?\n \nreply",
      "Happened to me too\u2026 Yes, it happened during syncing. I searched up on how to disable Kindle auto update and it\u2019s happening less, but last time I bought a book from Kindle store I got bit again. I guess it\u2019s a sign to stop  feeding the hand that bites me.\n \nreply",
      "It deletes all side-loaded books when you connect it to the internet, happened to me as well.\n \nreply",
      "Interesting thread. I wonder if this has anything to do with what I experienced with the one (two, actually) I set up for my mom years back. The books were visible on the filesystem but nowhere to be found when using the device.Fortunately, getting two units in a row with jarring pink blotches was enough of a non-starter to suggest that she just revert to a V4 without my having to explain the confusing behavior.\n \nreply",
      "That is strange. I have had a Kindle for years (probably around a decade), upgrading mutliple times. I am using a Paperwhite Signature for the past year or so and I have not experienced this with any books. I have probably 70% Amazon purchased books, but that still leaves a decent amount of side-loaded books (mostly epub).The only annoying thing I experience is that the cover art will unload on the side-loaded books where you just get the generic cover with the text of the book. But once you click the book it loads the artwork, which seems to last a few days before quickly going back to the generic cover. But the book itself never leaves the device. I can't say I have experienced this ever (except for once, mentioned below) and I have over 150 books on my 32Gb device.Just some random thoughts I am wondering:- Is it an ads-supported model (mine isn't)- Are the books in a broken-DRM .mobi? Not judging, i've done it too, just curious if Amazon has some sort of \"signature\" in mobis that allow it to detect a book that had DRM removed?- Are they standard .epub?- What is the size of the device? (Maybe smaller 8Gb devices will clean up and prioritize non-Kindle content to make way for \"official content)- Which device is it? (Scribe, Oasis, Paperwhite, older paperwhite, etc)The one time I had it deleting my book was a large book (it was 400Mb) and it was on an older 8Gb paperwhite. I still had PLENTY of storage space available (I was using around 4Gb) but it kept deleting that book. This was the only time I have seen it happen and it was with this one specific book. That was many years ago and I haven't seen it since. That book was a DRM-removed book I got through \"shared\" means. Which has led me to question if the Kindle removes it because it could detect that the DRM had been removed or because of its large size. But this was a one-off experience for me. The book was readable by the device. It would download for several weeks at a time before being removed. You could redownload it and it would work for weeks again before dissapearing. I never could figure it out.\n \nreply",
      "It doesn't do this for me. I've got side-loaded and Amazon store books on the same device, no problem.\n \nreply",
      "It also deleted all my side-loaded books.  That was the last straw for me.  I only buy DRM-free media from now on and only use respectful hardware.  I use my Remarkable 2 primarily for e-reading now, though I concede fully it's not the best user experience for reading.  But I don't have to worry that it will delete my books!  I can also now \"write in the margins\" which I've found to be a powerful way to take notes.  I can't bring myself to write on physical books, but with Remarkable you can have a copy that is stock and a copy with your notes on it.  Best of both worlds!\n \nreply"
    ],
    "link": "https://www.aboutamazon.com/news/devices/new-kindle-color-scribe-paperwhite-entry",
    "first_paragraph": "Amazon.comConditions of UseAmazon Privacy Policy\u00a9 1996-2024 Amazon.com, Inc. or its affiliates4 minOctober 16, 2024Written by Amazon Staff Written by Amazon Staff Because some reading experiences deserve nature\u2019s full palette.Get library books on your Kindle, show off book covers as your screensaver, and a lot more. If you\u2019ve recently purchased a Kindle Paperwhite and are ready to start putting it to use, we\u2019re here to help.Learn about the incredible digital playground that let kids safely discover a treasure trove of shows, books, podcasts, games, and more. Related Tags 4 minReading:Meet the all-new Kindle family, including the first color KindleTrending news and storiesAmazon.comConditions of UseAmazon Privacy Policy\u00a9 1996-2024 Amazon.com, Inc. or its affiliates",
    "summary": "In a revolutionary display of mediocrity, Amazon unveils its first color Kindle, proving that it can indeed keep up with technology from a decade ago. What's better than overpaying for a device that respects your ownership rights as much as a kleptomaniac in a cash-only flea market? Join the literary elite in the comments section, where survivors of unwanted book purges share their harrowing journey from Kindle to Kobo, as they reminisce about the \"good old days\" when side-loading a book didn't trigger an existential crisis. Whether it's disabling updates, juggling incompatible file formats, or just trying to make their device remember not to erase their entire library\u2014our tech-savvy bibliophiles never fail to entertain with their techno-thrillers more convoluted than any plot they could hope to download. \ud83d\ude02\ud83d\udcda\ud83d\udc4f"
  },
  {
    "title": "A solar gravitational lens will be humanity's most powerful telescope (2022) (phys.org)",
    "points": 65,
    "submitter": "amichail",
    "submit_time": "2024-10-15T11:52:48.000000Z",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41847563",
    "comments": [
      "The most complete plan for this was proposed by JPL's Slava Turyshev and team. It has been selected for Phase III of NASA Innovative Advanced Concepts. [0]> In 2020, Turyshev presented his idea of Direct Multi-pixel Imaging and Spectroscopy of an Exoplanet with a Solar Gravitational Lens Mission. The lens could reconstruct the exoplanet image with ~25 km-scale surface resolution in 6 months of integration time, enough to see surface features and signs of habitability. His proposal was selected for the Phase III of the NASA Innovative Advanced Concepts. Turyshev proposes to use realistic-sized solar sails (~16 vanes of 10^3 m^2) to achieve the needed high velocity at perihelion (~150 km/sec), reaching 547 AU in 17 years.> In 2023, a team of scientists led by Turychev proposed the Sundiver concept,[1] whereby a solar sail craft can serve as a modular platform for various instruments and missions, including rendezvous with other Sundivers for resupply, in a variety of different self-sustaining orbits reaching velocities of ~5-10 AU/yr.Here is an interview with him laying out the entire plan.[2] It is the most interesting interview that I have seen in years, possibly ever.[0] https://en.wikipedia.org/wiki/Slava_Turyshev#Work[1] https://www2.mpia-hd.mpg.de/~calj/sundiver.pdf[2] https://www.youtube.com/watch?v=lqzJewjZUkk\n \nreply",
      "Christian Ready made a great video on his Youtube channel, Launch Pad Astronomy, about NASA's plans for a solar gravitational lens. It's got some great graphics and visualisations, and is accessibly narrated. I was inspired and learnt a lot of new ideas.https://www.youtube.com/watch?v=NQFqDKRAROI (23 minutes)\n \nreply",
      "Is there anything stopping you from putting 2+ satellites out \"closer\" but in the path of the lensed light, capturing the light simultaneously, and then resolving the image via async computation later? I think this is called interferometry and I know it's hard because you need _very_ precise timing, but I'm curious if that would be possible or not. (Maybe you can get the timing in sync with atomic clocks, or by sending a laser to both from a central point that lets them keep time with some very tight tolerance?)Weird idea but I wonder if there are ways to take this from \"crazy tech\" to \"hard tech\".\n \nreply",
      "> Is there anything stopping you from putting 2+ satellites out \"closer\" but in the path of the lensed lightThe Sun. Literally.Satellites have to be that far for the Einstein ring to be bigger than the apparent size of the solar disk.Edit: to make it a bit more clear, the gravitational lens does not quite behave like a normal lens. Instead, you see the light from _behind_ the object. So if you're too close to the lensing object so that the Einstein ring is not larger than it, you'll just see a part of the object to be a bit more bright.Also, the gravitational lens does not actually _focus_ the image, it distorts it into a band around the lensing object.\n \nreply",
      ">to make it a bit more clear, the gravitational lens does not quite behave like a normal lens. Instead, you see the light from _behind_ the object.Or to put it another way: A gravity lens bends space so that the light from behind an object curves around it while travelling straight.\n \nreply",
      "On a cosmic scale what does simultaneously mean? Two object in a distinct orbit will be in different planes of reference\n \nreply",
      "Presumably it means that two light rays that leave the same point on the planet simultaneously (but going in slightly different directions) arrive at the two telescope satellites simultaneously\n \nreply",
      "The precision you need for interferometry depends on the wavelength, and being able to do this over astronomical distances at visible wavelengths would indeed be a challenge.  I think the scale is timing more accurate than 0.1 nanoseconds and distance accuracy on the order of 100 nanometers.  Near those orders of magnitude at least and over astronomical distances that might be measured in AU.Then again the precision of the gravitational wave instruments measure distance on the order of the width of a proton, so who knows.Terrestrial infrared and optical interferometry telescopes are on the bleeding edge right now.\n \nreply",
      "\"Boring\" cesium atomic clocks can do 50 ps per day with the best cutting edge optical clocks coming in at ~739 fs per day. Optical clocks would only need to resynchronize once every ~ 135 days while cesium clocks would need to do it every 2 days to get 0.1ns of accuracy.I think the bigger challenge may be how you would transport the clocks after synchronization to maintain it across astronomical distances since they're very sensitive to any kind of acceleration. Since you have to regularly re-synchronize them in space anyway, that feels like the engineering problem you'd have to solve - how do you synchronize two atomic; the current record is synchronizing to within 0.32fs at a distance of 300km [1].[1] https://spectrum.ieee.org/atomic-clock-femtosecond-accuracy\n \nreply",
      "Is this a similar principle to the concept in the 3 Body Problem series of books? As in, how one of the main characters is able to boost the transmission power of an earth bound antenna\n \nreply"
    ],
    "link": "https://phys.org/news/2022-10-solar-gravitational-lens-humanity-powerful.html",
    "first_paragraph": "\n\n                  Click here to sign in with\n                  \n\n\n                  or\n                  \n\n\n\n\nForget Password?\n\nLearn more\nshare this!16010ShareEmail\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tOctober 10, 2022\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t by Brian Koberlein, \t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t Universe Today\nOne of the central predictions of general relativity is that a massive object such as a star, galaxy, or black hole can deflect light passing nearby. This means that light from distant objects can be gravitationally lensed by objects closer to us. Under the right conditions, gravitational lensing can act as a kind of natural telescope, brightening and magnifying the light of distant objects. Astronomers have used this trick to observe some of the most distant galaxies in the universe. But astronomers have also thought about using this effect a little closer to home.One idea is to use the sun's gravity as a lens to study nearby exoplanets. Light coming from an exoplanet would be gravitationally f",
    "summary": "In a daring escape from reality, humanity plans to use the sun as a massive cosmic magnifying glass to spy on distant exoplanets, because apparently, building telescopes the size of a small country is just too mainstream. Phys.org publishes yet another article that will surely twist the knickers of the hobbyist physicist community into quantum knots over \"solar gravitational lensing.\" Commenters, in a spectacular display of missing the point, dive into a nerdfight over the feasibility of space-based interferometry with atomic clock precision. One could argue the practicality, but why bother when you can argue theoretical physics in the comment section of an article few truly understand? \ud83e\udd2f"
  },
  {
    "title": "Hofstadter on Lisp (1983) (gist.github.com)",
    "points": 295,
    "submitter": "Eric_WVGG",
    "submit_time": "2024-10-16T13:44:37.000000Z",
    "num_comments": 117,
    "comments_url": "https://news.ycombinator.com/item?id=41858975",
    "comments": [
      "In case anyone else is confused by what the functions named \"oval\" and \"snot\" mean in the following example:  > (cond ((eq (oval pi) pie) (oval (snot pie pi)))\n  (t (eval (snoc (rac pi) pi))))\n\nI realised after a few seconds that they are meant to be \"eval\" and \"snoc\" instead.  The above code should be written as the following instead:  (cond ((eq (eval pi) pie)\n         (eval (snoc pie pi)))\n        (t (eval (snoc (rac pi) pi))))\n\nThis article has been a fascinating read, by the way.  Kudos to the maintainer of the Gist post.  I am also sharing these corrections as comments on the Gist post.EDIT #1: Downloaded a copy of the original Scientific American article from https://www.jstor.org/stable/24968822 and confirmed that indeed the functions \"oval\" and \"snot\" are misspellings of \"eval\" and \"snoc\".EDIT #2: Fixed typo in this comment highlighted by @fuzztester below.\n \nreply",
      ">confirmed that indeed the functions \"oval\" and \"snot\" are misspellings of \"eval\" and \"snot\".Correction of your correction:confirmed that indeed the functions \"oval\" and \"snot\" are misspellings of \"eval\" and \"snoc\".And I guess snoc is cons  reversed and rac is car  reversed.\n \nreply",
      "> Correction of your correctionThanks!  Fixed.> And I guess snoc is cons reversed and rac is car reversed.Indeed!  That's exactly how those functions are introduced in the article.  Quoting from the article:> The functions rdc and snoc are analogous to cdr and cons, only backwards.\n \nreply",
      "OCR maybe?\n \nreply",
      "Maclisp goodness:  (compress (reverse (explode x)))\n\nElisp much improved:  (defun explode (x)\n    (if (symbolp x) (setq x (symbol-name x)))\n    (string-to-list x))\n  (defun compress (x) (concat x))\n \nreply",
      "I just love his writing so much -- he captures what I felt when I discovered Lisp. As a kid learning programming in the 80s, I had already done some BASIC, Fortran, Pascal and COBOL in high school and early college. There were differences, of course, but they had some fundamental commonality.At UC Berkeley, however, the first computer science class was taught in Scheme (a dialect of Lisp)...and it absolutely blew me away. Hofstadter is right: it feels the closest to math (reminding me a ton of my math theory classes). It was the first beautiful language I discovered.(edit: I forgot to paste in the quote I loved!)\"...Lisp and Algol, are built around a kernel that seems as natural as a branch of mathematics. The kernel of Lisp has a crystalline purity that not only appeals to the esthetic sense, but also makes Lisp a far more flexible language than most others.\"\n \nreply",
      "Have you tried Haskell? It feels much closer to math to me. Definitions, not procedures. It even looks like math.\n \nreply",
      "No! After about 10 years of writing software professionally, I moved over to product management, and my time spent coding decreased drastically (in the last 15 years, only some Python to show my kids a thing or two).But I'd love to try! Maybe I'll take an online class for fun.\n \nreply",
      "I can't recommend it highly enough. You're already familiar with laziness from Lisp, but purity is another head-trip. It made me a better programmer in any language, and even a better software architect before I've written a line of code.And algebraic data types make it possible to make your code conform to reality in ways that classes can't. Once you're exposed to them, it's very much like learning about addition after having been able to multiply for your whole life. (In fact that's more than a metaphor -- it's what's happening, in a category theoretic sense.)Haskell has other cool stuff too -- lenses, effect systems, recursion schemes, searching for functions based on their type signatures, really it's a very long list -- but I think laziness, purity and ADTs are the ones that really changed my brain for the better.\n \nreply",
      "Have you tried Coalton? It's a Common Lisp library that adds Haskell-esque (or near-Haskell) type wonders, and which smoothly interoperates with your Common Lisp code.Your comment is great though, consider me convinced. I've done a bit of messing with Lisp, but really would like to try write something in Haskell, or slog through a book or two, some day.\n \nreply"
    ],
    "link": "https://gist.github.com/jackrusher/5139396",
    "first_paragraph": "\n        Instantly share code, notes, and snippets.\n      In the mid-80s, while reading through my roommate's collection of Scientific American back issues, I encountered this introduction to Lisp written by Douglas Hofstadter. I found it very charming at the time, and provide it here (somewhat illegally) for the edification of a new generation of Lispers.In a testament to the timelessness of Lisp, you can still run all the examples below in emacs if you install these aliases:February, 1983IN previous columns I have written quite often about the field of\nartificial intelligence - the search for ways to program computers so\nthat they might come to behave with flexibility, common sense,\ninsight, creativity, self awareness, humor, and so on. The quest for\nAI started in earnest over two decades ago, and since then has\nbifurcated many times, so that today it is a very active and\nmultifaceted research area. In the United States there are perhaps a\ncouple of thousand people professionally inv",
    "summary": "**Hofstadter on Lisp (1983) (gist.github.com)**\n\nIn a world where programming nostalgia is apparently at premium, one brave soul \"somewhat illegally\" rescues a Douglas Hofstadter piece on Lisp from the dusty shelves of the 1980s and bravely pastes it onto GitHub. Comments quickly spiral into a pedantic quagmire, where Lisp enthusiasts tripping over their 'ovals' and 'snocs' seek to decode typos like medieval scholars deciphering lost scriptures. Meanwhile, other commenters wax poetic about Lisp's crispy syntax that evidently marries coding with abstract mathematics, a revelation sure to cure insomnia across the globe. A cameo by Haskell pleads for relevance in a corner. \ud83e\uddd9\u2728\ud83d\udcbe"
  },
  {
    "title": "We outsmarted CSGO cheaters with IdentityLogger (mobeigi.com)",
    "points": 134,
    "submitter": "mobeigi",
    "submit_time": "2024-10-16T18:18:07.000000Z",
    "num_comments": 159,
    "comments_url": "https://news.ycombinator.com/item?id=41862028",
    "comments": [
      "For UT2004, you can ban by player GUID (a hash of the CD key) or IP.  With the game abandoned by Epic, a number of key generators have cropped up, which makes GUID bans useless.  IP bans only go so far with VPNs costing $2 these days.The main solutions we have today are IP ban + VPN blocking using a database of known VPN subnets and adding them all to the firewall, and a similar fingerprinting technique which scans their folder structure of certain system folders.\n \nreply",
      "> IP bans only go so far with VPNs costing $2 these days.https://redman.xyz/doku.php/schachtmeister2 was made specifically against people using VPNs.It was made for Tremulous (ioquake3 fork) where people kept evading IP bans, but it can be used for any other games.It is not my project, but I know the author, and I could personally fork it and make it suitable for specific (or any) games if there is demand for it.You may also use heuristics, too, in schachtmeister2:  whois   -10     \"Hosting\"\n  whois   -10     \"hosting\"\n  whois   -7      \"Server\"\n  whois   -4      \"server\"\n  whois   -10     \"VPS\"\n  whois   -13     \"VPN\"\n  whois   -3      \"Private Network\"\n  whois   +7      \"residential\"\n  whois   +7      \"Residential\"\n  whois   -20     \"Dedicated Server\"\n\nEdit: I noticed that the git repository returns 502, contacted the maintainer.\n \nreply",
      "Wait, can you help maintain UT2004?  Because I love that game.I don't play online anymore because I get destroyed but it's still fun to pop in for a quick match against AI when I have 30 minutes to kill.\n \nreply",
      "This still leaves you wide open to cheaters using mobile data tethering and proxies. Have you considered more advanced network analysis? It's one of the areas I have an interest in (professionally and personally) so if you want any suggestions let me know.\n \nreply",
      "> This still leaves you wide open to cheaters using mobile data tethering and proxiesIs latency going to be good enough on mobile data (especially if they're also using proxies) for a FPS, though?  Sure, they're using cheating software, but I wouldn't be surprised if the software gets the information it needs to cheat too late often enough for it to be useful.\n \nreply",
      "Assuming obvious cheat, even 100ms or 200ms latency is unbeatable by a human. Especially since the cheat doesn't need time to aim.Even for non-obvious use-cases, it's hard to beat the advantage provided by knowing the position of players.On my own hotspot, I have less than 30ms of latency.\n \nreply",
      "I regularly played CSGO in Europe because the North American ranking system were screwed up.I got to Supreme (2nd highest rank) with 150 ms ping. The people I queued with hit Global.It's possible to play legitimately with very high ping. The higher ping put us at a disadvantage, but the skill gap between regions made it worth it to arbitrage.\n \nreply",
      "What was screwed up about the NA ranks?\n \nreply",
      "NA is (or at least was when I played) the most populated and visible regional zone, and attracts a lot of players attempting various kinds of rank manipulation. On the one hand you have smurfing, which is the practice of a relatively high skill player using a an account with relatively low rank so that they can dominate lower ranked players. On the other side you have boosting, which is a relatively high skill player ranking up new accounts for later sale.In practice this means at lower ranks, it was not at all uncommon to be matched with players with similar rank but vastly better skills.\n \nreply",
      "This was my experience too years ago when I played CSGO. The difficulty at higher ranks (up to a certain point) felt significantly easier than the lower ranks. Getting out of the silver and gold ranks (can't remember the exact names) was a hellish grind with lots of matches that ended in one sided stomps with one or two guys on the other team racking up some insane k/d. Past that was smooth sailing for a long long way.\n \nreply"
    ],
    "link": "https://mobeigi.com/blog/gaming/how-we-outsmarted-csgo-cheaters-with-identitylogger/",
    "first_paragraph": "",
    "summary": "**The Internet Tackles Cheaters, or at Least Pretends To**\n\nIn a groundbreaking display of cyber-detective prowess that would make Sherlock Holmes roll his eyes, the immensely self-satisfied tech mavens of the internet have assembled on mobeigi.com to take on CSGO cheaters. With strategies reminiscent of a paranoid tech-support guide, commenters toss around acronyms like GUIDs, VPNs, and IP bans with the gusto of a squirrel on espresso. One even suggests scanning system folders, because nothing screams \"user-friendly gaming experience\" like letting a random server rummage through your files. Meanwhile, another commenter, probably petting their Mr. Robot poster, whispers sweet nothings about network analysis and proxy wars, seemingly unaware that the article's topic was gaming, not cyber warfare."
  },
  {
    "title": "Show HN: Clean News - A cleaner curation of world news events (cleannews.fyi)",
    "points": 13,
    "submitter": "sumeruchat",
    "submit_time": "2024-10-16T23:31:32.000000Z",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=41864947",
    "comments": [
      "Pretty cool stuff. If I were building this, I'd add some tagging system so users could filter what they want to see. Politics, business, sports, entertainment, etc.Right now it seems heavily weighted towards world politics with a sprinkle of Victoria's SecretSpecifically I'd nake it like little tags that you can click on to filter for or click on an x to filter out, something like that (think like how you can \"solo\" or \"mute\" any given track on a DAW)\n \nreply",
      "Noted. I was thinking of those filtering tags in the top bar as well.\n \nreply",
      "How would you compare/contrast this to https://en.wikipedia.org/wiki/Portal:Current_events ?Edit:  You appear to be scraping it, actually?\n \nreply",
      "Yes and it is a pretty good MVP using that :)\n \nreply",
      "You might want to give appropriate credit!  And by \"might\" I mean specifically https://en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative...\n \nreply",
      "Good point will add that to the footer asap.\n \nreply",
      "This is awesome. Reminds me of the drudgereport when it was good. Only recommendation would be a way to see the article content summary without click through if I want a bit more info but not enough to click through and face ads. Food for thought.\n \nreply",
      "Thanks! Yeah thats coming soon within a month (using gpt4o).\n \nreply",
      "very biased and unrealiable sources though...\n \nreply",
      "Perhaps one piece of news seems to be positive and most seem about death and war. I respect the site and effort but I would worry about putting myself in such a headspace.  There is positive stuff in this world too just don't lose sight of it!\n \nreply"
    ],
    "link": "https://cleannews.fyi",
    "first_paragraph": "",
    "summary": "Welcome to Clean News, where the future of journalism is apparently a regurgitated Wikipedia feed with fancy tags. Today's culinary delight on Hacker News features a single seasoning of \"world politics\" with a tantalizing hint of Victoria's Secret. Our genius commenters excitedly debate invisible technologies and eagerly out themselves as people who think \"adding tags\" is a groundbreaking feature. Meanwhile, fearless defenders of the Creative Commons calmly remind a budding \"innovator\" to maybe, just possibly, <i>credit the source he's scraping</i>. Because, you know, ethics. \ud83d\ude44"
  },
  {
    "title": "Show HN: Automated smooth Nth order derivatives of noisy data (github.com/hugohadfield)",
    "points": 56,
    "submitter": "hugohadfield",
    "submit_time": "2024-10-16T20:17:46.000000Z",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41863398",
    "comments": [
      "This is great! I've taken sort of a passive interest in this topic over the years, some papers which come to mind are [1] and [2] but I don't think I've seen a real life example of using the Kalman filter before.[1] https://www.sciencedirect.com/science/article/abs/pii/002192...[2] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=924...\n \nreply",
      "Congratulations! Pardon my ignorance, as my understanding of mathematics at this level is beyond rusty, but what are the applications of this kind of functionality?\n \nreply",
      "I actually have one for this! Last week I had something really specific - a GeoTIFF image where each pixel represents the speed in \"x\" direction of the ice sheet surface in Antarctica and I wanted to get the derivative of that velocity field so I could look at the strain rate of the ice.A common way to do that is to use a Savitzky-Golay filter [0], which does a similar thing - it can smooth out data and also provide smooth derivatives of the input data. It looks like this post's technique can also do that, so maybe it'd be useful for my ice strain-rate field project.[0] - https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter\n \nreply",
      "I've been a heavy user of Savitzky-Golay filters (linear time series, rectangular grid images, cubic space domains | first, second and third derivitives | balanced and unbalanced (returning central region smoothed values and values at edges)) since the 1980s.The usual implementation is as a convolution filter based on the premise that the underlying data is regularly sampled.The pain in the arse occassional reality is missing data and|or present but glitched|spiked data .. both of which require a \"sensible infill\" to continue with a convolution.This is a nice implementation and a potentially useful bit of kit- the elephant in the room (from my PoV) is \"how come the application domain is irregularly sampled data\"?Generally (engineering, geophysics, etc) great lengths are taken to clock data samples like a metronome (in time and|or space (as required most)).I'm assuming that your gridded GeoTIFF data field is regularly sampled in both the X and Y axis?\n \nreply",
      "Thanks for that, it looks like my research today is cut out for me.\n \nreply",
      "No problem! Let's dream up a little use case:Imagine you have a speed sensor eg. on your car and you would like to calculate the jerk (2nd derivative of speed) of your motion (useful in a range of driving comfort metrics etc.). The speed sensor on your car is probably not all that accurate, it will give some slightly randomly wrong output and it may not give that output at exactly 10 times per second, you will have some jitter in the rate you receive data. If you naiively attempt to calculate jerk by doing central differences on the signal twice (using np.gradient twice) you will amplify the noise in the signal and end up with something that looks totally wrong which you will then have to post process and maybe resample to get it at the rate that you want. If instead of np.gradient you use kalmangrad.grad you will get a nice smooth jerk signal (and a fixed up speed signal too).\nThere are many ways to do this kind of thing, but I personally like this one as its fast, can be run online, and if you want you can get uncertainties in your derivatives too :)\n \nreply",
      "I'd been researching Kalman filters to smooth out some sampling values (working on mobile: anything from accelerometer values to voice activation detection), but hadn't got around to revising the mathematics, so I appreciate the explanation.  Out of curiosity, what other ways might this be achieved? I haven't seen much else beyond Kalman filters.\n \nreply",
      "Basically, approximating calculus operations on noisy, discrete-in-time data streams.\n \nreply",
      "This is what I was thinking, but stated much clearer than I'd have managed.\n \nreply",
      "That's useful. Can it generate a simple filter for later real-time use, based on the statistics of the noise? That would be useful for self-tuning controllers.\n \nreply"
    ],
    "link": "https://github.com/hugohadfield/kalmangrad",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Automated, smooth, N'th order derivatives of non-uniformly sampled time series data\n      kalmangrad is a python package that calculates automated smooth N'th order derivatives of non-uniformly sampled time series data. The approach leverages Bayesian filtering techniques to compute derivatives up to any specified order, offering a robust alternative to traditional numerical differentiation methods that are sensitive to noise. This package is built on top of the underlying bayesfilter package.Estimating derivatives from noisy data is a common challenge in fields like signal processing, control systems, and data analysis. Traditional numerical differentiation amplifies noise, leading to inaccurate results. Anyone who has naiivly attempted to differentiate sensor data has run into this problem. This repository implements a bayesian fi",
    "summary": "Title: Show HN: I reinvented noise reduction, but with more steps\n\nA GitHub warrior has just unleashed \"kalmangrad,\" an astonishing Python package that miraculously computes N'th order derivatives from data as noisy as Hacker News comment sections. Claiming to transcend mere mortal techniques like old school numerical differentiation, this approach brings in the heavy artillery from Bayesian filtering because, clearly, that's what every hobbyist data punter has been impatiently waiting for. Commenters erupted in joy, dusting off their calculus textbooks\u2014or pretending to while frantically Googling Kalman filters\u2014to craft replies that blend vague understanding and awe. Meanwhile, a brave wanderer inquires about real-world applications, perhaps not realizing that their Rusty mathematics will get no mercy here. \ud83d\ude80\ud83e\udd13"
  },
  {
    "title": "Should We Chat, Too? Security Analysis of WeChat's Mmtls Encryption Protocol (citizenlab.ca)",
    "points": 90,
    "submitter": "lladnar",
    "submit_time": "2024-10-16T20:06:58.000000Z",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=41863278",
    "comments": [
      "The Chinese government has direct access to the WeChat backend so it's unlikely that these weaknesses were government mandated. Probably just the result of overworked 996 developers:>The name 996.ICU refers to \"Work by '996', sick in ICU\", an ironic saying among Chinese developers, which means that by following the \"996\" work schedule, you are risking yourself getting into the ICU (Intensive Care Unit)https://github.com/996icu/996.ICU\n \nreply",
      "WeChat is basically one of the tools the communist party uses to control the population. If something is on there it is most likely by design.Off topic (or is it?): While back a western journalist in China reported that her wechat account was banned 10 minutes after changing her password to \"fuckCCP\"...\n \nreply",
      "The point being made in the preceding comment is that the threat model for WeChat already overtly includes its operators being able to puncture its confidentiality. It doesn't make a lot of operational sense to introduce complicated cryptographic backdoors (such as the IV construction, which the authors say could potentially introduce an AES-GCM key/IV brute forcing attack) when you control the keys for all the connections in the first place.\n \nreply",
      "Not only control keys, but control the software update mechanism (backdoor a la xz).\n \nreply",
      "The issue of accounts being banned after a password change is quite common, especially outside of China. This isn't related to the content of the new password.Additionally, it's unlikely that the protocol has government-mandated vulnerabilities, as such weaknesses could potentially allow foreign governments to spy on WeChat users that are abroad.  The Chinese government doesn't need such weaknesses, as they have access to the servers.\n \nreply",
      "I had my account banned for absolutely no reason (I didn't even use it to talk to anyone and was simply learning the interface myself to explain it later to a friend who was traveling to China). You can't infer anything from that story. Their \"security\" automation is even more paranoid than Google's, that's probably all there's to it.\n \nreply",
      "Chinese apps don't need encryption but pretends to, the government had direct access to all clear-text data. If you can't comply your business would be fucked one way or another.Security researchers need to stop beating the dead horse. The encryption mechanism is mostly used for compliance or certificate. In fact many on the shelf intranet middleboxes can decrypt wechat communications, it's not a bug, it's a feature.IRL people just treat wechat as somekind of Discord with payment options. If you say something slightly wrong your account would instantly get into trouble. Just assume your wechat chat records are public one way or another.\n \nreply",
      ">Generally, NIST recommends[1] not using a wholly deterministic derivation for IVs in AES-GCM since it is easy to accidentally re-use IVs.A quick skim of the referenced document did not show where NIST recommended against the use of deterministic IVs. The document actually spends a significant amount of text in discussing how one would do such a thing. Did I miss something?>Lack of forward secrecyThe article mentions that the key is forgotten when you close the app. Probably enough forward secrecy for most people.>Since AES-CBC is used alongside PKCS7 padding, it is possible that the use of this encryption on its own would be susceptible to an AES-CBC padding oracle, which can lead to recovery of the encrypted plaintext.This is a messaging app. Is there actually an available oracle? Does the implementation even generate a padding error?[1] https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpubli...\n \nreply",
      "The GCM IV thing didn't ring true to me either; in fact, the whole reason we have XAES-type constructions is to enable fully nondeterministic IVs, which don't fit comfortably in the GCM IV space.Regarding padding oracles: it is most definitely not necessary for a target to generate a \"padding error\", or even an explicit error of any sort, to enable the attack.\n \nreply",
      "There has to be some reverse channel to do an oracle. Timing? That might not be a thing for messaging. Signal apparently also uses CBC with the same type of padding. So the same shade could be thrown in that direction if someone really wanted to do so.I would be happier if there were fewer vague assertions in these sorts of writeups...\n \nreply"
    ],
    "link": "https://citizenlab.ca/2024/10/should-we-chat-too-security-analysis-of-wechats-mmtls-encryption-protocol/",
    "first_paragraph": "",
    "summary": "Title: Should We Chat, Too? Security Analysis of WeChat's Mmtls Encryption Protocol\n\nIn an unfathomable twist, Citizen Lab manages to blow the lid off the unknown: WeChat's encryption might as well be a paper door in a hurricane. Commenters eagerly play top trumps with paranoia anecdotes \u2013 ranging from authoritarian bans for password creativity to cryptographic facepalms about why bother encrypting when Big Brother has the master keys. Meanwhile, tedious technical tangents about IVs and padding oracles make everyone briefly consider a career in literally anything else. So keep chatting on WeChat, safe in the knowledge that it\u2019s secure as a sieve and just as transparent to those who built the sieve! \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcac\ud83d\udeaa"
  },
  {
    "title": "FTC announces \"click-to-cancel\" rule making it easier to cancel subscriptions (ftc.gov)",
    "points": 1200,
    "submitter": "pseudolus",
    "submit_time": "2024-10-16T13:09:04.000000Z",
    "num_comments": 562,
    "comments_url": "https://news.ycombinator.com/item?id=41858665",
    "comments": [
      "When people try and say that regulating stuff like this is impossible, I often think about how unreasonably great the regulations around \u201cUnsubscribe\u201d links in emails are.There really seems to be no loophole or workaround despite there being huge incentive for there to be one. Every time I click an \u201cUnsubscribe\u201d link in an email (it seems like they\u2019re forced to say \u201cUnsubscribe\u201d and not use weasel words to hide the link) I\u2019m either immediately unsubscribed from the person who sent me the email, or I\u2019m taken to a page which seemingly MUST have a \u201cremove me from all emails\u201d option.The level of compliance (and they can\u2019t even do malicious compliance!) with this is absurd. If these new rules work anything like that, they\u2019ll be awesome. Clearly regulating behavior like this is indeed possible.\n \nreply",
      "Unsubscribe links are a fantastic regulation, but there is a workaround. I must have received at least a dozen emails from Brown after graduating despite unsubscribing to every email they sent.The trouble is they're endlessly creative about the lists they put you on. I'd get one email from \"Alumni Connections\" and then another from \"Faculty Spotlight\" and then another from \"Global Outreach\" and then another from \"Event Invitations, 2023 series\". I'm making those names up because I forget exactly what they were called, but you get the idea. I hope this was in violation of the regulation: surely you can't invent a new mailing list that didn't used to exist, add me to it, and require me to unsubscribe from it individually.They finally stopped after I sent them an angry email.\n \nreply",
      "What I have noticed companies do is resume emails after a year or so. They probably think people would forget about unsubscribing them after a year, and for the most part they are right.If I catch any of these email lists not respecting my unsubscribing, I immediately mark them as \"spam\".Gmail then doesn't send them to my inbox anymore. I don't think just one person marking them as spam hurts them, but at least I feel gratified and my ego is satisfied.\n \nreply",
      "I go one step further and for the lists which I don't remember subscribing to, I never click \"Unsubscribe\" - it's \"Spam\" right away.\n \nreply",
      "Same for me. Spam or phishing, depending on how annoyed I am.Some site I haven't used in 5 years reminding me to login and check out their deals? Sounds like a phishing trap to me.\n \nreply",
      "One thing that probably happens, as some who attends a lot of events or at least used to, is that you end up getting repopulated in a lot of mailings through purchased lists or badge scans.\n \nreply",
      "This is where we need something like GDPR, which makes it so that they can't auto subscribe you to a new list whenever they feel like resubscribing you.\n \nreply",
      "Or they interpret any kind of interaction after a while of inactivity as \"yes please sign me up for all your newsletters, even though I previously explicitly told you to unsubscribe me\"\n \nreply",
      "The worst for this is Shopify. If you've ever given your email to shopify, they will absolutely share it to a page you visit, even if you don't check out.Throw something in the cart at a random website? Now you're on their mailing list and get reminders to finish checking out. Doesn't matter that you never consented. I don't know how this isn't a violate of the CAN-SPAM act\n \nreply",
      "Now is a good time to mention SimpleLogin. So... yeah. SimpleLogin.\n \nreply"
    ],
    "link": "https://www.ftc.gov/news-events/news/press-releases/2024/10/federal-trade-commission-announces-final-click-cancel-rule-making-it-easier-consumers-end-recurring",
    "first_paragraph": "An official website of the United States governmentHere\u2019s how you know\nThe .gov means it\u2019s official.\n\n                Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you\u2019re on a federal government site.\n              \nThe site is secure.\n\n                The https:// ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely.\n              We enforce federal competition and consumer protection laws that prevent anticompetitive, deceptive, and unfair business practices.View EnforcementFind legal resources and guidance to understand your business responsibilities and comply with the law.Browse legal resourcesView all Competition Matters Blog postsWe work to advance government policies that protect consumers and promote competition.View PolicyFind legal resources and guidance to understand your business responsibilities and comply with the law.Browse legal resourc",
    "summary": "**FTC Revolutionizes Subscription Cancellations, Web Users Skeptical**\n\nIn a stunning display of common sense rarely seen from governmental entities, the FTC champions the heroic \"click-to-cancel\" rule, poised to rescue the average Joe from the labyrinthine tactics of subscription models\u2014the kind where escaping charges often involves solving the Riddle of the Sphinx. Web commenters erupt into a frenzy of applause, each sharing war stories of email battles fought bravely against university alumni associations and underhanded e-commerce operations. Meanwhile, others school the comment section on the elite art of marking emails as spam as if performing an exorcism on their inbox. Who knew clicking 'unsubscribe' could feel so rebelliously gratifying? \ud83e\udd14\ud83d\udcac\ud83d\udeab"
  },
  {
    "title": "ArchiveBox is evolving: the future of self-hosted internet archives (sweeting.me)",
    "points": 443,
    "submitter": "nikisweeting",
    "submit_time": "2024-10-16T16:18:39.000000Z",
    "num_comments": 104,
    "comments_url": "https://news.ycombinator.com/item?id=41860909",
    "comments": [
      "@nikisweeting ArchiveBox is awesome and we'd really love it to be more awesome. And sustainable!I've posted issues and PRs for showstopper issues that took months to get merged in:\nhttps://github.com/ArchiveBox/ArchiveBox/issues/991\nhttps://github.com/ArchiveBox/ArchiveBox/pull/1026You have the opportunity for the community to lean in on ArchiveBox. I understand it's hard to do everything as a solo dev, we've seen many cases in the community where solo devs get burned out or have personal challenges that take priority etc.It's hard for us users to lean in on ArchiveBox when after a happy month of archiving, things start break and you're left with maintaining a branch of your own fixes that aren't in main. Meanwhile, your solution of soliciting one time donations just makes the whole project feel more rickety and fly-by-night. How about thinking bigger?We NEED ArchiveBox to be a real thing. Decentralized tooling for archiving is SO IMPORTANT. I care about it and I suspect many people do. I'm posting this so other people who care about it can also comment and chime in and suggest how it can become something we can rely on. Because archiving isn't just about the past, it's about the future.Maybe it needs to be a dev org of three committed part-time maintainers, and a small foundation that people recurrently support is what grants it? IDK. I'm not an expert at how to make open source resilient. There have been discussions about this in the past, but I think it's worth a serious look because ArchiveBox is IMPORTANT and I want it to work any month I decide to re-activate my interest in it. I invite people to discuss ways to make this valuable project more sustianable and resilient.\n \nreply",
      "Let chat more. I'm almost ready to raise some seed money, hire a second staff dev or find a cofounder, and I'm looking for people that care deeply about the space.It's only been during the last few months that I decided to go all in on the project, so this is still just the first few pages of a new chapter in the project's history.(I should also mention that if you're a commercial entity relying on ArchiveBox, you can hire us for dedicated support and uptime guarantees. We have a closed source fork that has a much better test suite and lots of other goodies)\n \nreply",
      "It looks like you're doing great work here, thanks a bunch; looking forward to seeing this project develop.Selling custom integrations, managed instances, white-glove support with an SLA, and so on seems like a reasonable funding model for a project based on an open-source, self-hostable platform. But I'm a little disheartened to read that you're maintaining a closed fork with \"goodies\" in it.How do you decide which features (better test suite?) end up in the non-libre, payware fork of your software? If someone contributed a feature to the open-source version that already exists in the payware version, would you allow it to be merged or would you refuse the pull request?\n \nreply",
      "The idea with the plugin system is that plugins are just git repos containing <pluginname>/__init__.py, and you can add any set of git repo plugins you want to your instance.The marketplace will work by showing all git repos tagged with the \"archivebox\" tag on github.My approval is only needed for PRs to the archivebox core engine.More info on free vs paid + reasoning why it's not all open source: https://news.ycombinator.com/item?id=41863539\n \nreply",
      "I love this project. I \"independently\" \"invented\" it in my head the other day, and happy to see it already exists!I'd love to see blockchain proof/notary support. The ability to say \"content matching this hash existed at this time.I'm exceptionally busy now but that being said, I may choose to contribute nonetheless.I'd love to connect directly, and will connect to the Zulip instance later.If we align on values, I may be able to connect you with some cash. People often call me an \"anarchist\" or \"libertarian\", though I'm just me, not labels necessary.\n \nreply",
      "\"I too would like commit access to your promising looking project's  git repo and CI/CD pipeline. Thanks, Jia Tan\"\n \nreply",
      "lololol\n \nreply",
      "Do you guys have a Discord by chance? I have a close friend who is insanely passionate about archiving, he has a personal instance of archivebox, and is working on a Video Downloading project as well. He has used it almost everyday and archived thousands of news articles over years. He's aware of a lot of the nuances.\n \nreply",
      "We have a Zulip which is similar to discord (but self hosted and it has better threading): https://zulip.archivebox.io\n \nreply",
      "https://github.com/ArchiveTeam/grab-site might be helpful. I'm a fan of the ability to create WARC archives from a target, uploard the WARC files to object storage (whether that is IA, S3, Backblaze B2, etc), and then keep them in cold storage or serve them up via HTTPS or a torrent (mutable, preferred). The Internet Archive serves a torrent file for every item they host; one can do the same with WARC archives to enable a distributed archive. CDX indexes can be used for rapidly querying the underlying WARC archives.You might support cryptographically signing WARC archives; Wayback is particular about archive provenance and integrity, for example.https://www.loc.gov/preservation/digital/formats/fdd/fdd0005... (\"CDX Internet Archive Index File\")https://www.loc.gov/preservation/digital/formats/fdd/fdd0002... (\"WARC, Web ARChive file format\")https://github.com/internetarchive/wayback/tree/master/wayba... (\"Wayback CDX Server API - BETA\")\n \nreply"
    ],
    "link": "https://docs.sweeting.me/s/archivebox-plugin-ecosystem-announcement",
    "first_paragraph": "",
    "summary": "**ArchiveBox Is Evolving, But Can It Keep Up With Its Own Ambition?**\n\nThe venerable tool for hoarding internet scraps, ArchiveBox, gets a *passionate* critique from a self-identified tech savior on <em>sweeting.me</em>. Apparently, our lone warrior developer is drowning under the weight of user needs and the solution is\u2014wait for it\u2014more *money* and *people*. Hold your applause, though, there's a treasury twist\u2014a secret stash of enhanced features locked behind a paywall, because nothing spells 'community' like a VIP lounge! Meanwhile, the commenters juggle between patting backs and subtly auditioning for tech messiah roles, because who needs titles when you're the self-proclaimed anarchist/libertarian savior of the digital archives. \ud83d\ude02\ud83d\udcdc\ud83d\udcbe"
  },
  {
    "title": "Traveling with Apple Vision Pro (azadux.blog)",
    "points": 327,
    "submitter": "tosh",
    "submit_time": "2024-10-16T13:48:51.000000Z",
    "num_comments": 424,
    "comments_url": "https://news.ycombinator.com/item?id=41859012",
    "comments": [
      "For air travel, I really like my Xreal Air glasses now that I have a newer iPhone 16pro.   Just plug in the USB-C cable, and you have a virtual 60\" screen in front of you which works perfectly for Netflix, etc.   And they cost less than 10% of the cost of an AVP, and are not limited to 2-3 hours of battery life (they get power from the phone).Note that if you have an older (lightning) iPhone, don't bother with these.  They require a pair of  dongles.  Not only does that make things really awkward, but one of the dongles ends up apparently blocking HDCP, and prevents you from using anything but ... your own... downloaded content.\n \nreply",
      "Never known this exists. How\u2019s the experience from HN users here? I may get the Aair as it is on-sale now.\n \nreply",
      "As I get older, a gin and tonic (or two) is what makes air travel more relaxing.\n \nreply",
      "Taking a flight as an opportunity to indulge in a moment of blissful idleness is great... on a three-hour flight.But on a 14-hour trans-continental flight, you've gotta have something to do. If nothing else, to distract you from how uncomfortable it is to be effectively confined to your seat + a few feet of narrow walkway for that long. That's more confined than a prison cell!\n \nreply",
      "Nah. I do Europe-Korea frequently, and I've definitely slowly settled into a happy optimum of sleeping 7-8 hours inbetween the meals.Killing time is a rather slower flight experience than being unconscious.I'm very happy WiFi continues to be an expensive opt-in product. If it was always-on, I'm sure I'd break the above habit.\n \nreply",
      "Good for you. I can't sleep sitting on plane. The only times I've been able to to sleep is (1) getting a free upgrade to 1st class where I could lie down (2) a mostly empty plane where I could stretch out across three seats.\n \nreply",
      "Like you, I cannot sleep on vehicles, but I used benzos on a recent flight, and it was like an instant time-skip forward during nighttime of the destination time zone, which helped with jet lag. Benzos are pretty terrible for you if used regularly, but if you have the self-control to limit your usage to a few long flights a year, it would be irrational not to take advantage of them. (I am not a doctor)\n \nreply",
      "I couldn't sleep on planes my entire life until one year I did Europe-Korea 8 times and it became so routine it started working.The routine part, I think, is what took the \"I won't sleep on this long flight\" anxiety away somehow, because I started learning the rhythm of the flight. For example, there is no point to try and sleep until the initial meal is served, you'll just get woken up anyway. So now I just spend that time thinking and daydreaming and being bored, then I quietly eat my meal but stop at feeling overly fully, and by the time it's over I start to get tired.I also realized that a sleeping mask is a great aid for me (on the other hand, I don't need earplugs/earphones). This came as a great surprise, as I don't typically need darkness to sleep well, but something about the sensory deprivation in the plane setting seems to do the trick.What I'm saying is, I used to describe myself as the \"I don't sleep on planes\" guy for a good decade, but then it started working; don't give up yet.BTW, to give this a software dimension: I was recently on a flight with an airline I hadn't used before, and I really liked a UI in their in-flight infotainment that showed the entire flight as a timeline with all meaningful events penciled in (when the meals are, etc.) and a recommendation during which blocks to sleep. That was really nice and thoughtful.\n \nreply",
      "> BTW, to give this a software dimension: I was recently on a flight with an airline I hadn't used before, and I really liked a UI in their in-flight infotainment that showed the entire flight as a timeline with all meaningful events penciled in (when the meals are, etc.) and a recommendation during which blocks to sleep. That was really nice and thoughtful.Do you recall which airline this was? Given similar pricing, this seems like a useful differentiator that would sway my decision of which airline to pick.",
      "Not sure why not mention the airline in the post directly. Is this some kind of avoiding advertising for the brand? Really curious. Can't be click bait or SEO on hn."
    ],
    "link": "https://azadux.blog/2024/10/08/traveling-with-apple-vision-pro/",
    "first_paragraph": "The Vision Pro has quickly become an essential item that I take onto every flight.It\u2019s a fantastic device to travel with\u2014Be it by train or by plane, it offers an unparalleled opportunity to selectively tune out your environment and sink into an engaging activity like watching a movie or just working on your laptop.\u00a0In this blog post, I\u2019ll outline what I\u2019ve learned about the Vision Pro while traveling, explain some of the functionality, shine light onto its drawbacks, as well as assess how it fares against solutions like a phone or a laptop.\u00a0For context, most of the traveling where I\u2019ve brought my Vision Pro along has been on airplanes, but this applies to traveling by train as well.\u00a0First and foremost, space in my suitcase or backpack while traveling is the most valuable commodity. Whenever I\u2019m packing for a trip, anything that adds unnecessary bulk to my setup is immediately discarded.\u00a0For that reason, I highly recommend that you do NOT purchase the behemoth of the $200 Vision Pro \u201ctr",
    "summary": "### Traveling with Apple Vision Pro: A Game-Changer or Just Another Gadget?\n\nThe digital nomads are restless yet again, buzzing about the latest in eye-wear gadgetry, the Apple Vision Pro. According to a diehard fan's blog, this Zeus of gadgets ensures you can \"tune out\" from the nightmare of public travel and immerse into a personal cinematic experience \u2014 all while the peasants around you bask in the aura of your tech halo. \ud83d\udcbb\u2708\ufe0f Meanwhile, in the comment trenches, digital wanderers passionately compare their travel hacks, from vintage methods like downing gin tonics to futuristic eyewear that promises to revolutionize boredom. One deeply philosophical debate unravels whether it's more Zen to stare into the abyss of flight anxiety or chemically induce oblivion via benzos. As the line between real travel woes and virtual escapism blur, who needs actual destinations anymore? \ud83c\udf0e\ud83d\ude34"
  },
  {
    "title": "Ichigo: Local real-time voice AI (github.com/homebrewltd)",
    "points": 68,
    "submitter": "egnehots",
    "submit_time": "2024-10-14T17:25:50.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41839686",
    "comments": [
      "Finally I can use one of the random facts that have entered my brain for decades now even though I can't remember where my keys are.If I remember correctly, \"ichigo\" means strawberry in japanese. You are welcome.\n \nreply",
      "There are strawberries all over the readme so I reck you're right.\n \nreply",
      "Is this a continuation of the meme that GPT can't identify the number of \"R\"s in \"strawberry\"?\n \nreply",
      "Getsuga tenshou!!\n \nreply",
      "Your keys are in the fridge with the remote control.\n \nreply",
      "Tatakae!\n \nreply",
      "This is a really cool project! What have people built with it? I'd love to learn about what local apps people are building on this.\n \nreply",
      "its amazing to see cool projects like this really REALLY based in opensource and open training like this wow\n \nreply",
      "Very cool, but a bit less practical than some alternatives because it does not seem to do request transcription.\n \nreply"
    ],
    "link": "https://github.com/homebrewltd/ichigo",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Llama3.1 learns to Listen\n      \n\n\nHomebrewed early-fusion speech modelNoteUpdate: September 30, 2024Warning\ud83c\udf53 Ichigo is an open research experiment\ud83c\udf53 Ichigo is an open, ongoing research experiment to extend a text-based LLM to have native \"listening\" ability. Think of it as an open data, open weight, on device Siri.It uses an early fusion technique inspired by Meta's Chameleon paper.We build train in public:\ud83c\udf53 Ichigo is an open research project. We're looking for collaborators, and will likely move towards crowdsourcing speech datasets in the future.Checkout this notebook to try our latest model:For detailed information on synthetic generation, please refer to the Synthetic Generation Guide.Restart shell nowYou can also download the model using tune:Setup the Dataset from HF path by change the path and change the name of the model in ",
    "summary": "<h1>Ichigo: The Future of Forgetting Even More Basic Things</h1>\nThe tech savants at Homebrew Ltd have unleashed their latest Frankencreation, <em>Ichigo</em>, upon the GitHub masses, promising a Siri-like experience without the polished veneer of corporate development. Shrouded in the soothing buzzwords of \"open research\" and \"early fusion technique,\" this digital strawberry \ud83c\udf53 is set to revolutionize how we don\u2019t listen to each other. The comment section, a delightful cesspool of anime references and misplaced keys, buzzes with half-baked excitement that only a true open-source mess can elicit. Watch in awe as the digital horizon expands to include more untranscribed requests and forgotten personal items! \ud83c\udf89"
  },
  {
    "title": "Jank development update \u2013 Moving to LLVM IR (jank-lang.org)",
    "points": 72,
    "submitter": "refset",
    "submit_time": "2024-10-15T06:50:51.000000Z",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=41845669",
    "comments": [
      "I love this writing style, very easy to read.\nBest of luck with the optimizations effort! looking forward to hearing more about this project.\n \nreply",
      "Incredible work. A native Clojure would be a dream come true!Wish jank the best of lucks. Hope I can contribute soon.\n \nreply",
      "Nice to see a language take developer UX seriously and focus on good compilation times. I wish this was more common.\n \nreply"
    ],
    "link": "https://jank-lang.org/blog/2024-10-14-llvm-ir/",
    "first_paragraph": "Hi everyone! It's been a few months since the last update and I'm excited to outline what's been going on and what's upcoming for jank, the native Clojure dialect. Many thanks to Clojurists Together and my Github sponsors for the support. Let's get into it!Heart of ClojureIn September, I flew from Seattle to Belgium to speak at Heart of Clojure. For the talk, I wanted to dig deep into the jank details, so I created a walk-through of implementing exception handling in jank. You can watch my talk here, or in the embed below.AnnouncementPart of my Heart of Clojure talk was an announcement that, starting in January 2025, I'll be quitting my job at EA to focus on jank full-time. Two years ago, I switched from full-time to part-time at EA in order to have more time for jank. Now, with the momentum we have, the interest I've gathered, and the motivation backing this huge effort, I'm taking things all the way.I don't have funding figured out yet, though. It's hard for companies to invest in ja",
    "summary": "**Jank Development Update: The Soliloquy of a Coding Hipster**\n\nAnother day, another techie decides to quit the corporate hamster wheel to evangelize about their pet project, *jank*, the latest entrant in the increasingly obscure language Olympics. From part-time to no-time at EA, our hero boldly spills details about their shindig at the \u201cHeart of Clojure\u201d - because what's more riveting than detailing exception handling in a language only its creator might love? Comment sections across the digital cosmos are now pulsating with the usual cocktail of empty cheers and fuzzy good lucks. No solid funding yet, but hey, optimism can surely pay the bills, right? \ud83d\ude43"
  },
  {
    "title": "Reflex (YC W23) Hiring Senior/Staff Engineer \u2013 Infrastructure (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-10-16T21:17:35.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/reflex/jobs/uBt9ZNP-senior-staff-engineer-infrastructure",
    "first_paragraph": "",
    "summary": "At Reflex (a name that believably encapsulates the spontaneous, unthinking reactions its engineering choices inspire), the desperate cries for talent resound like an echo chamber where innovation ostensibly happens. Zero details about the job kinda scream trust issues, but hey, YC-backed startup glitter makes everything sparkle. Commenters, blissfully oscillating between pedantic nitpicks and glorified tech evangelism, leap at the opportunity to flaunt their esoteric jargon while simultaneously missing the point. Is it a job description? A cult initiation? Who knows, but surely *something* awesome, 'cause YC. Right? \ud83d\ude44"
  },
  {
    "title": "Cell-Based Architecture Enhances Modern Distributed Systems (infoq.com)",
    "points": 16,
    "submitter": "gemanor",
    "submit_time": "2024-10-15T05:30:09.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.infoq.com/articles/cell-based-architecture-distributed-systems/",
    "first_paragraph": "A monthly overview of things you need to know as an architect or aspiring architect.View an example\nWe protect your privacy.\n\n                        Facilitating the Spread of Knowledge and Innovation in Professional Software Development\n                    \nBack to login\n\nBack to login\nJoe Rowell explores the use of unified memory on modern GPU, the low-level details of how unified memory is realized on an x86-64 system, and some of the tools to understand what's happening on a GPU.In this article series, we take readers on a journey of discovery and provide a comprehensive overview and in-depth analysis of many key aspects of cell-based architectures, as well as practical advice for applying this approach to existing and new architectures.Roland Meertens and Anthony Alford discuss the historical cycles of AI \"summers\" and \"winters\": periods of optimism and decline in AI research. The conversation follows the story of neural networks, to the resurgence of AI with backpropagation and ",
    "summary": "In the latest episode of *Making Complicated Things Sound More Complicated*, InfoQ strikes again with \"Cell-Based Architecture Enhances Modern Distributed Systems\". Aspiring architects and confused IT interns alike gather to bask in the jargon-laden glory, mistaking complexity for profundity. \ud83e\udd13 Meanwhile, in the comments, IT crusaders argue fiercely over whether the article is groundbreaking or just another rehash of their first-year computer science textbook. The consensus remains elusive, but everyone agrees the diagrams are pretty cool."
  },
  {
    "title": "Ask HN: How do you add guard rails in LLM response without breaking streaming?",
    "points": 15,
    "submitter": "curious-tech-12",
    "submit_time": "2024-10-15T05:35:30.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41845256",
    "comments": [
      "If it's the problem I think it is, the solution is to run two concurrent prompts.First prompt validates the input.  Second prompt starts the actual content generation.Combine both streams with SSE on the front end and don't render the content stream result until the validation stream returns \"OK\".  In the SSE, encode the chunks of each stream with a stream ID.  You can also handle it on the server side by cancelling execution once the first stream ends.Generally, the experience is good because the validation prompt is shorter and faster to last (and only) token.The SSE stream ends up like this:    data: ing|tomatoes\n    \n    data: ing|basil\n    \n    data: ste|3. Chop the\n\nI have a writeup (and repo) of the general technique of multi-streaming: https://chrlschn.dev/blog/2024/05/need-for-speed-llms-beyond... (animated gif at the bottom).\n \nreply",
      "Hi, I run the model serving team at Databricks. Usually you run regex filters, LLAMA Guard, etc on chunks at a time so you are still streaming but it's in batches of tokens rather than single tokens at a time. Hope that helps!You could of course use us and get that out of the box if you have access to Databricks.\n \nreply",
      "But ultimately, it's an unsolved problem in the field. Every single LLM has been jailbroken.\n \nreply",
      "You start streaming the response immediately and kick off your guardrails checks. If the guard rail checks are triggered you cancel the streaming response.Perfect is the enemy of good enough.\n \nreply",
      "We have just the product for you! We\u2019ve recently improved guardrail accuracy by 25% for a $5B client and would be happy to show you how we do it.You're right - prompt eng. alone doesn't work. It's brittle and fails on most evals.Ping me at shaunayrton@galini.ai\n \nreply",
      "> Hi all, I am trying to build a simple LLM bot and want to add guard rails so that the LLM responses are constrained.Give examples of how the LLM should respond. Always give it a default response as well (e.g. \"If the user response does not fall into any of these categories, say x\").> I can manually add validation on the response but then it breaks streaming and hence is visibly slower in response.I've had this exact issue (streaming + JSON). Here's how I approached it:\n1. Instruct the LLM to return the key \"test\" in its response. \n2. Make the streaming call.\n3. Build your JSON response as a string as you get chunks from the stream.\n4. Once you detect \"key\" in that string, start sending all subsequent chunks wherever you need.\n5. Once you get the end quotation, end the stream.\n \nreply",
      "Not sure about the exact nature of your project, but for something similar I\u2019ve worked on, I had success using a combination of custom stop words and streaming data with a bit of custom logic layered on top. By fine-tuning the stop words specific to the domain and applying filters in real-time as the data streams in, I was able to improve the response to users taste. Depending on your use case, adding logic to dynamically adjust stop words or contextually weight them might also help you.\n \nreply",
      "Depending on what sort of constraints you need on your output, a custom token sampler, logit bias, or verifying it against a grammar could do the trick.\n \nreply",
      "What's your stack? What type of response times are you looking for?\n \nreply",
      "have it format in yaml instead of json, incomplete yaml is still valid yaml\n \nreply"
    ],
    "link": "item?id=41845256",
    "first_paragraph": "",
    "summary": "**Hackers Ask for Stream-guard Train Wrecks**\n\nIn a groundbreaking display of overengineering, a thread emerges on Hacker News filled with the brightest <em>tech luminaries</em> solving a non-problem: how to baby-proof real-time streaming responses from a large language model. What problem are we bulldozing today? A user unwisely wants to slap a half-baked validation step in front of streaming content generation, sparking an orgy of replies each more \"here's-a-solution-looking-for-a-problem\" than the last. Between pitching Databricks as some sort of panacea and suggesting regex filters as if it\u2019s still 1975, the responses blend into an indistinguishable symphony of techno-babble and wishful selling. Meanwhile, another commenter is just trying to make sure their sad string of JSON doesn\u2019t break - because what\u2019s more important than string formatting in the quest for artificial intelligence? \ud83d\ude02"
  },
  {
    "title": "Optimizing the Ion compiler back end (spidermonkey.dev)",
    "points": 102,
    "submitter": "undercut",
    "submit_time": "2024-10-16T17:15:24.000000Z",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=41861442",
    "comments": [
      "Wow, that dominator tree algorithm I wrote as an intern (https://bugzilla.mozilla.org/show_bug.cgi?id=666426) seems to have lasted 13 years! At the time we assumed that graphs would be small enough to not warrant the constant factor against Lengauer-Tarjan. ...And of course, browser-based apps have gotten much bigger since then.Semi-NCA hadn't even been published yet and seems like the clear choice nowadays, so I'm glad to see it in place! Great work!\n \nreply",
      "> Semi-NCA hadn't even been published yet and seems like the clear choice nowadays[.]For those who are awkwardly lingering and casting longing glasses at the entrance door of compiler engineering like I am, and who were just as dismayed by this sentence, it wasn\u2019t \u201cproperly\u201d published but looks to have been described in a thesis from 2005[1] and in an extended abstract (ugh) before that[2].But also, the reduction of RMQ to NCA, really?.. Ouch. I\u2019m having flashbacks to my (very brief) competitive programming days, and not the good kind.[1] https://www.cs.princeton.edu/research/techreps/TR-737-05[2] https://www.cse.uoi.gr/~loukas/index.files/dominators_soda04...\n \nreply",
      "It was published prior to that in a paper named \"Finding dominators in practice\", published in ESA 2004[1].For once, the title is not actually an oversell, it actually covers that topic quite well for a conference paper.[1] https://link.springer.com/chapter/10.1007/978-3-540-30140-0_...\n \nreply",
      "Amusingly, the LLVM implementation was also written by an intern at one point :)Semi-NCA was actually published back then, just not in the cited paper.See \"Finding dominators in practice\", published in 2004, section 2.3.\nSo two decades ago.The main advantage of Semi-NCA (and why LLVM uses it) is that it makes for a good incremental update algorithm.\nSee https://reviews.llvm.org/D34258Truthfully, as much as I love Cooper and friends (they were responsible for a lot of very thoughtful, well engineered algorithms at a time when lots of stuff was just \"here's some research i think is better\"), the \"simple\" dataflow based algorithm was never worth it.Part of the thing i was good at way back then was keeping track of all of the research in compiler opts and which was any good - most of their stuff is very good.I used to go through every paper that came around and keep an up to date library of ones worth looking at (both now and in the future) that a bunch of folks used. This was harder back in the days of nobody really publishing code, i used to have to write a ton of prototype implementations to see which numbers were real and which were BS because they compared against crappy implementations or whatever.SEMI-NCA was an obvious win - it was simple enough to implement and test, equally as fast as what existed now, and could easily be extended to incremental updates.If you want to see what it takes to do incremental updates with LT, take a look at GCC's dominator update code back around that time period (I think it's unchanged since then, actually, but i haven't looked in a few years).  There were a fairly small number of people who could understand the data structures and algorithms involved.\n \nreply",
      "> extremely large functions\n> quadratic behavior*High five*I fixed several of these during my time as a compiler engineer too.It's true what they say, quadratic is the worst possible time complexity. Fast enough to work on all your test cases, slow enough to explode in prod.\n \nreply",
      "What is MIR in this context? On the one hand, given the mention of Cranelift, it seems like it could be referring to the Rust compiler's intermediate representation, but given the context perhaps it's referring to an independent intermediate representation that also just happens to be called MIR?\n \nreply",
      "In modern times I seldom reach for a linked list... cache friendly data structures almost always win.\n \nreply",
      "Yup. Almost every DS in my compiler is either an array or a hashmap.\n \nreply",
      "If anyone have a comparison with V8 that would be great!\n \nreply",
      "Not sure what kind of comparison you mean, but you can compare desktop browsers with [1].I just ran it on my mac M2 Max and got:    (F)irefox 131.0.3\n    (E)dge 129.0, V8 12.9.17.8\n    (S)afari 18.0 (20619.1.26.31.6)\n\n    Speedometer 3.0\n    (F) 25.9  \n    (E) 22.3\n    (S) 30.9\n\n    JetStream2\n    (F) 251.787\n    (E) 350.74\n    (S) 360.568\n\nSafari seems slightly faster in all benchmarks. I did not run motionmark because it takes forever :-/. The page says JetStream2 is what you want if you want to benchmark wasm.How this relates to TFA, no idea ... is not really easy to tell which version of SpiderMonkey is running on the installed Firefox.--1: https://browserbench.org/\n \nreply"
    ],
    "link": "https://spidermonkey.dev/blog/2024/10/16/75x-faster-optimizing-the-ion-compiler-backend.html",
    "first_paragraph": "\n        Oct 16, 2024\n      \u2022 \n          \nJan de MooijIn September, machine learning engineers at Mozilla filed a bug report indicating that Firefox was consuming excessive memory and CPU resources while running Microsoft\u2019s ONNX Runtime (a machine learning library) compiled to WebAssembly.This post describes how we addressed this and some of our longer-term plans for improving WebAssembly performance in the future.SpiderMonkey has two compilers for WebAssembly code. First, a Wasm module is compiled with the Wasm Baseline compiler, a compiler that generates decent machine code very quickly. This is good for startup time because we can start executing Wasm code almost immediately after downloading it. Andy Wingo wrote a nice blog post about this Baseline compiler.When Baseline compilation is finished, we compile the Wasm module with our more advanced Ion compiler. This backend produces faster machine code, but compilation time is a lot higher.The issue with the ONNX module was that the I",
    "summary": "**Crashing Through the Web: A Memory-Heavy Tale at Mozilla**\n\nIn an exhilarating twist that absolutely no one asked for, the wizards at Mozilla have finally realized their browser devours RAM like a digital black hole. Cue heroic efforts to optimize their Ion compiler back end after someone figured out the ONNX Runtime was bogged down by yesterday\u2019s garbage collection strategies. Meanwhile, in the comment section, a nostalgic former intern reminds everyone he once touched the code\u2014because apparently, it's bring-your-old-trophies-to-the-Internet day. \ud83c\udfc6\ud83d\ude44 Amid back-pats and obscure algorithm nostalgia, everyone else just wants to know why their browser tabs keep crashing during their hourly meme-scrolling sessions."
  },
  {
    "title": "Show HN: Greenmask 0.2 \u2013 Database anonymization tool (github.com/greenmaskio)",
    "points": 31,
    "submitter": "woyten",
    "submit_time": "2024-10-16T20:37:46.000000Z",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=41863600",
    "comments": [
      "I\u2019ve used https://postgresql-anonymizer.readthedocs.io/en/latest/ before to create trimmed down dev databases based on scrubbed and fuzzed production data.\n \nreply",
      "This is really awesome - and it's so amazing that you've build this as a standalone tool!I can absolutely speak to the pain of having a dozen pg_dump --exclude-table-data arguments and having a developer experience that makes it difficult to reproduce bugs due to drift between production data and test fixtures (even if they share the same schema, assumptions can change massively!).Secure and robust database cloning also enables preview apps that actually answer the stakeholder question \"can I see/play with what the new code would do, if applied to the actual [document/record/product listing] that motivated the feature/bugfix?\" Subsetting and PII masking are both critical for this, and it's amazing to see that you've thought about them as integral parts of the same product.I really want to see a product like this succeed! The easier the tool is to use, the harder it might be to monetize... but there are so many applications of a tool like this, including ones that can materially improve security at organizations large and small (https://nabeelqu.substack.com/i/150188028/secrets just posted here earlier today remarks on this!) that I'm sure you'll find the right niche!\n \nreply",
      "It's great seeing more tools in this space.I was recently researching ways of anonymizing production data for staging, and I also found existing tools either cumbersome to setup or lacking in features.I stumbled upon clickhouse-obfuscator[1], and really liked that it worked on standalone dump formats (CSV, Parquet, etc.) rather than any specific DBMS. I think that's a great approach for this, since it keeps things simple and generic, and it can be conveniently added as a middle step in the backup-restore pipeline. Unfortunately, the tool is quite barebones, and has issues maintaining referential integrity, so we had to abandon it.This is still an unsolved problem in our team, so I'll keep an eye on your tool. We would need support for ClickHouse as well, so it's good you're planning support for other DBMSs. Good luck![1]: https://clickhouse.com/docs/en/operations/utilities/clickhou...\n \nreply",
      "Having jumped from Replibyte to Greenmask already I can say it is a significantly better architecture - hands down.\n \nreply"
    ],
    "link": "https://github.com/GreenmaskIO/greenmask",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        PostgreSQL database anonymization and synthetic data generation tool\n      Greenmask is a powerful open-source utility that is designed for logical database backup dumping,\nanonymization, synthetic data generation and restoration. It has ported PostgreSQL libraries, making it reliable.\nIt is stateless and does not require any changes to your database schema. It is designed to be highly customizable and\nbackward-compatible with existing PostgreSQL utilities, fast and reliable.\n\n\n\n\n\n\n\nGreenmask has a Playground - it is a sandbox environment in Docker with\nsample databases included to help you try Greenmask without any additional actionsClone the greenmask repository and navigate to its directory by running the following commands:Once you have cloned the repository, start the environment by running Docker Compose:Greenmask is ideal for",
    "summary": "Welcome to another episode of HackerNews theater, where the star of the hour is \ud83c\udf1f **Greenmask 0.2** \ud83c\udf1f, a magical incantation for PostgreSQL that promises to anonymize your data without so much as a schema twitch. Ideal for those daring enough to clone a repository based on the all-powerful command of reading <em>documentation</em>. Comments, as expected, range from tech evangelists touting their \"before and after\" tool tales, to battle-scarred devs seeking refuge from the tyranny of data leakage, all while casually citing every other anonymization tool they've ever encountered. Watch in awe as they expertly debate nuances you never knew existed, all in the quest for the holy grail of data security \u2013 or at least something that won\u2019t crumble during the next demo."
  },
  {
    "title": "Efficient high-resolution image synthesis with linear diffusion transformer (nvlabs.github.io)",
    "points": 139,
    "submitter": "Vt71fcAqt7",
    "submit_time": "2024-10-16T14:56:22.000000Z",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41859805",
    "comments": [
      "This looks like quite a huge breakthrough, unless I'm missing something?~25x faster performance than Flux-dev, while offering comparable quality in benchmarks. And visually the examples (surely cherry-picked, but still) look great!Especially since with GenAI the best way to get good results is to just generate a large amount of them and pick the best (imo). Performance like this will make that much easier/faster/cheaper.Code is unfortunately \"(Coming soon)\" for now. Can't wait to play with it!\n \nreply",
      "> surely cherry-picked\n\nAs someone who works in generative vision, this is one of the most frustrating aspects (especially for those with less GPU resources). There's been a silent competition for picking the best images and not showing random results (even when there are random results they may be a selected batch). So it is hard to judge actual quality until you can play around.Also, I'm not sure what laptop that is but they say 0.37s to generate a 1024x1024 image on a 4090. They also mention that it requires 16GB VRAM. But that laptop looks like a MSI Titan, which has a 4090, and correct me if I'm wrong, but I think the 4090 is the only mobile card with 16GB?[0] (I know desktop graphics have 16 for most cards). The laptop demo takes 4s to generate a 1024x1024 image. But they are chopped down quite a bit[1]I wonder if that's with or without TensorRT[0] https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_proces...[1] https://gpu.userbenchmark.com/Compare/Nvidia-RTX-4090-Laptop...\n \nreply",
      "The GeForce RTX 3080 Mobile and GeForce RTX 3080 Ti Mobile also have 16 GB versions as noted directly above the linked section on [0].\n \nreply",
      "Thanks! I forgot about that (usually mobile cards have less VRAM, not more lol). I don't necessarily doubt the paper's generation claim, but there are of course many factors that could help clarify what that number actually represents\n \nreply",
      ">This looks like quite a huge breakthrough, unless I'm missing something?Looking at their methodology,  it seems like it's more of an accumulation of existing good ideas into the one model.If it performs as well as they say, perhaps you can say the breakthrough is discovering just how much can be gained by combining recent advances.It's sitting on just the edge of sounding too good to be true to me.  I will certainly be pleased if it holds up to scrutiny.\n \nreply",
      "If you read closer to the benchmark, it seems to be slightly worse than FLUX [dev] on prompt adherence and quality. However, the best is to evaluate the result oneself, and the track-record of PixArt Sigma (from the same author?) is pretty good!\n \nreply",
      "If you generate 25x more images, you can afford to cherry-pick.\n \nreply",
      "That transfers computer time to user time.    It's great when you want variations,  less so when you want precision and consistency.   Picking the best image tires the brain quite quickly, you have to take into account the at a glance quality without it overriding the detail quality.I'd be curious to see how a vision model would go if it were finetuned to select the best image match to a given criteria.It's possible that you could do O1 style training to build a final stage auto-cherrypicker.\n \nreply",
      "It would be interesting to have benchmarks that take this into account (maybe they already do or I\u2019m misunderstanding how those benchmarks work). I.e. when comparing quality between two different models of vastly different performance, you could be doing best-of-n in the faster model.\n \nreply",
      "That sounds like it could be an intiresting metric. Worth noting that there is a difference between an algorithmic \"best of n\" selection (via eg. an FID score) vs. manual cherry picking which takes more factors into account such as user preference and also takes time to evaluate, which is what GP was suggesting.\n \nreply"
    ],
    "link": "https://nvlabs.github.io/Sana/",
    "first_paragraph": "Exploring the Frontiers of Efficient Generative Foundation Models\nEnze Xie1*,\n                Junsong Chen1*,\n                Junyu Chen2,3,\n                Han Cai1,\n                Haotian Tang2,\nYujun Lin2,\n                Zhekai Zhang2,\n                Muyang Li2,\n                Ligeng Zhu1,\n                Yao Lu1,\n                Song Han1,2\n\n1NVIDIA, 2MIT, 3Tsinghua University\n                \n                *Project co-lead\n            We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 \u00d7 4096 resolution.\n                    Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed,\n                    deployable on laptop GPU. Core designs include:\n                    Deep compression autoencoder:  unlike traditional AEs, which compress images only 8\u00d7,\n                        we trained an AE that can compress images 32\u00d7, effectively reducing the number of latent tokens.\n     ",
    "summary": "In an unprecedented fusion of jargon and digital wizardry, a team from NVIDIA and some other schools you've heard of introduce \"Sana,\" an AI that apparently can conjure hyper-detailed digital wallpaper in seconds. Commenters, half of whom may someday find this blog on their quest to back-engineer a high school science project, are agog with anticipatory delight. Little do they know, the promise of playing with Sana is as intangible as the \"Coming Soon\" code release. Meanwhile, debates rage about whether the images are cherry-picked \u2013 a term still astonishingly taken as a revelation in the age of Instagram perfection."
  },
  {
    "title": "Reflections on Palantir (nabeelqu.substack.com)",
    "points": 182,
    "submitter": "freditup",
    "submit_time": "2024-10-16T02:18:29.000000Z",
    "num_comments": 188,
    "comments_url": "https://news.ycombinator.com/item?id=41855006",
    "comments": [
      "I read the article. It sounds like a Laudatio to amorality for a S&P500 behemoth whose goal is to enable other companies to purge human from their workflow, pardon... to digitalize the business. I'll give it a pass.\n \nreply",
      "Can someone explain to me what is the Palantir's business model ?\nI haven't heard any large, meaningful project they been involved in, but I keep hearing the company name & how hot they are and their stocks are going to blow-up any day (some of my friends kept their stocks for the last 4-5 years with very little gain compared to other software companies).\nI know of the smaller software companies that are less than 100 people and have a very meaningful impact in DoD & Gov space.\n \nreply",
      "When I interned at Palantir (summer 2014) their business was mostly in data ingestion, visualization, and correlation.A typical workflow for a Palantir customer was that Palantir would come in and dump a ton of data out of old crufty databases and into Palantir's datastore. Then, they'd establish connections between that data. This is all sounds kind of hand-wavy, but the gist of it is that a lot of government agencies have data that lives in separate databases and they can't easily correlate data between those two databases. Once the data was in Palantir's system, they could do queries against all their data, and make connections and correlations that they wouldn't otherwise be able to find when the data was previously siloed.One of the sample use cases was identifying people filling prescriptions for schedule II drugs multiple times on the same day, and correlating that with pharmacies run by people connected to known drug traffickers. Previously, this was hard to do because the database of prescription purchases was disconnected from the database of drug convictions.\n \nreply",
      "People dismiss this type of work as no big deal, but in my experience this is the actual hard work of producing something useful for companies, and what 90% of SaaS resellers will never be able to deliver on.\n \nreply",
      "Yes, it is very hard. But does Palantir succeed? Or do they like some other companies just trick customers with big wallets to buy?\n \nreply",
      "To me it seems they do \nhttps://logicmag.io/commons/enter-the-dragnet/\n \nreply",
      "In many of the enterprise orgs I've worked in, the two tech teams that are chronically understaffed are 1) info sec, 2) DBA/ data architecture/ data science. I'm lumping those 3 together on purpose, because they're always understaffed and typically not empowered to build anything.\n \nreply",
      "You're right to group Data teams together. They seem to share a common plight.In my experience, internal employees outside Data have a funny relationship with Data. They hate to manage it but they love to blame it, especially in analytical / decision-making scenarios. Teams that \"own\" the data usually get the blame, on top of having to deal with a mass of rotting pipes and noncompliant teams, while also losing out on credit when non-Data teams report big wins.Based on what the GP says, it sounds like Palantir knows how to exploit common internal politics around Data. They build up technical & social expertise in ETL'ing disparate data sources, and they can avoid blame by being hired by executives as an external third party.\n \nreply",
      "So it\u2019s hygiene and structure\n \nreply",
      "That, and a really powerful visualization suite. In the example I gave above, you could plot the prescription purchases on a map and see that people were driving along the highway and hitting up pharmacies along the interstate. Better yet, you could drop into Google Street view in front of one of the pharmacies, and look at it from the street level and see that it doesn't even have signage indicating it's a pharmacy.\n \nreply"
    ],
    "link": "https://nabeelqu.substack.com/p/reflections-on-palantir",
    "first_paragraph": "",
    "summary": "<h1>Exploring the Dark Arts of Data with Palantir</h1>\n\nIn the latest theatrical performance disguised as professional reflection, Nabeel at Substack decides to wax philosophical about Palantir, oozing praises for what essentially amounts to glorifying a sophisticated data-mining overlord. As expected, the commentators embark on a confused journey, oscillating between awe and skepticism, as they try to decode the spell Palantir casts over big data. From ex-interns reminiscing about their mystifying data sorcery summers to puzzled bystanders questioning the company\u2019s actual offerings, everyone seems tangled in a narrative that\u2019s part <em>Ghostbusters</em>, part <i>Enron</i>. Amidst the crossfire of techno-babble and cynical nods to the NSA's lesser-known cousin, it is clear: knowing too little about Palantir is just as good as knowing too much."
  }
]