[
  {
    "title": "It's always TCP_NODELAY (brooker.co.za)",
    "points": 423,
    "submitter": "todsacerdoti",
    "submit_time": "2024-05-09T17:54:49",
    "num_comments": 147,
    "comments_url": "https://news.ycombinator.com/item?id=40310896",
    "comments": [
      "The real issue in modern data centers is TCP. Of course at present, we need to know about these little annoyances at the application layer, but what we really need is innovation in the data center at level 4. And yes I know that many people are looking into this and have been for years, but the economic motivation clearly has not yet been strong enough. But that may change if the public's appetite for LLM-based tooling causes data centers to increase 10x (which seems likely).",
      "I've fixed multiple latency issues due to nagle's multiple times in my career. It's the first thing I jump to. I feel like the logic behind it is sound, but it just doesn't work for some workloads. It should be something that an engineer needs to be forced to set while creating a socket, instead of letting the OS choose a default. I think that's the main issue. Not that it's a good / bad option but that there is a setting that people might not know about that manipulates how data is sent over the wire so aggressively.",
      "Same here. I have a hobby that on any RPC framework I encounter, I file a Github issue \"did you think of TCP_NODELAY or can this framework do only 20 calls per second?\".So far, it's found a bug every single time.Some examples: https://cloud-haskell.atlassian.net/browse/DP-108 or https://github.com/agentm/curryer/issues/3I disagree on the \"not a good / bad option\" though.It's a kernel-side heuristic for \"magically fixing\" badly behaved applications.As the article states, no sensible application does 1-byte network write() syscalls. Software that does that should be fixed.It makes sense only in the case when you are the kernel sysadmin and somehow cannot fix the software that runs on the machine, maybe for team-political reasons. I claim that's pretty rare.For all other cases, it makes sane software extra complicated: You need to explicitly opt-out of odd magic that makes poorly-written software have slightly more throughput, and that makes correctly-written software have huge, surprising latency.John Nagle says here and in linked threads that Delayed Acks are even worse. I agree. But the Send/Send/Receive receive pattern that Nagle's Algorithm degrades is a totally valid and common use case, including anything that does pipelined RPC over TCP.Both Delayed Acks and Nagle's Algorithm should be opt-in, in my opinion. It should be called TCP_DELAY, which you can opt-into if you can't be asked to implement basic userspace buffering.People shouldn't /need/ to know about these. Make the default case be the unsurprising one.",
      "The problem with making it opt in is that the point of the protocol was to fix apps that, while they perform fine for the developer on his LAN, would be hell on internet routers.  So the people who benefit are the ones who don't know what they are doing and only use the defaults.",
      "Thanks for the reminder to set this on the new framework I\u2019m working on. :)",
      "Oh hey! It\u2019s been a while how\u2019re you?!",
      "> It should be something that an engineer needs to be forced to set while creating a socket, instead of letting the OS choose a default.If the intention is mostly to fix applications with bad `write`-behavior, this would make setting TCP_DELAY a pretty exotic option - you would need a software engineer to be both smart enough to know to set this option, but not smart enough to distribute their write-calls well and/or not go for writing their own (probably better fitted) application-specific version of Nagles.",
      "I agree, it has been fairly well known to disable Nagle's Algorithm in HFT/low latency trading circles for quite some time now (like > 15 years).  It's one of the first things I look for.",
      "I was setting TCP_NODELAY at Bear Stearns for custom networking code circa 1994 or so.",
      "Same in M&E / vfx"
    ],
    "link": "https://brooker.co.za/blog/2024/05/09/nagle.html",
    "first_paragraph": "It's not the 1980s anymore, thankfully.",
    "summary": "Title: Engineers Madly in Love with TCP_NODELAY: A Never-Ending Tech Saga\n\nThe year is 2023, but it\u2019s always 1986 in the hearts of those bravely battling the scourge known as TCP. Behold, the profound revelations from software warriors saving humanity by disabling Nagle's Algorithm, one socket at a time. Commenters, brandishing their GitHub links like mythical Excaliburs, enlighten the world on vital issues like why your app probably just <em>sucks</em>, and why every data hiccup should be fixed by forcibly wrestling with TCP_NODELAY settings. Meanwhile, stick around for side-splitting geek banter that reverberates through the hollows of decades-old email threads, where the question isn\u2019t just <i>how</i> TCP_NODELAY can fix your life, but who gets to claim they've been doing it since the internet was powered by steam."
  },
  {
    "title": "Sioyek is a PDF viewer with a focus on textbooks and research papers (github.com/ahrm)",
    "points": 100,
    "submitter": "simonpure",
    "submit_time": "2024-05-09T21:28:43",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=40313143",
    "comments": [
      "I took a bit more collaborative approach to reading and discussing research papers (https://www.scholars.io or https://app.scholars.io) and allow people to annotate, comment and collaborate on research.",
      "Does the table of contents work if the pdf doesn't come with a table of contents? (That's something I always wished my pdf reader could do)",
      "Sortof - sioyek will parse the text and attempt to create a table of contents (this does not modify the base pdf), but the accuracy/usefulness varies. For papers I have found it works reasonably well, just sometimes misses a few sections or includes a few junk entries. For textbooks, especially ones with heavy math typesetting, there are too many junk/missing entries a lot of the time in my experience.For papers or for textbooks I only need a small subset of, I find the autogenerated table of contents + bookmarks is typically sufficient. For other cases I like to use pdf.tocgen (https://github.com/Krasjet/pdf.tocgen) to semi-manually generate a correct table of contents.",
      "Thanks, I'll look into both!",
      "Yes\n> If set and the file doesn\u2019t have a table of contents, we use heuristic methods to create a table of contents. You can use max_created_toc_size to prevent creating very large table of contents.https://sioyek-documentation.readthedocs.io/en/latest/config...",
      "Tangentially related. does anyone know a good pdf reader with LLM based search integrated? Suppose I want search for \"cities in USA\",  in the document, it should show all occurences new york, los angeles and chicago, for example.",
      "[PDFgear](https://www.pdfgear.com/) has LLM integrated, it is currently free.",
      "Appreciate the response!",
      "Is there something like this for iPad? Reading research articles is a painful process having to flip back and forth between text, figures and references all the time",
      "https://www.liquidtext.net/ is unique in its feature set and use of gestures and stylus.It\u2019s definitely worth trying."
    ],
    "link": "https://github.com/ahrm/sioyek",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In the digital cesspool of GitHub, a beacon of unnecessary functionality emerges: Sioyek, a PDF viewer that pretends textbooks and research papers aren't already excruciating to digest. God forbid a PDF comes without a table of contents\u2014Sioyek's life mission is to <em>guess</em> it for you, with all the precision of a drunk darts player. Meanwhile, in the comments, tech enthusiasts engage in a tragic dance of suggesting even more obscure tools, because clearly, reading a document in 2023 isn't challenging enough without throwing in a dozen additional steps. Oh, and don't miss the guy earnestly asking for a PDF reader with a built-in large language model because manually searching 'cities in USA' is so 2022. \ud83d\ude44"
  },
  {
    "title": "Viking-Age Hunters Took Down the Biggest Animal on Earth (hakaimagazine.com)",
    "points": 17,
    "submitter": "benbreen",
    "submit_time": "2024-05-07T19:12:15",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://hakaimagazine.com/features/how-viking-age-hunters-took-down-the-biggest-animal-on-earth/",
    "first_paragraph": "In the fall of 1385, according to a 17th-century Icelandic text, a man named \u00d3lafur went fishing off the northwestern coast of Iceland. In the cold seas cradled by the region\u2019s labyrinthine fjords, \u00d3lafur reportedly came across an animal that would have dwarfed his open wooden boat\u2014a blue whale, the largest animal on record, known in the Icelandic language as steypirey\u00f0ur.",
    "summary": "<h1>Viking-Age Hunters in Epic Battle with Gigantic Fish: A True Story?</h1>\n<p>In an unparalleled display of historical fishing tales, a \"detailed\" report from the 17th-century (because records from the 14th-century just weren't dramatic enough) tells of \u00d3lafur, the Viking quasi-myth, who apparently grappled with a blue whale\u2014the ocean's equivalent of a skyscraper. Armed with nothing but sheer guts and probably an anachronistically large harpoon, \u00d3lafur embarks on the ultimate 'fisherman's tale' off the coast of Iceland. Commenters, swelling with newfound marine expertise, debate the practicalities of ancient whaling techniques while casually ignoring the narrative flair typical of century-old second-hand sagas. This saga floats somewhere between earnest history and the fantasy section, much like the fabled whale itself.</p>"
  },
  {
    "title": "Wprs \u2013 rootless remote desktop for Wayland (and X11, via XWayland) applications (github.com/wayland-transpositor)",
    "points": 17,
    "submitter": "happy-dude",
    "submit_time": "2024-05-09T23:02:14",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://github.com/wayland-transpositor/wprs",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In this week's edition of \"How the Internet Refuses to Let Old Technology Die,\" a brave soul has decided that what the universe needs right now is yet *another* way to remote desktop\u2014this time for Wayland (and begrudgingly, X11). Wprs, lovingly crafted from the spare parts of our collective despair, promises to revolutionize the five by five pixel patch of screen real estate you couldn't see with older tools. \"We read every piece of feedback,\" claims the GitHub page, desperately clinging to the hope that anyone else cares. Meanwhile, the comment section devolves into an all-out war between those who think this is the second coming of sliced bread, and aging sysadmins who have never forgiven the world for moving past telnet. \ud83c\udf5e\ud83d\udd25"
  },
  {
    "title": "Launch HN: Muddy (YC S19) \u2013 Multiplayer browser for getting work done",
    "points": 159,
    "submitter": "lele0108",
    "submit_time": "2024-05-09T15:38:48",
    "num_comments": 82,
    "comments_url": "https://news.ycombinator.com/item?id=40309342",
    "comments": [
      "The feature for sending messaging and posting comments to a tab is some pretty clever and creative UX. Seriously next level future stuff and congrats for just coming up with the concept.I like that it's all timeline based. For my use case, we currently use Front email thread and then link to a Shared Dropbox where we post everything (including links to like a Google Doc or webpage). I think having chronological bookmarks like you do would be clearly better. I also know many people who use Google Groups and Google Doc to document progress too -- which I think would be insane / nightmare but teams do it. You all definitely would solve that automatically.Couple other notes:- Whenever I screenshare with a team or others I see 1000 bookmarks or tabs on their browser. I could not imagine the nightmare of how that would impact my workflow or the timeline. Trusting AI to clean stuff up or hunt is not for me.- I can tell you all have been heads down blitzing (dog in video, phone ringing in background of another) but I think a separate \"Solutions\" page where you tackle specific examples would be nice to see or browse.- Maybe too much or not really your goal, but right now need some sort of client integration for an outside person. I can't imagine giving access to a client on a whim and training them on this. Instead, maybe automatic email integration where their emails show up in the timeline and can respond directly from there. Would produce a really great timeline for where things left off and when things are being communicated. Being able to sub-comment and share files/updates/things on Front on email threads is one of the most killer features for productivity and a team. Mixing this with what you all have could be even more next level. Again though, might not be the goal.Congrats and best of luck! Big fan of people trying to tackle PM stuff and think you all are doing a great job.",
      "Great idea for creating some specific videos for engineers, designers, etc.Browsers are interesting since they can do almost anything but \"you can do anything you want!\" is intimidating for many new users.We've given e-mail thought and it's certainly a door we are considering as we keep on building. Has anyone built a browser without thinking about an email client? :P",
      "Integrating an AI tab organizer of which they\u2019re very few and the chrome one is essentially garbage, would make this a definite purchase for me. The AI should organize my own tabs and the ones I share",
      "yup, that's Muddy. One of our users described it as a self healing slack channel with tab groups.",
      "Please stop doing this:https://i.imgur.com/2WeVGxK.pngIt's so frustrating. Early in your product lifecycle it should be painfully easy to get started and you shouldn't be worrying about this kind of security.",
      "I don't see any issue with this? It makes sense to keep the accounts secure, and very few people will be coming up with passwords by hand; they'll be using the auto-generated ones from their browser (or from their 3rd-party password manager for the more HN-y types).",
      "Very few?! I really doubt that. Most people use their pet password everywhere.",
      "Please keep doing this.",
      "I almost gave up after trying this like three times but as an ex browser developer really wanted to see what ya'll were up to. I think it's an interesting concept. But I bet you will lose tons of trials here.In fact it should just be google auth (not sure if I missed the option to do that).",
      "Google auth is good to have, but local auth should always be an option. It's not really good practice to exclude those who don't wish to cede control of their online life to Google."
    ],
    "link": "item?id=40309342",
    "first_paragraph": "",
    "summary": "**Launch HN: Muddy (YC S19) \u2013 Multiplayer browser for rocket scientists and unpaid interns alike**\n\nIn a world demanding innovation, Muddy introduces the revolutionary concept of \"using a browser together\" because apparently, emailing links is too pass\u00e9 for the modern telecommuter. Pioneering a timeline-based interface that's not confusing at all, Muddy is coolly reinventing the labyrinthine chaos of your current 1000+ browser tabs into a neat, chronological horror show. Meanwhile, Muddy enthusiasts, a lively bunch of futurists, predict this to single-handedly annihilate traditional project management tools, because why bother with just a shared Google Doc when you can <i>literally</i> see your colleague\u2019s digital hoarding habits live? As everyone wonders whether this is the future of work or just another way to watch cat videos in company, one can\u2019t help but marvel at the blurry lines between ingenuity and inanity. \ud83d\ude80\ud83d\udc31\u200d\ud83d\udcbb"
  },
  {
    "title": "The history of 'OK' (2023) (howstuffworks.com)",
    "points": 69,
    "submitter": "goles",
    "submit_time": "2024-05-09T20:07:30",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=40312434",
    "comments": [
      "Is it me or does the final paragraph totally contradict itself?> It's not that it was needed to 'fill a gap' in any language. Before 1839, English speakers had 'yes,' 'good,' 'fine,' 'excellent,' 'satisfactory' and 'all right.' What OK provided that the others did not was neutrality, a way to affirm or to express agreement without having to offer an opinion.That, as is then explained - response without judgement, is a really big and useful gap to fill.",
      "Then I don't see \"yes\" as being opinionated or biased.I'm assuming \"OK\" was needed because \"yes\" is otherwise used in so many contexts, including just as a nod to keep a conversation going, in no small part because of its neutrality. So a new word allows for more intentionality (which is important, and we still agree fills a really big gap)",
      "Also, I\u2019m not too sure that saying \u2018ok\u2019 is to \u2018affirm or to express agreement\u2019. Sometimes it can be used to simply acknowledge that someone has said something, such as when you say \u2018ok sure\u2019 or \u2018ok but\u2019.And if I am wrong, and it does mean to affirm, then doesn\u2019t that by necessity mean that it involves \u2018offering an opinion\u2019? Affirmation and/or agreement is not neutral.",
      "It's used in different contexts, including the original meaning of \"all correct\", which is apparently offering an opinion.",
      "Affirmative.",
      "Just a couple too many syllables.",
      "Ack",
      "Sure",
      "Yup.",
      "A deep knowledge of etymology, if nothing else, is useful for answering a 5 year old's infamous chain of \"why?\"sDisregarding potential inaccuracies, I love stories like these. Not particularly sure why, but I'm sure someone has a post somewhere about why we're so obsessed with the history of _our_ mundane."
    ],
    "link": "https://people.howstuffworks.com/history-ok.htm",
    "first_paragraph": "Advertisement",
    "summary": "In a daring escapade of pseudo-linguistic analysis, HowStuffWorks tackles the perilous history of the word \"OK,\" reminding everyone that, yes, language did indeed exist before 1839. Commentators, in their infinite wisdom, descend into a semantic slugfest, debating fiercely whether \"OK\" fills a mythical gap left by the apparently overbearing and emotionally charged \"yes\" and \"all correct.\" Meanwhile, the article gamely tries to convince readers that \"OK\" offers neutrality, a concept as revolutionary as discovering that water is wet. As the comments spiral into an existential debate about affirmation versus acknowledgment, the rest of the internet heaves a collective sigh and mutters, <em>\"OK, sure.\"</em>"
  },
  {
    "title": "ESP32 Drum Synth Machine (github.com/zircothc)",
    "points": 159,
    "submitter": "peteforde",
    "submit_time": "2024-05-09T16:17:14",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=40309759",
    "comments": [
      "Not sure if the author is around here or not but I might have something interesting for them in the next few weeks: an ESP32 implementation of AppleMIDI/rtp-midi for sending and receiving MIDI over Ethernet. I\u2019ve put a reminder in my calendar to reach out once I\u2019m ready to release it.",
      "You're aware that there are already a couple of these, right?  First one that pops up in a platformio search:    % pio pkg search rtpmidi                                                                                                                                       \n    Found 1 packages (page 1 of 1)\n\n    lathoub/AppleMIDI\n    Library \u2022 3.3.0 \u2022 Published on Sat Aug 27 11:57:15 2022\n    AppleMIDI (aka rtpMIDI) MIDI I/Os for Arduino. AppleMIDI (aka rtpMIDI) is a protocol to transport MIDI messages within RTP (Real-time Protocol) packets over Ethernet and WiFi networks. This major rewrite is faster, more stable and uses less memory. Read the Wiki page when migrating\n\n.. is pretty stable and fast.  I've used it in an ESP8266 project ..",
      "Of course, the existence of some implementation is no reason not to write or share another one.In fact, it's good for the ecosystem to have different people/teams exploring the problem space and it's good for individual developers to hone their craft on delicate technical problems even when they're ostensibly already covered.",
      "Yes, I\u2019m specifically targeting Zephyr. It\u2019s actually less of an ESP32 thing and more of a Zephyr thing, I just happened to have an ESP32 LyraT Mini kicking around to do the project with.And in general the Arduino/Platformio ecosystem causes me to break out into hives\u2026",
      "For a similar device (with a different UI) check out the Woovebox. I love mine as a portable groovebox and I'm experimenting with having it be the brain of my dawless setup.Would buy or build one of these in a heartbeat.",
      "The high-end version of this is Synthstrom Deluge. I love that piece of gear.Even the wood on the sides aesthetic is the same.",
      "Not sure if you meant the wood side panels are an intentional reference to the Deluge, but if so you may want to know that wood side panels are a common design touch for desktop synths, hearkening back to some of the first compact consumer synths (like the Roland SH-1000), which also had wood side panels.",
      "yeah i even had walnut ones made for my 100m because i didn't like the fake veneer",
      "Does it support touch-sensitivity and polyphony?",
      "Pocket operator-esque setup, very cool"
    ],
    "link": "https://github.com/zircothc/DRUM_2004_V1",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In a thrilling display of misallocated energy, a new Drum Synth Machine emerges on the precarious platform of ESP32. The original poster, a paragon of delusion, insists on <em>valuing</em> the feedback loop that might as well be a M\u00f6bius strip in its utility. Meanwhile, hobbyists in the comments section gear up for a tech-masquerade, throwing around terms like <em>AppleMIDI</em> and <em>rtp-midi</em>\u2014because reinventing the wheel is, apparently, a sign of innovation in boutique tech arenas. Amidst the chaos, the mention of wooden side panels evolves into a cultural dissertation, proving once again that tech enthusiasts are unparalleled when it comes to missing the point while also flexing their woodwork trivia. \ud83c\udfb9\ud83c\udf10\ud83d\udd28"
  },
  {
    "title": "The World Has (Probably) Passed Peak Pollution (sustainabilitybynumbers.com)",
    "points": 66,
    "submitter": "robertn702",
    "submit_time": "2024-05-09T22:11:41",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=40313451",
    "comments": [
      "That's encouraging.- Peak pollution - check.- Peak coal - not yet.- Peak oil - maybe 2019, but aftermath of COVID affects numbers.[1]- Peak baby - 2013. [2][1] https://www.resilience.org/stories/2023-04-19/the-status-of-...[2] https://www.thenewdaily.com.au/life/2022/11/12/the-stats-guy...",
      "Peak Coal - Probably, very likely, more or less about right now.Global coal supply likely peaked in 2023 and then to decline https://news.ycombinator.com/item?id=38652273Can't say for sure until another few years have passed, nonetheless every country in the world has coal on hold or in decline save for India and China.China is the largest producer of renewable technology and energy (?), India is building out the worlds largest solat array farm (albeit \"only\" good for 16 million households).",
      "oh, so just the two biggest centers of population in the world!",
      "In the same way that your local power plant isn't solely responsible for the pollution created in heating surrounding homes, it's probably not reasonable to look at the places that have done the fabricating, weaving, injection molding, spinning, etc for the rest of the world as being the major center of pollution.China specifically is decarbonizing fast, and at some expense to their own economy over the last decade or so.",
      "What of it?China + India = 2.864bnNot China or India = 5.244bnI think the characterization that a turning point has occurred is perfectly supportable.The article never even tried to say anything other than that it only just barely happened 11 minutes ago and a few metrics metrics don't show a clear past peak yet.",
      "Do you believe what you wrote here is a constructive, insightful comment?  The comment you are replying to addressed that even India and China are building renewables.It requires very little energy for you to spread negativity but potentially thousands of people then have to internalize it.  Why choose to do that?Like burning coal, your negativity pollutes the environment of HN and the internet at large.",
      "> - Peak baby - 2013. [2]I think your source's claim that \"The planet will never see more babies than it has in 2013\" is open to question.To just use the US as an example: mainstream Americans have a declining fertility rate, and we have no reason to think that is going to reverse in the foreseeable future. Immigrants have higher fertility, but they tend to converge to the mainstream after a generation or two. So, that seems to support the article's contention, at least for the US \u2013 and if we look at other countries, we observe things are broadly similar, so that in turn supports that contention for the planet as a whole.However, we also observe in the US small, ultra-conservative religious minorities, such as the Amish and ultra-Orthodox Jews, who still have a high fertility rate, and also have high youth retention (80-90% of their children will stay in the group as adults). Now, even though these groups are a tiny percentage of the population, the miracle of exponential growth means that within 2-3 centuries, they could be the majority of the US population, which could then result in a national baby boom. And, eventually, as they spread across the globe (they exist to varying degrees in other countries too), a global baby boom. So 2013's \"peak baby\" may end up being surpassed.Of course, there is no guarantee that is going to happen \u2013 maybe they'll get to a certain size, and then they'll stop growing. And of course, it is physically impossible to sustain exponential growth indefinitely. However, it remains a possible future that they won't stop growing until after they've gotten so big that 2013 turns out not to be the year of \"peak baby\" after all. Maybe it will actually be 2213 or 2313 instead. Our descendants (if one happens to have them) will find out.",
      "> Maybe it will actually be 2213 or 2313 instead. Our descendants (if one happens to have them) will find out.Peak-anything is just shorthand for \"the first peak on record\" because our culture and ability to record-keep has only really existed in a population and resource boom period.I agree that circumstances might mean that the global peak baby might be 200-300 years away, but the thing that makes the current round of peak-anything relevant is that we don't know how our social and economic systems will function when certain things are in long-term decline (even if they do pick up again in a few hundred years).If we knew what a few centuries of low birth rates or low oil consumption looked like, we wouldn't be nearly as interested in what \"peak-those-things\" means, the same way we're not all that interested in specific market peaks because we understand the boom/bust cycles and long-term productivity increases, etc.",
      "> the miracle of exponential growth means that within 2-3 centuries,I don't know if any realm of human sciences where extrapolating exponential growth yields trustworthy results, outside maybe of wealth accumulation, and even then not for 2 centuries.",
      "I never claimed that it is inevitable that they'll sustain their current growth rate for the next 2-3 centuries, only that it is entirely possible. And I don't see why it wouldn't be. If you look at ultra-Orthodox Jews in New York/New Jersey, I don't see how they'd hit any natural barriers to their continued growth until they are many times larger than they are now, at which point they'd be a serious challenger for becoming the population majority.The main ways it might not happen would be if either (1) they (gradually or suddenly) abandon their current culture for a less fertile one, (2) mainstream society persecutes them sufficiently. Both are entirely possible, but neither is anywhere near certain.The situation for the (Old Order) Amish is more difficult, since \u2013 unlike ultra-Orthodox Jews \u2013 they'll run out of enough land to sustain their agrarian lifestyle, and will have to transition to a more urban one. While the more urban lifestyle of ultra-Orthodox Jews demonstrates it is possible for insular high fertility religious minorities to exist in an urban setting, there is a real risk that they might lose their fertility and/or their insularity in the process.Also, people often bring up the problem Israel has with many ultra-Orthodox Jews not working and relying on government subsidies to live. That is much less of a problem in the US than it is in Israel, so the sustainability of that lifestyle is less of a barrier to future growth in the US than it is in Israel. Furthermore, the fact they manage to grow so much in the US without doing that, means being forced to stop doing that isn't necessarily going to stop their growth in Israel either."
    ],
    "link": "https://www.sustainabilitybynumbers.com/p/peak-pollution",
    "first_paragraph": "",
    "summary": "**Title: The World Has (Probably) Passed Peak Pollution (sustainabilitybynumbers.com)**\n\nIn a stunning display of environmental optimism, an article breathlessly announces that \ud83c\udf0d the world might just have crested the murky hill of peak pollution, giving armchair activists everywhere a reason to briefly glance up from their smartphones. The comment section, a notorious battleground for the misinformed and overly confident, quickly fills with declarations of victory over coal and sidelong glances at oil stats, peppered with the classic \"but what about China and India?\" rhetoric. As self-assured commenters juggle dubious data points and environmental jargon, the inevitable descent into questioning each other's understanding of basic science is both a spectacle and a snooze-fest. With apocalyptic glee, one user suggests we might avoid peak baby by 2213, if only our ultra-Orthodox and Amish friends manage to inherit the earth. \ud83d\ude80\ud83d\udc76\ud83d\udcc9"
  },
  {
    "title": "Algebraic Data Types for C99 (github.com/hirrolot)",
    "points": 307,
    "submitter": "bondant",
    "submit_time": "2024-05-09T11:31:27",
    "num_comments": 166,
    "comments_url": "https://news.ycombinator.com/item?id=40307098",
    "comments": [
      "Wikipedia has something interesting on this (how unions can be implemented using \"class hierarchy in object-oriented programming\"): \nhttps://en.wikipedia.org/wiki/Tagged_union#Class_hierarchies...There is a lengthy blog post about the same stuff, except that the author doesn't seem to have come across the said wiki section yet: \nhttps://nandakumar.org/blog/2023/12/paradigms-in-disguise.ht...Kudos to the dev of datatype99 for showing the problem with such ad-hoc methods in the readme right away.",
      "If I ever implement a product from scratch again, discriminated unions with compiler enforced exhaustive pattern matching is a hard requirement. It\u2019s too powerful to not have.",
      "Interesting.Algebraic Data Types are almost always one of the things I miss when I use imperative languages.  I have to do Java at work, and while I've kind of come around on Java and I don't think it's quite as bad as I have accused it of being, there's been several dozen instances of \"man I wish Java had F#'s discriminated unions\".Obviously I'm aware that you can spoof it with a variety of techniques, and often enums are enough for what you need, but most of those techniques lack the flexibility and terseness of proper ADTs; if nothing else those techniques don't have the sexy pattern matching that you get with a functional language.This C extension looks pretty sweet since it appears to have the pattern matching I want; I'll see if I can use it for my Arduino projects.",
      "Everyone who hasn't used ADTs and pattern matching doesn't get what the big deal is all about. Everyone who is used to ADTs and pattern matching doesn't get what the big deal is all about, until they have to work in a language that doesn't have them. And everyone who just found out about them can't shut up about them being the best thing since sliced bread.:)",
      "I\u2019m in the latter camp (from Ocaml) and now using Go. Go feels clunky and awkward.",
      "That's because Go is intentionally clunky and awkward in the name of \"simplicity\". IMO it's charming to some degree, but it's far from perfect and I think you'd need some pretty serious threats to get me to describe it as \"elegant\" in any way.Rust somehow has more elegance than Go, if only in small parts. Nothing compares to Scheme in the elegance category IMO :)",
      "I have mainly used them in Rust. They are nice I suppose, but nothing mindblowing.To me it feels very similar to an interface (trait) implemented by a bunch of classes (structs). I have multiple times wondered which of those two approaches would be better in a given situation, often wanting some aspects of both.Being able to exhaustively pattern match is nice. But being able to define my classes in different places is also nice. And being able to define methods on the classes is nice. And defining a function that will only accept particular variant is nice.From my perspective a discriminant vs a vtable pointer is a boring implementation detail the compiler should just figure out for me based on what would be more optimal in a given situation.",
      "> I have multiple times wondered which of those two approaches would be better in a given situation, often wanting some aspects of both.ADTs are closed to extension with new cases but open to extension with new functions, eg. anytime you want to add new cases, you have to update all functions that depend on the ADT, but you can add as many functions for that ADT as you like with no issues.Traits are open to extension with new cases but closed to extension with new functions, eg. you can add as many impl as you like with no issues (new cases), but if you want to add a new function to the trait you have to update all impl to support it.They are logical duals, and the problem of designing systems that are open to extension in both cases and functions is known as the expression problem:https://en.wikipedia.org/wiki/Expression_problem",
      "I suppose this is a genuine dichotomy, but I feel like it\u2019s missing a more critical difference: ADTs cleanly represent data, even when nothing can be, or needs to be, extended from outside.For example, a result is a success value or an error.  A stock order is a market order or a limit order, and nothing else, at least until someone updates the spec and recompiles the code. Situations like this happen all the time.  I don\u2019t want to extend a result to include gizmos in addition to success value or errors, nor do I generally want to extend the set of functions that operate on a certain sort of result.  But I very, very frequently want to represent values with a specific, simple schema, and ADTs fit the bill. A bunch of structs/classes, interfaces/traits and getters/setters can do this, but the result would look like the worst stereotypes of enterprise Java code to accomplish what a language with nice ADTs can do with basically no boilerplate.",
      "> For example, a result is a success value or an error. A stock order is a market order or a limit order, and nothing else, at least until someone updates the spec and recompiles the code.But that's just it, specs are rarely complete because reality is fluid. For example, a result is a success or an error, until maybe you want an errors to prompt the user to correct something and then the computation can be resumed (see resumable exceptions).Should you even have to recompile your code to handle new cases? Why can't you just add the new case, and define new handlers for the functions that depend on your ADT without recompiling that code? That's the expression problem."
    ],
    "link": "https://github.com/Hirrolot/datatype99",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In a brave new world where the antiquated notions of <em>C99</em> clash with the hipster allure of <em>Algebraic Data Types (ADTs)</em>, a repository on GitHub emerges as a beacon of hope for four lost souls who still remember what a pointer is. The single developer, armed with the audacity to bring ADTs to a language as low-level as C, quickly becomes the messiah in the comment section\u2014a blend of confusion, misplaced enthusiasm, and nostalgia for F#. Overflowing with connoisseur-like debates around the sexiness of pattern matching and the existential dread of missing out on discriminated unions in Java, the comments offer a perfect storm of tech clich\u00e9s and the classic programmer\u2019s hubris. After all, who needs simple code when one can juggle obscure typologies and mirror the elegance of Scheme while programming their coffee machine firmware? \ud83e\udd13"
  },
  {
    "title": "Being Green: A new book marvels at the strangeness of plants (slate.com)",
    "points": 22,
    "submitter": "Petiver",
    "submit_time": "2024-05-09T20:14:39",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=40312500",
    "comments": [
      "Subtitled:> A new book marvels at the strangeness of plants\u2014and tries a little too hard to explain how they\u2019re like people.If you're ever at risk of thinking that plants are like people, you can cure it by reading about alternation of generation in bryophytes.  If bryophytes were like humans it would be like if our (haploid) sperm/eggs went out and got a job and an apartment and a social life and only bothered spin up a (diploid) human for sexy times.  Plants are bizarre.",
      "To be fair, the article does mention:>> Rejecting the anthropomorphism that permeates the preceding 10 chapters, she cautions that \u201cputting too human a sheen on plant intelligence is a failure of imagination.\u201dA courtesy caveat emptor for the objective passersby suffering from antilibrary fatigue.",
      "Looks like an interesting book. The author will be giving a talk at Powell's in downtown Portland next week - I just might go and check it out."
    ],
    "link": "https://slate.com/culture/2024/05/light-eaters-zoe-schlanger-book-plant-intelligence.html",
    "first_paragraph": "Back when I only visited the countryside occasionally, I suffered from what botanists call \u201cplant blindness,\u201d described by Zo\u00eb Schlanger in her entertaining new book, The Light Eaters, as \u201cthe tendency to view plant life as an indistinguishable mass, a green smudge, rather than as thousands of genetically separate and fragile individuals, as distinct from one another as a lion is from a trout.\u201d Now that I live in small-town New England, I\u2019m a bit more literate in the flora I see. A stand of Japanese knotweed, the bamboo-like invasive I\u2019m constantly beating back to the margins of my own yard, indicates there must be a brook or other waterway out there, since that\u2019s how the plant spreads. One wild apple tree by the roadside may have sprouted from a core tossed out of a car, but two or more suggests an old farmstead now consumed by the surrounding brush. I now know I will pine in vain forever for my own patch of partridge berry, a ground cover that flourishes along my favorite trail becau",
    "summary": "**Hug A Tree, Save Your Sanity** \ud83c\udf33\ud83d\udc9a\n\nIn the grand tradition of urbanites discovering the \"great outdoors,\" a Slate writer overcomes *plant blindness*\u2014a tragic condition inhibiting the ability to see plants as anything more than leafy wallpaper. The enlightening page-turner, *The Light Eaters*, ventures to teach these poor souls that trees are more than just potential coffee tables. Meanwhile, the comment section bursts into intellectual bloom with assertions that plants could basically run a Starbucks, if only they weren't so busy photosynthesizing. Alert the presses: someone in New England spotted Japanese knotweed AND learned what it indicates. \ud83e\udd2f Nature is saved, folks!"
  },
  {
    "title": "Show HN: Exploring HN by mapping and analyzing 40M posts and comments for fun (blog.wilsonl.in)",
    "points": 308,
    "submitter": "wilsonzlin",
    "submit_time": "2024-05-09T12:31:04",
    "num_comments": 105,
    "comments_url": "https://news.ycombinator.com/item?id=40307519",
    "comments": [
      "This is impressive work, especially for a one man show!One thing that stood out to me was the graph of the sentiment analysis over time, I hadn't seen something like that before and it was interesting to see it for Rust. What were the most positive topics over time? And were there topics that saw very sudden drops?I also found this sentence interesting, as it rings true to me about social media  \"there seems to be a lot of negative sentiment on HN in general.\" It would be cool to see a comparison of sentiment across social media platforms and across time!",
      "Thanks! Yeah I'd like to dive deeper into the sentiment aspect. As you say it'd be interesting to see some overview, instead of specific queries.The negative sentiment stood out to me mostly because I was expecting a more \"clear-cut\" sentiment graph: largely neutral-positive, with spikes in the positive direction around positive posts and negative around negative posts. However, for almost all my queries, the sentiment was almost always negative. Even positive posts apparently attracted a lot of negativity (according to the model and my approach, both of which could be wrong). It's something I'd like to dive deeper into, perhaps in a future blog post.",
      "The sentiment issue is a curious one to me. For example, a lot of humans I interact with that are not devs take my direct questioning or critical responses to be \"negative\" when there is no negative intent at all. Pointing out something doesn't work or anything that the dev community encounters on a daily basis isn't an immediate negative sentiment but just pointing out the issues. Is it a meme-like helicopter parent constantly doling out praise positive so that anything differing shows negativity? Not every piece of art needs to be hung on the fridge door, and providing constructive criticism for improvement is oh so often framed as negative. That does the world no favors.Essentially, I'm not familiar with HuggingFace or any models in this regard. But if they are trained from the socials, then it seems skewed from the start to me.Also, fully aware that this comment will probably be viewed as negative based on stated assumptions.edit: reading further down the comments, clearly I'm not the first with these sentiments.",
      "Every helicopter gets a trophy",
      "wait, the parents get a trophy?",
      "I did something related for my ChillTranslator project for translating spicy HN comments to calm variations which has a GGUF model that runs easily and quickly but it's early days. I did it with a much smaller set of data, using LLM's to make calm variations and an algo to pick the closest least spicy one to make the synthetic training data then used Phi 2. I used Detoxify then OpenAI's sentiment analysis is free, I use that to verify Detoxify has correctly identified spicy comments then generate a calm pair. I do worry that HN could implode / degrade if there is not able to be a good balance for the comments and posts that people come here for. Maybe I can use your sentiment data to mine faster and generate more pairs. I've only done an initial end-to-end test so far (which works!). The model, so far is not as high quality as I'd like but I've not used Phi 3 on it yet and I've only used a very small fine-tune dataset so far. \nFile is here though: https://huggingface.co/lukestanley/ChillTranslator\nI've had no feedback from anyone on it though I did have a 404 in my Show HN post!",
      "its so interesting that in Likert scale surveys, I tend to see huge positivity bias/agreement bias, but comments tend to be critical/negative. I think there is something related to the format of feedback that skews the graph in general.On HN, my theory is that positivity is the upvotes, and negativity/criticality is the discussion.Personally, my contribution to your effort is that I would love to see a tool that could do this analysis for me over a dataset/corpus of my choosing. The code is nice, but it is a bit beyond me to follow in your footsteps.",
      "Great work! Would you consider adding support for search-via-url, e.g. https://hn.wilsonl.in/?q=sentiment+analysis. It would enable sharing and bookmarks of stable queries.",
      "Thanks for the suggestion, I've just added the feature:https://hn.wilsonl.in/s/sentiment%20analysis",
      "Anecdotally, I think anyone who reads HN for a while will realize it to be a negative, cynical place.Posts written in sweet syrupy tones wouldn\u2019t do well here, and jokes are in short supply or outright banned. Most people here also seem to be men. There\u2019s always someone shooting you down. And after a while, you start to shoot back."
    ],
    "link": "https://blog.wilsonl.in/hackerverse/",
    "first_paragraph": "",
    "summary": "A solitary Hacker News enthusiast decides to map 40 million posts and comments, confirming what everyone already suspected but didn\u2019t need graphed out: sentiment on HN is more negative than a dark room full of goths. Commenters fall over themselves to praise the \"impressive\" solo effort and quickly spiral into philosophical debates about whether negativity is just misunderstood positivity. One brave soul suggests comparing the industrious sea of negativity across different social media platforms, as if discovering that Twitter might be slightly less depressing could be considered a win. <*i>Hand out the trophies, everyone feels like they\u2019ve contributed intellectually here.*</i>"
  },
  {
    "title": "Show HN: An SQS Alternative on Postgres (github.com/tembo-io)",
    "points": 183,
    "submitter": "chuckhend",
    "submit_time": "2024-05-09T12:21:53",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=40307454",
    "comments": [
      "> Guaranteed \"exactly once\" delivery of messages to a consumer within a visibility timeoutThat's not going to be true. It might be true when things are running well, but when it fails, it'll either be at most once or at least once. You don't build for the steady state, you build against the failure mode. That's an important deciding factor in whether you choose a system: you can accept duplicates gracefully or you can accept some amount of data loss.Without reviewing all of the code, it's not possible to say what this actually is, but since it seems like it's up to the implementor to set up replication, I suspect this is an at-most-once queue (if the client receives a response before the server has replicated the data and the server is destroyed, the data is lost). But depending on the diligence of the developer, it could be that this provides no real guarantees (0-N deliveries).",
      "It will be true because of the \"within a visibility timeout\" right? Of course that makes the claim way less interesting.I took a peek at the code and it looks like their visibility timeout is pretty much a lock on a message. So it's not exactly once for any meaningful definition, but it does prevent the same message from being consumed multiple times within the visibility timeout.",
      "> it does prevent the same message from being consumed multiple times within the visibility timeout.... When there is no failure of the underlying system. The expectation of any queue is that messages are only delivered once. But that's not what's interesting: what matters is what happens when there's a system failure: either the message gets delivered more than once, the message gets delivered zero times, or a little of column A and a little of column B (which is the worst of both worlds and is a bug). If you have one queue node, it can fail and lose your data. If you have multiple queue nodes, you can have a network partition. In all cases, it's possible to not know whether the message was processed or not at some point",
      "The Two Generals Problem is a great thought experiment that describes how distributed consensus is impossible when communication between nodes has the possibility of failing.https://en.wikipedia.org/wiki/Two_Generals%27_Problem",
      "If the message never reaches the queue (network error, database is down, app is down, etc), then yes that is a 0 delivery scenario. Once the message reaches the queue though, it is guaranteed that only a single consumer can read the message for the duration of the visibility timeout. FOR UPDATE guarantees only a single consumer can read the record, and the visibility timeout means we don't have to hold a lock. After that visibility timeout expires, it is an at-least-once scenario. Any suggestion for how we could change the verbiage on the readme to make that more clear?",
      "You are talking about distributed systems, nobody expects to read \"exactly once\" delivery. If I read that on the docs, I consider that a huge red flag.And the fact is that what you describe is a performance optimization, I still have to write my code so that it is idempotent, so that optimization does not affect me in any other way, because exactly once is not a thing.All of this to say, I'm not even sure it's worth mentioning?",
      "> Once the message reaches the queue though, it is guaranteed that only a single consumer can read the message for the duration of the visibility timeout.But does the message get persisted and replicated before any consumer gets the message (and after the submission of the message is acked)? If it's a single node, the answer is simply \"no\": the hard drive can melt before anyone reads the message and the data is lost. It's not \"exactly once\" if nobody gets the message.And if the message is persisted and replicated, but there's subsequently a network partition, do multiple consumers get to read the message? What happens if writing the confirmation from the consumer fails, does the visibility timeout still expire?> After that visibility timeout expires, it is an at-least-once scenario.That's not really what \"at least once\" refers to. That's normal operation, and sets the normal expectations of how the system should work under normal conditions. What matters is what happens under abnormal conditions.",
      "As u/Fire-Dragon-DoL says, you can have an exactly-once guarantee within this small system, but you can't extend it beyond that.  And even then, I don't think you can have it.  If the DB is full, you can't get new messages and they might get dropped, and if the threads picking up events get stuck or die then messages might go unprocessed and once again the guarantee is violated.  And if processing an event requires having external side-effects that cannot be rolled back then you're violating the at-most-once part of the exactly-once guarantee.",
      "> you can have an exactly-once guarantee within this small systemIt's actually harder to do this in a small system. I submit a message to the queue, and it's saved and acknowledged. Then the hard drive fails before the message is requested by a consumer. It's gone. Zero deliveries, or \"at most once\".> If the DB is full, you can't get new messages and they might get droppedIn this case, I'd expect the producer to receive a failure, so technically there's nothing to deliver.> if the threads picking up events get stuck or dieWhile this obviously affects delivery, this is a concurrency bug and not a fundamental design choice. The failures folks usually refer to in this context are ones that are outside of your control, like hardware failures or power outages.",
      "> That's not going to be true. It might be true when things are running well, but when it fails, it'll either be at most once or at least once.Silly question as somebody not very deep in the details on this.It's not easy to make distributed systems idempotent across the board (POST vs PUT, etc.)Distributed rollbacks are also hard once you reach interacting with 3rd party APIs, databases, cache, etc.What is the trick in your \"on message received handler\" from the queue to achieve \"exactly once\"? Some kind of \"message hash ID\" and then you check in Redis if it has already been processed either fully successfully, partially, or with failures? That has drawbacks/problems too, no? Is it an impossible problem?"
    ],
    "link": "https://github.com/tembo-io/pgmq",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "The Haskell-enthusiast basement of Hacker News presents yet another testament to reinventing the wheel with \"An SQS Alternative on Postgres,\" a service that purports to deliver messages <em>\"exactly once\"</em>, which in the world of distributed systems is akin to catching a unicorn with a spaghetti strainer. Commenters, armed with an impenetrable shield of jargon and pessimism, quickly troll into the conversation to dissect, reconstruct, and argue over this claim, showcasing elaborate scenarios involving network partitions, server meltdowns, and metaphysical musings about the Two Generals Problem. But honestly, do these armchair engineers think that appending <i>\"within a visibility timeout\"</i> to their docs will make a quixotic feature suddenly seem viable? Ah, but fret not\u2014regardless of what the readme says, we all know this is just another over-engineered solution waiting to break spectacularly during your next product demo. \ud83c\udf7f\ud83d\ude2c"
  },
  {
    "title": "Show HN: Ellipsis \u2013 Automated PR reviews and bug fixes (ellipsis.dev)",
    "points": 62,
    "submitter": "hunterbrooks",
    "submit_time": "2024-05-09T16:14:47",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=40309719",
    "comments": [
      "We are using ellipsis and sweep both for our open source project and they are quite helpful in their own ways. I think selling them as an automated engineer is a little over the top at the moment but once you get the hang of it they can spot common problems in PRs or do small documentation related stuff quite accurately.Take a look at this PR for example:\nhttps://github.com/julep-ai/julep/pull/311Ellipsis caught a bunch of things that would have come up only in code review later. It also got a few things wrong but they are easy to ignore. I like it overall, helpful once you get the hang of it although far from a \u201cjunior dev\u201d.",
      "I\u2019ve been following sweep and aider for awhile and really love what they\u2019re both doing, especially sweep.Would love to get your thoughts on sweep. Does it meet your expectations? If not, where does it fall short?",
      "> I think selling them as an automated engineer is a little over the top at the momentIndeed. Amazon originally advertised CodeGuru as being \"like having a distinguished engineer on call, 24x7\".[^1] That became a punchline at work for a good while.I can definitely see the value of a tool that helps identify issues and suggest fixes for stuff beyond your typical linter, though. In theory, getting that stuff out of the way could make for more meaningful human reviews. (Just don't overpromise what it can reasonably do.)[^1]: https://web.archive.org/web/20191203185853/https://aws.amazo...",
      "As it stands today, Ellipsis isn't sold as an AI software engineer.One of our largest learnings is that state of the art LLM's aren't good enough to write code autonomously, but they are good enough to be helpful during code review.",
      "> Take a look at this PR for example: https://github.com/julep-ai/julep/pull/311I am still confused if vector size should be 1024 or 728 lol.",
      "A sampling of PRs looks pretty good code-wise, but the commit messages/descriptions don't.  They just summarize the changes done (something that can be gleaned from the diff) but don't give context or rationale around why the changes were necessary.",
      "It's most helpful when a GitHub/Linear issue is linked because the \"why\" is extracted, and also for larger PR's",
      "Automated review seems like a hard sell to me.If humans participation is reduced they could approve PR's without proper reviewing.Eventually this stochastic parrot could throw an ``rm -rv ${TEMP}/`` in there and you are roasted.",
      "Automated code reviews are a step in the software development lifecycle, not a replacement for human reviewers.Ellipsis can't commit code without your permission and approval, so this particular parrot can't feather your filesystem.",
      "Signed up for Beekeeper Studio, no idea how well it performs for a desktop app written with electron and vue.js, but we'll see!First two code reviews show no feedback."
    ],
    "link": "https://www.ellipsis.dev/",
    "first_paragraph": "Ellipsis",
    "summary": "Welcome to the latest Silicon Valley savior: Ellipsis! The tool that sort of helps with PRs, kind of like a \"junior dev\" but without the coffee runs. According to one thrilled user, Ellipsis caught errors that human eyes could only spot later \u2013 amidst the occasional flub. But don't worry, those are \"easy to ignore\" (famous last words before production crashes). Enjoy the journey of overestimated AI capabilities where each bug fix is treated like a divine revelation, and every commenter vies to be the least disillusioned tech enthusiast. Will Ellipsis replace your developers? Absolutely not. But it will give your hopeful commenters something to buzz about, reducing human code review to mere formality before the AI apocalypse. \ud83d\ude02"
  },
  {
    "title": "Scientists find 57,000 cells and 150M neural connections in tiny sample of brain (theguardian.com)",
    "points": 8,
    "submitter": "mr_toad",
    "submit_time": "2024-05-10T00:35:18",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://www.theguardian.com/science/article/2024/may/09/scientists-find-57000-cells-and-150m-neural-connections-in-tiny-sample-of-human-brain",
    "first_paragraph": "Harvard researchers teamed up with Google to analyse the makeup of the brain, much of which is not yet understood",
    "summary": "In a groundbreaking display of counting things better left uncounted, Harvard researchers and their new best friends at Google have dazzled the world by finding <em>57,000 cells and 150 million neural connections</em> in a brain crumb. Heralding the dawn of a new era, The Guardian explains this complex scientific marvel with colorful pie charts and snackable metaphors suited for its dear readers' presumed neuronal count. Meanwhile, the comment section morphs into a virtual battleground where self-proclaimed neuroscientists and die-hard sci-fi fans debate whether this discovery will enable mind reading, immortality, or just better-targeted ads. At this rate, we\u2019ll surely solve the enigma of consciousness, one online squabble at a time. \ud83e\udde0\ud83d\udcac"
  },
  {
    "title": "How bad are satellite megaconstellations for astronomy? (leonarddavid.com)",
    "points": 60,
    "submitter": "belter",
    "submit_time": "2024-05-09T20:11:21",
    "num_comments": 97,
    "comments_url": "https://news.ycombinator.com/item?id=40312469",
    "comments": [
      "There certainly is a real concern for astronomers, but the photo illustrations used in the article are selected to make things seem worse than they really are. They're wide field of view, long-duration exposures. That used to be the way astronomers imaged space, with film systems a century ago. But these days astronomical telescopes tend to have much narrower fields of view (like tiny soda straws peering into one particular spot in space) and use image stacking, a technique where many individual images are processed to form the final image (very simplistic overview: https://www.skyatnightmagazine.com/astrophotography/astropho... .) Using image stacking and armed with accurate catalogs that predict precisely where every satellite will be at any time, enables the removal of satellites during the image stacking process. Or they can just use that information for better scheduling: wait a minute or two to image a particular spot, so there won't be satellites in the field of view.The article definitely gets this part right:> \u201cSome astronomers see this as a true \u2018hair on fire\u2019 emergency, heralding irretrievable losses to space science; others present a more sanguine face, depicting this as yet another challenge to be surmounted in surveying a decreasingly pristine sky,\u201d Koplow remarks.Being involved in both space and astronomy plants me squarely in the latter camp. It takes a bit more work and software, but having so many satellites in space is a surmountable challenge for terrestrial astronomers. (Not to mention, these days some of the best astronomy is performed by telescopes in space, so astronomy overall benefits by having easier access to space.)",
      "> They're wide field of view, long-duration exposures. That used to be the way astronomers imaged space,Sorry, but one of the two example images (https://noirlab.edu/public/images/iotw1946a/) is a single 333-second exposure with a modern survey camera, the Dark Energy Camera.  This is not particularly long nor does it represent some outmoded observational strategy.  Large, wide-field imaging sky surveys (such as the upcoming Rubin Observatory) are among the highest-profile ground-based astronomy projects today.Masking and stacking can mitigate the problem but it does not of course compensate for the lost area and sensitivity.  And the brightest satellites (like BlueWalker) saturate the readout electronics and spoil the whole exposure.Narrow field instruments (such as spectrographs) have less geometric chance of seeing a satellite but tend to take longer exposures (tens of minutes), so there is a greater loss of telescope time when a streak does happen.Even space telescopes are affected by streaks (https://www.space.com/hubble-images-spoiled-starlink-satelli...).> It takes a bit more work and softwareEquivalently, it takes more money and time.  That just means less science, given flat to declining funding from Congress.",
      "> Masking and stacking can mitigate the problem but it does not of course compensate for the lost area and sensitivity. And the brightest satellites (like BlueWalker) saturate the readout electronics and spoil the whole exposure.Is this only a problem in systems that aren't aware of where the satellites will be? I naively assume that, if the system knew, it could start exposing in the next clear window. I naively assume that the window is almost always clear, especially for an individual sensor.",
      "Where do you see that the full well of those CCDs are fully saturated by a single satellite streak?Even if that was the case, that seems a very poor design for a sensor and not really an issue of satellites. If a satellite streak can saturate your well, then so could a decently dense star field.Do they blow out the individual pixels? Sure. But you just stack.You don't lose sensitivity just because your stack rejected pixels, that's not how stacking works. You wind up with a tiny bit more noise, how much more depends on your capture. But not lower sensitivity, stacking isn't just average an area",
      "Forget satellites; you can't do raw long-exposure for \"serious\" terrestrial-based astronomy anymore because you'll get planes in the way.",
      "> the photo illustrations used in the article are selected to make things seem worse than they really areExactly. I do a LOT of astrophotography and satellite trails are actually very easy to get rid of. Even the stupidest, most naive outlier rejection techniques, such as taking a stack of tracked images, finding their mean and standard deviation, throwing out any data points outside of 2 standard deviations, and then re-averaging, will get rid of the satellites very cleanly. You can go to more advanced techniques such as doing linear fits and RANSAC and whatnot, but you get the idea.NOT doing any outlier rejection, just taking the mean of all the images will show the satellite tracks, but very dimly. The people who are trying to make noise on social media deliberately and disingenuously take the max() instead of the mean() to make the problem seem worse than it is.That said -- in all of my imaging sessions, aircraft are a much, much, much bigger problem than satellites, and still easy to deal with. Yet nobody makes any noise about aircraft.",
      "Seriously, any time you have >3 images to stack satellites and planes are a complete non issue. The median pixel value is always going to exclude those. Five images is enough to eliminate any trails even if they overlap between images.",
      "You're not imaging with cryogenic CCDs --- satellite trail can blow out whole rows.",
      "On the one hand the constellations are doing considerable damage to surface based astronomy. On the other hand they are funding an enormously lower cost of access to orbit and beyond for astronomy up in the sky.",
      "Exactly my thoughts. Scientists drool at the few telescopes we have in orbit. If we could launch more of them for less money at the expense of a busier night sky I don't think they would want to miss on that"
    ],
    "link": "https://www.leonarddavid.com/blinded-by-the-light-megaconstellation-clash-with-astronomical-peer-groups/",
    "first_paragraph": "Wait a minute.Image credit: Barbara David",
    "summary": "LeonardDavid.com hastily assembles yet another high-stakes drama titled <em>\"How bad are satellite megaconstellations for astronomy?\"</em> injecting sheer panic among the three stargazers who haven\u2019t switched to Netflix for their cosmic fix. The article gracefully employs the \u201ccherry-picked imagery to fabricate an apocalypse\" method, showcasing wide field exposures that are apparently as outdated as using Morse code. The armchair astronomers in the comments, leveraging their expertise from that one time they stayed at a Holiday Inn Express, argue with the rigor of a flat-earth convention, passionately asserting that a sprinkle of software and a dash of wishful thinking can surely fix those pesky light streaks ruining their backyard telescope sessions. Meanwhile, the real astronomy moves to space faster than you can say \u201corbital traffic jam.\u201d"
  },
  {
    "title": "Logarithmic Scales (briefer.cloud)",
    "points": 51,
    "submitter": "lucasfcosta",
    "submit_time": "2024-05-08T11:25:49",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=40296734",
    "comments": [
      "another nice one is Inverse Hyperbolic Sine scale, which can reach 0 and plot negative values, whereas log scales cannot. imo it's nicer than a cube root scale, too.https://leeoniya.github.io/uPlot/demos/arcsinh-scales.htmlcube root: https://en.wikipedia.org/wiki/Cube_root#/media/File:Cube-roo...vs arcsinh: https://upload.wikimedia.org/wikipedia/commons/9/92/Inverse_...some interactive log scale demos:https://leeoniya.github.io/uPlot/demos/log-scales.htmlhttps://leeoniya.github.io/uPlot/demos/log-scales2.html",
      "I like the asinh scale, I\u2019ve come across it before as \u201csigned pseudolog scale\u201d - your name for it is nice and descriptive, do you have a writeup about it somewhere?https://win-vector.com/2012/03/01/modeling-trick-the-signed-...",
      "Wow. This is the first time in hearing about the inverse hyperbolic sine scale. I wonder if it's lesser known simply because many kinds of real-world underlying data that needs logarithmic scales simply don't have zeroes or negative values? For example we might be plotting some data that actually comes from an underlying exponential distribution?",
      "i think sometimes it's called symlog, though i'm not sure if that's just a convenient re-naming of arcsinh. i don't see how any kind of \"log\" scale can do <= 0.you can always y-flip the result of log(abs(value)) and show neg ticks on the axis, but it will not be a nicely-continuous, single scale that can cross 0.https://stackoverflow.com/questions/3305865/what-is-the-diff...",
      "Symlog is symmetric log. It's not arcsinh, instead it's a normal logarithmic scale, that turns into a linear scale around zero (for example in the range [-1,1]) and then it turns into a negative logarithm on the other side of zero. So similar but slightly different.https://matplotlib.org/stable/gallery/scales/symlog_demo.htm...for an example.",
      "> that turns into a linear scale around zerohow does this actually work? afaik, there is no log() fn that you can run which \"becomes linear\". i guess you can wrap a call to log() in another fn that simply does linear scaling below the defined threshold, but it's not a smooth transition without a bunch of extra [possibly slow] smoothing code.what you're describing is exactly how a straight call to Math.asinh() behaves, and what i have implemented in the above demo.",
      "Theres no smoothing involved, it's three distinct domains with different function definitions. Top part is log(x) from [a,\\infty), bottom part is -log(-x) from (-\\infty,-a] and the middle part is cx from (-a,a). With c chosen to ensure continuity in the transform.In the example from matplotlib I linked in the earlier comment they call out that the symlog transform has a discontinous gradient at the a's, and that the asinh transform can be used instead if that's a problem.Edit: On reflection it's probably not entirely correct to talk about it as choosing an appropriate c. Since that transform seems to kinda break apart around a=1. Simpler to consider it a matter of plotting on a logarithmic scale down to some value. Then continuing the plot on a linear scale until you reach the negative value on the other side and then plotting on a negative logarithmic scale (-log(|x|)).",
      "It's also harder to interpret. What the heck does `log(x + sqrt(x^2 + 1))` mean?It's a great transformation for data viz or machine learning / statistical modeling, but its not really obvious what an IHS-transformed variable represents.",
      "> It's also harder to interpret. What the heck does `log(x + sqrt(x^2 + 1))` mean?once you see the shape plotted, i dont think it's much harder than understanding the shape of logistic functions, which have similar formulations, and are used all over the place in ai/ml for activation, etc.https://en.m.wikipedia.org/wiki/Logistic_function",
      "This is brilliant. I used a logarithmic cosine scale in college physics for the degradation of a Cavindish balence. It won me both a guest lecture and an easy A.There are even wilder triple hyperbolic scales on three dimensions."
    ],
    "link": "https://briefer.cloud/blog/posts/logarithms/",
    "first_paragraph": "The first lesson I learned about logarithms is not to mention them when speaking to a large crowd. All the other lessons sucked, so I thought I'd create my own.",
    "summary": "Title: Sine, Cosine, Inverse Hyperbolic Pandemonium - A Brainy Mess at the Scale Olympics\n\nBreaking news from the unfathomable depths of mathematical complexity: an audacious blogger at briefer.cloud reveals a stark, ground-breaking lesson - logarithmic scales are not party conversation material! The riveting expos\u00e9 thus unleashes the creeps of the comment section to unleash their arsenal of even more obscure mathematical models, including the awe-inducing <em>\"Inverse Hyperbolic Sine Scale\"</em>, suspense-thickening <em>symlog</em>, and roller-coaster-ride-worthy <em>arcsinh vs cube root</em> scale showdown \ud83c\udfa2. Meanwhile, one dazed reader wonders if all this jargon could simply be code for \u201c<i>I need more friends who won't run away when I say math</i>.\u201d \ud83e\uddd1\u200d\ud83c\udf93\ud83d\udca8"
  },
  {
    "title": "Cubic millimetre of brain mapped in spectacular detail (nature.com)",
    "points": 108,
    "submitter": "geox",
    "submit_time": "2024-05-09T21:36:06",
    "num_comments": 51,
    "comments_url": "https://news.ycombinator.com/item?id=40313193",
    "comments": [
      "The interactive visualization is pretty great. Try zooming in on the slices and then scrolling up or down through the layers. Also try zooming in on the 3D model. Notice how hovering over any part of a neuron highlights all parts of that neuron:http://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-rele...",
      "My god. That is stunning.To think that\u2019s one single millimeter of our brain and look at all those connections.Now I understand why crows can be so smart walnut sized brain be damned.What an amazing thing brains are.Possibly the most complex things in the universe.Is it complex enough to understand itself though? Is that logically even possible?",
      "Crow/parrot brains are tiny but in terms of neuron count they are twice as dense as primate brains (including ours): https://www.sciencedirect.com/science/article/pii/S096098221...If someone did this experiment with a crow brain I imagine it would look \u201ctwice as complex\u201d (whatever that might mean). 250 million years of evolution separates mammals from birds.",
      "That shouldn't be too surprising, as a larger fraction of the volume of a brain should be taken up by \"wiring\" as the size of the brain expands.",
      "I expect we'll find that it's all a matter of tradeoffs, in terms of count vs size/complexity, much like how the \"spoken data rate\" of various human languages seems to be the same even though some have complicated big words versus more smaller ones etc.",
      "Interesting! Thank you. I didn\u2019t know that.",
      "We don\u2019t know what \u201cunderstanding\u201d means (we don\u2019t have a workable definition of it), so your question cannot be answered.",
      "I wonder if we manage to annotate this much level of detail about our brain, and then let (some variant of the current) models train on it, will those intrinsically end up generalizing a model for intelligence?",
      "I think you would also need the epigenetic side, which is very poorly understood: https://www.universityofcalifornia.edu/news/biologists-trans...We have more detail than this about the C. elegans nematode brain, yet we still no clue how nematode intelligence actually works.",
      "How's OpenWorm coming along?"
    ],
    "link": "https://www.nature.com/articles/d41586-024-01387-9",
    "first_paragraph": "Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\n            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\n            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\n            and JavaScript.",
    "summary": "In a feat that absolutely warranted the thousands of hours likely spent, scientists have mapped a cubic millimeter of the human brain, because the rest of the universe was just too boring this week. <i>Nature.com</i> cautions its readers to upgrade their browser from Stone Age Explorer to view this wonder in full CSS glory, effectively ensuring half the intended audience has already nodded off. Meanwhile, the comment section erupts with amateur neurologists marveling at neuron density with the kind of awed revelation usually reserved for late-night infomercial products. Is the human brain complex enough to understand itself? Don\u2019t ask the commenters, who can barely navigate browser updates, let alone existential inquiries into consciousness. \ud83e\udde0"
  },
  {
    "title": "Deaf girl is cured in world first gene therapy trial (independent.co.uk)",
    "points": 476,
    "submitter": "belter",
    "submit_time": "2024-05-09T11:38:14",
    "num_comments": 238,
    "comments_url": "https://news.ycombinator.com/item?id=40307138",
    "comments": [
      "My Mom was born with less than half of her hearing in left ear and barely anything at all in right ear. She had surgery as an adult and it slightly improved, especially on the right side.Pretty sure this little girl and my Mom don't share the same disability at all but she saw these news today and texted me so excited because future kids won't have to endure the same.Brings tears to my eyes, I'm so grateful for modern medicine and its stupendous advances.",
      "It might be the condition I have, otosclerosis, where the bones of the middle ear fuse. The surgery, a stapedectomy, involves removing the bones and replacing them with a metallic prosthesis. Unfortunately, in my case, the calcification is also happening in my cochlea which means that at some indeterminate time in the future, I will end up losing all hearing. Not to mention that even with the prosthetic, my hearing isn\u2019t 100% so I have a difficult time understanding speech. There might be some gene therapy that can remedy things, but I think what I may be hoping to see is the ability to grow a new inner and middle ear from stem cells and transplant them into my head, but I suspect that at age 55, I won\u2019t see this happen in time for me.",
      "I had no idea that was possible. If you'd asked me, I would have guessed that it wasn't.Astonishing. Sorry it's not a permanent fix for you, but it's impressive as hell that they could do anything.",
      "I was recently diagnosed with otosclerosis, have you found a hearing aid that works best for otosclerosis, my Doctor mentioned that most hearing aids don't work well for low frequency loss.",
      "I have moderate to severe hearing loss mostly in the low frequency region from otosclerosis...I use Widex Moment 440s BTE with custom ear moulds. They sound fantastic.  My only complaint is that require a necklace pendant to get Bluetooth connectivity but other than that I love them.  They're expensive but worth every penny IMO.",
      "Thank you to share about your personal experience.  Will a cochlear implant help for your condition?",
      "It will, but with the caveat that hearing with a cochlear implant is (at least at current technology levels), inferior to hearing with an actual cochlea. So there\u2019s a balancing act where they want to continue having me hear with my cochlea as long as possible. The other problem is that with a CI, I have no hearing at all without the receiver which means that I would be completely deaf while swimming, showering, etc. while I have at least some hearing without my hearing aids right now (although I had an ear infection in December which left me completely deaf for a week. It was a bit startling how much people were unwilling to engage in the smallest adaptations for me\u2014I found how to set up live transcriptions on my phone and I remember the cashier at the grocery store being unwilling to use that so I could see what she was saying).",
      "(With the live transcription feature, I was actually able to engage in normal-ish telephone conversations, maybe even a little more effective than I can with using the audio.)",
      "Another viable future tech for this might be neural implants similar to Neuralink. Not sure how viable it would be.",
      "Did your sense of taste change after your surgery? My sister is considering a similar procedure and is concerned that everything could start tasting like hot garbage."
    ],
    "link": "https://www.independent.co.uk/news/health/deaf-cure-girl-gene-therapy-b2541735.html",
    "first_paragraph": "Please refresh the page or navigate to another page on the site to be automatically logged inPlease refresh your browser to be logged in",
    "summary": "In a world-first gene therapy trial, a deaf girl allegedly hears again, suggests <em>Independent.co.uk</em>, perhaps disbelieving its own sensationalistic headline. Commenters leap into action, spinning tales of personal auditory woes and speculative biotech fantasies, as if determined to outdo one another in the \"my condition is more complex than yours\" Olympics. One dreams of stem-cell-grown ears; another braces for a taste of 'hot garbage'. The future, it seems, is ripe for the rediscovery of hearing, garnished generously with a side of medical misinformation and unrealistic expectations. What's next, bionic eyes that stream Netflix directly into your brain? \ud83d\ude44\ud83d\ude02"
  },
  {
    "title": "DNSecure \u2013 a configuration tool of DoT and DoH for iOS and iPadOS (github.com/kkebo)",
    "points": 37,
    "submitter": "conductor",
    "submit_time": "2024-05-09T18:11:46",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=40311110",
    "comments": [
      "Isn't this basically just putting a UI on top of configuration profiles?https://github.com/paulmillr/encrypted-dnsOr does an app have some advantages to it that I'm unaware of?",
      "configuration profiles you need to create the configuration files, send to the device, import.And if you need to make some changes you need to do it all again..with this tool you have an interface to configure everything, it only make make the process easier..",
      "This app is hardly better than what you describe though: you still need to install it on all your devices and configure it. Seems hardly any easier than just downloading a pre-made profile from the repo I linked above and installing it. shrug To each their own I guess.",
      "If you are managing many devices I agree, but for just a few.When I said it was easier I meant for a single device or if you need some unusual configuration.I have my own dns server, but my devices are already set so not sure will use this.If I was setting things up today I might give it a try..",
      "Or you can download one from NextDNS.io since you do need another end of the connection.",
      "I have my own DNS server using AdGuard Home that i host in a VPS so i can have adblocking anywhere as i have at home.But i have to maintain my own configuration profile, that is not hard but kind of a pain to make changes, not that i have to do that often.",
      "Related:  I've had bad luck with configuration generators for MacOS (OSX) ... somehow the configuration is weirdly tied to a specific network interface and then pukes if you change from wifi to wired ...... yet at the same time, the \"universal\" configuration will not work at all ...I don't understand why, in 2024, we can't have a plain old configuration dialog in OSX that allows the setting of DoH hosts.  Why is Apple restricting this to enterprise configurations ?",
      "Today I learned acronyms for DNS over TLS and DNS over HTTPS. Neat tool. I\u2019ll try it when I get home."
    ],
    "link": "https://github.com/kkebo/DNSecure",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In today's episode of \"Reinventing the Configuration Wheel,\" we meet DNSecure, the app that promises to add a shiny UI to the dark underworld of DNS setting tweaks \ud83c\udfa8. Its creator vows solemn fidelity to user feedback, possibly mistaking frequency of updates for quality. Meanwhile, the GitHub commentariat are flexing their tech muscles, arguing whether it\u2019s literally easier to configure DNS via smoke signals than use this app. One bright spark learns what DoT and DoH stand for, marking a significant personal milestone amidst the squabble. Each user seems to have brewed their own artisanal DNS solution, yet they gather 'round to debate the color of the bike shed called DNSecure. Isn't modern tech wonderful? \ud83c\udf08\u2699\ufe0f"
  }
]