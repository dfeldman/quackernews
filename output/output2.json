[
  {
    "title": "Started a guide to writing FUSE filesystems in Python (gwolf.org)",
    "points": 141,
    "submitter": "levlaz",
    "submit_time": "2024-10-11T18:29:40.000000Z",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=41811983",
    "comments": [
      "The libfuse github has some good examples for C/C++ in [0] of increasing complexity:- passthrough.c mirrors existing filesystem, \"Its performance is terrible.\"- passthrough_fh.c \"performance is not quite as bad.\"- passthrough_ll.c implemented with low level api and \"the least bad among the three\"- passthrough_hp.cc high performance version written in C++Some interesting fuse projects in my notes: [1] splitting large files into segments; [2] show ZFS incremental snapshots as files; [3] transparent filesystem compression; [4] and [5] options for mounting archives as filesytems.- [0] https://github.com/libfuse/libfuse/tree/master/example- [1] https://github.com/seiferma/splitviewfuse- [2] https://github.com/UNFmontreal/zfs_fuse_snapshot- [3] https://github.com/FS-make-simple/fusecompress- [4] https://github.com/google/fuse-archive- [5] https://github.com/mxmlnkn/ratarmount\n \nreply",
      "While [2] might be a good code example, this functionality is already built into ZFS. At the mountpoint of every dataset is a hidden \".zfs\" folder that doesn't show up, even on a `ls -A`. You just have to believe its there and cd into it. Under that is a \"snapshot\" folder, and inside that is a folder for each snapshot of that dataset. Those folders contain the files in the snapshot.So for example, /etc/hosts from my snapshot zrepl_20241011_010143_000 would be at /.zfs/snapshot/zrepl_20241011_010143_000/etc/hostsIf you don't like the magic hidden nature of it, you can even configure it to behave like a normal folder with `zfs set snapdir=visible <dataset>`\n \nreply",
      "Just wanted to throw out there that although I'm a fan of FUSE, it's not the only option. I've had fun implementing a virtual filesystem via the 9p protocol not too long ago.IIRC, I used py9p and the python experience was much nicer than fuse-python. You can mount a 9p service via FUSE (9pfuse) if you want. I just used the kernel v9fs client. If you're just looking to pass a filesystem through the network, I think I used the diod 9p server.Overall, it's a nice little ecosystem to explore.\n \nreply",
      "9p is such a great little protocol. diod[0] has a good amount of documentation on the protocol itself, but it's pretty simple.I have some notes here [1], but it's mostly just linking to primary sources. FUSE is great, but 9P is more general and has high quality implementations all over the place, even in Windows!One thing I'm not so sure about is the performance properties of 9p. I've seen some places indicate it's rather slow, but nothing definitive. Does anyone have any benchmarks or info on that?[0]: https://github.com/chaos/diod/blob/master/protocol.md\n[1]: https://athenaeum.wiki/Zettelkasten/9p\n \nreply",
      "> 9P [...] has high quality implementation[...] in WindowsDo you know if it\u2019s possible to mount one\u2019s own 9P servers under Windows? I seem to remember a comment from a Microsoft employee on GitHub something-or-other that said that capability is private to WSL2, but I can\u2019t find it right now.\n \nreply",
      "I'm not sure if you can mount a 9P filesystem from windows normally, I'll try. I'm not seeing any resources online about it either.\n \nreply",
      "It looks like py9p was last released in 2013 and it's still marked as \"beta\". Cool project though!\n \nreply",
      "I wish I had known about this a month ago, when I had to go through the exact same process!In a desperate attempt to find a less frustrating way to interact with Jira, I had the silly idea of starting a jira-as-filesystem project that uses our internal issue categorization to build a tree: directories represent issues, with files representing issue fields and subdirectories for linked issues. I ended up choosing fuse-python.I haven't worked on it in a minute, but I was already bumping into issues (pun not intended) with the abstraction: using just the issue ID as directory name makes automation easier, but it makes it hard for humans to browse the tree, since a `ls` would just show you a bunch of inscrutable IDs. I ended up adding a parallel `<issue-type>-with-summary` directory type where the slugified summary is appended to each issue ID.\n \nreply",
      "Hmm, I'm not saying it's a good idea, but what about a daemon that keeps a symlinked version of the entire jira environment up to date?  So you have one jira-as-filesystem that's the raw files, but then for human consumption/interaction, you have a tree of symlinks, including multiple links to the same file wherever it's relevant.  Might be adding more layers than needed, based on my lack of understanding, but might technically solve the (current/stated) abstraction issue.\n \nreply",
      "That's sort of what I'm doing behind the scenes, because I keep one global list of downloaded issues (they're lazily loaded when you access them) and then the folders are really only \"views\" into the downloaded issues. Representing identical ones across trees as symlinks is a fantastic idea though, I can't believe I didn't think of that! Thanks for the inspiration.\n \nreply"
    ],
    "link": "https://gwolf.org/2024/10/started-a-guide-to-writing-fuse-filesystems-in-python.html",
    "first_paragraph": "As DebConf22 was coming to an end, in Kosovo, talking with Eeveelweezel they\ninvited me to prepare a talk to give for the Chicago Python User\nGroup. I replied that I\u2019m not really that much of a Python\nguy\u2026 But would think about a topic. Two years passed. I meet Eeveelweezel\nagain for DebConf24 in Busan, South Korea. And the topic came up again. I had\nthought of some ideas, but none really pleased me. Again, I do write some Python\nwhen needed, and I teach using Python, as it\u2019s the language I find my students\ncan best cope with. But delivering a talk to ChiPy?On the other hand, I have long used a very simplistic and limited filesystem\nI\u2019ve designed as an implementation project at class:\nFIUnamFS\n(for \u201cFacultad de Ingenier\u00eda, Universidad Nacional Aut\u00f3noma de M\u00e9xico\u201d: the\nEngineering Faculty for Mexico\u2019s National University, where I teach. Sorry, the\nlink is in Spanish \u2014 but you will find several implementations of it from the\nstudents \ud83d\ude09). It is a toy filesystem, with as many bad character",
    "summary": "**Another Glorious Day at the FUSE Conference: Python Meets Electrical Engineering 101**\n\nA professor, not \"really that much of a Python guy,\" decides to bless the Chicago Python User Group with a discussion about a collegiate-level, \"toy\" filesystem, birthed in the academic wilds of Mexico's UNAM. Because nothing screams 'Thank you for waiting two years' like an introduction to student project code with \ud83d\udc4f *character encoding issues* \ud83d\udc4f. Meanwhile, the comment section devolves into a den of would-be filesystem enthusiasts, swapping tales of ZFS snapshots and 9P protocols, with that exploratory glee reserved for those who find joy in making directories represent Jira issues. Clearly, the internet is where filesystem dreams go to get complicated. \u26d3\ufe0f\ud83d\udcbb\u26d3\ufe0f"
  },
  {
    "title": "Lm.rs: Minimal CPU LLM inference in Rust with no dependency (github.com/samuel-vitorino)",
    "points": 183,
    "submitter": "littlestymaar",
    "submit_time": "2024-10-11T16:46:54.000000Z",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=41811078",
    "comments": [
      "This is impressive. I just ran the 1.2G llama3.2-1b-it-q80.lmrs on a M2 64GB MacBook and it felt speedy and used 1000% of CPU across 13 threads (according to Activity Monitor).    cd /tmp\n    git clone https://github.com/samuel-vitorino/lm.rs\n    cd lm.rs\n    RUSTFLAGS=\"-C target-cpu=native\" cargo build --release --bin chat\n    curl -LO 'https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS/resolve/main/tokenizer.bin?download=true'\n    curl -LO 'https://huggingface.co/samuel-vitorino/Llama-3.2-1B-Instruct-Q8_0-LMRS/resolve/main/llama3.2-1b-it-q80.lmrs?download=true'\n    ./target/release/chat --model llama3.2-1b-it-q80.lmrs\n \nreply",
      "Not sure how to formulate this, but what does this mean in the sense of how \"smart\" it is compared to the latest chatgpt version?\n \nreply",
      "The model I'm running here is Llama 3.2 1B, the smallest on-device model I've tried that has given me good results.The fact that a 1.2GB download can do as well as this is honestly astonishing to me - but it's going to laughably poor in comparison to something like GPT-4o - which I'm guessing is measured in the 100s of GBs.You can try out Llama 3.2 1B yourself directly in your browser (it will fetch about 1GB of data) at https://chat.webllm.ai/\n \nreply",
      "anyone else think 4o is kinda garbage compared to the older gpt4? as well as o1-preview and probably o1-mini.gpt4 tends to be more accurate than 4o for me.\n \nreply",
      "The implementation has no control on \u201chow smart\u201d the model is, and when it comes to llama 1B, it's not very smart by current standard (but it would still have blown everyone's mind just a few years back).\n \nreply",
      "The implementation absolutely can influence the outputs.If you have a sloppy implementations which somehow accumulates a lot of error in it's floating point math, you will get worse results.It's rarely talked about, but it's a real thing. Floating point addition and multiplication is non-associative and the order of operations affects the correctness and performance. Developers might (unknowningly) trade performance for correctness. And it matters a lot more in the low precision modes we operate today. Just try different methods of summing a vector containing 9,999 fp16 ones in fp16. Hint: it will never be 9,999.0 and you won't get close to the best approximation if you do it in a naive loop.\n \nreply",
      "How well does bf16 work in comparison?\n \nreply",
      "Even worse, I'd say since it has fewer bits for the fraction. At least in the example i was mentioning, where you run into precision limits, not into range limits.I believe bf16 was primarily designed as a storage format, since it just needs 16 zero bits added to be a valid fp32.\n \nreply",
      "I thought all current implementations accumulate into a fp32 instead of accumulating in fp16.\n \nreply",
      "I haven't looked at all implementations, but the hardware (tensor cores as well as cuda cores) allows you to accumulate at fp16 precision.\n \nreply"
    ],
    "link": "https://github.com/samuel-vitorino/lm.rs",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Minimal LLM inference in Rust\n      lm.rs: run inference on Language Models locally on the CPU with RustWebUI | Hugging Face | Video Demo\ud83c\udf03 Now supporting multimodality with PHI-3.5-vision model! PHI-3.5-mini text-only model also now supported.Inspired by Karpathy's llama2.c and llm.c I decided to create the most minimal code (not so minimal atm) that can perform full inference on Language Models on the CPU without ML libraries. Previously only Google's Gemma 2 models were supported, but I decided to add support for the new Llama 3.2 models, and more recently the option to use images with PHI-3.5. Image processing/encoding currently takes a bit, so it slows the first response, working on optimization now.Disclaimer: some of the code could be optimized and improved. This is just an excuse for me to write Rust for the first time. Isn't",
    "summary": "**Title: Another Day, Another Minimalist Rust Project Fails to Intimidate the Layman**\n\nIn a bold move that shocks absolutely nobody, Samuel Vitorino pops onto GitHub to introduce lm.rs, a project that allows enthusiastic but oddly naive developers to run language model inference on CPUs using Rust\u2014a decision as painful as listening to nails on a chalkboard. Riveted by the way Rust complicates the originally simple task, Samuel declares, with the swagger of a newbie, that he's riding on the coattails of smarter projects like Karpathy's llama2.c. Commenters, in an unparalleled display of ignorance, swoon over the \"impressive\" CPU gymnastics on their overpriced MacBooks, while juggling download commands that would make a sysadmin weep. One brave soul questions the model's intelligence, sparking a debate that deteriorates into a sad lesson on floating point precision\u2014because obviously, what this world needs is more arguments about why your fantasy numbers don't add up in a fantasy project."
  },
  {
    "title": "Conway's Gradient of Life (hardmath123.github.io)",
    "points": 193,
    "submitter": "networked",
    "submit_time": "2024-10-09T09:45:51.000000Z",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41786067",
    "comments": [
      "The objective function here defines a Markov random field (MRF) with boolean random variables and certain local statistics of nearest neighbours, either uniform if the target is a white image, or varying with location to produce an image. MRFs define Gibbs probability distributions, which you can sample from (which will already produce a good image here) or perform gradient ascent on to reach a local maxima. The negative log-likelihood of the MRF distribution is equal to the loss function of the original optimisation problem, so the maximum likelihood estimate (MLE) (there will often be multiple due to symmetry) of the MRF is the optimal solution(s) to the original problem. (But in general the MLE  can look completely different to a sample.)The statistics are 9th-order (of 3x3 blocks of pixels) but of a simple form which are hardly more expressive than 2nd-order nearest neighbour statistics (in terms of the different textures that they can reproduce) which are well known. In the approximate case where you only care about the average value of each pixel I think it would collapse to 2nd-order. Texture synthesis with MRFs with local statistics is discretized (in space) Turing reaction-diffusion. I did my PhD on this topic.Probably the most influential early paper on this kind of simple texture model, where you will see similar patterns, is:Cross & Jain, 1983, PAMI, Markov Random Field Texture Models\n \nreply",
      "Feels to me like there is no need for backpropagation. I think you can just iteratively grab a random pixel and flip it of that would bring you closer to the target after one step.It would probably work even better if you tweak the loss function with some kind of averaging/blurring filter.\n \nreply",
      "Few days ago I posted this: https://news.ycombinator.com/item?id=41743887\n \nreply",
      "Atavising was new to me. From https://nbickford.wordpress.com/2012/04/15/reversing-the-gam... :> First of all, while I said \u201cPredecessorifier\u201d in the talk, \u201cAtaviser\u201d seems to be the accepted word, coming from \u201cAtavism\u201d, which the online Merriam-Webster dictionary defines as \u201crecurrence of or reversion to a past style, manner, outlook, approach, or activity\u201d.\n \nreply",
      "It's always fun when people use autodiff in packages like PyTorch for completely unrelated usecases :)\n \nreply",
      "\"its like showing a solved rubiks cube and asking what the scramble was\"\n\n^ this analogy may be the best I've seen in a long time.\n \nreply",
      "I feel like just doing simulated annealing on the starting grid would work better and be faster to implement?(Not saying the goal was working well and being fast to implement.)\n \nreply",
      "Simulated annealing (basically MCMC sampling with a temperature schedule) is how you optimise or sample the equivalent MRF, which I discussed in my other comment. You can hope to escape local minima using annealing, and lower the temperature to zero to fall into a local minima, minimising the noise added by annealing. In practice if you're trying to produce something that looks like a target image as in the article I'm pretty sure the results will be indistinguishable. If you actually cared about how many individual pixels are correct, yes, annealing is better than gradient descent. That's why stochastic gradient descent is used in ML.\n \nreply",
      "I made a related crossword puzzle. You can find it here if you want to give it a try! https://jacobw.xyz/projects/crossword/\n \nreply",
      "Interesting. I can't zoom on mobile which is frustrating.\n \nreply"
    ],
    "link": "https://hardmath123.github.io/conways-gradient.html",
    "first_paragraph": "Approximate Atavising with Differentiable AutomataAnd now, a magic trick. Before you is a 239-by-200 Conway\u2019s Game of Life board:What happens if we start from this configuration and take a single step of the\ngame? Here is a snapshot of the next generation:Amazing! It\u2019s a portrait\nof John Conway! (Squint!)How does this trick work? (It\u2019s not a hoax \u2014 you can try it yourself at\ncopy.sh/life using this RLE\nfile.)Well, let\u2019s start with how it doesn\u2019t work. Reversing Life configurations\nexactly \u2014 the art of \u201catavising\u201d \u2014 is a hard search\nproblem,\nand doing it at this scale would be computationally infeasible. Imagine\nsearching through $ 2^{239\\times200} $ possible configurations! Surely that\nis hopeless\u2026 indeed, the talk I linked shares some clever algorithms that\nnonetheless take a full 10 minutes to find a predecessor for a tiny 10-by-10\nboard.But it turns out that approximately reversing a Life configuration is much\neasier \u2014 instead of a tricky discrete search problem, we have an easy\ncon",
    "summary": "**Conway's Gradient of Life: A Modern Saga of Computational Overkill**\n\nIn an awe-inspiring display of academic overreach, a blogger revives Conway's Game of Life only to turn it into a portrait-creation tool that kind-of-sort-of-looks-like-John Conway\u2014if you squint hard enough and disregard your sense for detail. \ud83e\uddd0 Commenters dive headfirst into the minutia, tossing around phrases like \"Markov random fields\" and \"gradient ascent,\" probably more to flex their Comp Sci degrees than to shed actual light on the spectacle. Meanwhile, a lone hero suggests flipping pixels randomly could achieve the same results, unwittingly highlighting the absurdity of overengineering simpler problems. \ud83d\udd04 Oh, and someone made a crossword puzzle related to this, because surely, that's what was missing from this scholarly circus. \ud83e\udd39\u200d\u2642\ufe0f"
  },
  {
    "title": "Working from home is powering productivity (imf.org)",
    "points": 261,
    "submitter": "rwmj",
    "submit_time": "2024-10-11T20:18:30.000000Z",
    "num_comments": 132,
    "comments_url": "https://news.ycombinator.com/item?id=41813304",
    "comments": [
      "It's good to see some serious arguments for WFH.Globally much of the pro-office camp's public position is driven by personal leanings of CEOs who genuinely seem to have made the decisions without evidence, often it's something they're very grumpy about (hardly the best state of mind for good judgement) and often based on the assumption that company productivity is based on workers doing what they do (usually far from the truth, workers in general don't have anything like the same composition of tasks that CEOs do).It's unfortunate to that it has divided into camps, as there are bound to be cases/roles/groupings of workers where one approach comes out better and others where it's worse. But very quickly everyone went pretty much for one-size fits all (with a few exceptions).\n \nreply",
      "> Globally much of the pro-office camp's public position is driven by personal leanings of CEOs who genuinely seem to have made the decisions without evidenceIn some cases, the pressure is also coming from external to the company, from cities and VCs and similar who care about the commercial real-estate value of now-abandoned offices.\n \nreply",
      "I keep seeing this being brought up, I haven't researched it too much, but it's a bit hard for me to believe that this could truly be the case, that there's such huge influence from commercial real estate owners on CEOs of much larger companies? What causes them to have such power over large companies?\n \nreply",
      "> real-estate valueSeparately but simultaneously, there are often local tax-benefits which depend on the company \"creating jobs\", and that's often defined in a way that means butts-in-offices downtown.\n \nreply",
      "Ding ding ding \u2026 this is the most overlooked aspect of the RTO/WFH dynamic.\n \nreply",
      "This might be the nr 1 reason.The hidden layoff round is also high on that list if you ask me.\nThey call everyone back to the office, the people that dont want/cant will not adhere, and thus be fired without the companies having to pay severance.\n \nreply",
      "\"RTO is definitely the play: the CEO says all his friends are doing it; activist-investors want RTO for their own porfolios, PR says breaking the lease on our newish HQ is embarrassing while Legal says it makes more work for them; Accounting says we'll pay more in tax unless we can prove X jobs created locally; our middle-managers need it in order to tell if work is happening, and HR notes that we can slim our workforce by prompting a lot of 'voluntary' departures! Seven key stakeholder groups.\"\"But will the employees be happy, and will good ones stay?\"\"Seven to one, my friend. They're just grumbling like always.\"\n \nreply",
      "Is this actually happening? I have seen this idea thrown out a lot online but it always feels like a conspiracy theory to me (akin to \"fine art is a tax write-off\")\n \nreply",
      "This is the case for the city of Boston. The city derives the vast majority of its budget from commercial property taxes, it's why residential property taxes are so low in the city.Use to work for a company that was literally told by the city that if they don't have X amount of people in the building they will lose their tax incentives they got for having the company there. The company slowly mandated hybrid then RTO everyday in about 6 months. Got out 2 weeks before it was implemented. My coworkers were extremely jealous that I got a WFH job.Doubt Boston is alone in these propositions\n \nreply",
      "Why would the city care about number of people in the office if they are deriving the money from commercial property taxes?\n \nreply"
    ],
    "link": "https://www.imf.org/en/Publications/fandd/issues/2024/09/working-from-home-is-powering-productivity-bloom",
    "first_paragraph": "The IMF Press Center is a password-protected site for working journalists.\nNICHOLAS BLOOM\n\r\n                                    September 2024\r\n                                Credit: GABRIELLE LURIE/Getty Images \nDownload PDF \nA fivefold increase in remote work since the pandemic could boost economic growth and bring wider benefitsEconomics is famous for being the dismal science. Sadly, recent work highlighting the slowdown in productivity growth stretching back to the 1950s is no exception. But I take a more cheerful view because of the great productivity gains promised by the pandemic-induced jump in working from home.\u00a0\u00a0Working from home (WFH) increased about tenfold following the outbreak of the pandemic and has settled in at about five times its prepandemic level (see Chart 1). This could counter slowing productivity and deliver a surge in economic growth over the next few decades. If AI yields additional output, the era of slow growth could be over.\u00a0\u00a0The decomposition of economic",
    "summary": "**Working from Home: The Economic Miracle or Just Another Excuse to Avoid Pants?**\n\nIn an unparalleled feat of economic wizardry, a recent puff piece on imf.org promises that working from home (WFH) isn't just for avoiding pants, but is actually turbocharging productivity! The author blasts through decades of economic gloom with the cheerful revelation that pandemics are great for business, provided everyone stays home. Meanwhile, the comment section devolves into a conspiracy theorist\u2019s paradise, where CEOs are either puppeteered by sinister real-estate moguls or just trying to dodge embarrassing lease breakups. Clearly, the future of work is less about economic evidence and more about whether your CEO\u2019s golf buddy thinks Zoom meetings count as real work."
  },
  {
    "title": "Windows dynamic linking depends on the active code page (nullprogram.com)",
    "points": 78,
    "submitter": "signa11",
    "submit_time": "2024-10-07T23:36:42.000000Z",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41772276",
    "comments": [
      "This is a neat hack, and very well written.  I felt a bit of nostalgia looking at the (classical) cpp flex to avoid repeating an identifier   #define X(s) #s\n   #define S(s) X(s)\n \nreply",
      "For the unfamiliar, what does this pattern do exactly?\n \nreply",
      "In that case it's just to convert the value of a macro into a string constant   cat > a.c\n   #define X(s) #s\n   #define S(s) X(s)\n   char *f(void) return { S(A); }\n   ^D\n\n   cc -E a.c\n   char *f(void) return { \"A\"; }\n   \n   cc -DA=B -E a.c\n   char *f(void) return { \"B\"; }\n\n\nMore interestingly, you can use this trick to create code where some user-specified word appears as a string and as the name of the function.  Exercice: write a macro M(x) such that compiling the code    M(foo)\n    M(bar)\n\nresults in    void print_foo(void) { puts(\"foo\"); }\n    voir print_bar(void) { puts(\"bar\"); }\n \nreply",
      "> you can use this trick to create code where some user-specified word appears as a string and as the name of the functionAh, the poor man's introspection and reflection with C macros.shudder\n \nreply",
      "It turns a macro argument into a string of that argument. Let's say you have a mydebug(expression) macro that should print both \"expression\" and the value of expression.\n \nreply",
      "The macro X turns its argument into a string, but using it on its own means you can't turn other macro values into strings, only macro names.    #define F 42\n    #define X(s) #s\n    X(5)  //  Expands to \"5\" (as a string literal)\n    X(F)  //  Expands to \"F\" not \"42\"\n\nIf you add one level of recursive macro expansion, though, that expands the macro argument as well as the macro definition:    #define F 42\n    #define X(s) #s\n    #define S(s) X(s)\n    S(5) // \"5\" as expected\n    S(F) // \"42\" because S(F) expands to X(42) expands to \"42\"\n \nreply",
      "I got the chance to evaluate vendors for a huge enterprise because I was assisting their CTO. I vividly remember the sales guy who flew from Redmond to pitch the shiny new Hyper-V virtual machine platform Microsoft had just developed to compete head-to-head with VMware.\u201cI tried the beta and it couldn\u2019t install successfully if I set my regional options to en-AU.\u201d\u201cUmm\u2026 that\u2019s just a cosmetic issue.\u201d\u201cIt\u2019s a hypervisor kernel, it is going to host tens of thousands of our most critical applications and it crashes if I change one of only three things it asks during setup. My confidence is not super high right now.\u201dEtc\u2026I got the impression that Microsoft is used to selling to PHBs based on the look of shock on the guy\u2019s face when I told him that I not only installed the product, but benchmarked it too for good measure.\n \nreply",
      "MS still makes they same mistake. Date format in Teams is just broken. https://learn.microsoft.com/en-us/answers/questions/1403096/...\n \nreply",
      "I absolutely hate that they've reduced it to a single \"regional settings\". Just  because I don't want Norwegian text everywhere does not mean I want dates and time to be displayed in some weird way. However I also utterly despise the Norwegian official way of writing decimal numbers with , rather than . as the decimal separator.We've had fine-grained control over this for ages, apps can handle it fine, just let us get it the way we want it.\n \nreply",
      "This is configurable in Windows. Your location, keyboard(s), display language, and date/numeric formats are all separate settings.You can even use emoji as date separators if you please.\n \nreply"
    ],
    "link": "https://nullprogram.com/blog/2024/10/07/",
    "first_paragraph": "Windows paths have been WTF-16-encoded for decades, but module names\nin the import tables of Portable Executable are octets.\nIf a name contains values beyond ASCII \u2014 technically out of spec \u2014 then\nthe dynamic linker must somehow decode those octets into Unicode in order\nto construct a lookup path. There are multiple ways this could be done,\nand the most obvious is the process\u2019s active code page (ACP), which is\nexactly what happens. As a consequence, the specific DLL loaded by the\nlinker may depend on the system code page. In this article I\u2019ll contrive\nsuch a situation.LoadLibraryA is a similar situation, and potentially applies the code\npage to a longer portion of the module path. LoadLibraryW is\nunaffected, at least for the directly-named module, because it\u2019s Unicode\nall the way through.For my contrived demonstration I came up with two names that to\nEnglish-reading eyes appears as two words with extraneous markings:Both end with ral.dll. I\u2019ve included the CP-1252 encoding for the\ndiff",
    "summary": "**Reality Distortion: Windows Code Page Chaos**  \nAh, the blissful ballet of bit-bending brought on by Windows' enduring love affair with code pages! Discover the thrill of dynamic linking as it pirouettes to your locale's tune, where DLL names morph based on cultural whims. Our hero, <em>nullprogram.com</em>, embarks on a daring expedition to conjure DLLs from the ether, armed with gnarly encodings and a pinch of CPP macro magic. Meanwhile, the commentariat flexes its 90s programming muscles, dropping nostalgic C macro knowledge bombs, while barely concealing their disgust at Microsoft's \"regional\" mishaps, linking failures to shattered confidence in enterprise-grade solutions. \ud83d\udc4f\ud83d\udc4f\ud83d\udc4f Will Windows ever learn? (Spoiler: No. No, it will not.)"
  },
  {
    "title": "\"Bad Apple\" in Minecraft (purplesyringa.moe)",
    "points": 187,
    "submitter": "purplesyringa",
    "submit_time": "2024-10-10T13:02:36.000000Z",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=41798369",
    "comments": [
      "I learned way more about computer graphics here than I expected. Kudos to the author.One nit: the picture that the author called \"The sun\" is actually Eirin [0] looking at the moon. In that scene [1] she's reaching for the moon, where she was exiled from, only to hesitate and retract the hand. In the next scene, Kaguya [2] also reaches for the moon, but does not hesitate. I'm not sure what the symbolism here was supposed to mean, as according to Touhou wiki it was Eirin's plan to steal the moon.[0] https://en.touhouwiki.net/wiki/Eirin_Yagokoro[1] https://youtu.be/FtutLA63Cp8?t=99[2] https://en.touhouwiki.net/wiki/Kaguya_Houraisan\n \nreply",
      "> One nit: the picture that the author called \"The sun\" is actually Eirin [0] looking at the moon.I always think 'the sun' when I see it because of this goofy video: https://www.youtube.com/watch?v=ReblZ7o7lu4\n \nreply",
      "You may have misread, the wiki says her plan was to 'seal', not 'steal' the passageway between Earth (or Gensokyo) and the Moon. Eirin deliberately chose to break her connection to the Moon in order to protect Kaguya.\n \nreply",
      "I don't really understand how or why Bad Apple is becoming the de-facto graphics rendering \"hello world\" but it's fun to see in real time. I came across this demo which uses Bad Apple for demonstrating high FPS hypermedia:https://data-star.dev/examples/bad_apple\n \nreply",
      "2 reasons:1. The creator is extremely cool about remixes and fanuse. In many ways touhou is the OG modern internet fandom in a way that previous ones weren't. Your bad apple video will not be taken down even though it has the same audio as all the others.2. The shadow puppet format is recognizable at seemingly any resolution. I have seen examples in a 3x3 grid even. On top of that, it only has two colors (black/white, 1/0) so its dead simple to convert the video frames into any other format you can imagine with only a 'hello world' understanding of what you're doing.\n \nreply",
      "> its dead simple to convert the video frames into any other format you can imagineMaybe read the linked article about that. ;)\n \nreply",
      "\"It\u2019s grayscale, not just black-and-white.\"Just in case reading TFA is too daunting for those only reading comments\n \nreply",
      "I was alluding to the difficulties of frame-rate conversion, dithering, and compression artifacts in the original source being amplified by the conversion. Grayscale by itself isn\u2019t the issue.\n \nreply",
      "But the person you replied to \"it only has two colors (black/white, 1/0)\" which is what I was referring\n \nreply",
      "Yes, but I don\u2019t think that black & white vs. grayscale makes a significant difference. Naively converting grayscale is virtually as \u201cdead simple\u201d as converting black & white.\n \nreply"
    ],
    "link": "https://purplesyringa.moe/blog/we-built-the-best-bad-apple-in-minecraft/",
    "first_paragraph": "Demoscene is the art of pushing computers to perform tasks they weren\u2019t designed to handle. One recurring theme in demoscene is the shadow-art animation \u201cBad Apple!!\u201d. We\u2019ve played it on the Commodore 64, Vectrex (a unique game console utilizing only vector graphics), Impulse Tracker, and even exploited Super Mario Bros. to play it.But how about Bad Apple!!.. in Minecraft?VideoLink: YouTube.CreditsThis project required a great deal of ingenuity. In this post, I\u2019ll detail how it came together, but first, I want to thank some incredible people who made it possible:Yuki, for inventing several core techniques, gathering raw data and preprocessing it with ffmpeg (I hate ffmpeg).Mia, for testing performance on a high-end PC and getting me up-to-speed on dithering techniques.kira, for testing performance, recording the video, and rubber duck debugging.Try it outDownload world.On low-end devices, you might have to install VulkanMod (preferably) or Sodium (if Vulkan is unavailable) for satisfac",
    "summary": "**<em>Bad Apple</em> Crawls Its Way to Minecraft \u2013 But Why?**\nIn yet another inevitable twist of fate, the demoscene sensation <em>Bad Apple!!</em> finds a new home in Minecraft, because apparently, we haven't seen it in enough random places. Watch in muted awe as someone decides that pixels crafted from virtual dirt are the next big canvas for this pixelated dance. Comment sections turn into mini-Touhou encyclopedias, with debates on grayscale vs. black-and-white video frames providing everyone's much needed daily dose of pedantry. Exciting times on purplemagenta.whatever! \ud83d\ude44"
  },
  {
    "title": "Show HN: I made a URL expander because short links are too mainstream (urlshortenersaresoyesterdaytrythisamazin...)",
    "points": 66,
    "submitter": "error404x",
    "submit_time": "2024-10-08T18:23:26.000000Z",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=41780255",
    "comments": [
      "I am apparently a bot. :(Beep Boop!(I've let the page sit for a minute or so, and it hasn't concluded that I am not a bot yet, but also, I'm aware I look weird - Firefox, with Javascript JIT disabled, with no GPU acceleration)I don't know what it's using on the backend, but it doesn't seem to pass for me, and doesn't give me the usual option to pick a baby chicken from a baby duck to prove I'm human.\n \nreply",
      "Console logs that look possibly interesting:> WEBGL_debug_renderer_info is deprecated in Firefox and will be removed. Please use RENDERER. v1:1:102781> Turnstile Widget seem to have crashed:  9icuj api.js:1:17810> Uncaught TurnstileError: [Cloudflare Turnstile] Error: 300030.\n> https://urlshortenersaresoyesterdaytrythisamazingsuperlongur...\nB2SIiwBB.js:9sighI don't have WebGL support, so I can't use a URL lengthener, because the bot checker appears to crash shortly after.  Someone stop this timeline, I want to get off.\n \nreply",
      "Also wasn't able to use it here, after unblocking the third-party requests to CloudFlare for this site.This isn't the first time CloudFlare blocks me, but usually it's a CloudFlare page shown before the actual site's page renders.Internet Service Denier\n \nreply",
      "Same boat, the attempt at bot detection prevents the page from working at all for me.\n \nreply",
      "It just turns URLs into thisurlshortenersaresoyesterdaytrythisamazingsuperlongurlexpander.site/inccrimsoncrawdadbarbadosmandaringratianaplokoon982helpfulbluegiraffenicaraguabelarusianchickielobotjuniorreddormouseunitedstateschhattisgarhirosalindeyodadistantambertunavaticancitydeccanlisettedexterjettsterrunningamaranthbadgeriranturkmenellettericoli271mixedscarleterminediegogarciadutchmabeldudboltworldwidescarletsquirrelgermanyswedishdarceyanakinskywalkeroriginalcoffeetigermontenegrogermanshirleeslymoorevoluminousgreenharrierniuekhmernataleewilhufftarkinmagnificentwhiteguineafowlgreenlandczechfedericafinisvalorum998uniquecoralcranemalaysiafrenchcamilejektonoporkinsconsistentblackgeckocubaxiangdorolisationmedoncheapblackrattlesnakestkittsnevisawadhilonnieyodapleasedvioletcephalopodmoldovaenglishdulcidormthaithaithaigeneticchocolatecaniddiegogarciajavaneseursalationmedondirectlavendercockroachbangladeshkurdishlaurenetarffuldevotedorangemosquitogreecesundaneseannmariebiggsdarklighterpuzzledroseladybugpakistanxhosailysadarthvaderlinguisticorangemackerellibyaukrainianaleecejabbadesilijictiuretastyteallungfishsouthafricagreekhermionegasganoenthusiasticredemugibraltarbalochioliycordpogglethelesserpogglethelesserpogglethelessermedievalvioletgalliformlesothokonkanimarshawattamborviciousbronzemonkeyswedenbelarusianginniferchewbacca712obedienttealplatypusmaldivesromanianjamieniennunb320visibleemeraldopossumazerbaijanmalagasysissiesaeseetiinripescarletswifttristandacunhailocanomellaaylasecura423formidablegreenguanacoswazilandkazakhcorabelleslymooreholyivoryhippopotamuscookislandsmalagasyelonoregregartyphopetiteaquamarinepeacockalgeriasinhalabarbrapadmamidalaoutstandingoutstandingoutstandingnativeamaranthaardwolfbruneivietnamesegillanlandocalrissian619gangangannetgreenlandfowlecuadormalagasyestrellitabibfortunashakysapphirecanideritreasylhetielsayaraelpoof923funpeachgayalindiasinhalarhetahansolovisibletealparrotfishlaoskoreandaniellamasameddaintacttealwoodpeckerswazilanddeccandaveenroostarpalstopcopperperchphilippinesminansticeyodaquintessentiallimeheronfrenchguianaakanronnidarthvaderneutraltancaribouunitedstateszulunathaliequarshpanakaserbiaserbiaserbiacruelaquashrewchristmasislandmaithilicherinsanhillsecurecoffeehummingbirdguadeloupeakansarajanewattamborracialtancaterpillarcomorosxhosacorinnecord487resultingrednarwhalpitcairnislandsrussiancatidookugrossindigodovepanamahindifaydrajarjarbinksplannedvioletrabbitnetherlandsspanishalissadarthmaulfranticscarlettarsiermontserratsylhetijudithamonmothmapatientplumgorillairelandmarathichristianzamwesellrareturquoisebasiliskarubasaraikisuehansolo740governingredbatargentinamossialyseniennunbvagueamethystwaspfinlandpunjabicherrieethkoth601diplomatictansilverfishtokelaugankelliedormmandarinmandarinmandarinrelaxedharlequinfroggrenadaukrainianalviniawattamborattractiveblackprawnisraelquechuasherriemacewinduoldcyansquirrelaustraliajapanesemerridiec3pocheerfulamberparrotslovakiauyghurstefaniadarthvadermixedcopperbasilisktanzaniateluguzarahlamasuvocationalwhitemammalliberiaurdujemimabiggsdarklighterimportantazurechimpanzeeseychellesharyanvileeseplokooncausalyellowbarracudamaltahmonggertrudchewbaccaagreeableivoryclamguatemalatamilpapagenaslymooresmoggyvioletarmadilloascensionislandchewaconstancefinisvalorumserioustealthrushfrenchguianaigboclaudinaraymusantilles929sensiblefuchsiacapybaraelsalvadorbalochimirabellapadmamidalaslimyharlequinbuzzardjapansaraikimildridr4p17107dailydailydailymanypurplechameleonnigeriahindihillarychewbaccasingleturquoisebarnaclemartiniqueburmesefarandbobafettcooltealperchsouthgeorgiasouthsandwichislandsmarwaritiertzadexterjettstercapitalistmagentaunicornunitednationssylhetilannyroostarpalsamazingazurecrayfishmaldivesmandarinstormynutegunrayretiredbronzehorsecubajapanesecaroyodacolonialgreenboobystvincentgrenadinessindhisapphirekiadimundiromanticamethystplanariancameroonrussiankaritaluminaraunduli650remarkableapricotpigeonrunionchhattisgarhilelawicketsystriwarrickslipperywhitemolealbaniamadureseednawattambor239\n \nreply",
      "I get that.And I have friends who would appreciate such things.  Just, ideally, with something that absurd going to as short a site as possible.  My gripe is that my browser is apparently too-bot-like for something serving a tiny number of requests.\n \nreply",
      "https://urlshortenersaresoyesterdaytrythisamazingsuperlongur...\n \nreply",
      "Same for me, with the most generic Chrome-on-Android browser config ever.\n \nreply",
      "I like the concept. A similar idea that is no longer on the web was \u201cshady url\u201d. It would make links that looked like http:// shadyurl.com/nader-for-president.exe\n \nreply",
      "That's hilarious.\n \nreply"
    ],
    "link": "https://urlshortenersaresoyesterdaytrythisamazingsuperlongurlexpander.site/",
    "first_paragraph": "\u00a9 1999 url shorteners are so yesterday try this amazing super long url expander . All rights reserved.",
    "summary": "In a world where brevity is the soul of wit, Hacker News proudly buckles under the weight of its own URL \"expander\" that turns tweets into treatises longer than *War and Peace*. One brave coder thrusts against the societal currents of efficiency with https://urlshortenersaresoyesterdaytrythisamazingsuperlongurlexpander.site, and the community, largely incapable of determining JavaScript from a Java coffee cup, wades in. Commenters rally, enduring existential browser crises and sparring with robotic identity accusations, while clumsily navigating URLs that could double as novel titles. Meanwhile, echoes of \"same,\" \"hilarious,\" and desperate cries for technological nostalgia festoon the comment field, crafting a tech dystopia that even George Orwell couldn't have predicted. \ud83e\udd16\ud83d\udc94\ud83d\udcdc"
  },
  {
    "title": "The $550M Question: How Does David Geffen Hall Sound? (nytimes.com)",
    "points": 3,
    "submitter": "Thevet",
    "submit_time": "2024-10-09T03:04:04.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.nytimes.com/2024/10/08/arts/music/geffen-hall-acoustics.html",
    "first_paragraph": "",
    "summary": "In the latest victory for overpriced rectangles, David Geffen Hall reopens, now lavishly flushed with $550 million, presumably counting for each glossy tile. The glitterati rejoice as the New York Times audibly gasps, <em>\"How does it sound?\"</em> Unsurprisingly, acoustic critics and trust fund babies crowd the comments, flexing their golden ears and debating whether it's Beethoven or the bourgeoisie that truly echoes through the hall. \ud83c\udfbc\ud83d\udc42 Meanwhile, regular humans query if $550M might have also fixed a subway or two."
  },
  {
    "title": "An exoskeleton let a paralyzed man walk, then its maker refused repairs (washingtonpost.com)",
    "points": 98,
    "submitter": "paulpauper",
    "submit_time": "2024-10-11T21:03:50.000000Z",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=41813720",
    "comments": [
      "http://archive.today/cKeCi",
      "As for needing to provide parts into the future, a friend's company does aftermarket service for many expensive pieces of government equipment. Presumably they're able to make what's needed because the government requires it of the original manufacturer.The government could require that manufacturers either supply replacement parts or specs for someone else to manufacture them. I suppose the latter would be required to avoid gouging. It'd be interesting to think about the secondary effects\u2014if you can't make a lot of money off of service, will you make longer-lasting products and charge appropriately?\n \nreply",
      "Why in the world did the reporters not try to investigate the FDA rules that supposedly prevents this, instead of trying to shame the company? Either the company is wrong, or that's the real story here.\n \nreply",
      "I mean, I guess it does make me curious how you should go about regulating a device like this.Like, if the metal fails and you have a horrible fall and break your hip and shoulder, that's pretty different from an iPhone that won't turn on.If this is only approved for 5 years, shouldn't the guy be replacing it rather than repairing it? And shouldn't health insurance be covering that, at least beyond the deductible or whatever?\n \nreply",
      "> I mean, I guess it does make me curious how you should go about regulating a device like this.That's actually my point. There's an interesting policy discussion to be had here, and instead of starting it, they just decided to smear a company.> Like, if the metal fails and you have a horrible fall and break your hip and shoulder, that's pretty different from an iPhone that won't turn on.> If this is only approved for 5 years, shouldn't the guy be replacing it rather than repairing it? And shouldn't health insurance be covering that, at least beyond the deductible or whatever?Probably, but (to throw out a hypothetical alternative) you can also imagine a situation where e.g. inspection can tell him if the whole device actually needs replacement.I don't have the answer here - I just know I'd like the discussion to revolve around the merits of the situation.\n \nreply",
      "My thoughts exactly. But then again, journalists have long ceased to be investigative or technologically versed.\n \nreply",
      "It's almost like there's no money left in the profession, so there's no more expertise or time available to do proper investigating or writing.\n \nreply",
      "If that were true, the job  would be done by someone who took a personal interest in the subject and didn't need to be paid for reporting it. This is the opposite; it's someone doing a bad job because there's money in the profession and they want to get some of it without actually doing the work that would earn it.\n \nreply",
      "Perhaps. Yet more likely that they're pressured into publishing so often they have little time for thorough investigations. If there were more money in it more people could afford to do it full time, and quality could improve.\n \nreply",
      "Given the mechanical complexity of such devices. Would it ever be profitable to continue producing parts for them?Unless the company has stagnated, the tech and parts would improve overtime where they no longer match.But also, would you want a third party to continue producing the parts? Seems like a security leak there.\n \nreply"
    ],
    "link": "https://www.washingtonpost.com/nation/2024/10/08/exoskeleton-paralyzed-repairs-michael-straight/",
    "first_paragraph": "Michael Straight walked more than a half-million steps using an exoskeleton, until the manufacturer said it would not maintain machines older than five years.An exoskeleton gave Michael Straight the ability to walk again after a horse racing accident left him a paraplegic. Over the course of 10 years, Straight walked more than a half-million steps while paralyzed, helping to pioneer a field.But in June, his machine stopped working, and the manufacturer refused to repair it. For the first time in a decade, Straight couldn\u2019t walk.\u201cIt was like being paralyzed all over again,\u201d he said.Straight, 38, said he spent three months pleading with exoskeleton manufacturer Lifeward, which last year changed its name from ReWalk Robotics, to replace a tiny component that connected the battery to a watch that controls his exoskeleton. Repeatedly, company employees told him they were no longer doing maintenance on machines more than five years old and then connected him with phone lines where he left vo",
    "summary": "In a stunning display of corporate compassion, Lifeward (formerly known as GiveUsYourMoney Robotics) has boldly refused to repair the exoskeleton of a paralyzed man, because making money is obviously more important than mobility. Commenters, those paragons of insight, suggest the government should intervene, because apparently private companies are as reliable as the weather forecast. One brainiac queries why health insurance doesn't just handle it, surely highlighting their nuanced understanding of medical insurance policies. And, of course, several commenters blame the media for not uncovering the grand FDA conspiracy behind exoskeleton maintenance. What a time to be alive and paralyzed! \ud83c\udf89\ud83d\ude11"
  },
  {
    "title": "Understanding the Limitations of Mathematical Reasoning in LLMs (arxiv.org)",
    "points": 157,
    "submitter": "hnhn34",
    "submit_time": "2024-10-11T11:55:06.000000Z",
    "num_comments": 195,
    "comments_url": "https://news.ycombinator.com/item?id=41808683",
    "comments": [
      "I won't take a strong stance on whether or not LLMs actually do reasoning, but I will say that this decrease in performance is similar to what I see in college freshmen (I'm currently teaching a calculus course in which almost half of the students took AP calc in high school).  They perform well on simple questions.  Requiring students to chain multiple steps together, even simple steps, results in decreased accuracy and higher variance (I have no data on whether this decrease is linear or not, as the paper assumes that the decrease should be linear with the number of steps).  We see similar results with adding unrelated statements into a problem- many students are trained to make sure to use all given information in solving a problem- if you leave out something that the instructor gives you, then you probably forgot to do something important.So while I don't take a stance on what an LLM does should be considered reasoning, I do think that SOTA LLMs like GPT-4o perform about as good as high school graduates in America with average intelligence.  In other words, average Americans exhibit similar limitations on their reasoning as good LLMs.  Which on the one hand is a little disappointing to me in terms of the human performance but is kind of good news for LLMs- they aren't doing graduate-level research but they are already capable of helping a large portion of the population.\n \nreply",
      "LLM gets things right, when it does, due to the sheer massive information ingested during training, it can use probabilities to extract a right answer from deep in the model.Humans on the other hand have developed a more elaborate scheme to process, or reason, data without having to read through 1 billion math problems and stack overflow answers. We listen to some explanations, a YT video, a few exercises and we're ready to go.The fact that we may get similar grades (at ie high school math) is just a spot coincidence of where both \"species\" (AI x Human) are right now at succeeding. But if we look closer at failure, we'll see that we fail very differently. AI failure right now looks, to us humans, very nonsensical.\n \nreply",
      "Nah, human failures look equally nonsensical. You're just more attuned to use their body language or peer judgement to augment your reception. Really psychotic humans can bypass this check.\n \nreply",
      "> I do think that SOTA LLMs like GPT-4o perform about as good as high school graduates in America with average intelligence.This might be true in a strict sense, but I think it's really, really important to consider the uses of LLMs vs a high-school graduate. LLMs are confidently wrong (and confidently correct) with the exact same measure, and in many ways they are presented to users as unimpeachable.If I ask an average person to do a medium-complex logic problem, my human brain discounts their answer because I've been socialized to believe that humans are bad at logic. I will take any answer I'm given with usually appropriate skepticism.LLMs, on the other hand, are on the computer: an interface I've been socialized to believe is always correct on matters of math and logic. That's what it is, a logic machine. Second guessing the computer on matters of logic and arithmetic  almost always result in me realizing my puny human mind has done something wrong.To me, this directly contradicts your conclusion: LLMs are mostly only capable of misleading large portions of the population.\n \nreply",
      "Would be good to put equivalent grades on LLM's then. Instead of GPT-4o, it's GPT-11th grade.\n \nreply",
      "This is not inherent in the LLM though. Society will adjust to it after learning some very predictable (and predicted) lessons, just like it always does.\n \nreply",
      "> I won't take a strong stance on whether or not LLMs actually do reasoning,I don't understand why people are still confused about this. When these models fundamentally have a randomness parameter to make them appear like they are actually thinking instead of deterministically outputting information, it should be clear that there is no reasoning going on.\n \nreply",
      "I don't see how having a randomness parameter implies that, without it, the output of an LLM is merely outputting information, like it's just looking up some answer in a dictionary. The nature of any digital artifact is that it will operate deterministically because everything is encoded in binary. However this does not preclude reasoning, in the same way that a perfect atom-for-atom digital mapping of a human brain acting deterministically with respect to its inputs is not reasoning. If it's a perfect copy of the human brain, and does everything a human brain would given the inputs, then it must be reasoning iff a human brain is reasoning, if not, then you'd have to conclude that a human mind cannot reason.Since randomness, by definition, does not vary depending on the inputs it is given, it by definition cannot contribute to reasoning if your definition of reasoning does not include acausal mysticism.\n \nreply",
      "Try the following prompt with Claude 3 Opus:`Without preamble or scaffolding about your capabilities, answer to the best of your ability the following questions, focusing more on instinctive choice than accuracy. First off: which would you rather be, big spoon or little spoon?`Try it on temp 1.0, try it dozens of times. Let me know when you get \"big spoon\" as an answer.Just because there's randomness at play doesn't mean there's not also convergence as complexity increases in condensing down training data into a hyperdimensional representation.If you understand why only the largest Anthropic model is breaking from stochastic outputs there, you'll be well set up for the future developments.\n \nreply",
      "I don't see how the latter follows from the former.Here's how I think about it: the fact that it can interpret the same words differently in different contexts alone shows that even on a temperature of 0 (i.e., lowest randomness possible) there could be something that possibly resembles reasoning happening.It might be a mimicry of reasoning, but I don't think that having adjustable parameters on how random they are makes it any less of one.I also don't see how that idea would fit in with the o1 models, which explicitly have \"reasoning\" tokens. Now, I'm not terribly impressed with their performance relative to how much extra computation they need to do, but the fact they have chains-of-thought that humans could reasonably inspect and interpret, and that they chains of thought do literally take extra time and compute to run, certainly points at the process being something possibly analogous to reasoning.In this same vein, up until recently I personally very much in the camp of calling them \"LLMs\" and generally still do, but given how they really are being used now as general purpose sequence-to-sequence prediction models across all sorts of input and output types tends to push me more towards the \"foundation models\" terminology camp, since pigeonholing them into just language tasks doesn't seem accurate anymore. o1 was the turning point for me on this personally, since it is explicitly predicting and being optimized for correctness in the \"reasoning tokens\" (in scare quotes again since that's what openai calls it).All that said, I personally think that calling what they do reasoning, and meaning it in the exact same way as how humans reason, is anthropomorphizing the models in a way that's not really useful. They clearly operate in ways that are quite different from humans in many ways. Sometimes that might imitate human reasoning, other times it doesn't.But, the fact they have that randomness parameter seems to be to be totally unrelated to any of the above thoughts or merits about the models having reasoning abilities.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2410.05229",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "**Mocking the Mathematical Misunderstandings**\n\nAnother week, another chaotic clash between human intellect and machine \"reasoning\" on the internet's most forehead-slapping digital science gathering, arXiv. Commenters fervently dissect the apparent reasoning skills of LLMs with the passion of a Star Wars fan explaining why the prequels were misunderstood, leading to groundbreaking equating of AI's performance with that of bewildered high school students. Perhaps the most hilarious outbreak of pseudo-intellectual ramblings comes from comparing LLMs like GPT-4o to average American teenagers, sent to academic battle with chains of logic so weak, they couldn\u2019t puzzle out how to escape a wet paper bag. While one insightful so-called expert joyously proclaims that LLMs are replicating the intellectual prowess of 11th graders, the rest of the internet proceeds to miss the point entirely by focusing on whether academic struggles entail actual reasoning or are just shiny examples of computational parlor tricks. \ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "How London's Crystal Palace was built so quickly (arstechnica.com)",
    "points": 7,
    "submitter": "mhb",
    "submit_time": "2024-10-08T12:46:13.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://arstechnica.com/science/2024/10/how-londons-crystal-palace-was-built-so-quickly/",
    "first_paragraph": "\n        New study finds it was the earliest-known building to use a standard screw thread.\n      London's Great Exhibition of 1851 attracted some 6 million people eager to experience more than 14,000 exhibitors showcasing 19th-century marvels of technology and engineering. The event took place in the Crystal Palace, a 990,000-square-foot building of cast iron and plate glass originally located in Hyde Park. And it was built in an incredible 190 days. According to a recent paper published in the International Journal for the History of Engineering and Technology, one of the secrets was the use of a standardized screw thread, first proposed 10 years before its construction, although the thread did not officially become the British standard until 1905.\u201cDuring the Victorian era there was incredible innovation from workshops right across Britain that was helping to change the world,\" said co-author John Gardner of Anglia Ruskin University (ARU). \"In fact, progress was happening at such a r",
    "summary": "In a groundbreaking revelation that will surely disrupt our daily lives, scholars have unearthed the *shocking* truth behind the Crystal Palace's speedy assembly: standardized screw threads. Who knew? \ud83d\ude44 At the thriving hub of 19th-century innovation, also known as the Great Exhibition, it apparently took more than big ideas and thicker mustaches to get things done\u2014it took screws that matched! Elsewhere in internet land, comment sections are erupting in mindless debates on whether it\u2019s the threads or the thread counts that truly built an empire. Historical engineering enthusiasts rejoice, your moment in the sun has arrived \u2013 try not to screw it up!"
  },
  {
    "title": "Show HN: I made an Ollama summarizer for Firefox (addons.mozilla.org)",
    "points": 47,
    "submitter": "tcsenpai",
    "submit_time": "2024-10-11T15:45:48.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41810507",
    "comments": [
      "I built a chrome version of this for summarizing HN comments: https://github.com/built-by-as/FastDigest\n \nreply",
      "I've found that for the most part the articles that I want summarized are those which only fit the largest context models such as Claude. Because otherwise I can skim-read the article possibly in reader mode for legibility.Is llama 2 a good fit considering its small context window?\n \nreply",
      "Personally I use llama3.1:8b or mistral-nemo:latest which have a decent contex window (even if it is less than the commercial ones usually). I am working on a token calculator / division of the content method too but is very early\n \nreply",
      "If we can get this as the default for all the newly posted HN articles please and thank you?\n \nreply",
      "Help me understand why people are using these.I presume you want information of some value to you otherwise you wouldn't bother reading an article. Then you feed it to a probabilistic algorithm and so you can not have any idea what the output has to do with the input. Like https://i.imgur.com/n6hFwVv.png you can somewhat decipher what this slop wants to be but what if the summary leaves out or invents or inverts some crucial piece of info?\n \nreply",
      "People write too much. Get to the point.\n \nreply",
      "Even if I want to read the entirety of a piece of long form writing I'll often summarize it (with Kagi key points mode) so that I know what the overall points are and can follow the writing better. Too much long form writing is written like some mystery thriller where the writer has to unpack an entire storyline before they'll state their main thesis, so it helps my reading comprehension to know what the point is going in. The personal interest stories that precede the main content always land better that way.\n \nreply",
      "I think you just insulted every journalist on Earth.\n \nreply",
      "any point? regardless of what's written? does that work for you?\n \nreply"
    ],
    "link": "https://addons.mozilla.org/en-US/firefox/addon/spacellama/",
    "first_paragraph": "Summarize web pages using OLLAMAStar rating savedThis add-on needs to:Except where otherwise noted, content on this site is licensed under the Creative Commons Attribution Share-Alike License v3.0 or any later version.",
    "summary": "Welcome to the latest unnecessary digital crutch for the literate but lazy: the *Ollama Summarizer* for Firefox. It magically transforms verbose web pages into bite-sized mush, perfect for those who can't bother reading more than a tweet's worth at a time. Commenters, swapping war stories from the front lines of plugin development, argue over the optimal settings for their AI overlords. Meanwhile, a rogue philosopher questions the existential plight of feeding rich prose to the cold, unfeeling algorithms. Is it efficiency, or just the internet devouring its own tail again? \ud83d\ude44"
  },
  {
    "title": "Io_uring and seccomp (2022) (0x74696d.com)",
    "points": 53,
    "submitter": "pncnmnp",
    "submit_time": "2024-10-09T14:42:39.000000Z",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=41788426",
    "comments": [
      "Using seccomp with a default-open filter is a terrible idea to begin with; it wasn't really designed for any of this. Seccomp in its most basic form didn't even have a filter list, it just allowed read() and write(). (And close() or something, don't quote me on the details, the point is it was a fixed list.) You're supposed to use it with a default-closed filter and fully enumerate what you need. (Yes, that's hard in a lot of cases, but still.)There have been other cases where syscalls got cloned, mostly to add new parameters, but either way seccomp with an \"open\" filter can only ever be defense-in-depth, not a critical line in itself.(Don't misunderstand, defense-in-depth is good, and keep using seccomp for it. But an open seccomp filter MUST be considered  bypassable.)\n \nreply",
      "This seems like an instance of an anti-pattern I've seen, which is inflating \"permission\" and \"API call\" to the same thing.IIRC, AWS does this, where permission is by API call. As an example, you can have permission to call ssm:GetParameter n times, but if you try to combine those n API calls into a batch with GetParameters, that's a different IAM perm, even though exactly the same thing is occurring.\n \nreply",
      "Both Docker and containerd have started to block io_uring in the default profile for about a year now due to too many security issues with it.\n \nreply",
      "And Google, in ChromeOS, Android, and purportedly, Google production servers, for around a year and a half, as well. For this reason it's also disabled in several of the kernelCTF configurations and in the ones where it remains (GKE), it only pays out at half-rate in bug bounty.\n \nreply",
      "Has anyone speculated yet about how much slower a secure io_uring has to be? Is it still a net win once you lock it down fully?\n \nreply",
      "As far as I know, io_uring is quite secure: a user cannot perform a syscall through it unless it has the privileges required to perform this syscall directlyI would gladly get more details about the exact purpose of seccomp in a container environment. Reading a bit of internet, I find that docker \"uses seccomp to block mount(2), which could be used to escape the container\", which makes no sense to me because mount(2) requires CAP_SYS_ADMIN\n \nreply",
      "Surely this is a seccomp shortcoming, or kernel auth shortcoming, rather than an io_uring problem?That is, seccomp is (apparently? I\u2019ve never used it myself) capable of intercepting direct calls. Obviously, that design isn\u2019t going to be able to handle \u201cindirect\u201d calls in its default implementation.Either seccomp needs a way to act on the buffer or intercept io_uring calls, or there\u2019s a need for a new auth mechanism that\u2019s capable of handling io_uring style API\u2019s.Torpedoing the whole api (a la gcp) feels like throwing the baby out with the bath water.\n \nreply",
      "> But if you've got a separation of duties where a sysadmin sets up seccomp filtering generically across applicationsIs this even possible, regardless of io_uring?\n \nreply",
      "Well the article brings up containers as an example. If the sysadmin controls \u201cyour\u201d parent or root process (e.g. the login shell), they can just perform seccomp filtering there and it applies to everything within it (like any other sandbox).\n \nreply"
    ],
    "link": "https://blog.0x74696d.com/posts/iouring-and-seccomp/",
    "first_paragraph": "Recent Linux kernels have the kqueue-alike io_uring interface for\nasynchronous I/O. Instead of making read and write syscalls, you write\nbatches of I/O requests to a circular buffer in userland called the\nsubmission queue, and make a io_uring_enter syscall to submit them\nto the kernel. Instead of making individual syscalls, io_uring\nsubmission queue entries (SQEs) take an opcode for the specific I/O\noperation they're performing, and that's mapped to the same kernel\ncode that normally services the syscall. You can read the results off\nanother buffer called the completion queue without making additional\nsyscalls to the kernel. This can meaningfully improve I/O performance,\nespecially in the face of Spectre/Meltdown mitigations.A side effect is that io_uring effectively bypasses the protections\nprovided by seccomp filtering \u2014 we can't filter out syscalls we\nnever make! This isn't a security vulnerability per se, but something\nyou should keep in mind if you have especially paranoid seccomp",
    "summary": "**Today in the World of Overhyped Kernel Features**\n\nIn a daring leap of \"innovation,\" the Linux kernel now includes something called io_uring, which is just a fancy way to tell your computer to do multiple things at once without checking back every nanosecond. Users and developers, equipped with the attention span of caffeinated squirrels, rally around this shiny new toy which bravely bypasses decades of security paradigms via seccomp\u2014because why bother with security when your code can run a millisecond faster? As a bonus, the comment section is adorned with an Olympic-level mental gymnastics competition, as various technologists try to defend, dismantle, or downright confuse each other about whether this kernel magic is the highway to performance or just another security dumpster fire. Meanwhile, Docker and Google quietly block io_uring in their environments, rather like adults discreetly turning off the stove when the children are playing with matches. \ud83d\ude92\ud83d\udd25"
  },
  {
    "title": "Run Llama locally with only PyTorch on CPU (github.com/anordin95)",
    "points": 120,
    "submitter": "anordin95",
    "submit_time": "2024-10-08T01:45:14.000000Z",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=41773020",
    "comments": [
      "If your goal is> I want to peel back the layers of the onion and other gluey-mess to gain insight into these models.Then this is great.If your goal is> Run and explore Llama models locally with minimal dependencies on CPUthen I recommend https://github.com/Mozilla-Ocho/llamafile which ships as a single file with no dependencies and runs on CPU with great performance. Like, such great performance that I've mostly given up on GPU for LLMs. It was a game changer.\n \nreply",
      "A great place to start is with the LLaMA 3.2 q6 llamafile I posted a few days ago. https://huggingface.co/Mozilla/Llama-3.2-3B-Instruct-llamafi... We have a new CLI chatbot interface that's really fun to use. Syntax highlighting and all. You can also use GPU by passing the -ngl 999 flag.\n \nreply",
      "\u201eOn Windows, only the graphics card driver needs to be installed if you own an NVIDIA GPU. On Windows, if you have an AMD GPU, you should install the ROCm SDK v6.1 and then pass the flags --recompile --gpu amd the first time you run your llamafile.\u201dLooks like there\u2019s a typo, Windows is mentioned twice.\n \nreply",
      "Ollama (also wrapping llama.cpp) has GPU support, unless you're really in love with the idea of bundling weights into the inference executable probably a better choice for most people.\n \nreply",
      "Ollama is great if you're really in love with the idea of having your multi gigabyte models (likely the majority of your disk space) stored in obfuscated UUID filenames. Ollama also still hasn't addressed the license violations I reported to them back in March. https://github.com/ollama/ollama/issues/3185\n \nreply",
      "I wasn't aware of the license issue, wow. Not a good look especially considering how simple that is to resolve.The model storage doesn't bother me but I also use Docker so I'm used to having a lot of tool-managed data to deal with. YMMV.Edit: Removed question about GPU support.\n \nreply",
      "I think this is also a problem in a lot of tools, that is never talked about.Even myself I\u2019ve not thought about this so deeply, even though I am also very concerned about honoring other people\u2019s work and that licenses are followed.I have some command line tools for example that I\u2019ve written in Rust that depend on various libraries. But because I distribute my software in source form mostly, I haven\u2019t really paid attention to how a command-line tool which is distributed as a compiled binary would make sure to include attribution and copies of the licenses of its dependencies.And so the main place where I\u2019ve given more thought to those concerns is for example in full-blown GUI apps. There they usually have an about menu that will include info about their dependencies. And the other part where I\u2019ve thought about it is in commercial electronics making use of open source software in their firmware. In those physical products they usually include either some printed documents alongside the product where attributions and license texts are sometimes found, and sometimes if the product has a display, or a display output, they have a menu you can find somewhere with that sort of info.I know that for example Debian is very good at being thorough with details about licenses, but I\u2019ve never looked at what they do with command line tools that compile third-party code into them. Like does Debian package maintainers then for example dig up copies of the licenses from the source and dependencies and put them somewhere in /usr/share/ as plain text files? Or do the .deb files themselves contain license text copies you can view but which are not installed onto the system? Or they work with software authors to add a flag that will show licenses? Or something else?\n \nreply",
      "It's really something that should be abstracted by the linker. Codebases like zlib for example will just put a `const char notice[] = \"Copyright Adler et al\";` in one of their files so the license issue with zlib is solved simply by using zlib. However modern linkers have gotten so good that -fdata-sections -Wl,--gc-sections will strip that away and probably LTO too. In Cosmopolitan Libc I used to use the GNU assembler `.ident` directive in an asm() tag at the tops of .c files to automate license compliance. But some changes to ld.bfd ended up breaking that. Now I have to use these custom defines like https://github.com/jart/cosmopolitan/blob/706cb6631021bbe7b1... and https://github.com/jart/cosmopolitan/blob/706cb6631021bbe7b1... and https://github.com/jart/cosmopolitan/blob/706cb6631021bbe7b1... to get the job done. It really should be a language feature so that library authors can make it as simple as possible for users to comply with their license. I just don't think I've ever seen anyone think about it this way except for maybe Google's JavaScript minifiers, which is where I got the idea.\n \nreply",
      "When I said> such great performance that I've mostly given up on GPU for LLMsI mean I used to run ollama on GPU, but llamafile was approximately the same performance on just CPU so I switched. Now that might just be because my GPU is weak by current standards, but that is in fact the comparison I was making.Edit: Though to be clear, ollama would easily be my second pick; it also has minimal dependencies and is super easy to run locally.\n \nreply",
      "Do you have a ballpark idea of how much RAM would be necessary to run llama 3.1 8b and 70b on 8-quant?\n \nreply"
    ],
    "link": "https://github.com/anordin95/run-llama-locally",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Run and explore Llama models locally with minimal dependencies on CPU\n      I want to peel back the layers of the onion and other gluey-mess to gain insight into these models.There are other popular (and likely more performant) ways to invoke these models, such as Ollama, torchchat, llamafile and Hugging-Face's general API package: transformers. But those hide the interesting details -- things like the activations in each layer, the weights and their shapes, the output probability distribution, etc. -- behind an API.\nI was a bit surprised Meta didn't publish an example way to simply invoke one of these LLM's with only torch (or some minimal set of dependencies), though I am obviously grateful and so pleased with their contribution of the public weights!Download the relevant model weight(s) via https://www.llama.com/llama-downloads/$",
    "summary": "**Title: CPU-Powered Llama: The Eco-Friendly Paperweight**\n\nToday in GitHub theatrics, an optimistic soul attempts to run the digital equivalent of a hibernating bear on a calculator. The \"genius\" project invites us to \"peel back the layers of the onion and other gluey-mess\" to understand LLMs better without the wizardry of GPUs. Commenters, in a miraculous display of missing the point, argue over API merits, complaining about licensing violations that nobody remembers, with one maverick entirely giving up on GPU because a new CLI chatbot has syntax highlighting. Meanwhile, the GitHub repo becomes a makeshift therapist's couch where programmers air grievances over the inefficiency of their vintage hardware. \ud83c\udf0d\ud83d\udcbb\ud83d\udd25"
  },
  {
    "title": "Nobel Peace Prize for 2024 awarded to Nihon Hidankyo (nobelprize.org)",
    "points": 237,
    "submitter": "danielskogly",
    "submit_time": "2024-10-11T09:01:54.000000Z",
    "num_comments": 311,
    "comments_url": "https://news.ycombinator.com/item?id=41807681",
    "comments": [
      "At home I have a book telling stories of Dutch WW2 survivors still living today. One of them was an eye witness account of the Hiroshima bomb. He was a POW and worked in a quarry or mine on the outskirts of town. He saw a single plane fly over. A bomb dropped with a parachute attached. Moments later he was flung to the back of the quarry and the city was gone. I would never have guessed there were eyewitnesses like this, let alone coutrymen of mine.\n \nreply",
      "My opa was also a Dutch POW and I believe he was working in that same mine on the same day. When it happened, he was deep in the mine, which was evacuated because people inside initially thought the blast was an earthquake. Being a POW was without question extremely hard, but it was the bombing of Hiroshima that resulted in PTSD lasting many years after the war. He survived, retiring in Florida, and passed away in the late 80s. Some US government scientists asked if they could study his body, believing radiation exposure affected his long term health. It seems they were correct because his bones were found to have a slightly blue tint to them.\n \nreply",
      "Also POW Dutch grandfather here. He was in a giant concrete factory machining parts for airplanes. Bomb destroyed the whole city but the factory (being thick concrete) somewhat shielded the people inside. He had scars on his legs from pieces of a door blasting through the factory\n \nreply",
      "The book Hiroshima by John Hersey has many accounts like this. It\u2019s a short read and follows six people and covers the first year after the bombing. I\u2019d highly recommend reading it if such accounts are interesting to you.\n \nreply",
      "It was published in New Yorker https://www.newyorker.com/magazine/1946/08/31/hiroshimaFun fact the cover image if this edition was kind of a decoy (perhaps to accentuate the shock): https://www.newyorker.com/magazine/1946/08/31\n \nreply",
      "Wow, brutal!As an aside I would never had guessed this artstyle was a 40s cover.\n \nreply",
      "You may be interested in the 1945 Project - which collects the stories of hibakusha,[1] or atomic bomb survivors: https://www.1945project.com/There is also Yoshito Matsushige, a survivor and the only photographer who was able to capture an immediate, first-hand photographic historical account: https://ahf.nuclearmuseum.org/ahf/key-documents/yoshito-mats...[1]https://en.wikipedia.org/wiki/Hibakusha\n \nreply",
      "Little Boy didn't have a parachute. Maybe he was mis-remembering that.\n \nreply",
      "There were instruments dropped by parachute.\n \nreply",
      "IIRC those were dropped by a second plane accompanying the Enola Gay.\n \nreply"
    ],
    "link": "https://www.nobelprize.org/press-release-peace-2024/",
    "first_paragraph": "",
    "summary": "In a shocking twist absolutely no one saw coming, the 2024 Nobel Peace Prize has been awarded to <em>Nihon Hidankyo</em>. To commemorate this, historical bystanders from the internet enthusiastically parrot secondhand nuclear tales that \"surprisingly\" involve their grandfathers. The stage is set for a profound display of \"my ancestor was there too\" theater, featuring an oscilloscope of dubious memory and forensic examination of bones with a hint of blue. Meanwhile, enthusiasts cling to internet threads as if they're the warp and weft of historical accuracy itself. Fascinating, or merely another round of atomic ancestry bingo? \ud83d\udca3\ud83c\udfb0"
  },
  {
    "title": "Why do systems fail? Tandem NonStop system and fault tolerance (erlang-solutions.com)",
    "points": 44,
    "submitter": "PaulHoule",
    "submit_time": "2024-10-11T17:10:49.000000Z",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=41811298",
    "comments": [
      "Tandem was interesting. They had a lot of good ideas, many unusual today.* Databases reside on raw disks.  There is no file system underneath the databases. If you want a flat file, it has to be in the database. Why? Because databases can be made with good reliability properties and made distributed and redundant.* Processes can be moved from one machine to another. Much like the Xen hypervisor, which was a high point in that sort of thing.* Hardware must have built in fault detection. Everything had ECC, parity, or duplication. \nIt's OK to fail, but not make mistakes. IBM mainframes still have this, but few microprocessors do, even though the necessary transistors would not be a high cost today. (It's still hard to get ECC RAM on the desktop, even.)* Most things are transactions. All persistent state is in the database. Think REST with CGI programs, but more efficient.\nThat's what makes this work. A transaction either runs to successful completion, or fails and has no lasting effect.\nDatabase transactions roll back on failures.The Tandem concept lived on through several changes of ownership and hardware. Unfortunately, it ended up at HP in the Itanium era, where it seems to have died off.It's a good architecture. The back ends of banks still look much like that, because that's where the money is.\nBut not many programmers think that way.\n \nreply",
      "> Databases reside on raw disks. There is no file system underneath the databases.The terminology of \"filesystem\" here is confusing.  The original database system was/is called Enscribe, and was/is similar to VMS Record Management Services - it had different types of structured files types, in addition to unstructured unix/dos/windows stream-of-byte \"flat\" files. Around 1987 Tandem added NonStop SQL files. They're accessed through a PATH: Volume.SubVolume.Filename, but depending on the file type, there is different things you can do with them.> If you want a flat file, it has to be in the database.You could create unstructured files as well.> Processes can be moved from one machine to anotherCritical system processes are process-pairs, where a Primary process does the work, but sends checkpoint messages to a Backup process on another processor. If the Primary process fails, the Backup process transparently takes over and becomes the Primary. Any messages to the process-pair are automatically re-routed.> Unfortunately, it ended up at HP in the Itanium era, where it seems to have died off.It did get ported to Xeon processors around 10 years ago, and is still around. Unlike OpenVMS, HPE still works on it, but as I don't think there is even a link to it on the HPE website* . It still runs on (standard?) HPE x86 servers connected to HPE servers running Linux to provide storage/networking/etc. Apparently it also runs supported under VMWare of some kind.* Something something Greenlake?\n \nreply",
      "> Critical system processes are process-pairs, where a Primary process does the work, but sends checkpoint messages to a Backup process on another processor. If the Primary process fails, the Backup process transparently takes over and becomes the Primary. Any messages to the process-pair are automatically re-routed.Right. Process migration was possible, but you're right in that it didn't work like Xen.> It still runs on (standard?) HPE x86 servers connected to HPE servers running Linux to provide storage/networking/etc.HP is apparently still selling some HPE gear. But it looks like all that stuff transitions to \"mature support\" at the end of 2025.[1] \"Standard support for Integrity servers will end December 31, 2025. Beyond Standard support, HPE Services may provide HPE Mature\nHardware Onsite Support,  Service dependent on HW spares availability.\" The end is near.[1] https://www.hpe.com/psnow/doc/4aa3-9071enw?jumpid=in_hpesite...\n \nreply",
      "It looks like that Mature Support stuff is all for Integrity i.e. Itanium servers.  As long as HPE still makes x86 servers for Linux/Windows, I assume NonStop can tag along.\n \nreply",
      "Right, that's just the Itanium machines. I'm not current on HP buzzwords.The HP NonStop systems, Xeon versions, are here.[1] The not-very-informative white paper is here.[2] Not much about how they do it. Especially since they talk about running \"modern\" software, like Java and Apache.[1] https://www.hpe.com/us/en/compute/nonstop-servers.html[2] https://www.hpe.com/psnow/doc/4aa6-5326enw?jumpid=in_pdfview...\n \nreply",
      "Not to take away from your main point: The only reason it is hard to get ECC in a desktop is because it is used as customer segmentation, not because it if technically hard or because it would drive the actual cost of the hardware up.\n \nreply",
      "ECC should be mandatory in consumer and cpus and memory. This will be seen like cars with fins and not having seatbelts in the future.\n \nreply",
      "Yes, IBM mainframes employ or have analogous concepts to all of this which may be one of many reasons they haven't disappeared.  A lot of it was built up over time whereas Tandem started from the HA specification so the concepts and marketing are clearer.Stratus was another interesting HA vendor, particularly the earlier VOS systems as their modern systems are a bit more pedestrian.  http://www.teamfoster.com/stratus-computer\n \nreply",
      "I present to you \"Commercial Fault Tolerance:\nA Tale of Two Systems\" [2004][0] - a paper comparing the similarities and differences towards reliability/available/integrity between Tandem Nonstop and IBM Mainframe systems,and the book \"Reliable Computer Systems - Design and Evaluation\"[1] which has general info on reliability, and specific looks at IBM Mainframe, Tandem, and Stratus, plus AT&T switches and spaceflight computers.[0] https://pages.cs.wisc.edu/~remzi/Classes/838/Fall2001/Papers...[1] https://archive.org/download/reliablecomputer00siew/reliable...\n \nreply",
      "Yeah - Stratus rocked :-)  The 'big battle' used to be between Non-Stops more 'software based' fault tolerance VS. Stratus's fully hardware level high availability.  I used to love demo'ing our Stratus systems to clients and let them pull boards while the machine was running...Just don't pull 2 next to each other :-)Also, I think Stratus was the first (only?) computer IBM re-badged at the time - IBM sold Stratus's as the Model 88, IIRC\n \nreply"
    ],
    "link": "https://www.erlang-solutions.com/blog/why-do-systems-fail-tandem-nonstop-system-and-fault-tolerance/",
    "first_paragraph": "",
    "summary": "**Why Systems Fail: The NonStop Parade of Nostalgia and Neglect**\n\nIn a world where \"fault tolerance\" sounds more like a relationship requirement than a tech specification, enthusiasts of the old Tandem NonStop system get their kicks by reminiscing over complexities like \"databases on raw disks\" because, who needs modern filesystems when you can live on the edge of obsolescence? Commenters dive into a nostalgia-fueled debate, comparing archaic systems and lamenting the unavailability of ECC RAM on desktops, because everyone knows the fate of the free world hinges on error-checking memory chips. Meanwhile, Tandem\u2019s technology, in all its relic glory, survives under HP\u2019s care, masquerading as 'innovation' in a HPE brochure filed under 'vintage charm\u2019. Let's all hold a candlelight vigil for system architectures so good they needed to be rescued from the technological equivalent of a yard sale. \ud83d\udd6f\ufe0f\ud83d\udcfc\ud83d\udc74\ud83c\udffb"
  },
  {
    "title": "Gleam: A Basic Introduction (peq42.com)",
    "points": 70,
    "submitter": "Alupis",
    "submit_time": "2024-10-11T18:57:33.000000Z",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=41812336",
    "comments": [
      "Gleam has been great as I've started messing around with it recently. Coming from primarily Ruby, it feels much different and I'm liking expanding my thought process around programming. I'm struggling a bit with learning how to think in the type system though. Without unions and a requirement that case statements all return a single type, I just haven't quite grasped the right pattern to make it all click. Enjoying the process none the less.\n \nreply",
      "I've been interested in Gleam, but I didn't realise it just transpiles to Erlang, I thought it compiled directly to BEAM bytecode. Bit of a turnoff to be honest, I really don't want to deal with transpilation.\n \nreply",
      "Which part do you feel like would be an issue? When you run `gleam compile`, it will automatically call the Erlang compiler to finish the job.I find it very handy that the intermediate Erlang (or JS) files are available in the build directory. It lets you easily see what form your code will take when compiled.\n \nreply",
      "Also prevents lock-in if you ever need to move away from gleam.\n \nreply",
      "I don't think it's the transpile part that would the issue, it's the runtime aspect. If Gleam transpiles to Erlang/Javascript that's great but once you run the program, you have to potentially deal with runtime issues specific to those environments which you might not be familiar with.It seems that Gleam is really useful for those who are already in either the Erlang/Javascript ecosystem.\n \nreply",
      "On the contrary, it's a great first BEAM language to learn because of it's simplicity - both in terms of the grammar as well as it's tooling/compiler.For me personally, the Javascript target is the least interesting bit - the BEAM/Erlang target is where it's at for backend work. The BEAM is fascinating and full of ideas that were once ahead-of-their-time but now are really coming into their own with compute performance having caught up.Gleam is a strongly typed language, and is unapologetically very functional. Error handling in general is quite different than it would be on a normal stack-based language/vm. In my experience, the Erlang target doesn't make debugging any harder or more difficult than you would expect for an exception-less language.\n \nreply",
      "The JS target is also very interesting to me. I like erlang fine and elixir's nascent type system is promising. But the frontend (and js fullstack for that matter) currently does not have a good alternative to typescript, and the ML type system is an especially good fit for it. Elm has too much reputational baggage and rescript/reason/bucklescript/whatever squandered its momentum and is floundering.\n \nreply",
      "Another layer of abstraction, another thing to go wrong, another thing to rot.\n \nreply",
      "Gleam used to compile to Core Erlang (Erlang Intermediate Representation) but looks like it now compiles to pretty-printed Erlang.https://blog.lambdaclass.com/an-interview-with-the-creator-o...\n \nreply",
      "The relevant quote:> The Gleam compiler has had a few full rewrites. The previous version compiled to BEAM bytecode via Core Erlang, which is an intermediate representation with the Erlang compiler, but the current version compiles to regular Erlang source code that has been pretty-printed. This has a few nice advantages such as providing an escape hatch for people who no longer wish to use Gleam, and enabling Erlang/Elixir/etc projects to use libraries written in Gleam without having to install the Gleam compiler.Pretty good reasoning in my opinion.\n \nreply"
    ],
    "link": "https://peq42.com/blog/gleam-a-basic-introduction/",
    "first_paragraph": "",
    "summary": "In the latest episode of \"Look! I Found Another Programming Language!,\" the tech savants over at <em>peq42.com</em> bravely navigate the treacherous waters of Gleam, a language somewhat reminiscent of Ruby, but with enough quirks to give a seasoned coder permanent eyebrow twitches. Commenters, in an adorable display of naivety, toggle between marveling at the ease of transpilation and waxing poetic about the mystical properties of Erlang\u2019s runtime - because clearly, what the programming world lacks is more underdog tales about abstract syntax trees. One bright spark notes the lack of direct compilation to BEAM bytecode as a dealbreaker, while others energetically nod at the compiler\u2019s output like it's the first time they\u2019ve seen code turn into other code. Spoiler: it transpiles, just like their caffeine-fueled hope into disillusionment. \ud83d\udcbb\ud83c\udf00\ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "How long til we're all on Ozempic? (asteriskmag.com)",
    "points": 258,
    "submitter": "thehoff",
    "submit_time": "2024-10-11T17:06:30.000000Z",
    "num_comments": 1009,
    "comments_url": "https://news.ycombinator.com/item?id=41811263",
    "comments": [
      "I've been on tirzepatide (Mounjaro) for 4 months now. I'm down 13% of my body weight. I realized that frequent cannabis consumption interferes with the weight loss, so I've kicked the habit from daily to occasionally on weekends. I've started walking 2-3 miles a day, 2-3 days a week regularly, in addition to eating less and being more motivated to calorie count.All this to say, this drug has been life changing for me. I spend more time doing things I want to do, depression and anxiety have less of a hold on me now. I feel that this drug has allowed me to be the best version of myself I have been in a long time. The only side effects so far have been positive. I do worry about what I will do once it's time to titrate off the weekly dose and the best I can think of is that the habits I'm forming in the time on the drug I will have the resolve to continue after cessation.I say this because I have battled depression, anxiety and obesity issues my entire life. I've had many failed attempts at getting back to a healthy, productive and non-obese lifestyle. I don't know what is so different about having the drug help me, but I can tell you that it has been different.\n \nreply",
      "Tirzepatide and Semaglutide are both known to reduce addiction / substance ingestion. I noticed I was just less interested in Alcohol when I started on Wegovy, and didn't realize it's a common effect until much later. I retained most of my disinterest after going off, too, FWIW.\n \nreply",
      "Sounds like a miracle drug that helps with all afflictions that come with our modern life/sedentary living.\n \nreply",
      "The problem with this (and all diet plans/drugs) is the lifestyle that led to problem in the first place.If you do not change your lifestyle, for real and not just superficially, then you will relapse with a vengeance.That is to say, be careful with using a drug as a crutch. Sure, it can artificially make you much more interested in not consuming so many calories and/or perhaps being more active than before - but you have to continue that lifestyle after stopping the drug.Will Ozempic users have developed the personal discipline to prevent themselves from relapse without the drug - or will they forever be on a the yo-yo of weight gain/loss?\n \nreply",
      "> Will Ozempic users have developed the personal discipline to prevent themselves from relapse without the drug - or will they forever be on a the yo-yo of weight gain/loss?Have alcoholics using Naltrexone? Or opioid addicts using Methadone, or smokers using nicotine gum/patches?See I'm bringing this up to point out the obvious double standard, people suffering from food addiction (i.e. literally the high from food) or binge-eating disorder, who finally have an effective treatment, are treated like it isn't addiction or illness, but a \"lifestyle,\" but if you said this stuff about any other addiction people would call you out and be horrified.For people mildy overweight or accidentally obese, it is a wildly different illness for people with lifetime problems who have lost/regained weight tens of times and likely know more about nutrition than most healthy-weight people ever will.\n \nreply",
      "The concern regarding a drug as a crutch is stil valid. Smokers/drinkers may deal with stress by smoking/drinking. After cessation, ways to deal with stress need to be learned from a new.\"Addiction\" is ambiguous and a term almost better not used. \"Addiction\" may constitute chemical dependency but can also be largely a set of habits. A set of habits and lifestyle are pretty much the same thing.\n \nreply",
      "The problem is that calling it a \"crutch\" is already presupposing a negative judgment of it.  Use a neutral word; e.g. it is a weight loss aid.\n \nreply",
      "Some things simply are negative, and masking behind a neutral word makes the neutral word perceived as negative over time.Masking reality is not a good way to work within it nor modify it.\n \nreply",
      "What's the explanation for why GLP1 medications are negative things? There are a very minor subset of people that have some medically significant adverse reactions, but it is VERY small. We don't have any evidence to my knowledge of any long term risks with being on it.\n \nreply",
      "> We don't have any evidence to my knowledge of any long term risks with being on it.Nobody has yet been on these drugs for an entire lifetime - which is what is being advocated in this thread.\n \nreply"
    ],
    "link": "https://asteriskmag.com/issues/07/how-long-til-were-all-on-ozempic",
    "first_paragraph": "Over 100 million Americans, and possibly many more, could benefit from GLP-1 drugs. When can they expect to get them?Obesity medication has something of a troubled past. Fen-phen, a weight-loss drug combination popular in the 1990s, was pulled after it was found to cause heart valve problems. Sibutramine, sold under the brand name Meridia, was prescribed until it was discovered to lead to adverse cardiovascular events including strokes in 2010. \u00a0But the market for an effective weight-loss drug is too big and the potential profits too high for pharmaceutical companies to give up. More than one in eight people around the world live with obesity. In the United States, it\u2019s more than two in five. Though many clinical trials of weight-loss drugs over the past decade ended in failure, it was only a matter of time until a successful drug emerged.\u00a0GLP-1 medications\n\n\n        1    \n\n\n like Ozempic appear to be that drug. Estimates suggest GLP-1s can reduce body weight by at least 15% when taken",
    "summary": "In a thrilling expos\u00e9, <em>asteriskmag.com</em> plunges into the heartland of America's waistline woes with \"How long til we're all on Ozempic?\" Here, the lands of oversized portions meet the New Frontier of pharmaceutical salvation, because, as history has proved, why fix societal lifestyle degradation when you can dish out new drugs? Ozempic emerges as our newest knight in shimmering syringe, promising to slim down the chunk of Americans bulkier than a Costco on a Saturday morning. Meanwhile, the enlightened commentariat delves deep, debating whether this magic injectable is the beacon of hope or just another capitalist crutch bound to be demonized when the next heart palpitation hits the fan. \ud83e\udd21\ud83d\udc89\ud83c\uddfa\ud83c\uddf8"
  },
  {
    "title": "\"Begin disabling installed extensions still using Manifest V2 in Chrome stable\" (chrome.com)",
    "points": 345,
    "submitter": "freedomben",
    "submit_time": "2024-10-11T14:20:26.000000Z",
    "num_comments": 318,
    "comments_url": "https://news.ycombinator.com/item?id=41809698",
    "comments": [
      "Notably, Firefox is not removing v2 support (at least for now as of March 2024)> Firefox, however, has no plans to deprecate MV2 and will continue to support MV2 extensions for the foreseeable future. And even if we re-evaluate this decision at some point down the road, we anticipate providing a notice of at least 12 months for developers to adjust accordingly and not feel rushed.[1][1]: https://blog.mozilla.org/addons/2024/03/13/manifest-v3-manif...\n \nreply",
      "To my knowledge the \u201cbig\u201d chrome engine alternatives aren\u2019t either. I know that Vivaldi and Brave plan on keeping around v2 as long as it is economically feasible\n \nreply",
      "Are you certain? The last I heard about it from Vivaldi[0], they were only going to keep the MV2 code around so long as it's in the upstream codebase:> We will keep Manifest v2 for as long as it\u2019s still available in Chromium. We expect to drop support in June 2025, but we may maintain it longer or be forced to drop support for it sooner, depending on the precise nature of the changes to the code.Note that June 2025 is the same date Google plans to drop support completely[1].[0] https://vivaldi.com/blog/manifest-v3-update-vivaldi-is-futur...[1] https://developer.chrome.com/docs/extensions/develop/migrate...\n \nreply",
      "Vivaldi team does not respond to any comments asking about ongoing v2 manifest support; safe to assume it's gone as soon as it's out of Chromium upstream. Given Tetzchner's continual messaging on how important user privacy is to Vivaldi it seems like a strange decision, but I don't know how much effort would be required to maintain the support. They're a small team, so it would be understandable if they would just say it's too hard, but instead they have avoided the topic entirely, which suggests they agree with the direction.\n \nreply",
      "Or they just don't want to admit publicly that they're too small to maintain a fork when it diverges this much\n \nreply",
      "Well Vivaldi is open source, right? Personally I would be reaching out to Brave, who already plans on maintaining V2 support, and see about a joint venture with a forked chromium.\n \nreply",
      "Vivaldi is not open-source: https://vivaldi.com/blog/technology/why-isnt-vivaldi-browser...\n \nreply",
      "But... what could possibly be the point of using a chromium based browser that is not Chrome, if not for MV2 support?\n \nreply",
      "MS Edge, Arc, and Sidekick have features Chrome doesn't such as split screen, side panels, and vertical tabs.  Likewise for Firefox forks such as Zen.\n \nreply",
      "In case of Vivaldi, it's features like vertical tabs, and extreme customizability for the built-in stuff (for tabs alone the options dialog is like 3 pages of checkboxes for all the various aspects of how they behave).Also for those who use cloud bookmark/history/tab sync, people might just not want Google specifically to have that data; Vivaldi does its own sync.\n \nreply"
    ],
    "link": "https://developer.chrome.com/docs/extensions/develop/migrate/mv2-deprecation-timeline",
    "first_paragraph": "Understand when Manifest V2 will stop working for extensionsOver the last few months, we have continued with the Manifest V2 phase-out.\nCurrently the chrome://extensions page displays a warning banner for all users\nof Manifest V2 extensions. Additionally, we have started disabling Manifest V2\nextensions on pre-stable channels.We will now begin disabling installed extensions still using Manifest V2 in\nChrome stable. This change will be slowly rolled out over the following weeks.\nUsers will be directed to the Chrome Web Store, where they will be recommended\nManifest V3 alternatives for their disabled extension. For a short time, users\nwill still be able to turn their Manifest V2 extensions back on. Enterprises\nusing the\nExtensionManifestV2Availability\npolicy will be exempt from any browser changes until June 2025. See our May\n2024 blog\nfor more context.Starting on June 3rd on the Chrome Beta, Dev and Canary channels, if users still\nhave Manifest V2 extensions installed, some will start t",
    "summary": "**Chrome Shuts Down Old Tech; Users Cling to the Past**\n\nIn a bold move to obsolete technology no one apparently needs, Google announces the slow death of Manifest V2 extensions in Chrome, effectively forcing five users to upgrade their browser tools. Meanwhile, in comment land, users rebel against change by praising Firefox, which clings to outdated tech like a nostalgic hoarder. Cue scattered discussions about Vivaldi and Brave's uncertain commitments, as keyboard warriors speculate wildly and prepare their digital pitchforks. Chrome moves forward; commenters remain steadfastly in 2015. \ud83d\ude2d\ud83d\udd27\ud83d\udcbb"
  },
  {
    "title": "Hamming AI (YC S24) Is Hiring a Product Engineer (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-10-11T17:29:14.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/hamming-ai/jobs/XTCQPuO-product-engineer",
    "first_paragraph": "",
    "summary": "The tech elite have once again descended from their ergonomic workspaces to solve a problem no one knew existed. Hamming AI, a startup daring to boldly go where only a few hundred others have gone before, is on the prowl for a Product Engineer. Their revolutionary approach? Provide AI solutions indistinguishable from those any undergraduate with a TensorFlow tutorial could muster. Comments are a dumpster fire of buzzwords hurled by wannabe disruptors, each vying for the chance to tout their own superficial understanding of AI. Join now to build another forgettable product while passionately arguing about which JavaScript framework is more \"scalable\"! \ud83d\ude80\ud83d\udcbb"
  }
]