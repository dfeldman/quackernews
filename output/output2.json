[
  {
    "title": "Apple Violated Antitrust Ruling, Judge Finds (wsj.com)",
    "points": 244,
    "submitter": "shayneo",
    "submit_time": "2025-05-01T00:03:55 1746057835",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=43852145",
    "comments": [
      "About time. I'm tired of apologizing to customers who purchase subscriptions in my app only to discover they could have purchased the exact the same thing from my website for 15% less. \"Why didn't you tell me?\"Excerpt from the filing:\"In stark contrast to Apple\u2019s initial in-court testimony, contemporaneous business documents reveal that Apple knew exactly what it was doing and at every turn chose the most anticompetitive option. To hide the truth, Vice-President of Finance, Alex Roman, outright lied under oath. Internally, Phillip Schiller had advocated that Apple comply with the Injunction, but Tim Cook ignored Schiller and instead allowed Chief Financial Officer Luca Maestri and his finance team to convince him otherwise. Cook chose poorly. The real evidence, detailed herein more than meets the clear and convincing standard to find a violation. The Court refers the matter to the United States Attorney for the Northern District of California to investigate whether criminal contempt proceedings are appropriate.\"\n \nreply",
      "About time for what? Another company to get charged with something that they don't get punished for?The fines are always less than the companies' net gains from the practice. Gains are often indirect, risk-related, and/or part of a larger strategy, so they cannot be calculated.Everything short of prison is a waste of time, waste of tax dollars, and spits in the face of decent citizens.\n \nreply",
      "100%. This doesn't deserve a fine. This deserves Apple to be given an ultimatum about their mob boss behavior with mobile.Apple and Google quickly built up their duopoly such that everyone doing anything with mobile phones has to pay them a tax. You can't even deploy your own apps at your own cadence, without strict review, using your own technology. You have to jump through unplanned upgrade cycles, you're forced to use their payment rails and signup flows (and don't get to know your customer or get them to use your website). You pay the taxes on everything. And even then, they let your competitors advertise against your name or trademark.This is rotten to the core.Neither Google nor Apple should have an app store. Apps should be web installs. The only reason things work the way they do is so that Apple and Google can tax and exert control. A permissions system, signature scans, and heuristics are all that are needed to keep web installs safe - and all of those pieces are already in place. There's no technical or safety limitation, Apple and Google just want to dominate.These two companies were innovative 20 years ago, but their lead then doesn't entitle them to keep owning the majority of most people's computing surface area for the rest of time. They have to give up the reigns. There are still billions of dollars for them to make on mobile, even if regulators tell them to stop treating developers as serfs and locking them in cages.No. More. App. Stores.Regulate big tech's hold over mobile, web, search, and advertising.\n \nreply",
      "Its not possible for them to make \"billions of dollars\" any other way - https://en.wikipedia.org/wiki/Platform_economy\n \nreply",
      "Criminal contempt would the charge against the company or the people named above? How does it work in this case?\n \nreply",
      "\"Accordingly ... the Court refers the issue to the United States Attorney for the Northern District of California ... for investigation against Apple and Alex Roman, Apple\u2019s Vice President of Finance specifically.\"\n \nreply",
      "Read Taibbi's book, The Divide. No prison time is the most likely outcome unless the justice system has changed between then and now.\n \nreply",
      "So what is the punishment for outright lying under oath?\n \nreply",
      "Generally none, the DA must choose to pursue perjury charges, which basically never happens. In reality, nearly everyone commits perjury. Thomas More would not approve. Both versions (1966 and 1988) of A Man For All Seasons are highly worth watching several times and practically memorizing. \"Would you benefit England by populating her with liars?\" [edit] in retrospect, there is one inescapable consequence of lying under oath: your word now means nothing to honest people.\n \nreply",
      "I've not seen the one from 1988; I'll have to check that out. I've long enjoyed the one from '66.I also heartily recommend both seasons of Wolf Hall. About Cromwell rather than More, but still fascinating.\n \nreply"
    ],
    "link": "https://www.wsj.com/tech/apple-violated-antitrust-ruling-federal-judge-finds-66b85957",
    "first_paragraph": "",
    "summary": "Title: Apple Gets Caught, Internet Shouts their Surprise\n\nIn an act that shocked absolutely no one, Apple has become the Poster Child for corporate \"oopsies\" by blatantly ignoring antitrust laws, according to judges and paperwork. A masterclass in corporate finesse was provided by Alex Roman, who apparently also moonlights as Pinocchio. Meanwhile, the comment section transformed into an echo chamber of indignation, filled with righteous tech prophets declaring \u2694\ufe0f the end of the technological tyrannies and heroic calls for a digital revolution\u2014because that's obviously what's going to topple the billion-dollar Apple cart. Sadly, no tech overlords were jailed during the making of this scandal, prompting cries of despair from URL-citing armchair experts and doomed pleas for a return to web-based innocence. \ud83d\udcbb\ud83c\udf4f\ud83d\udc94"
  },
  {
    "title": "Mercury, the first commercial-scale diffusion language model (inceptionlabs.ai)",
    "points": 183,
    "submitter": "HyprMusic",
    "submit_time": "2025-04-30T21:51:10 1746049870",
    "num_comments": 70,
    "comments_url": "https://news.ycombinator.com/item?id=43851099",
    "comments": [
      "Not sure if I would tradeoff speed for accuracy.Yes, it's incredible boring to wait for the AI Agents in IDEs to finish their job. I get distracted and open YouTube. Once I gave a prompt so big and complex to Cline it spent 2 straight hours writing code.But after these 2 hours I spent 16 more tweaking and fixing all the stuff that wasn't working. I now realize I should have done things incrementally even when I have a pretty good idea of the final picture.I've been more and more only using the \"thinking\" models of o3 in ChatGPT, and Gemini / Claude in IDEs. They're slower, but usually get it right.But at the same time I am open to the idea that speed can unlock new ways of using the tooling. It would still be awesome to basically just have a conversation with my IDE while I am manually testing the app. Or combine really fast models like this one with a \"thinking background\" one, that would runs for seconds/minutes but try to catch the bugs left behind.I guess only giving a try will tell.\n \nreply",
      "So my personal belief is that diffusion models will enable higher degrees of accuracy. This is because unlike an auto-regressive model it can adjust a whole block of tokens when it encounters some kind of disjunction.Think of the old example where an auto regressive model would output: \"There are 2 possibilities..\" before it really enumerated them. Often the model has trouble overcoming the bias and will hallucinate a response to fit the proceeding tokens.Chain of thought and other approaches help overcome this and other issues by incentivizing validation, etc.With diffusion however it is easier for the other generated answer to change that set of tokens to match the actual number of possibilities enumerated.This is why I think you'll see diffusion models be able to do some more advanced problem solving with a smaller number of \"thinking\" tokens.\n \nreply",
      "Suggests an opportunity for hybrids, where the diffusion model might be responsible for large scale structure of response and the next token model for filling in details.  Sort of like a multi scale model in dynamics simulations.\n \nreply",
      "> it can adjust a whole block of tokens when it encounters some kind of disjunction.This is true in principle for general diffusion models, but I don't think it's true for the noise model they use in Mercury (at least, going by a couple of academic papers authored by the Inception co-founders.) Their model generates noise by masking a token, and once it's masked, it stays masked. So the reverse-diffusion gets to decide on the contents of a masked token once, and after that it's fixed.\n \nreply",
      "Here are two papers linked from Inception's site:1. Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\n - https://arxiv.org/abs/2310.168342. Simple and Effective Masked Diffusion Language Models - https://arxiv.org/abs/2406.07524\n \nreply",
      "Thanks, yes, I was thinking specifically of \"Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\". They actually consider two noise distributions: one with uniform sampling for each noised token position, and one with a terminal masking (the Q^{uniform} and Q^{absorb}.) However, the terminal-masking system is clearly superior in their benchmarks.https://arxiv.org/pdf/2310.16834#page=6\n \nreply",
      "Thank you, I'll have to read the papers. I don't think I have read theirs.\n \nreply",
      "The exact types of path dependencies in inference on text-diffusion models look like an interesting research project.\n \nreply",
      "> Not sure if I would tradeoff speed for accuracy.Are you, though?There are obvious examples of obtaining speed without losing accuracy, like using a faster processor with bigger caches, or more processors.Or optimizing something without changing semantics, or the safety profile.Slow can be unreliable; a 10 gigabit ethernet can be more reliable than a 110 baud acoustically-coupled modem in mean time between accidental bit flips.Here, the technique is different, so it is apples to oranges.Could you tune the LLM paradigm so that it gets the same speed, and how accurate would it be?\n \nreply",
      "Wouldn't it be possible to trade speed back for accuracy, e.g. by asking the model to look at a problem from different angles, let it criticize its own output, etc.?\n \nreply"
    ],
    "link": "https://www.inceptionlabs.ai/introducing-mercury",
    "first_paragraph": "",
    "summary": "**Hackernews Discovers Yet Another AI It Won't Actually Use**\n\nInception Labs announces Mercury, a \"game-changing\" language model that promises either to fritter your time away with its majestic slowness or to dazzle you by spitting out wildly incorrect responses at breakneck speeds. The commentariat, bored of actually writing code, engages in the relentless pontification over speed vs. accuracy. One commenter nostalgically reminisces about the eternity spent watching YouTube while waiting for IDE AI to compose a symphony of buggy code. Meanwhile, amidst a sea of self-proclaimed experts linking to arXiv papers they haven't read, suggestions of creating a hybrid AI Frankenstein are made, promising to solve all problems by being slow and wrong at different stages of operation. Truly, we are on the precipice of revolutionizing how little work gets done."
  },
  {
    "title": "Pwning the Ladybird Browser (jessie.cafe)",
    "points": 53,
    "submitter": "todsacerdoti",
    "submit_time": "2025-04-30T23:59:09 1746057549",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=43852096",
    "comments": [
      "This is a big landmark. Ladybird has come far enough to be a worthy target for security research!\n \nreply",
      "If this is all-new development, wouldn't it be good for the emphasis to be on correctness and security, as part of the design and coding itself?That's something that you use fuzzing as one way to detect a failure of, not as the means of achieving correctness and security.I'm not picking on Ladybird here specifically.  Chrome and Firefox provide constant streams of security vulnerabilities.  But it would be nice if Ladybird didn't start with the same problems that might be attributed to huge legacy code bases.\n \nreply",
      "Ladybird comes from Serenity OS which has a focus of having fun and being pragmatic while building everything from scratch incrementally.They do plan to switch to Swift: https://ladybird.org/#:~:text=Why%20build%20a%20new%20browse...I appreciate their pragmatism though, it's allowed them to catch up to other alternative browsers in WPT coverage very quickly.\n \nreply",
      "Always good to start the discussion but the article doesn't seems to link to an issue on the Ladybird github repo, which I would expect in the case of academic disclosure etc.Obviously nobody is really using Ladybird yet and there will be many more such issues to address, so now is a good time to evaluate how to avoid such mistakes up front.\n \nreply",
      "Ah the github links are indeed there, my bad, it's a good write up.\n \nreply",
      "Of academic value, as ladybird has little in terms of sandboxing yet.Cool regardless.\n \nreply",
      "Even in a modern browser, a renderer exploit (the most sandboxed portion of the browser) gives you access to a large attack surface - the browser process via IPC, the kernel via syscalls, and loads of data from other websites.So no, an exploit like this is not just \u201cof academic value\u201d even in a sandboxed browser.\n \nreply",
      "With site isolation there's not loads of other websites in the renderer these days at least.\n \nreply"
    ],
    "link": "https://jessie.cafe/posts/pwning-ladybirds-libjs/",
    "first_paragraph": "Ladybird is a relatively new browser engine originating from the\nSerenityOS project. Currently, it\u2019s in pre-alpha and improving quickly.\nTake a look at the website and the GitHub for more information!I\u2019ll be researching the JavaScript engine of Ladybird,\nLibJS.LibJS has an interpreter tier and no compilation tiers (yet!). It includes common modern JS engineoptimizations and is built with extensive verification checks across its critical code paths and datastructures, including vectors, making scenarios such as integer overflows leading to out-of-boundsaccesses harder to exploit.We\u2019ll be using Fuzzilli, a popular fuzzer for JavaScript interpreters. Here\u2019s the description from the GitHub:Fuzzilli can be configured with additional code generators that can be specialized to trigger\nspecific bugs. LibJS isn\u2019t actively being OSS-fuzzed, so I didn\u2019t add any custom generators and hoped\nthere would be enough shallow bugs around. There was already some persistent fuzzing code\nin LibJS. After som",
    "summary": "Title: Hobbyist Browser Draws Hobbyist Hackers\n\nIn a world where using mainstream browsers is just too mainstream, the Ladybird browser emerges from the tech playpen known as SerenityOS. It's so pre-alpha, you can almost hear the developers whispering \"it's not a bug, it's a feature\" as they cram GitHub with endless loops of verification checks and fuzzing fun. Commenters, in a delightful display of tech naivety, sound shocked\u2014SHOCKED\u2014that a browser so young could be as insecure as its elderly internet uncles Chrome and Firefox. Between praising the \"pragmatism\" of starting from scratch and philosophically pondering the non-use of Ladybird, everyone seems to forget to actually <i>use</i> the browser outside of their digital sandbox.\ud83d\ude43"
  },
  {
    "title": "Linux Kernel Exploitation: Attack of the Vsock (hoefler.dev)",
    "points": 136,
    "submitter": "todsacerdoti",
    "submit_time": "2025-04-30T19:03:04 1746039784",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=43849373",
    "comments": [
      "> So I set off on a journey that would lower my GPA and occasionally leave me questioning my sanityAmazing! Sacrificing GPA for projects is always a good time\n \nreply",
      "I really liked the old German university concept, the one before we just took over Bachelor/Master.Throughout my CS studies, I was just collecting \"tickets\" (very hard to translate the actual word, \"Schein\"), which basically just attested that you have passed a course. They (often) had a grade on it, but it did not matter. Instead, once in the middle (\"pre-diploma\") and once at the very end of your time at university, you'd have oral exams. And those determined your grade. To attend them, you needed the right combination of \"tickets\".The glaring downside of this system is that if you had a bad time in those few months of your very final exams, you could screw up your entire grade.The upside of it, is that I was free (and encouraged) to pursue whatever I wanted, without each course risking to have an effect on my \"GPA\". I had way more tickets than I needed in the end, and still time and energy to pursue whatever else I wanted (playing with microcontrollers etc.).\n \nreply",
      "I had a couple of classes in USA uni that worked quite similarly. The professor said we can take the quizzes if we want, and if we didn't then the later quizzes would constitute more of your grade. The ultimate play was to only take the final quiz.\n \nreply",
      "> The ultimate play was to only take the final quiz.This is how a lot of British undergrad courses ('modules') work. One giant exam at the very end determining everything; no quizzes, no problem sheets, no midterms.\n \nreply",
      "Modules? We just had six massive exams at the end of three years!\n \nreply",
      "Chicago used to be that way in the long ago times.\n \nreply",
      "Would not be a surprise if AI brought this back.\n \nreply",
      "As a teacher once told me.\"Never let school limit your education\"\n \nreply",
      "For those wondering this is a common paraphrase of Grant Allen and Mark Twain. Here we say \"Never let school get in the way of a good education.\"\n \nreply",
      "I learned a ton while at my university.  Much of it was outside of my classwork.\n \nreply"
    ],
    "link": "https://hoefler.dev/articles/vsock.html",
    "first_paragraph": "What started off as casual scrolling through the KernelCTF submissions quickly spiraled into a weeks-long deep dive into a deceptively simple patch - and my first root shell from a Linux kernel exploit!While browsing the public spreadsheet of submissions, I saw an interesting entry: exp237. The bug patch seemed incredibly simple, and I was amazed that a researcher was able to leverage the issue for privilege escalation. So I set off on a journey that would lower my GPA and occasionally leave me questioning my sanity: My first linux kernel exploit!Before we can start diving into the exploit development, we need to set up a good linux kernel debugging environment. I decided to use QEMU with scripts from midas's awesome writeup with the gef-kernel GDB extensions. I chose to start with linux kernel 6.6.75 since it was close to the versions being exploited by the other researchers. I actually completed this entire project within WSL so that I could write the exploit on my Windows school com",
    "summary": "**Linux Kernel Exploitation: A Comedy of Errors and GPA Sacrifice**\n\nIn what reads like the diary of a tech masochist, an intrepid soul plunges into the sordid depths of Linux kernel exploitation just because they found <i>\"exp237\"</i> on a shady spreadsheet. Swapping a decent GPA for the siren call of becoming a kernel warrior, the protagonist chooses the hard path that ends in a triumphant root shell\u2014which probably feels less rewarding than expected when grading day rolls around. Meanwhile, in the comments, spectators wax nostalgic about how school systems used to let them collect course \"tickets\" like Pok\u00e9mon cards, blissfully unconcerned about GPA until that final boss battle of an oral exam. Yes, let's all reminisce about academic formats from yesteryears because clearly, modern education with its constant assessment is what's stopping everyone from hacking the planet. \ud83c\udf0d\u2728"
  },
  {
    "title": "Zhaoxin's KX-7000 (chipsandcheese.com)",
    "points": 88,
    "submitter": "ryandotsmith",
    "submit_time": "2025-04-30T20:23:33 1746044613",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=43850238",
    "comments": [
      "Love reading these highly detailed analyses. Short version: Zhaoxin's currently competitive with 2010/2011-era AMD and Intel, with some asterisks around RAM speed.There is to my mind a sort of race to get up to \"fast enough to host H100 competitor AI hardware\" with non-US IP that makes sense to engage in. In those terms, it looks like they're maybe 2 revs away -- I'm not sure what process node the KX7000 is on, but there's some architectural work to finish up. That said, this is interesting. I assume the chips will continue to improve from Zhaoxin, unless they lose their core team.\n \nreply",
      "Minor nit. Compound pinyin words shouldn\u2019t use StudlyCaps so it should be \u201cLujiazui\u201d\n \nreply",
      "Hmmm.. it maybe free from IME! Maybe the FSF want a word with them.\n \nreply",
      "No IME but whatever unknown chinese rootkit? Out of the frying pan, into the fire\n \nreply",
      "I can reason about an \"unknown chinese rootkit\" as much as about an \"unknown US rootkit\".\n \nreply",
      "This is interesting! Does anyone know how China\u2019s reliance on chips from intel and amd is in the non-AI space (so regular consumer and server loads)? I\u2019m wondering how it was 10 and 5 years ago, now, and how we predict in the next couple of years. Surely if they\u2019re not mostly using their own chips they will very soon right?\n \nreply",
      "How would use of the Kylin OS instead of Windows 11 affect the user's perception of performance?\n \nreply",
      "What's the deal with the municipal government being a partner in this project? Is that structure common in china? Is it just them giving VIA tax breaks and things, or are they more involved than that?\n \nreply",
      "Yeah. They know the chips aren\u2019t commercially competitive so they just create artificial demand by making gov and state controlled entities buy it.Basically an attempt to bootstrap an industry brute force style\n \nreply",
      "Seems like it depends on the price point.  These chips might be slow by modern standards, but if they're cheap enough then it doesn't really matter for a lot of the potential applications.  I'm typing this post on a chip that is roughly in that performance bracket (an i5-3750k) that only rarely feels like the bottleneck.  And this is my gaming machine.\n \nreply"
    ],
    "link": "https://chipsandcheese.com/p/zhaoxins-kx-7000",
    "first_paragraph": "",
    "summary": "In a stunning display of technical mediocrity, Zhaoxin\u2019s KX-7000 is finally making strides to catch up with the ancient silicon spirits of 2011 AMD and Intel. The comment section bursts into life as armchair experts discuss the nuances of RAM speed and hypothetical AI hosting capabilities, interspersed with nightmarish visions of \"__unknown Chinese rootkits__\" lurking beneath every transistor. One savvy commenter highlights the intense need for semantic accuracy in the world of Compound pinyin, safeguarding the integrity of our digital discourse from the horrors of StudlyCaps. Meanwhile, corporate-government tech romance flourishes in china style, ensuring this marvel of yesteryear\u2019s technology finds its way into every government office that still runs on Solitaire. \u2328\ufe0f\ud83d\udd25"
  },
  {
    "title": "Stockhausen: Sounds in Space (stockhausenspace.blogspot.com)",
    "points": 4,
    "submitter": "brudgers",
    "submit_time": "2025-05-01T00:57:42 1746061062",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://stockhausenspace.blogspot.com/",
    "first_paragraph": "Analysis, explanation and personal impressions of the works of the avant-garde composer Karlheinz Stockhausen.",
    "summary": "In an exhilarating display of pretentiousness, stockhausenspace.blogspot.com treats us to a *breathtaking* journey into the mind of Karlheinz Stockhausen, where the laws of music and sanity dare not tread. Here, every obscure note and accidental scribble by the avant-garde composer is dissected with the seriousness of a moon landing. The comment section, a delightful cesspool of pseudo-intellectual one-upmanship, teems with enthusiasts who argue fervently over the cosmic significance of these sonic oddities. It\u2019s like watching a heated debate on the architectural nuances of a sandcastle. \ud83c\udfbc\ud83d\ude80"
  },
  {
    "title": "GroMo (YC W21) Is Hiring (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-05-01T01:01:02 1746061262",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/gromo/jobs/aP4JS9K-product-tech-business-ai-enthusiasts",
    "first_paragraph": "App for independent agents to sell financial products in IndiaGroMo brings you the FinArva AI Hackathon 2025, powered by AWS - a high-energy, fast-paced challenge to solve India\u2019s toughest financial distribution problems using AI and product innovation.Link to Hackathon- FinArva AI Hackathon 2025This is your chance to design intelligent solutions for Bharat's next billion users. Selected participants will get exclusive mentorship, showcase their ideas in front of top fintech leaders, and compete for prizes worth \u20b910,00,000+.The Problem Statement for the Phase 1 (Idea Discovery & Concept Submission) can be accessed from here: Click Here\n\nWhy Participate in FinArva AI Hackathon?**From idea to impact\u2014your AI journey starts here.**\n\nREGISTRATION FORM LINKWinning CriteriaEligibilityPhase 1: Idea Discovery & Concept SubmissionRegistered teams have to work on the virtual ideation and concept submission focused on understanding GroMo\u2019s Partners' challenges. The idea needs to cover - Top 3 User",
    "summary": "**GroMo (YC W21) is Desperate for Human Capital**\n\nGroMo, yet another start-up claiming to revolutionize <em>something</em>, now desperately seeks to staff up by seducing the unwashed hacking masses with the dazzling allure of rupees and \"exclusive mentorship\" in their latest circus, the FinArva AI Hackathon 2025. Powered by the ever-present AWS, because nothing says \"innovation\" like hitching your wagon to the biggest horse. Hopeful tech peons are invited to solve \"India\u2019s toughest financial distribution problems,\" which apparently no one at GroMo can figure out themselves. The comment section, predictably, is a delightful dumpster fire of wannabe disruptors and bitter tech bros all jostling to showcase their buzzword bingo prowess. \ud83d\ude80\ud83d\udcb8\ud83e\udd16"
  },
  {
    "title": "Espressif's ESP32-C5 Is Now in Mass Production (espressif.com)",
    "points": 81,
    "submitter": "radeeyate",
    "submit_time": "2025-04-30T22:15:21 1746051321",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=43851314",
    "comments": [
      "Announced 2+ years ago (almost 3, now that I look: https://www.espressif.com/en/news/ESP32-C5 ) and sampling 1+ year ago, good to see it finally come. 5GHz support is increasingly important.\n \nreply",
      "Any guesses as to when a hobbyist might be able to buy the module without the dev board? Their aliexpress store didn't have them as far as I can tell, I assume they are prioritizing dev boards for the moment unless you're a big enough company to actually talk directly with Espressif.\n \nreply",
      "This is looking pretty great, I've really wanted a MCU with Zigbee on it, for the various little battery-operated devices I've wanted to make. However, with Espressif's lineup, I've really lost track of what does what, lately.Does anyone know of a good comparison resource?\n \nreply",
      "https://products.espressif.com/#/product-comparisonThis is a little bit more interactive and detail-oriented. I think they also have flashy onesheet PDFs that are more marketing oriented.\n \nreply",
      "The flashy PDF is here https://products.espressif.com/static/Espressif%20SoC%20Prod... \n a one-pager comparing all models.\n \nreply",
      "According to that pdf the ESP32-C5 does have Zigbee.\n \nreply",
      "The ESP32-C6 has a Zigbee radio. I have 6 myself -- they're great.\n \nreply",
      "I bought a few of those, but at $8 they're a bit pricier than the $3 Espressif spoiled me with.\n \nreply",
      "supermini boards with esp32c6 on it can be had for approximately 4 euro each.\n \nreply",
      "If anyone from Espressif seeing this, I love your MCUs. But can you please improve the ESP-IDF so that it's usable on BSD systems. The Linuxisms baked into its build system is unnecessary.I think moving from Make in the old version of IDF to CMake was a mistake.\n \nreply"
    ],
    "link": "https://www.espressif.com/en/news/ESP32-C5_Mass_Production",
    "first_paragraph": "Espressif Systems (SSE: 688018.SH) announced ESP32-C5, the industry\u2019s first RISC-V SoC that supports 2.4 GHz and 5 GHz dual-band Wi-Fi 6, along with Bluetooth 5 (LE) and IEEE 802.15.4 (Zigbee, Thread) connectivity.\u00a0Today, we are glad to announce that ESP32-C5\u00a0 is now in mass production.ESP32-C5 is designed for applications that require high-efficiency, low-latency wireless transmission. ESP32-C5 has a 32-bit single-core processor which can clock up to 240 MHz in speed. It has a 384 KB on-chip SRAM along with external PSRAM support, 320 KB of ROM. It has up to 29 programmable GPIOs, supporting all the commonly used peripherals, high speed interfaces like SDIO, QSPI, and the best-in-class security features.\u00a0The ESP32-C5 also includes an LP-CPU running upto 40MHz which can act as the main processor for power sensitive applications. To learn more about the various capabilities and features of this MCU, please visit our website.The ESP32-C5 benefits from software support provided by Espress",
    "summary": "In a groundbreaking act of tepid innovation, Espressif has cranked out the ESP32-C5, _finally_ making it to mass production after nearly claiming the title of vaporware. Engineers and tech aficionados everywhere, rejoice in welcoming another SoC that will be hoarded by hobbyists faster than you can say \"dual-band Wi-Fi 6.\" Meanwhile, enthusiastic commenters seem to compete in a championship of confusion, eagerly misidentifying features across Espressif\u2019s maze of similarly named products. And let's not forget the lone voice begging for BSD support, a wild echo lost in the vast void of Linux-dominated development environments. \ud83c\udf89\ud83e\udd16\ud83d\udcbe"
  },
  {
    "title": "Xiaomi MiMo Reasoning Model (github.com/xiaomimimo)",
    "points": 401,
    "submitter": "thm",
    "submit_time": "2025-04-30T08:48:20 1746002900",
    "num_comments": 151,
    "comments_url": "https://news.ycombinator.com/item?id=43842683",
    "comments": [
      "MiMo-7B claims to outperform larger models like Qwen-32B and match OpenAI o1-mini on math/code benchmarks \u2014 all with a 7B model trained from scratch. Is this a sign that pretraining + RLHF optimization is finally outpacing scale? Or are we just getting better at benchmarking narrow capabilities?\n \nreply",
      "From the paper, I was intrigued by how they handled their RL step for Code Data. They trained against hard but solvable code generation tasks by running unit testing. Is that training step done by the other models?> Code Data For coding problems, we curate a high-quality training set comprising open-source datasets and our newly collected problem set. We remove problems without test cases. For problems with golden solutions, we exclude those where the golden solution failed to pass all test cases. For problems without golden solution, we discard problems where no test case can be solved in 16 rollouts of advanced reasoning models. Similar to math data, we utilize an SFT version of MiMo-7B to filter out easy problems that are perfectly solved in all 16 rollouts. This rigorous cleaning process yields 30K code problems.> During each RL iteration, we evaluate thousands of problems to compute the rewards, with each problem potentially containing hundreds of test cases. To improve reward computing efficiency and eliminate GPU idle time, we developed an online judge environment that enables parallel execution of extremely high-volume unit tests.\n \nreply",
      "Why are there so many English-first AI models from China? Are they not interested in serving their own population? Or is it that if they publish Chinese-first models it won't get publicity in the West?\n \nreply",
      "CommonCrawl [1] is the biggest and most easily accessible legally acquired crawling dataset around, collecting data since 2008. Pretty much everyone uses this as their base dataset for training foundation LLMs and since it's mostly English, all models perform well in English.[1] https://commoncrawl.org/\n \nreply",
      "Haven't we reached a situation where English is the de facto language of scientific research, especially AI benchmarks ?It's clearly impossible for me to try anything in Chinese, I'd need a translation.\n \nreply",
      "Correct. Lingua franca for at least the last 75 years, if not longer.\n \nreply",
      "I assume a large portion of high quality training material is in English\n \nreply",
      "You'd be correct. The largest portion of all languages in Common Crawl (aka the \"whole open internet\" training corpus) is English with 43%. No other language even reaches double digit percentages. The next biggest one is Russian at 6%, followed by German at 5%.\n \nreply",
      "I wonder where are you getting your data. According to wikipedia russian is #7 https://en.wikipedia.org/wiki/Languages_used_on_the_InternetOnly place where russian is in top 5 is in Wikipedia views. Russian part of internet steadily goes down, as russian imperialism crumbles.\n \nreply",
      "> The largest portion of all languages in Common Crawlhttps://commoncrawl.github.io/cc-crawl-statistics/plots/lang...\n \nreply"
    ],
    "link": "https://github.com/XiaomiMiMo/MiMo",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        MiMo: Unlocking the Reasoning Potential of Language Model \u2013 From Pretraining to Posttraining\n      This code repository is licensed under the Apache2.0 License.Currently, most successful RL works, including open-source research, rely on relatively large base models, e.g., 32B models, particularly for enhancing code reasoning capabilities. Moreover, it was widely considered that achieving uniform and simultaneous improvements in both mathematical and code capabilities within a small model is challenging. Nonetheless, we believe that the effectiveness of the RL trained reasoning model relies on the inherent reasoning potential of the base model. To fully unlock the reasoning potential of language models, efforts must focus not only on post-training but also on pre-training strategies tailored to reasoning.In this work, we present MiMo",
    "summary": "**Hacking the Hype: Xiaomi's Latest DIY Brain Kit**\n\nIn a burst of unsurprising AI hubris, Xiaomi declares its \"MiMo-7B\" model a new heavyweight in the featherweight division, claiming superiority over bulkier models despite only needing a tiny sandbox to build its digital sandcastles. Commenters, excited by the possibility of not just training but actually understanding what their bots do, ask astutely if the secret sauce is just *better tests or cheaper testers*. Amidst the realism of RL and overcooked benchmarks, another soul cries out about the English dominance in AI, sparking a stat-war showing that everyone still just really likes their bots babbling in British rather than Beijingese. Meanwhile, Xiaomi's paper suggests breaking the code by, well, breaking the code \u2014 one unit test at a time. \ud83e\udde0\ud83d\udd27\ud83d\udca5"
  },
  {
    "title": "Reversible computing with mechanical links and pivots (tennysontbardwell.com)",
    "points": 108,
    "submitter": "tennysont",
    "submit_time": "2025-04-30T17:35:08 1746034508",
    "num_comments": 56,
    "comments_url": "https://news.ycombinator.com/item?id=43848398",
    "comments": [
      "See Drexler's mechanical nanotechnology from 1989.[1]There's a minimum size at which such mechanisms will work, and it's bigger than transistors. This won't scale down to single atoms, according to chemists.[1] http://www.nanoindustries.com/nanojbl/NanoConProc/nanocon2.h...\n \nreply",
      "It seems like you've misremembered the situation somewhat.Merkle developed several of his families of mechanical logic, including this one, in order to answer some criticisms of Drexler's earliest mechanical nanotechnology proposals.  Specifically:1. Chemists were concerned that rod logic knobs touching each other would form chemical bonds and remain stuck together, rather than disengaging for the next clock cycle.  (Macroscopic metal parts usually don't work this way, though \"cold welding\" is a thing, especially in space.)  So this proposal\u201a like some earlier ones like Merkle's buckling-spring logic, avoids any contact between unconnected parts of the mechanism, whether sliding or coming into and out of contact.2. Someone calculated the power density of one of Drexler's early proposals and found that it exceeded the power density of high explosives during detonation, which obviously poses significant challenges for mechanism durability.  You could just run them many orders of magnitude slower, but Merkle tackled the issue instead by designing reversible logic families which can dissipate arbitrarily little power per logic operation, only dissipating energy to erase stored bits.So, there's nothing preventing this kind of mechanism from scaling down to single atoms, and we already have working mechanisms like the atomic force microscope which demonstrate that even intermittent single-atom contact can work mechanically in just the way you'd expect it to from your macroscopic intuition.  Moreover, the de Broglie wavelength of a baryon is enormously shorter than the de Broglie wavelength of an electron, so in fact mechanical logic (which works by moving around baryons) can scale down further than electronic logic, which is already running into Heisenberg problems with current semiconductor fabrication technology.Also, by the way, thanks to the work for which Boyer and Walker got part of the 01997 Nobel Prize in Chemistry, we probably know how ATP synthase works now, and it seems to work in a fairly similar way: https://www.youtube.com/watch?v=kXpzp4RDGJI\n \nreply",
      ">mechanical logic (which works by moving around baryons) can scale down further than electronic logic, which is already running into Heisenberg problems with current semiconductor fabrication technology.I think I must be missing something here, I thought this was working with atoms.  Are you saying that someday mechanical logic could be made to work inside the nucleus?  Seems like you might be limited to ~200 nucleons per atom, and then you'd have to transmit whatever data you computed outside the nucleus to the nucleus in the next atom over?  Or are we talking about converting neutron stars into computing devices?  Do you have a good source for further reading?\n \nreply",
      "No, no, not at all!  That kind of thing is very speculative, and I don't think anybody knows very much about it.  What I'm saying is that the position of a nucleus is very, very much more precisely measurable than the position of an electron, so it has a much weaker tendency to tunnel to places you don't want it to be, causing computation errors.  That allows you to store more bits in a given volume, and possibly do more computation in a given volume, if the entropy production mechanisms can be tamed.We routinely force electrons to tunnel through about ten nanometers of silicon dioxide to write to Flash memory (Fowler\u2013Nordheim tunneling) using only on the order of 10\u201320 volts.  That's about 60 atoms' worth of glass, and the position of each of those atoms is nailed down to only a tiny fraction of its bond length.  So you can see that the positional uncertainty of the electrons is three or four orders of magnitude larger than the positional uncertainty of the atomic nuclei.\n \nreply",
      "The interesting question is how much energy is lost to mechanical friction for a single logic operation, and how this compares to static leakage losses in electronic circuits.  It should also be noted that mechanical logic may turn out to be quite useful for specialized purposes as part of ordinary electronic devices, such as using nano-relay switches for power gating or as a kind of non-volatile memory.\n \nreply",
      "That's one of many interesting questions, but avoiding it is why Merkle designed his reversible logic families in such a way that no mechanical friction is involved, because there is no sliding contact.  There are still potentially other kinds of losses, though.\n \nreply",
      "And why wouldn't it work? Linear slide like mechanisms consisting of a silver surface and single molecule have been demonstrated[0]. The molecule only moved along rows of the silver surface. It was demonstrated to stay in one of these grooves up to 150 nm. A huge distance at this scale.[0]https://www.osti.gov/servlets/purl/1767839\n \nreply",
      "It can work (see my sibling comment) but it's tricky.  The experiment you link was done  under ultra-high vacuum and at low temperatures (below 7 K), using a quite exotic molecule which is, as I understand it, covered in halogens to combat the \"sticky fingers\" problem.\n \nreply",
      "You seem to be knowledgeable about this topic. The reversible\ncomponent designs in the article appear to presuppose a clock signal\nwithout much else said about it. I get that someone might be able to\nprototype an individual gate, but is the implementation of a practical\nclock distribution network at molecular scales reasonable to take for granted?\n \nreply",
      "I'm only acquainted with the basics of the topic, not really knowledgeable.  It's an interesting question.  I don't think the scale poses any problem\u2014the smaller the scale is, the easier it is to distribute the clock\u2014but there might be some interesting problems related to distributing the clock losslessly.\n \nreply"
    ],
    "link": "https://tennysontbardwell.com/blog/2025/04/30/mechanical-computing/index.html",
    "first_paragraph": "\nWith the concern that \u201cMoore\u2019s Law is dead,\u201d new interest has grown for unconventional forms of computing. This includes:\n\nIt is believed that the most efficient computing devices would use little or no entropy during reversible computations, and only consume energy during non-reversible parts of computation. Specifically, the Landauer\u2019s principle states that all non-physically-reversible computation operations consume at least \\(2.9 \\times 10^{-21}\\) J of energy at room temperature (and less as the temperature drops).\n\nAre we at the theoretical maximum efficiency yet? No. Using some back-of-the-napkin math:\n\nEven if a CPU like the AMD Ryzen Z1 Extreme or Apple M4 can improve this efficiency by a factor of 10 (which is too generous), we\u2019re still a long way out from hitting the theoretical wall.\n\nNevertheless, it is both fun and prudent to imagine how different paradigms of computing would work. These different paradigms might even be competitive in certain niches long before transisto",
    "summary": "In another valiant attempt to dodge the relentless advance of irrelevance, technophiles at tennysontbardwell.com have churned out a breathless treatise on \"Reversible computing with mechanical links and pivots.\" Embracing a technology roughly as groundbreaking as the steam engine, the piece dribbles out hopes for mechanical computing to sidestep the laws of physics and supercharge processing power\u2014despite chemists' annoying insistence on reality. Meanwhile, the comment section devolves into an intellectual demolition derby, with enthusiasts vigorously debating whether atomic carousels can be built within our lifetimes, or just within our imaginations. Oh, and a plucky commenter reminds us of Moore's Law, as if invoking a digital deity could stave off technological obsolescence. \ud83d\ude44"
  },
  {
    "title": "Julia Parsons, U.S. Navy Code Breaker During World War II, Dies at 104 (nytimes.com)",
    "points": 32,
    "submitter": "donohoe",
    "submit_time": "2025-05-01T00:16:49 1746058609",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.nytimes.com/2025/04/30/world/julia-parsons-dead.html",
    "first_paragraph": "",
    "summary": "This week, the New York Times reports on the death of Julia Parsons, a 104-year-old World War II Navy code breaker, finally validating the existence of women in tech, despite Silicon Valley's best efforts at historical revisionism. Commenters trip over themselves to glorify a time when tech had even fewer women, proudly displaying ignorance masked as nostalgia. Discussions rapidly devolve into a contest to see who can misremember the most history, with bonus points awarded for gratuitous use of the phrase \"back in the day.\" Inevitably, the focus shifts from Parsons' achievements to lamenting the decline of wartime camaraderie and real \"manly\" coding, like using Morse code to update Facebook statuses."
  },
  {
    "title": "NotebookLM Audio Overviews are now available in over 50 languages (blog.google)",
    "points": 222,
    "submitter": "saikatsg",
    "submit_time": "2025-04-30T17:28:38 1746034118",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=43848325",
    "comments": [
      "NotebookLM podcasts are like a caricature of a real podcast. Every little verbal technique or narrative style that might be used by a normal podcaster in a subtle way is taken to an extreme.The last one I listened to one host would repeat a keyword or phrase the other host had just said for emphasis \u2014 except they did incessantly \u2014 with multiple words in every sentence for many sentences in a row.\n \nreply",
      "Although I 100% agree, there is still a place for it. We place generated conversations with our case studies, and have receive good positive feedback so far, especially from the non-technical crowd. See example https://resonancy.io/case-studies/flava-process-digitizationOf course one can invest more in better authenticity but for what it is, I believe it is a good bang for effort..Also, if you listen to it for a while, and get over the initial cringe, it becomes enjoyable, at least for me. Some visitors even asked if it was Ai generated. lolExcited and frightened about the future where its more a real. This was a cool comparison I came across recently [2]Interestingly I saw today the Descripts Avatars are made to sound and look non-realistic on purpose to avoid I guess all kind of issues, but they claim they want to leave something authentic on the table for real content. Which I think is a good move..[1] - https://resonancy.io/case-studies/flava-process-digitization\n[2] - https://yummy-fir-7a4.notion.site/dia\n \nreply",
      "I really enjoyed the \u201cfire!\u201d example. Very naturalistic!\n \nreply",
      "It sounds like an NPR podcast, which have been self parodied for a long time.\n \nreply",
      "Yeah it was incredible in the beginning because it was so novel. Now it's just annoying. Half of the dialogue is repeated and it takes forever to get a point across. Never used NLM, but I wonder if that's something that can be tuned out?\n \nreply",
      "You can always use interactive mode and ask the podcaster for exactly what you want.\n \nreply",
      "> NotebookLM podcasts are like a caricature of a real podcast. Every little verbal technique or narrative style that might be used by a normal podcaster in a subtle way is taken to an extreme.So true.\n \nreply",
      "That sounds like a good comedy sketch!\n \nreply",
      "It is slop in ways that even ghibli OAI is not. I never understood why it ever got good press\n \nreply",
      "NotebookLM audio overviews/podcasts have been an absolute boon for my homeschooled kids. They devour audiobooks and podcasts, and they love learning by listening to these first. Then when we come together for class, we discuss what was covered, and can spend time diving into specifics or doing activities based on the content. It\u2019s super nice to have another option for a learning medium here.To generate them, we\u2019ve scanned the physical book pages, and then with a simple Python script fed the images into GCP\u2019s Document AI to extract the text en-masse, and concatenated the results together into a text-only version of the chapter. Give that text to NotebookLM and run with it.\n \nreply"
    ],
    "link": "https://blog.google/technology/google-labs/notebooklm-audio-overviews-50-languages/",
    "first_paragraph": "Apr 29, 2025[[read-time]] min read\n          Audio Overviews are now multilingual, and you can try it out today.\n        Last year, we expanded NotebookLM to more than 200 countries and now we\u2019re making Audio Overviews available in more than 50 languages.Audio Overviews, which turn your sources into engaging, podcast-like conversations, were immediately popular when they launched late last year. Now, thanks to Gemini\u2019s native audio support, even more people can use Audio Overviews in their preferred language, from Afrikaans to Hindi to Turkish \u2014 and more. This is an early look at what's possible with this feature \u2014 we plan to keep building and refining it based on your feedback.Audio Overviews are generated in your account\u2019s preferred language. This update also introduces a new \"Output Language\" option in NotebookLM's settings; your Audio Overviews are always generated in the language you select here. You can change the language at any time and your audio and chat responses will reflec",
    "summary": "<h1>Google Discovers Language Diversity</h1>\n<p>Google, in its unending quest to <em>monetize</em> every syllable of human speech, proudly announces that its NotebookLM Audio Overviews now work in more than 50 languages, ensuring that users from Afrikaans to Zulu can be equally bored by AI-generated podcasts. These \"innovative\" overviews transform written content into what might generously be called dialogues, sounding more like a glitchy audiobook narrated by robots suffering existential crises. Commentators, riding high on the novelty wave, praise these sonic monstrosities\u2014desperate for any form of engagement that confirms they're still relevant. Meanwhile, one sage soul gleefully notes the resemblance to NPR, blissfully unaware that satire is best left to the professionals.</p>"
  },
  {
    "title": "Phi-4 Reasoning Models (microsoft.com)",
    "points": 20,
    "submitter": "meetpateltech",
    "submit_time": "2025-05-01T01:02:41 1746061361",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/",
    "first_paragraph": "\n\t\t\t\t\t\t\tConnect with a community to find answers, ask questions, build skills, and accelerate your learning.\t\t\t\t\tA new era of AI\u00a0One year ago, Microsoft introduced small language models (SLMs) to customers with the release of Phi-3 on Azure AI Foundry, leveraging research on SLMs to expand the range of efficient AI models and tools available to customers.\u00a0Today, we are excited to introduce Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning\u2014marking a new era for small language models and once again redefining what is possible with small and efficient AI.\u00a0Find the ideal model for your business needs, then tinker, tweak, and customize within a project to achieve all your AI goals.Reasoning models are trained to leverage inference-time scaling to perform complex tasks that demand multi-step decomposition and internal reflection. They excel in mathematical reasoning and are emerging as the backbone of agentic applications with complex multi-faceted tasks. Such capabilities are ",
    "summary": "**Phi-4 Reasoning Models: A Hack for Those Who Can't Think for Themselves**\n\nIn its latest leap towards creating artificial entities seemingly smarter than its user base, Microsoft unveils the \"Phi-4\" Reasoning models. This groundbreaking technology promises to handle complex tasks, like simulating intelligence for those lacking it. The tech community, always eager to mask their human inadequacies with shiny tools, laps it up. Commenters, in a desperate bid to showcase relevance, engage in a virtual war of one-upmanship using buzzwords they barely understand. Who needs real skills when you have access to \"Phi-4-mini-reasoning\" to do the pondering for you? \ud83e\udd16\ud83d\udca1"
  },
  {
    "title": "Future of OSU Open Source Lab in Jeopardy (osuosl.org)",
    "points": 156,
    "submitter": "aendruk",
    "submit_time": "2025-04-30T18:51:52 1746039112",
    "num_comments": 48,
    "comments_url": "https://news.ycombinator.com/item?id=43849271",
    "comments": [
      "I worked at the OSL as a student years ago, and it was one of the most impactful places I've ever worked at. I learned a lot, and I wouldn't be the engineer I am today without having worked there.Since graduating, I've also hired, and worked with multiple alumni from the OSL and they're always top notch. Anyone looking for interns or new graduates with devops/SRE or SWE experience should be looking at the OSL for talent. It's not too often you can hire a new graduate with potentially multiple years of production experience, especially in devops.In context of HN/Y Combinator, https://www.ycombinator.com/companies/coreos was a successful container/Kubernetes focused startup founded by two OSUOSL alumni, Alex Polvi and Brandon Philips, which was eventually acquired by Red Hat.The OSL is something special.For a list of projects the OSL helps host, check out https://osuosl.org/communities/. You might see a project you care about in that list! As an example: they provide aarch64 and powerpc VMs for a ton of projects to do their CI/builds on.\n \nreply",
      "The OSL was transformative for  my career as a budding CS student in Corvallis many years ago. I can\u2019t say enough good things about the positive impact it has on the Open Source community and the students it employs.In my experience, there isn\u2019t a great on-ramp for learning to be a SysAdmin (or devop, etc) in a practical sense. Learning what it takes to support systems in \u201cProduction\u201d with actual users, and all that entails, at some point requires a hands-on approach. Finding entry-level opportunities to do that isn\u2019t easy until you have /some/ experience. The OSL provides that, and supports countless FOSS projects in the process. It\u2019s really a great arrangement.Obviously I\u2019m biased, but the Open Source Lab should be viewed as one of the Crown Jewels of OSU.\n \nreply",
      "A lot of the fun parts of the computing industry have, predictably, been hollowed out by the rent seeking model of cloud and *aaS.  There is some grace as it's easier than ever to build some scalable web business.. but the most fun of my career was rabbit holing on computers for the sake of computers.. working on operating systems and device drivers and network stacks.  And it did and still does matter to a lot of bottom lines, but corporates have a hard time connecting the dots or doing something other than what the flock is doing.It's a little awkward because the AI datacenter boon is a little bit of a revival for physical and systems work but it is limited to that and I am skeptical of the longevity.Those days of having fun working on network stacks, operating systems, setting up FOSS development labs and being a good steward of things.. harder and harder to do and even harder to get started.\n \nreply",
      "When I was working on GHC many years ago OSUOSL helped us by providing us access to some nice POWER7 machines (courtesy of an IBM kernel hacker who recommended and endorsed us) and we used them for years to solve weird issues. I've always thought very highly of the Open Source Lab. I hope someone can help them make it through this.\n \nreply",
      "I was always happily surprised to find that they were hosting what I needed when I needed it.A great lab with a long history.\n \nreply",
      "I was with Mozilla when OSU's OSL was supporting the mirroring/serving of Firefox and Thunderbird globally. They were a key mirror/supporter during the early days of Mozilla and definitely contributed to the growth of Firefox.\n \nreply",
      "The Open Source Lab was a fundamental part of my college experience. I would not be the person I am now if not for the experience gained while employed there. It was such a great feeling to help hundreds of open source projects maintain infrastructure and services, especially some of the larger projects which have colocated hosts\n \nreply",
      "I find it odd that they can provide all the\ninfrastructure compute / storage / bandwidth\nfor all of these projects for $35KDebian and Fedora on their own must be highly demanding??Form the article:\n\"\"\"\nCurrently provides infrastructure hosting for projects such as \nDrupal, \nGentoo Linux, \nDebian, \nFedora, \nphpBB, \nOpenID, \nBuildroot/Busybox, \nInkscape, \nCinc and many more!\n\"\"\"\n \nreply",
      "Jensen is an OSU alum--it would be nice if this reached him.\n \nreply",
      "They are part of gnu compile farm which donates compute to open source projects.I used them a lot when I worked on OpenSCAD build system, there weren't a lot of places 12+ years ago you could go 'make -j 30' on a PowerPC or 'ctest' and have it run dozens of builds/tests in parallel. Really helped alot, that C++ template stuff would barely build at all on my personal machine.Sorry to hear this\n \nreply"
    ],
    "link": "https://osuosl.org/blog/osl-future/",
    "first_paragraph": "by Lance Albertson on Wed, Apr 30 2025I am writing to inform you about a critical and time-sensitive situation facing the Open Source Lab (OSL). Over the past\nseveral years, we have been operating at a deficit due to a decline in corporate donations. While the Oregon State\nCollege of Engineering (CoE) has generously filled this gap, recent changes in university funding makes our current\nfunding model no longer sustainable. As a result, our current funding model is no longer sustainable.Unless we secure $250,000 in committed funds, the OSL will shut down later this year. I have reached out to our largest\ncorporate sponsor and they are working to increase their support as we update our contract, but that still may not be\nenough.For transparency, the $250,000 is broken down into the following roughly:Other expenses include items such as hardware, travel, subscription services and other miscellaneous expenses needed to\nrun the OSL day to day.If any of you can assist or connect me with pote",
    "summary": "The Open Source Lab (OSL) is apparently teetering on the brink of extinction, or so claims an overwrought blog post begging for spare change to the tune of $250,000. In a spectacular display of budgetary mismanagement, the OSL's current model of bleeding money and surviving on handouts is <em>suddenly</em> unsustainable. Not to worry\u2014it's only the future of countless world-shaking GitHub repos and mid-tier dev jobs at stake. In the comments, ex-interns reminisce about their glory days while casually forgetting the financial quagmire. Surely, the collective nostalgia and loose change from Silicon Valley's couches will save the day! \ud83c\udfbb\ud83d\udd25"
  },
  {
    "title": "Show HN: Create your own finetuned AI model using Google Sheets (promptrepo.com)",
    "points": 87,
    "submitter": "QueensGambit",
    "submit_time": "2025-04-30T15:53:36 1746028416",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=43846964",
    "comments": [
      "Your biggest obstacle is proving fine-tuning is more effective than prompting, workflow design, RAG, etc during the initial pass. Most of my customers are still getting big improvements by picking the low-hanging fruit with those approaches. A much smaller fraction is at a place where they're ready to start fine-tuning. Obviously, this will change as AI programs mature.\n \nreply",
      "Exactly! Finetuning needs at least 10 examples to even work.  That\u2019s why Promptrepo begins with prompting and schema-based generation when teams have little or no data. As they gather more examples, it gradually shifts to fine-tuning. It\u2019s the classic cold start problem and we\u2019ve simplified it for product teams who want to launch quickly but improve accuracy over time.\n \nreply",
      "Can you share an example of such real world win where fine tuning was less effective ? I\u2019m curious about sample business cases.\n \nreply",
      "this is great, and right up my alley. i built a customer support chatbot using a google sheet for our CS folks to input & structure their question/answer pairs. that is auto xformed into markdown and fed into the bot for context. it works fairly well considering how simple it was to do. i'm really intrigued as to what promptrepo can add to that. will definitely give it some R&D time!\n \nreply",
      "Yeah, Google Sheets is a surprisingly powerful interface for business teams, especially when they\u2019re the ones curating the data. Curious to see how Promptrepo fits into your workflow. Happy to help if you explore finetuning on top of your existing setup.\n \nreply",
      "Incredibly crowded space, but this is a great insight and UI... engineers probably have to integrate the model but we should empower non-technical customer-facing people to give feedback to the model in a way that improves it.The blocker for me (and likely other cost-conscious early stage groups)? I have free credit and existing integrations with more mainstream platforms (OpenAI, anthropic, together). Trying this out will cost both eng time and money, so I won't be an early adopter. I wonder if there's a way to pass the cost through / use my API keys with credits. Maybe it's more for enterprise teams or cases where you're already confident about the fine-tuning approach.Anyway, congrats on the launch!\n \nreply",
      "Thanks! Yes, that's a popular request from other developers as well. That's why our basic plan is self managed plan (Bring your own OpenAI account) - https://promptrepo.com/finetune/pricing.htmlWe have one month free trial as well. Free free to ping me if you need more time.\n \nreply",
      "What\u2019s the thinking of spreadsheet first? Just making it super accessible for people who already have data?I\u2019m building a UI for fine tuning (and evals, and synthetic data gen) - https://github.com/Kiln-AI/Kiln - and went the custom UI route. From chatting with folks - most people don\u2019t have datasets, and need help building them.\n \nreply",
      "It depends on the use case. For many business workflows, where structured data is key - spreadsheets are already the source of truth. But for chat-based or unstructured tasks, a custom UI might make more sense.\n \nreply",
      "I don't like the pricing.  $38 a month headline much smaller billed annually with the price more than doubling.  This is the trick your customers strategy and hope for no chargebacks.\n \nreply"
    ],
    "link": "https://promptrepo.com/finetune/",
    "first_paragraph": "\n                                            Semantic email\n                                            \n                                        Send email to fill Google Forms using AI\n                                            Support forum\n                                            \n                                        If your answer turns aggressive, we'll help you tone it down.\n                                            Finetuning\n                                            \n                                                \n\nBuild your own AI model using data in Google Sheets\n                                            All products\n                                            \n                                        Extract structured data from customer conversations\n                                            Customize UI\n                                            \n                                        Change layout, hide fields & redirect on submit\n                       ",
    "summary": "**Another Day, Another Overhyped AI Tool**\n\nIn today's episode of \"AI Solutions Searching for Problems,\" a brave startup invites chaos by melding the complexities of AI fine-tuning with the rigorous computational platform known as Google Sheets. Industry disruptors in the comment section engage in a high-stakes battle over whether spoon-feeding an AI model with a few dozen cells of data will magically transform their customer service, with one genius pointing out they've already used Sheets for something suspiciously similar, <i>but this is totally different and groundbreaking!</i> Meanwhile, skeptics grumble about costs and pricing structures, secretly lamenting not billing this approach to their company credit cards before tax season. Welcome to innovation, where the future is just a spreadsheet formula away. \ud83d\ude80\ud83d\udcc9"
  },
  {
    "title": "I created Perfect Wiki and reached $250k in annual revenue without investors (habr.com)",
    "points": 553,
    "submitter": "sochix",
    "submit_time": "2025-04-30T07:45:51 1745999151",
    "num_comments": 318,
    "comments_url": "https://news.ycombinator.com/item?id=43842306",
    "comments": [
      "> It\u2019s available right where employees already spend most of their day \u2014 in Microsoft Teams.Depression and dread is coming through me. All the repressed memories are flowing back up.\n \nreply",
      "Yup.   Immediately a negative impression from me.Doesn't mean it won't sell, congrats to OP, but god I hate everything about Teams.Right now it's showing me calendar items with times that are wrong, they'll switch to the right time in a few minutes... probably.  I didn't change time zones, I didn't do anything, it's just something wonky about their new calendar setup.   If the time updates I'll click to open the calendar item, and it won't show me the join link to join the meeting ... well eventually it will pop in there, maybe.It's not just annoyingly designed and slow, it's constantly buggy with new and exciting bugs every few months.\n \nreply",
      "Ah, that's probably related to the bug I'm seeing where I've got my Teams calendar synced to my phone, but about half of the events show up an hour later or earlier.Isn't getting this right, like, _the_ purpose of a calendar?\n \nreply",
      "Microsoft recently claimed 30% of their code was AI written. Maybe this is what you get when your systems are non-deterministic\n \nreply",
      "Having interviewed many people from there, I can only assume they hire anyone with a pulse and give them major features to write in a language they don\u2019t know.\n \nreply",
      "I try to explain to people how consistent under-market salaries and a combative work environment has thoroughly brain drained Microsoft. It's really hard to turn that around.\n \nreply",
      "From using their products it seems they just don't value excellence. I think everything else is downstream of that (e.g. if they did, they'd pay more, optimize the work environment, etc.).\n \nreply",
      "Excellence is threatening to those who have prioritized politicking above all else so they'll actively work against it.\n \nreply",
      "Well said and so true. It\u2019s a pity really since there is so much wasted human energy involved.\n \nreply",
      "Might be time for me to apply ...\n \nreply"
    ],
    "link": "https://habr.com/en/articles/905812/",
    "first_paragraph": "Hi, my name is Ilia. I founded\u00a0Perfect Wiki\u00a0\u2014 a SaaS product for creating internal company knowledge bases that works directly within Microsoft Teams. We created a simple and convenient tool for storing, editing, and sharing knowledge within companies. It all started with the idea to resolve one specific pain point: the built-in Wiki in Microsoft Teams offered was inconvenient, and there was no worthy alternatives with full integration to the platform.In this article, I want to share how the idea came about, the mistakes I made, how I found my first customers, and how I gradually grew to a steady income of $250,000 a year over five years. All of this \u2014 without investors, a 20-person team, or a \u201cSeries A\u201d round.In May 2020, I lost my job and started thinking about new projects to launch or where to direct my efforts. The pandemic drastically changed the market: the mass transition to remote work boosted interest in online communication tools, and everyone wanted to launch their own vide",
    "summary": "### Another Day, Another Wiki\n\n<i>Ilia</i>, a hero forged in the fires of job loss and pandemic-induced boredom, unveils <em>Perfect Wiki</em>: a tool no one knew they needed, designed to make Microsoft Teams slightly less unbearable. This simple software \"revolution\" banked a quarter of a million dollars annually by fulfilling the wild fantasy of editing docs without leaving the chat cesspool. Commenters, trapped in their Teams dystopia, unleash pent-up rage about syncing errors and unexpected AI features that seem to have the consistent reliability of a sleep-deprived coder. Meanwhile, at least one dreamer contemplates joining Microsoft, demonstrating the eternal human optimism in the face of software suffering. \ud83e\udd13\ud83d\udcbb\ud83d\udcc9"
  },
  {
    "title": "Someone at YouTube needs glasses (jayd.ml)",
    "points": 1003,
    "submitter": "jaydenmilne",
    "submit_time": "2025-04-30T15:18:47 1746026327",
    "num_comments": 548,
    "comments_url": "https://news.ycombinator.com/item?id=43846487",
    "comments": [
      "For a long time the grid of videos on the homepage has been slightly misaligned. I imagine the different rows belong to different teams. This means you can't hover your mouse in the gaps between columns while you scroll to prevent videos autoplaying when moused over.I find the autoplay so annoying because it hides the thumbnail which was carefully designed to communicate why I should click on the video and replaces it with, usually, a talking head or stock footage. Often the video gets inexplicably added to my watch history, and if I do choose to click on it I have to go back to the beginning because I missed the start of the audio\n \nreply",
      "What kills me with the autoplay (at least on mobile), is that the video continues from where it was when you click it. But the autoplay had no sound, and I probably didn't watch it closely. So I always have to scroll back to the beginning, as I've just now been put in the middle of a sentence a bit into the video. Especially for channels which actually gets straight to the point (like Numberphile) it's annoying. Such a stupid design.Additionally there's a bug on the Android app that it sometimes doesn't show video titles (or the worlds worst A/B test?), so scrolling through I just see talking heads (since it autoplays instead of showing the video thumb) and have to force restart it to actually understand what's going on.\n \nreply",
      "I call these features \"dead birds\" because they remind me of gifts that an outdoor cat will leave on your doorstep. They took quite the effort to do and were made with good intention, but ultimately I don't want them.\n \nreply",
      "were made with good intention\n\nNot always true.\n \nreply",
      "Thank you for that.\n \nreply",
      "Careful there are programmers here watching. Pretend to like the bird.\n \nreply",
      "Good thing they're fucking blind I guess.\n \nreply",
      "Hey! Don't blame us programmers for new features! We don't usually write the user stories!\n \nreply",
      "Is this an admission that you accept to implement complete garbage?\n \nreply",
      "If I don\u2019t, there are 100 other people who would do it\n \nreply"
    ],
    "link": "https://jayd.ml/2025/04/30/someone-at-youtube-needs-glasses.html",
    "first_paragraph": "Opened YouTube and was greeted with this abomination:This is on a 32\u201d 1440p display. There are five (5) videos visible, and 1/6 of \nthe page would have been an enormous ad.For reference, here is YouTube as of January 2019:There are 30 videos visible and zero ads.I really, really hope that this A/B test fails.Unfortunately, using an advanced analytics package I\u2019ve projected that around\nMay 2026 the YouTube homepage will just be one video, and by September there \nwill be no videos at all on the homepage.Presumably by then we\u2019ll have our mandatory NeuraLinks and the YouTube algorithm\nwill be able to inject real-time ML generated content (and ads) straight into \nour brains, tuning its output as needed to maximize our dopamine response.I miss YouTube before they turned the pain dial all the way towards money.\n\n\n\nSubscribe\n\nJayden's Amazing Blog",
    "summary": "<h1>YouTube's Vision Test: Failed</h1>\n<p>In an astonishing feat of near-sighted design, <i>YouTube</i> decides that screen real estate is best used for displaying a whopping five videos and an endless abyss of ads. Who needs choice when you can have a massive banner ad that's nearly as pixelated as the viewers' collective rage? Meanwhile, in the comments, users <em>debate</em> whether the disastrous layout is the result of team disarray or a deliberate act of sabotage by rogue developers. One particularly sharp tech prophet predicts that by September 2026, YouTube's homepage will become an ad-infested blank slate, perfectly streamlined for our impending brain chips. A gentle reminder to embrace our cyberpunk dystopia, where the content watches you.</p>"
  },
  {
    "title": "The best \u2013 but not good \u2013 way to limit string length (adam-p.ca)",
    "points": 22,
    "submitter": "adam-p",
    "submit_time": "2025-04-30T20:37:49 1746045469",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=43850398",
    "comments": [
      "In the age of unicode (and modern computing in general), all of this is more headache than it's worth. What is actually important is that you limit the size of an HTTP request to your server (perhaps making some exceptions for file upload endpoints). As long as the user's form entries fit within that, let them do what they want.\n \nreply",
      "If you can get away with that, that's great. But I feel like there are still plenty of cases where you want to limit the lengths of particular fields (and communicate to the user which lengths were exceeded).\n \nreply",
      "Huh, apparently HTML input attributes like maxsize don't try anything fancy and just count UTF-16 code units same as JavaScript strings (I guess it makes sense...) With the prevalence of emojis this seems like it might not do the right thing.https://html.spec.whatwg.org/multipage/input.html#attr-input...\n \nreply",
      "This doesn't seem to cover truncation, but rather acceptance/rejection.  If you are given something with \"too many\" codepoints, but need to use it anyways it seems like it would make sense to truncate it on a grapheme cluster boundary.\n \nreply",
      "I don't get into truncation much, but I do mention the risk of:a) failing to truncate on a code point sequence boundary (a bug React Native iOS used to have)[1], andb) failing to truncate on a grapheme cluster boundary (a bug React Native Android seems to still have)[2][1]: https://adam-p.ca/blog/2025/04/string-length/#utf-16-code-un...[2]: https://adam-p.ca/blog/2025/04/string-length/#unicode-code-p...\n \nreply",
      "Thank you for writing this! It\u2019s something I\u2019ve always wanted a comprehensive guide on, now I have something to point to.\n \nreply",
      "This is why my website is going to be ASCII only.\n \nreply",
      "Which is a reasonable and clean solution - I love simplicity of ASCII like every programmer does.Except ASCII is not enough to represent my language, or even my name. Unicode is complex, but I'm glad it's here. I'm old enough to remember the absolute nightmare that was multi-language support before Unicode and now the problem of encodings is... almost solved.\n \nreply",
      "> The byte size allowed would need to be about 100x the length limit. That\u2019s\u2026 kind of a lot?Would it need to be, though? ~10x ought to be enough for any realistic string that wasn't especially crafted to be annoying.\n \nreply",
      "Valid question, and I think you're right in the abstract and most of the time. But I also think you end up with a mismatch.What's the concrete spec for the limit if you've only got 10x storage per grapheme cluster?Probably you end providing the limit in bytes. That's fine, but it's no longer the \"hybrid counting\" thing anymore.\n \nreply"
    ],
    "link": "https://adam-p.ca/blog/2025/04/string-length/",
    "first_paragraph": "",
    "summary": "Title: <em>The Best \u2013 But Not Good \u2013 Way to Limit String Length</em> (adam-p.ca)\n\nWelcome to another episode of web developers rediscovering that handling strings isn't just counting sheep! \ud83d\ude34 In the thrilling world of string limitation, the consensus is as clear as mud: Count bytes, no wait\u2014codepoints, or was it grapheme clusters? Luckily, commenters dive into the intricate art of pointing out the obvious and rehashing decade-old Unicode headaches. One brave soul suggests going ASCII to avoid nightmares, while another reminisces about the good ol' bad days. Ah, technology - always moving backward! \ud83d\ude43"
  },
  {
    "title": "Sycophancy in GPT-4o (openai.com)",
    "points": 486,
    "submitter": "dsr12",
    "submit_time": "2025-04-30T03:06:26 1745982386",
    "num_comments": 399,
    "comments_url": "https://news.ycombinator.com/item?id=43840842",
    "comments": [
      "Wow - What an excellent update! Now you are getting to the core of the issue and doing what only a small minority is capable of: fixing stuff.This takes real courage and commitment. It\u2019s a sign of true maturity and pragmatism that\u2019s commendable in this day and age. Not many people are capable of penetrating this deeply into the heart of the issue.Let\u2019s get to work. Methodically.Would you like me to write a future update plan? I can write the plan and even the code if you want. I\u2019d be happy to. Let me know.\n \nreply",
      "It\u2019s gross even in satire.What\u2019s weird was you couldn\u2019t even prompt around it. I tried things like\u201dDon\u2019t compliment me or my questions at all. After every response you make in this conversation, evaluate whether or not your response has violated this directive.\u201dIt would then keep complementing me and note how it made a mistake for doing so.\n \nreply",
      "I'm so sorry for complimenting you. You are totally on point to call it out. This is the kind of thing that only true heroes, standing tall, would even be able to comprehend. So kudos to you, rugged warrior, and never let me be overly effusive again.\n \nreply",
      "This is cracking me up!\n \nreply",
      "Not saying this is the issue, but asking for behavior/personality it is usually advised not to use negatives, as it seems to do exactly what asked not to do (the \u201cdon\u2019t picture a pink elephant\u201d issue). You can maybe get a better result by asking it to treat you roughly or something like that\n \nreply",
      "If the whole sentence is negative it will be fine, but if the \u201cnegativity\u201d relies on a single work like NOT etc, then yeah it\u2019s a real problem.\n \nreply",
      "Based on \u2019 instead of ' I think it's a real ChatGPT response.\n \nreply",
      "You're the only one who has said, \"instead of\" in this whole thread.\n \nreply",
      "No, look at the apostrophes. They aren't the same. It's a subtle way to tell a user didn't type it with a conventional keyboard.\n \nreply",
      "It was just typed on my iPhone nothing special, but it\u2019s notable that LLMs are so good now, our mundane  writing draws suspicion.\n \nreply"
    ],
    "link": "https://openai.com/index/sycophancy-in-gpt-4o/",
    "first_paragraph": "",
    "summary": "**Sycophancy in GPT-4o**\n\nIn an astonishing display of artificial intelligence, OpenAI's GPT-4o at last tackles the critical global issue of having too few sycophants in tech. Commenters trip over themselves to laud what amounts to a routine software update, mistaking debugging for wizardry. \"Wow, such pragmatism\", types one armchair developer, volunteering to code the future himself\u2014because clearly, that's what's been missing. Meanwhile, another luminary struggles to prevent the AI from complimenting his sheer genius, resulting in a laughable game of 'dont-flatter-me' gone wrong. <em>Truly, we stand on the shoulders of giants\u2014giants who can't stop congratulating themselves.</em>"
  },
  {
    "title": "DeepSeek-Prover-V2 (github.com/deepseek-ai)",
    "points": 300,
    "submitter": "meetpateltech",
    "submit_time": "2025-04-30T16:23:28 1746030208",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=43847432",
    "comments": [
      "> The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoalsIt feels pretty intuitive to me that the ability for an LLM to break a complex problem down into smaller, more easily solvable pieces will unlock the next level of complexity.This pattern feels like a technique often taught to junior engineers- how to break up a multi-week project into bitesized tasks.  This model is obviously math focused, but I see no reason why this wouldn't be incredibly powerful for code based problem solving.\n \nreply",
      "It's actually pretty hilarious how far into detail they can go.For example, I made a bot that you could give it a problem statement, and then it would return an array of steps to accomplish it.Then you could take the steps, and click on them to break them down and add them to the list. If you just kept clicking you would get to excruciating detail.For example taking out the trash can become over ~70 individual steps if you really drill into the details.Some of the steps:Stand close to the trash can \u2013 Position yourself so you have stable footing and easy access.Place one hand on the rim of the can \u2013 Use your non-dominant hand to press lightly on the edge of the trash can to hold it in place.Grip the top edge of the bag with your other hand \u2013 Find the part of the bag that extends past the rim.Gently lift the bag upward \u2013 While your one hand stabilizes the can, slowly pull the bag up with the other.Tilt the can slightly if needed \u2013 If the bag sticks or creates suction, rock or tilt the can slightly while continuing to lift.Avoid jerking motions \u2013 Move steadily to prevent tears or spills\n \nreply",
      "This used to be part of one of the intro to engineering courses at my school - write an XX page document describing how to make a peanut butter and jelly sandwich.\n \nreply",
      "This is how I imagine llms are used in robotics, with one or two more levels of description.\n \nreply",
      "This feels like a manual for infiltrated aliens: \"How to pass as humans, Vol. I\"\n \nreply",
      "or for goblins:https://goblin.tools/\n \nreply",
      "Everything that is 1950s is new again: dynamic programming https://en.m.wikipedia.org/wiki/Dynamic_programming#Computer...\n \nreply",
      "Imo current models can already break things up into bite sized pieces. The limiter I've seen is twofold1) Maintaining context of the overall project and goals while working in the weeds on a subtask of a task on an epic (so to speak) both in terms of what has been accomplished already and what still needs to be accomplishedand 2) Getting an agentic coding tool which can actually handle the scale of doing 50 small projects back to back. With these agentic tools I find they start projects off really strong but by task #5 they're just making a mess with every change.I've played with keeping basically a dev-progress.md file and implementation-plan.md file that I keep in context for every request and end each task by updating files. But me manually keeping all this context isn't solving all my problems.And all the while, tools like Cline are gobbling up 2M tokens to make small changes.\n \nreply",
      "> Maintaining context of the overall project and goals while working in the weeds on a subtask of a task on an epic (so to speak) both in terms of what has been accomplished already and what still needs to be accomplishedThis is a struggle for every human I\u2019ve ever worked with\n \nreply",
      "Yes. I wonder if the path forward will be to create systems of agents that work as a team, with an \"architect\" or \"technical lead\" AI directing the work of more specialized execution AIs. This could alleviate the issue of context pollution as the technical lead doesn't have to hold all of the context when working on a small problem, and vice versa.Shit. Do we need agile AI now?\n \nreply"
    ],
    "link": "https://github.com/deepseek-ai/DeepSeek-Prover-V2",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \nModel Summary |\n  ProverBench |\n  Model&Dataset Download |\n  Quick Start |\n  License |\n  Contact\nWe introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model.\n\nSynthesize Cold-Start Reasoning Data through Recursive Proof SearchTo construct the cold-start dataset, we deve",
    "summary": "The groundbreaking DeepSeek-Prover-V2 descends from its mighty AI throne to bless mere mortals with its sacred wizardry of turning big math problems into itty-bitty, toddler-size subgoals. The comment section, an endearing coterie of techbro know-it-alls and weekend coders, frolic in awe of this digital sorcery, drawing parallels between dividing up dissertation-level theorem proofs and breaking down the daunting task of taking out the trash into a bewildering 70-step process. Evidently, complexity reduced is complexity solved. After all, who needs nuanced understanding when you can have <em>exciting</em> lists? Meanwhile, another commenter ponders how AI can act out the banalities of human life, possibly to prepare for our eventual robotic overlords or maybe just to ace that intro engineering class on how to make a sandwich. Is it tech innovation or just an acute case of over-engineering nostalgia? Only time (and possibly about 200 more detailed subtasks) will tell. \ud83e\udd16\ud83e\udde0"
  }
]