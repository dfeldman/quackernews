[
  {
    "title": "Reclaim the Stack (reclaim-the-stack.com)",
    "points": 127,
    "submitter": "dustedcodes",
    "submit_time": "2024-09-08T22:11:02.000000Z",
    "num_comments": 58,
    "comments_url": "https://news.ycombinator.com/item?id=41483675",
    "comments": [
      "I wish _I_ had a business that was successful enough to justify multiple engineers working 7 months on porting our infrastructure from heroku to kubernetes\n \nreply",
      "Knowing the prices and performance of Heroku (as a former customer) the effort probably paid for itself. Heroku is great for getting started but becomes untenably expensive very fast, and it's neither easy nor straightforward to break the vendor lock in when you decide to leave.\n \nreply",
      "I find AWS ECS with fargate to be a nice middle ground. You still have to deal with IAM, networking, etc. but once you get that sorted it\u2019s quite easy to auto-scale a container and make it highly available.I\u2019ve used kubernetes as well in the past and it certainly can do the job, but ECS is my go-to currently for a new project. Kubernetes may be better for more complex scenarios, but for a new project or startup I think having a need for kubernetes vs. something simpler like ECS would tend to indicate questionable architecture choices.\n \nreply",
      "Moving from Heroku to Render or Fly.io is very straight forward; it\u2019s just containers.\n \nreply",
      "If you use containers. If you're big enough for the cost savings to matter, you're probably also not looking for a service like Render or Fly. If your workload is really \"just containers\" you can save more with even managed container services from AWS or GCP.\n \nreply",
      "Unless you relied on heroku build packs.\n \nreply",
      "In my experience you can get pretty far with just a handful of vms and some bash scripts. At least double digit million ARR. Less is more when it comes to devops tooling imo.\n \nreply",
      "> you can get pretty far with just a handful of vms and some bash scripts. At least double digit million ARR.Using ARR as the measurement for how far you can scale devops practices is weird to me. Double-digit million ARR might be a few hundred accounts if you're doing B2B, and double-digit million MAUs if you're doing an ad-funded social platform. Depending on how much software is involved your product could be built by a team of anywhere from 1-50 developers.If you're a one-developer B2B company handling 1-3 requests per second you wouldn't even need more than one VM except maybe as redundancy. But if you're the fifty-developer company that's building something beyond simple CRUD, there are a lot of perks that come with a full-fledged control plane that would almost certainly be worth the added cost and complexity.\n \nreply",
      "Of course you can get away with that if your metric is revenue. (I think Blippi makes about that much with, I suspect, nary a VM in sight!The question is what you're doing with your infrastructure, not how much revenue you're making. Some things have higher return to \"devops\" and others have less.\n \nreply",
      "+1 or just use App Engine, deploy your app and scale\n \nreply"
    ],
    "link": "https://reclaim-the-stack.com",
    "first_paragraph": "",
    "summary": "**Reclaim the Stack: Where Silicon Valley Waste Meets First-World Problems**  \nIn a heroic tale of misguided engineering heroics, modern-day Sisyphuses earnestly debate the merits of **moving from Heroku to Kubernetes**\u2014because, clearly, ditching an expensive platform _just to end up managing your own infrastructure_ is peak tech brilliance. Embrace the joys as commenters leapfrog from expert advice on using AWS ECS (it's just \"easy peasy lemon squeezy\" after that first blood sacrifice to the IAM gods), to promoting the simplicity of bash scripts over a VM for those who measure their tech prowess in ARR rather than actual technology sophistication. Meanwhile, someone ardently believes that throwing words like \"Fargate\" and \"Render\" into a discussion will definitely mask their existential dread over their architecture choices. \ud83d\ude80\ud83d\ude44\ud83e\udd21"
  },
  {
    "title": "Creating a Git Commit: The Hard Way (avestura.dev)",
    "points": 47,
    "submitter": "avestura",
    "submit_time": "2024-09-04T18:47:59.000000Z",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=41449128",
    "comments": [
      "This is pretty cool. Worth noting that Git does not actually only store full copies of files every time you make a change, this article I found does a really good job at explaining Git's packing: https://gist.github.com/matthewmccullough/2695758\n \nreply",
      "Nice writeup. Reminds me of a Julia Evans post (which is the highest praise I could give it).\n \nreply",
      "I used to find it hard to properly format and write my git messages and often used to forget the git commandsThen I started using this https://github.com/zerocorebeta/Option-KSimply write your commits and it does all formatting and rewriting for youHere's a video explaining this magic: https://asciinema.org/a/mktcuXSPDTr2Mp1XVbo5tqhK1\n \nreply"
    ],
    "link": "https://avestura.dev/blog/creating-a-git-commit-the-hard-way",
    "first_paragraph": "Many of us create a few Git commits daily, either through GUI tools or\r\nthe command line. It can be as simple as following these steps:Here, we've used Git high-level commands (also known as Porcelain commands) like\r\ngit add, and git commit.\r\nHowever, there is another group of Git commands, known as\r\nPlumbing commands,\r\nthat handle the low-level operations.In this blog post, we want to create a Git commit using these low-level operations, and\r\nnot the git commit command.Before diving into low-level commands and crafting a Git commit, we need to understand\r\na few Git basics. Let's start with the states of a file in Git.Files in Git can be in one of these three states:Similarly, A Git project has three sections:Now that we understand the different sections of a Git project, we\r\nneed to know what exactly a Git commit is.A git commit is a git object. There are several types of objects in git,\r\nincluding blob, tree, and commit objects.\r\nThese objects can be found inside the .git/objects fol",
    "summary": "**Creating a Git Commit: The Hard Way (avestura.dev)**\n\n<i>Today in the life-depleting depths of developer blogs</i>, a brave soul reinvents the wheel by ditching user-friendly <em>'Porcelain'</em> commands for the cryptic delights of Git's <em>'Plumbing'</em> commands. Because evidently, making a simple commit isn't exhilarating enough unless it's done the hard way. Commenters, starved for complexity, swoon over this arcane knowledge. Watch as they compare this monumental achievement to Julia Evans, celebrate the avoidance of actually remembering Git commands, and share more tools to add even more steps to your already overcrowded workflow. Git ready for a wild ride through unnecessary complication! \ud83d\ude44\ud83d\udd27\ud83d\udcbb"
  },
  {
    "title": "The Fennel Programming Language (fennel-lang.org)",
    "points": 64,
    "submitter": "tosh",
    "submit_time": "2024-09-08T21:06:40.000000Z",
    "num_comments": 15,
    "comments_url": "https://news.ycombinator.com/item?id=41483216",
    "comments": [
      "I've talked about it a few times before when it comes up but I love this language and think it's just an incredible technical accomplishment.Lua is beloved and I have a ton of respect for that project in itself, it's definitely one of the seven wonders of the programming world. But the (well-considered) compromises for its intended use as an embedded scripting language make it pretty rough for large complex projects.Fennel is just an extremely focused aid to some of the worst lua warts. Pattern matching alone is incredible for the overloaded tables lua uses. The macro system is excellent, and since a lot of the time what you're doing with lua is defining a DSL anyway it gives you more powerful tools for that.I initially recoiled but I now think it was a genius decision to do it with just a few special forms over the lua semantics. Other than the lisp syntax there's very little new to learn to use it. And the best part is that it hooks into the lua module loader system so you can freely mix tables and functions between the two, a life changer in legacy codebases.Can't say enough good things about fennel. There are a lot of languages that I like but it's one of the only ones I think is actually good. I rarely write normal lua anymore it's just so flexible.\n \nreply",
      "For folks who are curious, it is possible to use Fennel w/ the Lua in Lualatex:https://wiki.fennel-lang.org/Fennel-in-LuaTeXbut no activity for this on tex.stackexchange (yet?) --- given the difficulties in doing recursion (see: https://tex.stackexchange.com/questions/723897/breaking-out-... ) using something like to Lisp could potentially be very helpful.\n \nreply",
      "Related:Why Fennel? - https://news.ycombinator.com/item?id=37497131 - Sept 2023 (102 comments)Language Showcase: Fennel - https://news.ycombinator.com/item?id=32349491 - Aug 2022 (2 comments)Fennel: A Practical Lisp - https://news.ycombinator.com/item?id=31029478 - April 2022 (85 comments)Fennel: A Practical Lisp - https://news.ycombinator.com/item?id=30963628 - April 2022 (2 comments)Fennel 1.0.0 Released - https://news.ycombinator.com/item?id=29221245 - Nov 2021 (1 comment)Fennel 0.8.0 Released - https://news.ycombinator.com/item?id=25842797 - Jan 2021 (1 comment)Raymarching with Fennel and L\u00d6VE - https://news.ycombinator.com/item?id=24835766 - Oct 2020 (3 comments)Fennel \u2013 Lisp in Lua - https://news.ycombinator.com/item?id=24390904 - Sept 2020 (112 comments)Neovim Configuration and Plugins in Fennel Lisp - https://news.ycombinator.com/item?id=21676606 - Dec 2019 (19 comments)Fennel \u2013 Lisp in Lua - https://news.ycombinator.com/item?id=18016168 - Sept 2018 (62 comments)\n \nreply",
      "\"you can use Fennel right here without installing anything\"It sits at 99% forever.\n \nreply",
      "With \"[sprintf] unexpected placeholder\".This is presumably https://github.com/fengari-lua/fengari/issues/147. You can workaround it by substituting \"%.f\" for \"%.0f\" in https://fennel-lang.org/fennel/fennel.lua.However Chrome won't let me override the contents of this URL for some reason. And the first Firefox response override extension I tried ended up confusing the page. So you can alternatively override the contents https://fennel-lang.org/fengari-web.js as follows:`\"string\" == typeof o.response ? a.f = yt(o.response.replace(\"%.f\",\"%.0f\")) : a.f = new Uint8Array(o.response.replace(\"%.f\",\"%.0f\"));`(Just before `var l = Ee(a);`)Opened a bug for it: https://github.com/bakpakin/Fennel/issues/485\n \nreply",
      "There's also a console message for an unhandled promise rejection with an error message about an unexpected sprintf placeholder.\n \nreply",
      "What I would love to have is a Lisp which is as easy to embed (and as portable) as Lua but comes with all the niceties of Lisp like a restart-system, REPL-driven development, numeric tower, hygienic and non-hygienic macros, etc.Unfortunately, no such thing seems to exist.\n \nreply",
      "I don't think it has absolutely all that, but janet might be close?https://janet-lang.org/\nhttps://janet.guide/\n \nreply",
      "Janet has a way to break into REPL, but AFAIK no way to restart (a la condition system).Fennel has something a bit more robust (assert-repl) and it can restart, but can only return a single value from the REPL, which often creates more errors downstream.\n \nreply",
      "It's been a couple years but I think janet has the primitives for this in the netrepl module which I think is part of the standard library. I remember being really surprised at how much introspection and control over the env when you responded to a repl connection, just at that time no one had put it together into what you want. I also seem to remember the actual janet repl being a different implementation which always seemed weird to me.\n \nreply"
    ],
    "link": "https://fennel-lang.org/",
    "first_paragraph": "Fennel is a programming language that brings together  the simplicity, speed, and reach of Lua  with the flexibility of a  lisp syntax and macro system.Anywhere you can run Lua code, you can run Fennel code.See the  install instructions in the setup guide.Fine, you can use Fennel right here without installing  anything:Curious about how a piece of code compiles? See for yourself  with a side-by-side  view how Fennel turns into Lua  and vice-versa.Looking for other versions?  Docs are generated for:Fennel's repository is on Sourcehut, and discussion occurs on  the mailing list  and the #fennel channel on Libera.Chat  and  on Matrix . There is a  read-only mirror of the repository on GitHub  for those who prefer it.Come meet the Fennel community and make friends at any of our community events.  Community interactions are subject to the code of conduct.  We periodically run  surveys  of the community.Bug reports are tracked in Sourcehut or Github.  See the  security page  for details abou",
    "summary": "The Fennel Programming Language, a nifty syntactical bridge to enlightenment for those enthralled by Lua's simplicity yet plagued by its \"seven wonders of the programming world\" level quirks, heralds its own suite of enhancements conveniently wrapped in Lisp flavor. Hooray, now developer hipsters and code ninjas alike can ponder the existential benefits of macro systems and pattern matching in a world where embedding Lisp just \"isn't quirky enough\" \ud83d\udd76\ufe0f. In overzealous pursuit, commenters\u2014armed with endless anecdotes of their newfound coding nirvana\u2014rival over who transformed their legacy codebase the fastest using Fennel's magical constructs, while simultaneously mourning the unmet pinnacle of embedding ease and REPL utopia. Meanwhile, practical issues involving unexpected placeholders spiral into a debugging odyssey ignored by most, but passionately lamented by the select philosophical few questioning their life choices in comment sections. \ud83e\uddd0"
  },
  {
    "title": "Core: an experimental new way to write videogames (github.com/damn)",
    "points": 184,
    "submitter": "resatori",
    "submit_time": "2024-09-08T18:21:00.000000Z",
    "num_comments": 118,
    "comments_url": "https://news.ycombinator.com/item?id=41482060",
    "comments": [
      "Oooh, this is cool. I always like to see different approaches to game dev (despite never having published a game!). So far I've tried- Bevy (Rust ECS engine), which is nice at first but has a lot of problems with its implementation and can become rather messy. I think it's heavily dependent on the game. Part of it will be my own incompetence.- Unity. IMO the system of gameobjects with composed modular components is the most utilitarian - it gets out of the way, and it's easy to avoid spaghetti without requiring a really strict engine-dictated structure.- Godot. I hated it. All of the awful heirarchy of OOP, a really poor builtin language, and \"signals\", which are meant to decrease spaghetti but only increased it for me. Maybe I was using it wrong? I very rarely use inheritance to the point of being bad at using it.- Pygame, back when I first learnt to code. It's quite nice for small projects - it's procedural at heart but you can make your own OOP or functional layers over it. There have been some surprisingly large projects made in it.I don't know Clojure, but it's interesting to see someone make a functional implementation of something that stereotypically seems like a good fit for OOP.\n \nreply",
      "As a professional game dev who has years of experience producing real products in both Unity and Godot, I am totally not with you about your thoughts on Godot vs unity.Godots signals are such a huge step up over Unity's built in classes having a lack of modularity.How do you even make sense of that? Godot vs Unity basically have the same scene/node/component model except Godot does it better imo. Eg) What is the difference between a prefab vs a scene in Unity anyways? Basically nothing, it's just (I'm speculating) a tech debt mistake in their design, probably still going because of how light maps work today.Unity's advantage over Godot is it's 3D renderer, built in physX, il2cpp backend for C#, profiler, general runtime performance and console support.Godots design is objectively more cohesive, as Unity has simply splintered into 10 different design directions since ~2018.I'm not trying to be a hater, I just think signals are a huge advantage for writing modular, simple stuff in Godot. I think there's plenty of great reasons to prefer Unity to Godot but \"signals\" is not one of em.\n \nreply",
      "I think the original commenter just really likes \"plug and play\" solutions with a lot of hand holding which is what Unity is excellent at. The problems come down the line.Godot is objectively a way way better tool\n \nreply",
      "> Godot is objectively a way way better toolIf you like things like Godot, Godot is the type of thing you will like.Seriously though, Godot works way better for me using C# than it does with GDScript and the OOP structure means I can refer to classes by their identity.\n \nreply",
      "Unity is a bit of a mess but to compare apples to apples when Unity has more features seems not quite fair.Are signals significantly better than Unity events or is it more that the API uses them heavily?Honestly, why doesn't Unity retool SendMessage() and public API callbacks to be an exposed Unity event?  Unlike merging prefabs and scenes, that's not even a breaking change.\n \nreply",
      "GDScript sucks big time, it's not even close to C#. They should drop that Python like language and go full in C# integration.\n \nreply",
      "The core developers are too enamored with GDScript, but that's fine, C# and C++ (GDExtension) support is excellent. There are only a handful of API functions which are hampered by their need to support GDScript, so it's something you can basically just ignore.You don't even have to use Nodes much, though they're great for lots of stuff. For real performance, you can drop all the way down to their thin wrapper over Vulkan and do whatever you want.\n \nreply",
      "GDScript makes simple things simple and complicated things complicated. It's pretty good for the simple parts of your game. You can still use C# for its complicated parts as they integrate well.\n \nreply",
      "I love everything about Godot so much, except for its UI systemIt is just so confusing to me and took me over a day to get a simple menu aligned how I wantedit might just be me not understanding how it works though\n \nreply",
      "Godot\u2019s UI system is easier when you realize that Control-derived nodes are even more granular than even HTML elements. They\u2019re more like Tailwind classes.Yes, your UI will be a tree of Control nodes 20 levels deep, and that\u2019s fine.Your root UI node will probably be an HBoxContainer, or VBoxContainer, each of which simply arranges their children horizontally or vertically, respectively. Between those two nodes you can create 90% of UIs you\u2019d want to.\n \nreply"
    ],
    "link": "https://github.com/damn/core",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Experiment - can videogame development be simple?\n      Core is an experimental new way to write videogames in the form of an Action-RPG Maker Tool&Engine&Property-Editor. The only thing missing is a game ;)It uses a simple component system, where components are just clojure vectors of [keyword value] and the different entities are clojure maps.Side effects in the game are just components like [:tx/foo param] named 'tx=transaction' similar to the datomic structure.The whole game state is stored in one atom: app/state and entities are again atoms inside the main atom (like in our universe).The whole content of the application is stored in one resources/properties.edn and uses malli-schemas for validation and can be edited with a GUI as seen in the screenshot below.Just type:It will start the application and also:The assets used are p",
    "summary": "<title>Another Day, Another Developer's Nirvana</title>\nIn a valiant attempt to mask complexity with simplicity, the 'Core' emerges as a flamboyant new playground for game developers on Github. With promises of an Action-RPG paradise through editable vectors and maps, the Core gently reminds that you still need to make that pesky game. Commenters, akin to experts in every conceivable engine, make a Beijing Olympic sport out of parading their biases, oblivious to the irony of discussing component systems while probably never finishing \u201cHello, World!\u201d \ud83d\udc7e Critics of each other\u2019s favorite toys \u2014 from Godot\u2019s 'confusing' UI to Unity\u2019s spaghetti mess \u2014 these keyboard warriors showcase the full spectrum of developer rage and utopia, accomplishing little beyond flexing their vocab of technical jargon. Where\u2019s the game though? \ud83c\udfae"
  },
  {
    "title": "Linux's Bedtime Routine (tookmund.com)",
    "points": 19,
    "submitter": "JNRowe",
    "submit_time": "2024-09-08T22:31:20.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://tookmund.com/2024/09/hibernation-preparation",
    "first_paragraph": "A place for my random thoughts about softwareHow does Linux move from an awake machine to a hibernating one?\nHow does it then manage to restore all state?\nThese questions led me to read way too much C in trying to figure out\nhow this particular hardware/software boundary is navigated.This investigation will be split into a few parts, with the first one going\nfrom invocation of hibernation to synchronizing all filesystems to disk.This article has been written using Linux version 6.9.9,\nthe source of which can be found in many places, but can be navigated\neasily through the Bootlin Elixir Cross-Referencer:https://elixir.bootlin.com/linux/v6.9.9/sourceEach code snippet will begin with a link to the above giving\nthe file path and the line number of the beginning of the snippet.These two system files exist to allow debugging of hibernation,\nand thus control the exact state used directly.\nWriting specific values to the state file controls the exact sleep mode used\nand disk controls the speci",
    "summary": "**Linux's Bedtime Routine: A Geeky Lullaby**  \nIn a dazzling display of insomnia, a brave soul ventures deep into the abyss of C code to unravel the mysteries of Linux's snooze functionality. Apparently, *Linux* doesn't just \"turn off\"; it partakes in a ceremonial dance of code, syncing file systems as if performing rhythmic gymnastics. Commenters, armed with their anecdotal evidence and self-awarded computer science degrees, engage in heated debates about the effectiveness of this nuanced bedtime process, sharing tales of systems that either sleep like babies or suffer from incurable insomnia. \ud83d\udda5\ufe0f\ud83d\udca4\ud83d\ude02"
  },
  {
    "title": "Exploiting CI / CD Pipelines for fun and profit (razzsecurity.com)",
    "points": 30,
    "submitter": "mukesh610",
    "submit_time": "2024-09-08T21:52:04.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41483541",
    "comments": [
      "The real problem is keeping sensetive information in .git directory. Like WTH would you put your password, in plaintext, in some general ini file? (or into a source file for that matter)?When I see things like those, they look so wrong to me. But sadly it's apparently uncommon nowadays: not only random bloggers, even my coworkers see nothing wrong with putting passwords or tokens into general config or source code files. \"it's just for a quick test\"1 they say and then they forget about it and the password is getting checked in, or shown at screenshare meeting.Maybe that's why there are so many security problems in industry? /rant(For those curious: for git specifically, use ssh with key auth. If for some reason you don't want this, you can set up git's credential helper to use your OS key store; or use plaintext git-crendetials, or even just good-old .netrc. For source code, something like  \"PASSWORD = open(\"/home/user/.config/mypass.txt\").read().strip()\" is barely longer than hardcoding it, but 100% eliminates chance of accidental secret checkin or upload)\n \nreply",
      "> 100% eliminates chance of accidental secret checkin or uploadYou've never worked with humans, have you?\n \nreply",
      "I\u2019ve never deployed a .git folder and wonder what systems/approaches lead to such a thing. How does that happen?\n \nreply",
      "Its easy to miss that you need to duplicate your .gitignore into your .dockerignore\n \nreply",
      "To not have to remember that one can not use a .dockerignore at all and instead explicitly pick the files and directories from the build context that need to be in the image.\n \nreply",
      "100% this!\n \nreply",
      "It's pretty common in systems where the final output to be deployed is the same as the root of the source tree. More often than not, lazy developers tend to just git clone the repo and point their web server's document root to the cloned source folder. In default configurations, .git is happily served to anyone asking for it.This seems to be automatically mitigated in systems which might have a \"build\" / \"compilation\" phase, because for the application to work in the first place, you only need the compiled output to be deployed. For instance, Apache Tomcat.\n \nreply"
    ],
    "link": "https://blog.razzsecurity.com/2024/09/08/exploitation-research/exploiting-ci-cd-pipelines-for-fun-and-profit/",
    "first_paragraph": "In today\u2019s world of fast-paced development and continuous integration, security vulnerabilities can be easy to overlook. Recently, I discovered a severe exploit chain, starting from a publicly exposed .git directory, which led to a full server takeover. This blog will walk through the chain of events, outlining how each weak point compounded the issue.A surprising number of websites still expose their .git directories to the public. When scanning for such exposures on a target, I noticed that the .git folder was publicly accessible. This is a serious vulnerability because .git stores the entire version history of a project, including configuration files, which might contain sensitive information.Impact: With access to .git/config, I found credentials, which opened the door to further exploitation. I could just clone the entire repository using the URL found inside the config file.After cloning the repository using the found credentials, I began exploring the repository\u2019s contents. In t",
    "summary": "**Hacker Heaven: The .git Catastrophe Workout**\n\nToday on RazzSecurity, an intrepid blogger once again **shocks** the world by discovering that water is wet, sky is blue, and developers routinely shove sensitive data into their .git directories like it's a digital junk drawer. Who knew? Through a death-defying journey of clicking URLs, the hero of our story uncovered credentials and took over a server, reminding everyone that the true CI/CD pipeline stands for \"Compromise Internally/Compromise Devastatingly.\" In the comments, there's an uproar as armchair experts compete to yell the loudest about how *they* would never make such rookie mistakes, entirely missing the irony that they, too, probably use \"password123\" for their bank account. One commenter suggests the revolutionary idea of not using .git for password storage, applauded by peers as if they've just solved quantum gravity. \ud83d\ude44"
  },
  {
    "title": "Htmx, Raku and Pico CSS (rakujourney.wordpress.com)",
    "points": 49,
    "submitter": "librasteve",
    "submit_time": "2024-09-08T19:51:20.000000Z",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=41482679",
    "comments": [
      "I have been in web dev since 1999, and recently switched to a backend role.I am never going back.The endless thrashing on front end borders on psychosis. Something is clearly broken, and that thing is my feeble mind.\n \nreply",
      "Like the article's writer, I remember the Netscape days and when JS came into existence. I was only a kid back then, like 9 years old, and was _just_ getting into coding in C and playing around with HTML/CSS. The results I produced, as you might have imagined, were akin to the results a toddler would produce given a wet loaf of bread to play with: mess everywhere.Anyway, back then the HTML was simple (but it was tables and spacer GIFs ha ha!) and the CSS even simpler. Now it's just such a chaotic mess that people actually think Web Assembley is a good idea; as if somehow they've invented something new: the sandbox and the machine code. Like running a process at the OS level and making _that_ better, more secure, etc. wasn't the right path.HTMLx is as close to the UI as I'll come. Everything else is an absolute headache to get into.\n \nreply",
      "> I am a simple sole, I want to reduce the cognitive load in my web projects. The general idea is to go back to the halcyon early days of the web before Netscape dropped the JS-bomb. You know HTML for the layout and CSS for the style. An elegant division of roles.(I'm not quite sure if this is the author's sentiment), but the point shouldn't be to escape JS entirely, but make it into something that can be used in a repeated pattern that works in lockstep with your application, such that you are neither creating custom JS for each page (e.g. React), nor blindly manipulating the DOM (like JQuery).The division of roles between CSS and HTML is an almost contradictory point - your styling and layout should be coupled if you are to impose any meaningful order in your design. If you are rejecting the \"decoupling\" of front-end and back-end that React gives you, then why would you expect to be able to do it between HTML and CSS?\n \nreply",
      "I\u2019ve been using htmx for over a year now for our internal management application (order monitoring, partner management, CAD model uploading and management type stuff) and I continue to be delighted at how quickly I can add features and iterate existing ones. I write way less client side JS, the app is very fast and responsive and I don\u2019t have to write the app twice like with a SPA + API.\n \nreply",
      "I love htmx for internal dashboards. I find htmx difficult to use for user facing applications because it's difficult to get everyone on board with the constraints of htmx (no optimistic uis, simple ui/ux). When building complicated frontends with lots of popovers, modals, optimistic state, I like react.\n \nreply",
      "I don't think htmx should be used on its own when implementing ui/ux - htmx has the job of getting the blocks of html, with data embedded, from the back-end to the front-end (and posting back up as necessary); once it's there, front-end client-only ui/ux can be handled by other tools in JavaScript\n \nreply",
      "i used it for simple dashboard, worked well but for complex projects it leads to spaghetti code for me, I had to stick to react for that.\n \nreply",
      "One thing I've been struggling with using HTMX... with an app and a frontend REST API I've found I can really kinda quickly craft a frontend by filtering down whatever resources I need (though it's honestly pretty wasteful at times).With HTMX I'm finding myself needing to have as many backend views as I have ways of interacting with a page. I still have to write the frontend, and on top of that I gotta make a bunch of one-off backend views for many interactions. What am I doing wrong?\n \nreply",
      "Check these out and see if they provide any insight for your specific issues :)- https://htmx.org/essays/template-fragments/- https://htmx.org/essays/10-tips-for-ssr-hda-apps/\n \nreply",
      "I make htmx sites built around view fragments rather than pages. And when I need a page it's just a set of fragments. I make \"API\" endpoints fir the needed fragments and call via ssr on first paint, then as needed from the htmx side.\n \nreply"
    ],
    "link": "https://rakujourney.wordpress.com/2024/09/08/htmx-raku-and-pico-css/",
    "first_paragraph": "Open::Journey\n\t\t\tmy personal open source journey\t\tThis post is kind of part 3, coming off last week\u2019s thrilling episode.I am a simple sole, I want to reduce the cognitive load in my web projects. The general idea is to go back to the halcyon early days of the web before Netscape dropped the JS-bomb. You know HTML for the layout and CSS for the style. An elegant division of roles.When I read about HTMX it was clear that Raku and Cro are ideal candidates for the back end HTML assembly, defining routes and serving RESTful APIs. As we have seen in the previous posts, HTMX eliminates the need for JS to make dynamic web content. Lovely.Remember \u2013 we are talking simpler ways to build attractive, dynamic, modern websites. While HTMX is well suited to 90% of this, if you are building a webapp like FaceBook or Google Maps, then it\u2019s not for you.But what to do about style and CSS?Well HTMX is neutral to CSS \u2026 it can be used with Bootstrap, Tailwind, SASS and so on. But many of these CSS tools hav",
    "summary": "**Nostalgia Overload in Modern Web Development**\n\nIn today's episode of \"Ancient Coders Try New Tricks,\" a sentimental developer embarks on a quixotic journey to make web development great again (MWGA) by reviving the prehistoric era of HTML and rejecting the demonic forces of JavaScript. The blog post, dripping with nostalgia, cries for a return to the so-called simpler days when websites were as dynamic as a fossil exhibit and stylesheets ruled supreme\u2014because apparently, suffering from CSS was less painful than understanding JavaScript. Commenters, in a dazzling display of one-upmanship, compete in reminiscing about the 'good ol' days' of web tables and spacer GIFs while subtly bragging about minimal use of JavaScript in their current projects using HTMX\u2014because why advance when you can comfortably regress? One wonders if their web browsers are also time machines, whisking them back to a 1998 where these complaints might still be relevant. \ud83d\udd78\ufe0f\ud83d\udc74\ud83c\udffb\ud83d\udcbe"
  },
  {
    "title": "alphaXiv: Open research discussion on top of arXiv (alphaxiv.org)",
    "points": 455,
    "submitter": "sahebjot",
    "submit_time": "2024-09-08T06:57:18.000000Z",
    "num_comments": 153,
    "comments_url": "https://news.ycombinator.com/item?id=41478690",
    "comments": [
      "Great idea.- The frontpage should directly show the list of papers, like with HN. You shouldn't have to click on \"trending\" first. (When you are logged in, you see a list of featured papers on the homepage, which isn't as engaging as the \"trending\" page. Again, compare HN: Same homepage whether you're logged in or not.)- Ranking shouldn't be based on comment activity, which ranks controversial papers, rather papers should be voted on like comments.- It's slightly confusing that usernames allow spaces. It will also make it harder to implement some kind of @ functionality in the comments.- Use HTML rather then PDF. Something that could be trivial with HTML, like clicking on an image to show a bigger version, requires you to awkwardly zoom in with PDF. With HTML, you would also have one column, which would fit better with the split paper/comments view.\n \nreply",
      "> Use HTML rather then PDF.The PDF is the original paper, as it appears on arXiv, so using PDF is natural.In general academics prefer PDF to HTML. In part, this is just because our tooling produces PDFs, so this is easiest. But also, we tend to prefer that the formatting be semi-canonical, so that \"the bottom of page 7\" or \"three lines after Theorem 1.2\" are meaningful things to say and ask questions about.That said, the arXiv is rolling out an experimental LaTeX-to-HTML converter for those who prefer HTML, for those who usually prefer PDF but may be just browsing on their phone at the time, or for those who have accessibility issues with PDFs. I just checked this out for one of my own papers; it is not perfect, but it is pretty good, especially given that I did absolutely nothing to ensure that our work would look good in this format:https://arxiv.org/html/2404.00541v1So it looks like we're converging towards having the best of both worlds.\n \nreply",
      "> In general academics prefer PDF to HTML. In part, this is just because our tooling produces PDFs, so this is easiest.The tooling producing PDF by default absolutely makes the preference for PDF justifiable. However, tooling is driven by usage - if more papers come with rendered HTML (e.g. through Pandoc if necessary), and people start preferring to consume HTML, then tooling support for HTML will improve.> But also, we tend to prefer that the formatting be semi-canonical, so that \"the bottom of page 7\" or \"three lines after Theorem 1.2\" are meaningful things to say and ask questions about.Couldn't you replace references like \"the bottom of page 7\" with others like \"two sentences after theorem 1.2\" that are layout-independent? This would also make it easier to rewrite parts of the paper without having to go back and fix all of your layout-dependent references when the layout shifts.HTML has strong advantages for both paper and electronic reading, so I think it's worth making an effort to adopt.When I print out a paper to take notes, the margins are usually too narrow for my note-taking, and I additionally have a preference for a narrow margin on one side and a wide margin on the other (on the same side, not alternating with page parity like a book), which virtually no paper has in its PDF representation. When I read a paper electronically, I want to eliminate pagination and read the entire thing as a single long page. Both of these things are significantly easier to do with HTML than LaTeX (and, in the case of the \"eliminate pagination\" case, I've never found a way to do it with LaTeX at all).(also, in general, HTML is just far more flexible and accessible than PDF for most people to modify to suit their preferences - I think most on HN would agree with that)\n \nreply",
      "HTML still lacks one key feature: a way of storing the entire document as a single file that remains fully functional offline and can be reasonably expected to be widely supported for decades. Research papers are used both for communicating new results and for archiving them. The long-term stability needed for the latter has never been a strong point of web technology.\n \nreply",
      "> Couldn't you replace references like \"the bottom of page 7\" with others like \"two sentences after theorem 1.2\" that are layout-independent?Yes, but I think such references are inherently harder to locate. Personally I try to just avoid making references to specific locations in the document and instead name anything that needs to be referenced (e.g. Figure 5, Theorem 3.2).\n \nreply",
      "Yes, I absolutely agree - I just figured that there had to be a reason that someone would want to do that. Chesterton's Fence and whatnot.\n \nreply",
      "I increasingly recommend against the Arxiv HTML version. I thought it had an acceptable start and they would fix the remaining problems and rapidly become on par with the PDF, but that seems to not be happening.The HTML version is seriously buggy; and the worst part is, a lot of those bugs take the form of silently dropping or hiding content. It's bad enough when half the paper is gone, because at least you notice that quickly, but it'll also do things like silently drop sections or figures, and you won't realize that until you hit a reference like 'as discussed in Section 3.1' and you wonder how you missed that. I filed like 25 bugs on their HTML pages, concentrating on the really big issues (minor typographic & styling issues are too legion to try to report), and AFAIK, not a single one has been fixed in a year+. Whatever resources they're devoting to it, it's apparently totally inadequate to the task.\n \nreply",
      "I think development on the TeX-to-HTML compiler has slowed down at some point, and it's far from perfect yet. Some of the issues are probably HTML5 limitations, unlikely to be fixed any time soon (unless one wants formulas to become graphics).But there is another problem: It takes too long to load on mobile and doesn't reflow. I thought mobile was one of the reasons people wanted HTML in the first place!\n \nreply",
      "In PDFs on arXiv, syntax highlighted codeblocks are graphics.\n \nreply",
      "> That said, the arXiv is rolling out an experimental LaTeX-to-HTMLSome history: https://www.arxiv-vanity.com/\n \nreply"
    ],
    "link": "https://www.alphaxiv.org/",
    "first_paragraph": "",
    "summary": "**alphaXiv: Because Reading Papers Wasn't Already Fun Enough**\n\nIn an online world desperate for another platform that nobody asked for, alphaXiv swoops in to save the day! Academics, now you can argue over the benefits of PDF and HTML formats as *definitively* as you dispute the merits of string theory. Brace yourselves for animated comment sections where the enthusiasm for HTML\u2019s clickable images is matched only by the outrage over PDF\u2019s nostalgic page references. And let\u2019s not forget the groundbreaking usernames with spaces, because what really matters is making sure no one can properly mention you during heated debates about syntax-highlighted codeblocks. \ud83d\udcc4\ud83d\udd25\ud83e\udd13"
  },
  {
    "title": "16th Century Irish Hipsters (irisharchaeology.ie)",
    "points": 14,
    "submitter": "slater",
    "submit_time": "2024-09-04T00:21:47.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41440620",
    "comments": [
      "This sounds fun but the link is ded. Anybody have it somewhere else?\n \nreply",
      "https://web.archive.org/web/20240530213529/http://irisharcha...\n \nreply"
    ],
    "link": "http://irisharchaeology.ie/2013/08/16th-century-irish-hipsters/",
    "first_paragraph": "\nI recently spotted what appeared to be remarkably modern looking haircuts in Albrecht Druer\u2019s woodcut of 1521 AD[i]. This image shows a group of Irish soldiers[ii], most likely mercenaries, who were fighting on the European continent during the early 1520s. I soon discovered that, far from being unusual, this distinctive hairstyle was actually very popular amongst the native Irish during the 16th century.Referred to as a \u2018glib\u2019 this style involved the hair at the back and side of the head being trimmed short, \u00a0while at the front and top it was allowed to grow long, resulting in a large fringe, which fell down over the face.In 1517 Laurent Vital described this distinctive Irish hair style thus: \u2018for they (Irish men) were shorn and shaved one palm above the ears, so that only the tops of their heads were covered with hair. But on the forehead they leave about a palm of hair to grow down to their eyebrows like a tuft of hair which one leaves hanging on horses between the two eyes\u2019[iii].S",
    "summary": "Title: 16th Century Irish Hipsters (irisharchaeology.ie)\n\nOnce again, the internet resurrects something nobody asked for, with a riveting expos\u00e9 on historical hipsterdom. Irish mercenaries from the 1520s apparently sported hairdos that could easily pass for modern-day barista or indie-band frontman, complete with an obnoxiously long fringe (or 'glib,' if you want to get technical and sound like you\u2019re *not* from this century). Commenters, in a desperate scramble to view the iconic 'do, mourn a dead link like they're at the world's saddest funeral, while someone heroically pastes yet another archive URL, fueling our never-ending nostalgia for ye olde hipster looks. Yawn, can we go back to not caring about the 16th-century hairstyling, please? \ud83d\udc74\ud83c\udfb8\ud83d\udcdc"
  },
  {
    "title": "Unconventional Case Study of Neoadjuvant Oncolytic Virotherapy for Breast Cancer (mdpi.com)",
    "points": 107,
    "submitter": "Amezarak",
    "submit_time": "2024-09-06T16:20:55.000000Z",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41467503",
    "comments": [
      "The science is not new, but the reason why this paper is raising some eyebrows is the following:Institutional Review Board StatementThis is a case of self-experimentation. As such, it does not require ethics committee review [33,34,35]. The study was feasible only due to the unique situation in which the patient was also an expert virologist. The patient was fully aware of her illness as well as of available therapies, and as a scientist in the field of virology, she was aware of the potential of oncolytic virotherapy. After two recurrences of the same tumour, she wanted to try an innovative approach in a scientifically sound way. Her oncologists (the leading oncologists in Croatia for breast cancer) accepted to monitor the progress of the treatment, primarily with the aim of discontinuing the injections and intervening with conventional therapy if there were untoward effects or if the tumour progressed.\n \nreply",
      "Definitely eyebrow raising but I'm always a fan of a bit of \"mad scientist\" self-experimentation (within reason), whether it be Barry Marshall chugging H. pylori or Thought Emporium giving himself lactase gene therapy.\n \nreply",
      "So the treatment worked, and she appears to be recovered from what might otherwise have been treatment-resistant, fatal cancer.But she was only able to do so because of having the skill and access needed to self-administer the cure. How many people die every year because we don't dare risk any deaths from uncertain treatments like these? Sure, they aren't ever going to be perfect, but- what's the net?\n \nreply",
      "The counter-argument is \"how many more would die if we just let desperate people yolo on themselves.\" Which brings us back to the current compromise solutions.\n \nreply",
      "We probably should let them if they\u2019ve exhausted other options right?\n \nreply",
      "I think the main worry is scam artists who essentially pray on desperate people with \"cures\" that do nothing at best and shorten their lives at worse.I'm fine with letting people try whatever they want if there are no other options (their life, although informed consent is tricky) however i dont think we should allow marketing such things to patients or allow anyone making money off it.\n \nreply",
      "Let them what? Enroll in a clinical trial?  Buy drugs from some website?Get a PhD to try and cure their own disease?https://www.wired.com/story/sleep-no-more-crusade-genetic-ki...\n \nreply",
      "I believe the main point is to allow the patient to choose from a set of treatments. Correct me if I\u2019m wrong, but isn\u2019t this already happening at some level, where patients receive treatments that are in advanced stages of drug trials?I imagine there could be other options available, and many times the issues are related to logistics, bureaucracy, and corruption. For instance, many people around the world go hungry due to logistical challenges (e.g., political interference) [1].It seems we accept the status quo, while some companies are sending cheaper spacecraft into space.[1] https://www.reddit.com/r/NoStupidQuestions/comments/zye4gm/w...\n \nreply",
      "People can enroll in clinical trials.https://clinicaltrials.gov/\n \nreply",
      "That page is for information, not for enrolling.\n \nreply"
    ],
    "link": "https://www.mdpi.com/2076-393X/12/9/958#",
    "first_paragraph": "",
    "summary": "Title: DIY Cancer Treatment: Because Who Needs Doctors Anyway?\n\nIn a world where the phrase \"self-care\" has taken a rather radical turn, a virologist decides to treat her own recurrent breast cancer with some oncolytic viruses casually lying around in her lab. <em>Because, why undergo standard treatment protocols when you can turn yourself into a living lab experiment?</em> The comment sections erupt with a mix of homebrew pharmacologists and armchair ethicists debating the merits of self-medication and the horrors of regulatory oversight. <strong>Should patents include a DIY section?</strong> Stay tuned as the audience discusses whether the next step in personalized medicine involves a scalpel and an online tutorial."
  },
  {
    "title": "ATProto for Distributed System Engineers (atproto.com)",
    "points": 7,
    "submitter": "danabramov",
    "submit_time": "2024-09-09T00:04:01.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://atproto.com/articles/atproto-for-distsys-engineers",
    "first_paragraph": "Sep 3, 2024AT Protocol is the tech developed at Bluesky for open social networking. In this article we're going to explore AT Proto from the perspective of distributed backend engineering.If you've ever built a backend with stream-processing, then you're familiar with the kind of systems we'll be exploring. If you're not \u2014 no worries! We'll step through it.The classic, happy Web architecture is the \u201cone big SQL database\u201d behind our app server. The app talks to the database and handles requests from the frontend.As our application grows, we hit some performance limits so we toss some caches into the stack.Then let's say we scale our database horizontally through sharding and replicas.This is pretty good, but we're building a social network with hundreds of millions of users; even this model hits limits. The problem is that our SQL database is \u201cstrongly consistent\u201d which means the state is kept uniformly in sync across the system. Maintaining strong consistency incurs a performance cost ",
    "summary": "The elite engineers at Bluesky have concocted the revolutionary <em>AT Proto</em>, a cure-all protocol promising to make every distributed system engineer's day either a dream come true or a throbbing migraine. Designed for those who can't resist turning a single database into an overly complex, cache-juggling, shard-splitting circus of servers, AT Proto is heralded as the Swiss Army knife for social networking scalability. Commenters, buzzing with the usual prophetic confidence, outdid each other with crypto-bro lexicon and fierce advocacy for their favorite database flavors. Who knew <i>distributed systems</i> could spark such an impassioned brawl over consistency models? \ud83c\udf7f\ud83e\udd2f"
  },
  {
    "title": "Charging lithium-ion batteries at high currents first increases lifespan by 50% (eurekalert.org)",
    "points": 80,
    "submitter": "snazz",
    "submit_time": "2024-09-08T22:06:44.000000Z",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=41483654",
    "comments": [
      "Such a cool finding if it pans out in production. A hidden process variable hiding in plain sight.\n \nreply",
      "They'll never do it because it means decreased profits.There are articles that appear here and elsewhere semi-frequently about how doing something simple extends battery lifetimes a huge amount, but those never get implemented in practice except perhaps for highly niche applications.Instead what usually happens is they'll then find a way to make them last the same amount of time, but with higher energy density. The \"high voltage\" lion cells (>4.2V end of charge) are an example of that process; they will last much longer than previous types if charged to 4.2V, but they'd rather advertise them as 4.3 or 4.35 or even 4.4V(!) and the extra capacity that gives.\n \nreply",
      "Hm this doesn't seem to be panning out in practice. Loads of devices have grown \"optimize charging\" style features in the recent-ish past, and those features are explicitly there to extend battery longevity (at the expense of consumer convenience even!). Clearly, the market forces are more complex than \"short battery lifetime = more frequent device upgrades = profit\" (although that effect is certainly *a part of& the equation).\n \nreply",
      "A lot of energy research is speculative and it can take decades for research to go from the lab to the consumer.This finding, however, specifically integrates with existing infrastructure; no new, unproven technology is needed, we just simply juice the batteries more during initial charge. If it pans out after extensive testing, we can see this technique hitting the market within 2 years.\n \nreply",
      "TLDR: During a battery's initial \"formation\" charge, some of the lithium deactivates, forming a squishy, protective layer around the negative electrode, called the solid electrolyte interphase (SEI).  Today, manufacturers typically do a slow formation charge, during which about 9% of the lithium is lost to the SEI.  It was thought this was needed to form a robust layer.  But the researchers found at the higher initial charge currents used in this study, 30% becomes SEI - so you loose some battery capacity (for a given amount of lithium), but wind up with a beefier protective layer on your electrode and better longevity across subsequent charge cycles.\n \nreply",
      "Whats a battery lifespan? Is it capacity degradation or random failure?If discovery slows down capacity degradation, but now your EV battery is 100x more likely spontaneously fail ($$$) - it's not really an improvement. Maybe ok for consumer device tho.\n \nreply",
      "There are two lifespans.  The shelf life and the number of charge cycles (less of a span perhaps) where you charge to 100% and discharge to near 0.  If you keep your charge/discharge to 80/20 then your battery life is limited primarily by the shelf life.  eg. keep your Nissan Leaf in the 20-80% state of charge range and it will probably last 20 years, DC fast charge it every time to 100% you'll probably only get 2000 cycles (5-7 years) out of it.\n \nreply",
      "It isn't that black and white, plus the Leaf without active battery temperature management isn't a representative example.Modern Tesla's show fairly similar long-tail degradation that is nearly identical for cars that strictly home charge and those that only supercharge (based on customer vehicle tracking). Most will level off at 85-90% of original capacity.\n \nreply",
      "TL;DR the high current causes a layer on the negative electron to form a bit differently (and obviously faster), previously it was thought that a slower initial charge led to better formation. This is a process tweak incremental improvement, not anything truly fundamental.\n \nreply",
      "> This is a process tweak incremental improvement, not anything truly fundamental.Regardless of whether this is a \"process tweak\" or something \"truly fundamental\", a 50% increase in battery lifespan would be huge, regardless.The conspiracy theorist in me though thinks that a lot of consumer electronics makers wouldn't like this, because lower battery capacity has to be a big driver of upgrade cycles. I'm guessing a lot of folks are similar to me: these days, somewhere in the 2-3 year mark my cell battery capacity starts degrading noticeably. My phone otherwise works great, and I certainly don't need the features in the latest model phone, and of course I know I can pay for just a battery replacement, but sometimes I think \"Well, if I need to replace the battery, I might as well get a new phone - it's got <some feature that is marginally better but that I'm now convincing myself is super cool to justify my not-really-necessary upgrade purchase>\".I think with 50% more battery lifespan I would rarely, if ever, use dwindling battery capacity as an excuse for an upgrade purchase.\n \nreply"
    ],
    "link": "https://www.eurekalert.org/news-releases/1056171",
    "first_paragraph": "\n                    Charging lithium-ion batteries at high currents just before they leave the factory is 30 times faster and increases battery lifespans by 50%, according to a study at the SLAC-Stanford Battery Center\n                DOE/SLAC National Accelerator Laboratoryimage:\u00a0Giving lithium-ion batteries their first charge at high currents before they leave the factory is 30 times faster and increases their lifespans by 50%.\n\nview more\u00a0Giving lithium-ion batteries their first charge at high currents before they leave the factory is 30 times faster and increases their lifespans by 50%.Credit: Greg Stewart/SLAC National Accelerator LaboratoryMenlo Park, Calif. \u2014\u00a0A lithium-ion battery\u2019s very first charge is more momentous than it sounds. It determines how well and how long the battery will work from then on \u2013 in particular, how many cycles of charging and discharging it can handle before deteriorating.In a study published today in\u00a0Joule, researchers at the SLAC-Stanford Battery Cent",
    "summary": "**SLAC Scientists Revolutionize Battery Charging, No One Will Use It**\n\nIn an electrifying display of common sense that was almost mistaken for actual innovation, researchers have decided that zapping lithium-ion batteries with *high currents* at birth magically extends their lifespans by 50%. Of course, in the swirling vortex of internet wisdom, commentators quickly pointed out that this will never hit your smartphone because, dear consumer, your incessant need to upgrade is the lifeblood of capitalism. \ud83d\udcf1\ud83d\udcb8 Another commenter, likely wearing a tin foil hat, suggests that if we can't even figure out regular battery life expectations, perhaps doubling down on ones with accelerated charging isn't the brightest idea. Meanwhile, tech aficionados everywhere toggle between dismay and apathy, ever-prepared to lament the death of their devices precisely two months post-warranty."
  },
  {
    "title": "\"Unstripping\" binaries: Restoring debugging information in GDB with Pwndbg (trailofbits.com)",
    "points": 102,
    "submitter": "aa_is_op",
    "submit_time": "2024-09-08T17:23:14.000000Z",
    "num_comments": 12,
    "comments_url": "https://news.ycombinator.com/item?id=41481682",
    "comments": [
      "GDB loses significant functionality when debugging binaries that lack debugging symbolsIMHO from experience with other debuggers, GDB is actually hostile to debugging at the Asm level, due to many perplexing design choices which may or may not be deliberate. Things like needing to add a superfluous asterisk when breakpointing on an address, the \"disassemble\" command not being able to do what it says and instead complaining about a lack of functions, etc.\n \nreply",
      "I have definitely gotten to the point of \"Fine, I'll just use IDA Pro then\".Of course, visual debuggers are another story entirely, but I'm not really thrilled with them either. For example I don't recall there being a good way to say \"Decode the address at rax as a WNDCLASSA\" or something like that in IDA. (I'm crossing my fingers for a Cunningham's Law here.)\n \nreply",
      "I don't recall what it was called in the menu, but it was definitely possible to assume a struct on a particular address. Muscle memory tells me the button is U, even though actual memory fails me.\n \nreply",
      "+1 there are many pain points, probably for historic reasons. *nix almost always comes with the source, so binary only debugging is never a priority.\n \nreply",
      "I read \u201cpwndgb\u201d as Welsh for a good 5 seconds before realising which site I was on.\n \nreply",
      "\u201cWelsh or C standard library name?\u201dhttps://www.reddit.com/r/ProgrammerHumor/comments/1f123qw/le...\n \nreply",
      "Beautiful\n \nreply",
      "I\u2019m curious how this compares with https://github.com/mahaloz/decomp2dbg\n \nreply",
      "sounds like an interesting direction, but I don't understand why should we have it coupled to specific tool (pwndbg)? Why not implement a BinaryNinja plugin to dump all user-defined names (function names, stack variables), together with an original (stripped) binary to the new ELF/.exe file, with symbol table and presumably with DWARF section?\n \nreply",
      "I've developed a Ghidra extension that exports object files. I've considered generating debugging symbols in order to improve the debugging experience when reusing these object files in new programs, but I keep postponing that feature for various reasons.Executable formats have at least one and often multiple debugging data formats which are very different from each other: ELF has STABS and DWARF version 1 to 5, MSVC has at least COFF symbols and PDB (which isn't documented)... Even discarding the old or obsolete stuff, there's no universal solution here. gdb+pwndbg seems to side-step this issue by integrating the debugger with Binary Ninja.Projecting reverse-engineered information into a debugging data format would also be a technical challenge once you go past global variables and type definitions. Debuggers already have a terrible user experience when stepping through functions in an optimized executable ; I doubt that reverse-engineered debugging data would be any better.Toolchains also don't do a lot of validation or diagnostics on their inputs and I can tell from experience that writing correct object files from scratch is already quite tricky. I expect that serializing correct and meaningful debugging data would be much harder than that.Doing this at the native executable level has the obvious advantage of working out of the box with standard tooling, but it would be a lot of work. I've already taken 2 1/2 years to make an object file exporter that's good enough for my needs and I'm still balking at generating DWARF debugging data every time I've considered it. I'm resigned to a terrible debugging experience and so far I've managed to muddle through it.\n \nreply"
    ],
    "link": "https://blog.trailofbits.com/2024/09/06/unstripping-binaries-restoring-debugging-information-in-gdb-with-pwndbg/",
    "first_paragraph": "By Jason AnGDB loses significant functionality when debugging binaries that lack debugging symbols (also known as \u201cstripped binaries\u201d). Function and variable names become meaningless addresses; setting breakpoints requires tracking down relevant function addresses from an external source; and printing out structured values involves staring at a memory dump trying to manually discern field boundaries.That\u2019s why this summer at Trail of Bits, I extended Pwndbg\u2014a plugin for GDB maintained by my mentor, Dominik Czarnota\u2014with two new features to bring the stripped debugging experience closer to what you\u2019d expect from a debugger in an IDE. Pwndbg now integrates Binary Ninja for enhanced GDB+Pwndbg intelligence and enables dumping Go structures for improved Go binary debugging.To help improve GDB+Pwndbg intelligence during debugging, I integrated Pwndbg with Binary Ninja, a popular decompiler with a versatile scripting API, by installing an XML-RPC server inside Binary Ninja, and then querying",
    "summary": "At Trail of Bits, the quest for turning GDB into something <em>less miserable</em> for debugging stripped binaries continues with Jason's summer project \u2014 because apparently, being able to make sense of what you're debugging without resorting to psychedelic substance use is a real goal now \ud83e\udd2a. Let's all take a moment to appreciate the audacious attempt to make GDB not just a cryptic relic by integrating it with Binary Ninja \u2014 because who doesn't want more complexity in their debugging tools? Comments reveal a unanimous nostalgia for when debugging was more akin to sorcery than science. One brilliant soul misread \u201cpwndbg\u201d as a Welsh term, contributing more to the field of comedy than debugging ever will."
  },
  {
    "title": "Jd \u2013 JSON Diff and Patch (github.com/josephburnett)",
    "points": 26,
    "submitter": "smartmic",
    "submit_time": "2024-09-08T19:48:36.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41482661",
    "comments": [
      "What I've done in the past is pipe JSON into gron then diff. Works sufficiently well for eyeballing.\n \nreply",
      "This tool looks great! I\u2019ve been using difftastic lately, which does a fairly good job but struggles with big json files.One feature I\u2019ve yet to see is applying jq query syntax to the jsons before the diff\n \nreply"
    ],
    "link": "https://github.com/josephburnett/jd",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        JSON diff and patch\n      jd is a commandline utility and Go library for diffing and patching JSON and YAML values. It supports a native jd format (similar to unified format) as well as JSON Merge Patch (RFC 7386) and a subset of JSON Patch (RFC 6902). Try it out at http://play.jd-tool.io/.To get the jd commandline utility:To use the jd web UI:setkeys This option determines what keys are used to decide if two objects 'match'. Then the matched objects are compared, which will return a diff if there are differences in the objects themselves, their keys and/or values. You shouldn't expect this option to mask or ignore non-specified keys, it is not intended as a way to 'ignore' some differences between objects.Note: import only release commits (v1.Y.Z) because master can be unstable.output:apply these change to another deployment:\n     ",
    "summary": "**Hacker News Presents: \"Jd \u2013 JSON Diff and Patch,\" Where Too Much Free Time Meets Overengineering.** In the latest episode of GitHub theater, a brave developer unleashes \"jd,\" a tool designed to diff and patch JSON like it's 1999. Commenters, frothing for anything that can run on command line, trip over themselves praising the utility, while subtly flexing their own arcane workflow hacks with other obscure tools. One enthusiastic keyboard warrior dreams out loud about integrating `jq` queries, because clearly, what the world needs is more complexity in a simple diff tool! Truly, jd is the hero we never asked for, but all apparently deserve. \ud83c\udfad\ud83d\udd27"
  },
  {
    "title": "Telum II at Hot Chips 2024: Mainframe with a Unique Caching Strategy (chipsandcheese.com)",
    "points": 37,
    "submitter": "mfiguiere",
    "submit_time": "2024-09-08T18:51:59.000000Z",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41482276",
    "comments": [
      "\"Why Do Mainframes Still Exist? What's Inside One? 40TB, 200+ Cores, AI, and more! - Dave explores the IBM z16 mainframe from design to assembly and testing.  What's inside a modern IBM z16 mainframe that makes it relevant today?\" - by Dave Plummer.This is an amazing 23-minute video by the Microsoft programmer who developed the Windows NT Task Manager, among other things. He visits IBM and talks to engineers about the Telum chip architecture (Hot Chips 2023), used in the z16 mainframe. Special attention is paid to the cache.https://youtu.be/ouAG4vXFORc\n \nreply",
      "That's a huge amount of effort to let most of the transistors in a computer (in the RAM) sit idle most of the time. Surely there are viable non-Von Neuman alternatives that could be spun out into general purpose computing.\n \nreply",
      "The vast majority of transistors in any modern CPU are \u2018idle\u2019 at any given moment.\n \nreply",
      "To bad the mainframe business will not be spun off from IBM.\nThen you may see innovation, but IBM see it as a cash cow.\n \nreply",
      "Do the customers want innovation?\n \nreply",
      "The CPUs has been a tour de force from the S/360, they have never relented, so empirically yes the customers care a lot or they wouldn't keep doing this.The software side seems to be more a tale of dichotomy.  The MVS lineage is technically impressive but undoubtedly bizarre and old feeling.  The TPF lineage seems like eventually somewhere the cloud movement will dip for certain cases so it is ahead of time.  Linux is neither stale nor avant-garde, I guess that is their strategy to remain \"contemporary\".  VM was always the most delightful one but internally forever the odd one out.\n \nreply",
      "I wonder what workloads would benefit from having an L4 victim cache on another CPU, but that other CPU doesn't need its own L2 cache.\n \nreply",
      "The claimed latency for it seems not far off from some other vendor's L3 caches which may be by virtue of rethinking where to share and therefore paying interconnection coherency taxes.The innovation here seems to be adaptive sizing so if by whatever algorithm/metric a remote core is idle, it can volunteer cache to L4.Presumably the interconnect is much richer than contemporary processors in typical IBM fashion and they can do all the control at a very low level (hw state machines&microcoding) so it is fast and transparent.  It will be interesting to hear how it works in practice and if POWER12 gets a similar feature since it shares a lot of R&D.\n \nreply",
      "At a guess, a single thread which benefits from as much cache as it can get.\n \nreply",
      "Sure, but having to buy entire CPUs filled with idle cores to scale up cache seems very expensive.\n \nreply"
    ],
    "link": "https://chipsandcheese.com/2024/09/08/telum-ii-at-hot-chips-2024-mainframe-with-a-unique-caching-strategy/",
    "first_paragraph": "",
    "summary": "Title: IBM Clings to Relevance with Shiny New Mainframe Cache Tricks at Hot Chips 2024\n\nIn an adrenaline-fueled quest to justify the existence of mainframes in a world swarming with more agile technologies, IBM pulls a rabbit out of its proverbial hat with the Telum II at Hot Chips 2024. Here, Dave Plummer, a respected relic from Microsoft\u2019s glory days, dives exuberantly into the antiquated abyss of IBM\u2019s z16, a machine boldly resisting the siren call of obsolescence. Commenters, a delightful mix of nostalgia addicts and tech cynics, debate passionately about whether stuffing a dinosaur with AI and *40TB caching tricks* is innovation or merely a pricey way to keep old transistors company. \ud83e\udd95\ud83d\udcbe Meanwhile, classic concerns such as \"Why don't they just spin off the whole mainframe gig?\" mingle with geeky ponderings about \"adaptive cache sizes\" and the esoteric mysteries of L4 victim caches, illustrating the enduring charm of *expensive solutions in search of a problem.* \ud83d\ude05"
  },
  {
    "title": "LibrePythonista allows running IPython code in a LibreOffice spreadsheet (github.com/amourspirit)",
    "points": 117,
    "submitter": "buovjaga",
    "submit_time": "2024-09-04T07:56:48.000000Z",
    "num_comments": 29,
    "comments_url": "https://news.ycombinator.com/item?id=41443012",
    "comments": [
      "Built extension available here: https://extensions.libreoffice.org/en/extensions/show/99231\n \nreply",
      "I know this comment is a lot of words, and may feel critical, but I assure you I mean them in the spirit of \"for your consideration\" and want to start off by saying thank you for sharing such a cool toy with the world under a permissive license. Great job, and thank you!I was trying to find out which version of python it offered, and while digging into that I found surprising references to win32 binaries in the pyproject <https://github.com/Amourspirit/python_libre_pythonista_ext/b...> although the extensions page says it's for all 3 major OSes.Related to that permalink, please do consider creating a formal tag that represents the code that went into the 0.1.1 binary you linked to. It'll greatly help those trying to track down bugs if they don't have to $(git bisect) in order to find out which sha created the extension they're usingKind of related to that, future you (and folks who clone your repo) are going to be sad if you keep putting the release artifacts in git, since it will make your repo grow without bound. If you want to make the .oxt available to folks outside of the libreoffice.org URL, that's another fine reason to create a tag since GitHub will cheerfully hold on to the .oxt with a permalink forever, but outside of your repo. It'll also motivate you, or a contributor, to create a GitHub Action showing how normal people could possibly build the release artifact for themselves\n \nreply",
      "Ok, can you report that to the developer: https://github.com/Amourspirit/python_libre_pythonista_ext/i...I'm not affiliated with this project.\n \nreply",
      "If Python in a spreadsheet is what you want, you might like Grist:https://www.getgrist.com/Disclaimer: I work there. I'm trying to make it easier to self-host. Send me github issues or pull requests if you have ways of making it better. :)\n \nreply",
      "I have been meaning to use Grist for an educational org I volunteer for - mainly to manage student data. We would definitely self-host, and it is great that there is already a Class Enrollment template. Your self-hosting docs [1] seem pretty straightforward. However,* I am annoyed that the the app requires providing inputs through command line args or environment variables rather than a fixed config file that I can store in git.\n* Some directions on creating a service that starts the app on server restart etc would be good.[1] https://support.getgrist.com/self-managed/\n \nreply",
      "Grist is not really a spreadsheet, it's a row-orientated database-system. Quite different in usage.\n \nreply",
      "How does this compare to using something like visual foxpro?\n \nreply",
      "This is off topic but I feel like Pythonista should have been called Parseltongue. Total missed opportunity.\n \nreply",
      "Why not just use pyspread?https://pyspread.gitlab.io/\n \nreply",
      "because they want the sheet in LibreOffice?\n \nreply"
    ],
    "link": "https://github.com/Amourspirit/python_libre_pythonista_ext",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          LibrePythonista is an extension for LibreOffice Calc. The extension allows Interactive Python code to be run directly in a spreadsheet.LibrePythonista is currently in beta and is subject to change based on feedback.LibrePythonista is free and open source software so there are no fees to start using it.LibrePythonista bring the power of Pandas, Matplotlib and much more to LibreOffice Calc.All  python code is executes on your local computer. This can alleviate many concerns around data privacy as your dataUsing LibrePythonista is it possible to create Data frame's, Series, custom Graphs an much more directly in a spreadsheet.LibrePythonista is built using OOO Development Tools which removes many barriers when working with the LibreOffice API.LibrePythonista brings the power of Python analytics into LibreOffice Calc. Use it to process data in C",
    "summary": "In a world craving unnecessary complexity, <em>LibrePythonista</em> swoops in to save the day by allowing users to run Python in a LibreOffice spreadsheet. Because, you know, using a plain old Python script is just too mainstream for the avant-garde spreadsheet jockey. Commenters, drunk on open source Kool-Aid and mostly ignoring the finer details of software bloat and repository pollution, line up to offer kudos, slightly off-topic alternatives, and obligatory GitHub issues. Dive in, if spaghetti code in spreadsheets is your kind of Friday night party! \ud83d\udc0d\ud83d\udcca"
  },
  {
    "title": "LLM_transcribe_recording: Bash Helper Using Mlx_whisper (gist.github.com)",
    "points": 6,
    "submitter": "Olshansky",
    "submit_time": "2024-09-08T23:14:01.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://gist.github.com/Olshansk/888533614765cd13139515d55573d676",
    "first_paragraph": "\n        Instantly share code, notes, and snippets.\n      ",
    "summary": "In the latest testament to developer indolence, a bright spark has finally murdered the last shred of transcription productivity by cobbling together a Bash script that uses Mlx_whisper to transcribe recordings. The repository, flung onto gist.github.com like a digital paper airplane, promises to \"instantly share code, notes, and snippets,\" because remembering anything for longer than a nanosecond is so last millennium. The comments section unfolds as a predictable symphony of misplaced awe and half-baked troubleshooting advice, as GitHub warriors congratulate themselves on revolutionizing the unnecessary. Truly, we live in the golden age of reinventing the wheel with more steps."
  },
  {
    "title": "Open Props \u2013 Supercharged CSS Variables (open-props.style)",
    "points": 12,
    "submitter": "homarp",
    "submit_time": "2024-09-08T21:12:14.000000Z",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41483249",
    "comments": [
      "I can\u2019t point my finger at it, but something really feels off about the layout of this page. A bunch of different spacing sizes mashed together, maybe? Doesn\u2019t look good for a design token library website. Love the idea though!\n \nreply",
      "I thought so too, tried putting a max width on the body and centering it. Ended up looking substantially better to my eye.\n \nreply",
      "Curious to know if anyone has compared this to Pollen:https://www.pollen.style/I\u2019ve used Pollen as an alternative to the bulk of Tailwind and have been very happy with it.\n \nreply",
      "Tailwind is fine for some use cases, but this and https://pollen.style are probably better baselines for apps and following a design system.  They let you define classes and components with some global consistency, without forcing the micro class names into every HTML element.(Yes I know you could make UI components that use Tailwind classes, but if you have a diverse stack or legacy code, it's not easy to bolt in the TW build system or assume one technology like React, etc)\n \nreply"
    ],
    "link": "https://open-props.style/",
    "first_paragraph": "18 props + normalize.css10 props8 props + normalize.css10 props16 props+2 importsExample loads only the animations and easings for a springy scale-in animationLight + dark HTML5 minimal stylesIt's non-prescriptive.File sizes and links to source:The following sizes are for the minified files and after Brotli compression.These props come in many flavors: CSS, PostCSS, JSON, or Javascript. Get em from a CDN or NPM. Try it in browser with Preact, Vite, Vanilla Extract, Lit, Qwik, you name it, it's in this helpful Stackblitz collection\nNeed help? \n              Ask a question on Discord\nNo installation required.View all \u2197Not all props can be represented as a design token.Follows the (draft) Design Tokens SpecCommunity created setup instructions.These props are scoped to :host for use in shadow DOM.Only ship the props you use. Learn more.Stop importing Open Props in your CSS (if you were).This plugin adds them to your stylesheet as you use them \ud83d\ude42Open Props includes Open Color, an open-source",
    "summary": "<h3>Open Props: Self-Proclaimed CSS Wizardry</h3>\nIn an era where every CSS framework promises to shave milliseconds off your load times and give your projects an aesthetic uplift, <em>Open Props</em> pitches itself with the allure of \"supercharged CSS variables,\" kind of like giving espresso shots to your already hyperactive stylesheet. Users engage in the comments with a mix of confusion over layout decisions and pseudo-technical musings, debating whether <em>Open Props</em> can dethrone the bulwark of awkward utility classes known as Tailwind. Meanwhile, style aficionados spar over alternatives like <i>Pollen</i>, demonstrating that CSS framework discourse could, in fact, be the tech equivalent of rearranging deck chairs on the Titanic. Forget cohesive UI consistency\u2014let's pile on more variables! \u26a1\ud83c\udfa8"
  },
  {
    "title": "Infisical (YC W23) Is Hiring a Developer Who Love Writing (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-09-08T17:00:02.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/infisical/jobs/snlbWtr-technical-content-marketer",
    "first_paragraph": "Open-source secrets manager for developersInfisical is the #1 open source secret management platform for developers. In other words, we help organizations manage API-keys, DB access tokens, certificates, and other credentials across all parts of their infra! In fact, we\u00a0process over 100M of such secrets per day.Our customers range from some of the largest public enterprises to fastest-growing startups (e.g., companies like Hugging Face, Delivery Hero). Developers love us and our community is growing every day! Join us on a mission to make security easier for all developers \u2013\u00a0starting from secret management.Infisical is looking for a customer success engineer to help grow Infisical\u2019s customer base and ensure great product/onboarding experience. You will be working closely with our CEO and the rest of the engineering team on:Overall, you\u2019re going to be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.This job will require you to have th",
    "summary": "**Infisical: Cryptic Code Custodians Wanted!**\n\nAt Infisical, the almighty keeper of secrets (yes, things developers scribble and then forget), they're on a manic quest to snag a medieval scribe who can code\u2014or was it a coder who can scribble? Either way, they manage oodles of secrets a day, ensuring even your grandpa\u2019s Yahoo password stays safe from his old poker buddies. They lure you in with big names like Hugging Face \ud83e\udd17, presumably because a friendly emoji implies robust data security. Meanwhile, the comment section is ablaze with keyboard warriors battling over tab versus space indents, bravely ignoring the job ad but fiercely committed to securing their spot as the clairvoyant of all secret hoarding lore. Join Infisical and become a secret-keeper\u2014or at least pretend to manage the chaos unleashed by developers who can't remember what they had for lunch. \ud83d\ude80"
  },
  {
    "title": "The Problem with the \"Hard Problem\" (edwardfeser.blogspot.com)",
    "points": 4,
    "submitter": "KqAmJQ7",
    "submit_time": "2024-09-04T18:42:15.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "http://edwardfeser.blogspot.com/2024/09/the-problem-with-hard-problem.html",
    "first_paragraph": "\"One of the best contemporary writers on philosophy\" National Review\n\n\"A terrific writer\" Damian Thompson, Daily Telegraph\n\n\"Feser... has the rare and enviable gift of making philosophical argument compulsively readable\" Sir Anthony Kenny, Times Literary Supplement\n\nSelected for the First Things list of the 50 Best Blogs of 2010 (November 19, 2010)The \u201chard problem\u201dWhether by\ndesign or not, the article marks the thirtieth anniversary since David Chalmers\nintroduced the phrase \u201chard problem of consciousness\u201d to label what has in\nrecent analytic philosophy of mind become a focus of obsessive attention.\u00a0 Introducing the problem, Kuhn notes:Key indeed are qualia, our internal, phenomenological, felt\nexperience \u2013 the sight of your newborn daughter, bundled up; the sound of\nMahler's Second Symphony, fifth movement, choral finale; the smell of garlic,\ncooking in olive oil. \u00a0Qualia \u2013 the felt\nqualities of inner experience \u2013 are the crux of the mind-body problem.Chalmers describes qualia as \u201cth",
    "summary": "\ud83c\udf93\ud83d\udcda\ud83e\udd14 *The Problem with the \"Hard Problem\"* sees yet another armchair philosopher bravely battle the winds of the unfathomable, grappling with \"qualia\" as if they were scattered lego blocks on the floor of modern philosophy. Our valiant blogsmith, backed by a pedigree of glowing commendations that smell suspiciously of old library books\u2014*National Review* insists he's \"one of the best\" (but what competition was there, really?)\u2014delves deep into the quagmire that real scientists gave up on when they realized they could publish more by sticking to things that actually exist. In the comments, a delightful circus of pseudointellectual regurgitation: everyone agreeing fervently without understanding a word, quoting Nietsche and Kant like they're battling it out for a philosopher king crown at a cosplay event. Truly, philosophy remains the art of talking in circles, chased fervently by those entranced by the sound of their own text. \ud83d\ude43\ud83c\udfa9\ud83d\udcac"
  }
]