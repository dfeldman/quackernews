[
  {
    "title": "Stark et al. vs. Patreon, Inc. Class Action Settlement (patreonsettlement.com)",
    "points": 29,
    "submitter": "pabs3",
    "submit_time": "2024-10-18T23:59:43.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41884543",
    "comments": [
      "From the FAQ at https://www.patreonsettlement.com/faq/> 1. What is happening in this lawsuit?> A group of people (called Plaintiffs) filed a class action lawsuit against Patreon, Inc. (\"Patreon\")\u2014the defendant in this lawsuit. These Plaintiffs claimed that Patreon disclosed its users' identities and video-viewing preferences without consent in violation of federal law. Patreon denies these allegations.> In April 2024, Patreon agreed to pay $7.25 million to settle the claims against it. The group of people that Patreon has agreed to pay is called the Settlement Class and it includes everyone who, between April 1, 2016 and September 23, 2024, requested or obtained video content on the Patreon website (patreon.com) while in the United States and at a time the person had a Facebook account and a Patreon account. If you are in this group and want to get paid, you must submit a claim. Details on how to submit a claim are provided below.\n \nreply",
      "> After deduction of the costs of notice and settlement administration, any Court-approved award of attorneys\u2019 fees (up to 30% of the Settlement Fund), litigation costs, and any service awards for the Class Representatives (up to $7,500 to each of the six individual Class Representatives), the Settlement Fund will be divided equally among eligible class members.> We will not know the final amount that each class member will receive until all claims are completed. Based on claims rates in similar cases, eligible class members may receive between $35 and $175 each. The actual amount that each class member receives will be determined once all claims are evaluated, and may be higher or lower than these estimates.I guess if there's a general trend of $35 per privacy violation companies might take a bit more serious.\n \nreply",
      "> I guess if there's a general trend of $35 per privacy violation companies might take a bit more serious.So far I received a payout of 210\u20ac for one privacy violation. THAT is a more adequate sum!\n \nreply",
      "> I guess if there's a general trend of $35 per privacy violation companies might take a bit more serious.The pricing of settlements is frequently tied to the size of insurance policies. So you\u2019re right, but it may take a few hops for the trend to percolate.\n \nreply",
      "You could always sue them yourself. no need to become part of the class action.\n \nreply",
      "Not trying to defend a big corporation, but I don't understand what Patreon did wrong...\n \nreply"
    ],
    "link": "https://www.patreonsettlement.com/",
    "first_paragraph": "",
    "summary": "Welcome to the latest episode of \"Privacy Schmivacy: The Patreon Edition.\" In an astonishing display of corporate benevolence, Patreon has graciously decided to cough up $7.25 million after apparently doing a little oopsie with their users' identities and binge-watching habits. Members of the \"We Saw What You Did There\" club could pocket somewhere between the dizzying heights of $35 and $175, sure to make up for any emotional trauma and/or unexpected celebrity from their viewing secrets being spilled. Meanwhile, commenters play armchair lawyer, doling out insights like \"just sue them yourself\" and pondering deep, philosophical questions like \"what did Patreon do wrong?\" \ud83d\udcb8\ud83d\udc40 Embrace the drama, folks. It's worth at least $35 of entertainment."
  },
  {
    "title": "New Mersenne Prime discovered (probably) (mersenne.org)",
    "points": 53,
    "submitter": "sdsykes",
    "submit_time": "2024-10-16T11:51:33.000000Z",
    "num_comments": 23,
    "comments_url": "https://news.ycombinator.com/item?id=41858024",
    "comments": [
      "I can swear something like 20+ years ago I found a new one too, but I didn\u2019t realize the importance of it. I had just downloaded GIMPS and I was just messing around with it, and when I saw the message I thought \u201cok, cool!\u201d and proceeded to turn it off.\n \nreply",
      "If it was literally around \"20+ years ago\", like 2004 or slightly before, it might have been M40 or M41.https://en.wikipedia.org/wiki/List_of_Mersenne_primes_and_pe...If this happened the way you remember, it's really unfortunate, but it wouldn't have stopped the prime in question from being discovered, because GIMPS always at least eventually gives out numbers to multiple people to check, and doesn't mark Mersenne numbers as checked until a computer actively reports that they were checked.However, your name could have ended up on that Wikipedia list as a discoverer. :-)\n \nreply",
      "Do you have any slightest idea about the exponent, including how many digits in the exponent? I assume you had no account (otherwise there should have been some logs for that).\n \nreply",
      "I'd believe it. Many years ago when I was around 10 years old and not understanding the concept of probability properly, I decided I had come up with a way to enumerate the lottery numbers and come up with a reasonably sized set of numbers to place bets for. I proceeded to write 9 pages of numbers for my father to place bets for. It is a 6/49 lottery so 6 balls are drawn from a set of 49 and you need to get all of them right to get the jackpot.It would have cost a little under 95$ to have played all my numbers (for a jackpot around $1.5M) I gave him however it would have taken a lot of effort to manually enter them. My father just does one page because it is silly. The numbers are silly, everything about this is silly. I completely understand in hindsight. But it turned out page 7 had the winning combination.\n \nreply",
      "I've seen that movie too\n \nreply",
      "Time for Bruce Schneier to change the combination to his luggage again\n \nreply",
      "For anyone who didn't get the joke, this is a reference tohttps://www.schneierfacts.com/facts/365from the \"Bruce Schneier Facts\" series (which was inspired by the \"Chuck Norris Facts\").\n \nreply",
      "Nice and tentative congratulations.I use to run Mersenne Prime Search (GIMPS), but now all I have is laptops.  It runs to hot on the Laptops I have :(Will need to play with throttling some more.Edit: found mprime (mprime-bin-24.14) is available in NetBSD pkgsrc.  But this uses 32 bit linux emulation to execute, I have been trying to avoid it, but may try it.\n \nreply",
      "Why don't they say what it is?\n \nreply",
      "Because the EFF Cooperative Computing Awards for the first discovery of large enough prime numbers are still active. Publishing the probable prime in advance would risk someone verifying faster than GIMPS.\n \nreply"
    ],
    "link": "https://www.mersenne.org/",
    "first_paragraph": "\nOctober 13, 2024 \u2014 Yesterday, a participating GIMPS computer reported a successful Fermat probable prime\r\n      test to the server!  A proof of work was uploaded and certified.  This certification proves there were no calculation errors during the probable\r\n      prime test.  Several Lucas-Lehmer tests are underway to definitively prove the number prime.  These tests will be run using several\r\n      different programs and on different hardware.  Soon after the result is officially confirmed an announcement will be made\r\n      with all the details.  Assuming the result holds, this new prime will be GIMPS first new prime in nearly six years.\r\n      Version 30.19 is now available.  ECM stage 2 is now much faster if you can give prime95 lots of memory to use.\r\n\t  This is similar to the improvements to P-1 stage 2 in version 30.8.  There are other minor bug fixes and tweaks.  This is not a required upgrade -- version 30.3 and\r\n\t  later can be used to hunt for new Mersenne primes.  Should y",
    "summary": "In the latest episode of \"Digital Needle in a Haystack,\" GIMPS enthusiasts have once again stumbled upon what might be a **new Mersenne prime**, sparking an *exciting* wave of Lucas-Lehmer tests and several self-congratulatory forum posts. A comment reminisces about almost hitting mathematical stardom two decades ago\u2014because remembering unlogged colossal discoveries is entirely believable and not at all a desperate cry for attention. Meanwhile, others ponder the financial rewards of landing a prime find, unaware that using the same energy to play the lottery might yield quicker returns. Don't worry, the collective brainpower of commenters will likely unlock the mysteries of the universe, right after they figure out how to prevent their laptops from overheating while running GIMPS."
  },
  {
    "title": "Secret 3D scans in the French Supreme Court (cosmowenman.substack.com)",
    "points": 584,
    "submitter": "abetusk",
    "submit_time": "2024-10-18T08:50:00.000000Z",
    "num_comments": 223,
    "comments_url": "https://news.ycombinator.com/item?id=41877513",
    "comments": [
      "> The court ruled that the museum\u2019s revenue, business model, and supposed threats from competition and counterfeiting are irrelevant to the public\u2019s right to access its scans, a dramatic rejection of the museum\u2019s positionIt would have helped the museum and government ministry if this had been clear before the government-funded scanning program was started. (Maybe it was, I don't know.)I was initially sympathetic to the museum, as it's common for public funding to be tight, and revenue from the gift shop or commercial licencing of their objects can fill the gap. I don't know about France, but I expect the ministry has been heavily pushing public museums to increase their income in this way.However, that doesn't justify the deception described by the article.\n \nreply",
      "This same person fought for years to get the Berlin Egyptian museum to release 3D scans of the famous Nefertiti bust. The museum also claimed it would undermine its revenue streams through the gift shop, but as the case progressed, that turned out to be very misleading - the museum had made less than 5000 EUR over ten years from 3D scans.https://reason.com/2019/11/13/a-german-museum-tried-to-hide-...\n \nreply",
      "What's to stop a replica maker from scanning a replica bought from the gift shop? I am very skeptical that a trinket purchaser will care about or be able to identify any scanning errors introduced.\n \nreply",
      "They\u2019re afraid of losing out on the revenue from selling replicas, etc. which is probably a very reasonable fear given that the guy filing suit and writing this blog post runs a company that creates replica artwork?\n \nreply",
      ">They\u2019re afraid of losing out on the revenue from selling replicasYou're aware that you don't need a 3D scanner (much less a 3D scan) to produce a replica of a sculpture (..or a replica of a replica), right?There's this ancient technique, known as casting, still in use today - which was used to produce some of the very sculptures being scanned in the first place!\n \nreply",
      "The huge piles of revenue?> SPK confirmed it had earned less than 5,000 euro, total, from marketing the Nefertiti scan, or any other scan for that matter. SPK also admitted it did not direct even that small revenue towards digitization, explaining that it was not obliged to do so. In the nearly 10 years since it had created the Nefertiti scan, SPK had completely failed to commercially exploit the valuable data idling on its hard drives.\n \nreply",
      "Right, no one is buying the digital scans. But tons of people buy physical replicas- I have been a volunteer at a different museum and our physical models of our most famous artifacts were very nice money makers for us, so I presume they would be for them as well. And using that digital scan you can make your own competing physical replica. Which is why the museum doesn't really want to make it easy for any 3D printer to compete with them.\n \nreply",
      "You could just buy one replica and scan it yourself.\n \nreply",
      "Not sure if you are intentionally missing the distinction I\u2019m making? Your comment just restates the GP\n \nreply",
      "I'm not sure what distinction you're trying to make, no.\n \nreply"
    ],
    "link": "https://cosmowenman.substack.com/p/secret-3d-scans-in-the-french-supreme",
    "first_paragraph": "",
    "summary": "In a dramatic twist that no one saw coming except perhaps every single person ever, the French Supreme Court decides that hoarding 3D scans in a dusty digital drawer is not quite in the spirit of public rights to cultural heritage. Commenters, armed with their absolutely <em>essential</em> law degrees from the University of Twitter, wrestle with complex legal notions about property rights and scanning technology that most six-year-olds could probably grasp better. Meanwhile, heartache ensues in gift shops across France as souvenir sales face the mortal threat of high-res digital models that maybe three people on Earth know how to print. Will fake Nefertiti busts flood the markets? Stay tuned to find out if civilization can withstand this terrifying possibility. \ud83d\udc80\ud83c\udfa8\ud83d\udda8\ufe0f"
  },
  {
    "title": "Show HN: Go Plan9 Memo (pehringer.info)",
    "points": 233,
    "submitter": "pehringer",
    "submit_time": "2024-10-18T14:36:27.000000Z",
    "num_comments": 75,
    "comments_url": "https://news.ycombinator.com/item?id=41879854",
    "comments": [
      ">  4 represents \u201cNOSPLIT\u201d which we need for some reasonFor those who are curious: \"In the general case, the frame size [the parameter after NOSPLIT] is followed by an argument size, separated by a minus sign. (It's not a subtraction, just idiosyncratic syntax.) The frame size $24-8 states that the function has a 24-byte frame and is called with 8 bytes of argument, which live on the caller's frame. If NOSPLIT is not specified for the TEXT, the argument size must be provided. For assembly functions with Go prototypes, go vet will check that the argument size is correct.\"Source: https://go.dev/doc/asm\n \nreply",
      "> After doing some preliminary research I discovered that Go uses its own internal assembly language called Plan9.Is the language actually called that?\n \nreply",
      "No. It's just Go assembly.\n(It happens to be a Plan 9-derived syntax, but we call it Go assembly.)\nSee https://go.dev/doc/asm.\n \nreply",
      "nice to see you around still :)\n \nreply",
      "This is a fair question. Initially I just assumed this was true. Because someone who did research on this topic would not get something like this wrong. And besides that, if you know a little about the project this name could make some sense.But the more I look into it, the more I think this is just a LLM hallucination.The doc about the 'assembly' format doesn't give a proper name. It just calls 'go assembler'.And I think the source of this hallucination was this first paragraph:> The assembler is based on the input style of the Plan 9 assemblers, which is documented in detail elsewhere. If you plan to write assembly language, you should read that document although much of it is Plan 9-specific. The current document provides a summary of the syntax and the differences with what is explained in that document, and describes the peculiarities that apply when writing assembly code to interact with Go.\n \nreply",
      "No, it doesn't have a name. Plan 9 is an operating system, and this style of assembly language syntax originates from the assembler used on this operating system. Its like saying \"The GNU Compiler Collection uses its own internal assembly language called Unix.\"\n \nreply",
      "Well, linux is just a mechanism to make more copies of GCC, so that tracks.\n \nreply",
      "Kinda like GNU (Unix reimplementation) was a fancy platform to run Emacs on top.\n \nreply",
      "basically: https://www.gnu.org/gnu/rms-lisp.en.html\n \nreply",
      "A couple points on Go assembly:1. On amd64 those ints are actually 64bit. If you used int32 then they would be be word aligned in the parameter list. However, there is a gotcha with that. The return values will always start at a dword aligned offset on 64bit system.2. NOSPLIT is defined in \"textflag.h\" which Go's compiler automatically provides. However, NOSPLIT is, from everything I've read, only respected on runtime.XX functions, so it's not doing anything there, and it's also not necessary. NOSPLIT tells the compiler not to insert code to check if the stack needs to split because it's going to overflow, which is technically unnecessary if the function doesn't need any stack space. It's basically only there on the function that checks for stack splits, to prevent that code from being injected into itself.\n \nreply"
    ],
    "link": "https://pehringer.info/go_plan9_memo.html",
    "first_paragraph": "I want to take advantage of Go\u2019s concurrency and parallelism for some of my upcoming projects, allowing for some serious number crunching capabilities. But what if I wanted EVEN MORE POWER?!? Enter SIMD, Same Instruction Muliple Data [\u201csim\u201d-\u201cdee\u201d]. Simd instructions allow for parallel number crunching capabilities right down at the hardware level. Many programming languages either have compiler optimizations that use simd or libraries that offer simd support. However, (as far as I can tell) Go\u2019s compiler does not utilizes simd, and I cound not find a general propose simd package that I liked. I just want a package that offers a thin abstraction layer over arithmetic and bitwise simd operations. So like any good programmer I decided to slightly reinvent the wheel and write my very own simd package. How hard could it be?After doing some preliminary research I discovered that Go uses its own internal assembly language called Plan9. I consider it more of an assembly format than its own lan",
    "summary": "**Web Developer Discovers Assembly Language, Hilarity Ensues**\n\nIn a thrilling episode of reinventing the wheel, a brave keyboard warrior decides that the Go programming language just isn\u2019t \ud83d\udcaa enough for \"serious number crunching.\" Armed with an insatiable appetite for power and an apparent allergy to existing libraries, our hero stumbles upon *Plan9* assembly, mistaking it for a programming language rather than just an assembly format from a bygone OS era. Commenters, ever helpful in splitting hairs finer than a quantum string, school the would-be Assembler in the actual nature of Go's assembly dialect while reminiscing about GNU, Linux, and other relics of geekdom. Meanwhile, functional SIMD code remains conspicuously absent, but hey, at least everyone got a good lecture on assembly language syntax!"
  },
  {
    "title": "setBigTimeout (evanhahn.com)",
    "points": 103,
    "submitter": "cfj",
    "submit_time": "2024-10-17T18:01:01.000000Z",
    "num_comments": 67,
    "comments_url": "https://news.ycombinator.com/item?id=41872010",
    "comments": [
      "The default behaviour of setTimeout seems problematic. Could be used for an exploit, because code like this might not work as expected:    const attackerControlled = ...;\n    if (attackerControlled < 60_000) {\n      throw new Error(\"Must wait at least 1min!\");\n    }\n\n    setTimeout(() => {\n      console.log(\"Surely at least 1min has passed!\");\n    }, attackerControlled);\n\n\nThe attacker could set the value to a comically large number and the callback would execute immediately.  This also seems to be true for NaN. The better solution (imo) would be to throw an error, but I assume we can't due to backwards compatibility.\n \nreply",
      "A scenario where an attacker can control a timeout where having the callback run sooner than one minute later would lead to security failures, but having it set to run days later is perfectly fine and so no upper bound check is required seems\u2026 quite a constructed edge case.The problem here is having an attacker control a security sensitive timer in the first place.\n \nreply",
      "The exploit could be a DoS attack. I don't think it's that contrived to have a service that runs an expensive operation at a fixed rate, controlled by the user, limited to 1 operation per minute.\n \nreply",
      "A minimum timing of an individual task is not a useful rate limit. I could schedule a bunch of tasks to happen far into the future but all at once for example.Rate limits are implemented with e.g., token buckets which fill to a limit at a fixed rate. Timed tasks would then on run try to take a token, and if none is present wait for one. This would then be dutifully enforced regardless of the current state of scheduled tasks.Only consideration for the timer itself would be to always add random jitter to avoid having peak loads coalesce.\n \nreply",
      "> I don't think it's that contrived to have a service that runs an expensive operation at a fixed rate, controlled by the userMaybe not contrived but definitely insecure by definition. Allowing user control of rates is definitely useful & a power devs will need to grant but it should never be direct control.\n \nreply",
      "Can you elaborate on what indirect control would look like in your opinion?No matter how many layers of abstraction you put in between, you're still eventually going to be passing a value to the setTimeout function that was computed based on something the user inputted, right?If you're not aware of these caveats about extremely high timeout values, how do any layers of abstraction in between help you prevent this? As far as I can see, the only prevention is knowing about the caveats and specifically adding validation for them.\n \nreply",
      "> that was computedOr comes from a set of known values. This stuff isn't that difficult.This doesn't require prescient knowledge of high timeout edge cases. It's generally accepted good security practice to limit business logic execution based on user input parameters. This goes beyond input validation & bounds on user input (both also good practice but most likely to just involve a !NaN check here), but more broadly user input is data & timeout values are code. Data should be treated differently by your app than code.To generalise the case more, another common case of a user submitting a config value that would be used in logic would be string labels for categories. You could validate against a known list of categories (good but potentially expensive) but whether you do or not it's still good hygiene to key the user submitted string against a category hashmap or enum - this cleanly avoids using user input directly in your executing business logic.\n \nreply",
      "That's just terrible input validation and has nothing to do with setTimeout.If your code would misbehave outside a certain range of values and you're input might span a larger range, you should be checking your input against the range that's valid. Your sample code simply doesn't do that, and that's why there's a bug.That the bug happens to involve a timer is irrelevant.\n \nreply",
      "> That's just terrible input validation and has nothing to do with setTimeout.Except for the fact that this behaviour is surprising.>  you should be checking your input against the range that's valid. Your sample code simply doesn't do that, and that's why there's a bug.Indeed, so why doesn't setTimeout internally do that?\n \nreply",
      "> Indeed, so why doesn't setTimeout internally do that?Given that `setTimeout` is a part of JavaScript's ancient reptilian brain, I wouldn't be surprised it doesn't do those checks just because there's some silly compatibility requirement still lingering and no one in the committees is brave enough to make a breaking change.(And then, what should setTimeout do if delay is NaN? Do nothing? Call immediately? Throw an exception? Personally I'd prefer it to throw, but I don't think there's any single undeniably correct answer.)Given the trend to move away from the callbacks, I wonder why there is no `async function sleep(delay)` in the language, that would be free to sort this out nicely without having to be compatible with stuff from '90s. Or something like that.\n \nreply"
    ],
    "link": "https://evanhahn.com/set-big-timeout/",
    "first_paragraph": "In short: JavaScript\u2019s setTimeout breaks after ~25 days. I made setBigTimeout, a silly module to get around this problem. See the package on npm or the source code.setTimeout is JavaScript\u2019s way of delaying code. You provide a timeout, in milliseconds, and a function to call after that time has elapsed.1So far, so good.In most JavaScript runtimes, this duration is represented as a 32-bit signed integer. That means the maximum timeout is about 2.1 billion milliseconds, or about 24.9 days. That\u2019s plenty of time for most people, but not for freaks like me. If you try to set a bigger timeout, weird things happen.For example, if you try setting a timeout to 40 days, the function runs immediately.This feels like a bug to me. Shouldn\u2019t this wait 40 days?Enter setBigTimeout, a cursed module I made to address this.setBigTimeout works just like setTimeout2 but it can take very big delays.You can also pass bigint delays to delay things for an extremely long time:It does this by chaining multiple ",
    "summary": "In an Earth-shattering breakthrough for those needing to dodge JavaScript fatigue, Evan Hahn introduces <em>setBigTimeout</em>, a module so niche it makes quantum physics look mainstream. Who needs a script delayed for over 25 days? Apparently, people safeguarding their New Year resolutions in code. Commenters dive deep, oscillating between praising the utility and devising security scenarios so implausible they make Flat Earthers blush. From DoS attacks at Bingo night to philosophizing about NaN behaviors as if they're discussing Schr\u00f6dinger's cat, the thread is a merry-go-round of tech obsession where practical use cases are as rare as meaningful comments in minified JS files. \ud83c\udfa1\ud83d\udcbb"
  },
  {
    "title": "JSON Patch (zuplo.com)",
    "points": 169,
    "submitter": "DataOverload",
    "submit_time": "2024-10-17T17:46:17.000000Z",
    "num_comments": 115,
    "comments_url": "https://news.ycombinator.com/item?id=41871850",
    "comments": [
      "I quite like JSON Patch but I've always felt that it's so convoluted only because of its goal of being able to modify every possible JSON document under the sun. If you allow yourself to restrict your data set slightly, you can patch documents much simpler.For example, Firebase doesn't let you store null values. Instead, for Firebase, setting something to null means the same as deleting it. With a single simple restriction like that, you can implement PATCH simply by accepting a (recursive) partial object of whatever that endpoint. Eg if /books/1 has    { title: \"Dune\", score: 9 }\n\nyou can add a PATCH /books/1 that takes eg    { score: null, author: \"Frank Herbert\" }\n\nand the result will be    { title: \"Dune\", author: \"Frank Herbert\" }\n\nThis is way simpler than JSON Patch - there's nothing new to learn, except \"null means delete\". IMO \"nothing new to learn\" is a fantastic feature for an API to have.Of course, if you can't reserve a magic value to mean \"delete\" then you can't do this. Also, appending things to arrays etc can't be done elegantly (but partially mutating arrays in PATCH is, I'd wager, often bad API design anyway). But it solves a very large % of the use cases JSON Patch is designed for in a, in my humble opinion, much more elegant way.\n \nreply",
      "The article has a section at the bottom \"Alternatives...\" [1].\nIt links to \"JSON Merge Patch\" which is what you are describing: https://zuplo.com/blog/2024/10/11/what-is-json-merge-patchThat's the format that people tend to naturally use. The main problem is that arrays can only be replaced.[1] https://zuplo.com/blog/2024/10/10/unlocking-the-power-of-jso...\n \nreply",
      "json merge patch is pretty good. I think it just needs an optional extension to specify an alternative magical value for \u201cdelete\u201d. null is a pretty good default, and comports well with typical database patterns, but is outright bad for some things.I think it also needs a \u201creplace\u201d option at the individual object update level. Merge is a good default, but the semantics of the data or a particular update could differ.You\u2019re almost surely doing something wrong if replace doesn\u2019t work for arrays. I think the missing thing is a collection that is both ordered and keyed (often not by the same value). JSON by itself just doesn\u2019t do that.So maybe what\u2019s missing is a general facility for specifying metadata on an update, which can be used to specify the magical delete value, and the key/ordering field for keyed, ordered collections.\n \nreply",
      "> You\u2019re almost surely doing something wrong if replace doesn\u2019t work for arrays. I think the missing thing is a collection that is both ordered and keyed (often not by the same value). JSON by itself just doesn\u2019t do that.Yeah, you could assign an identity value to each element of the array and then use a subresource to manipulate those elements by identity value. Then you could PUT using the same JSON merge mechanism to clear individual fields, and you could DELETE to remove items from the array by subresource.This just seems like a reinvention of a crufty piece of XML.\n \nreply",
      "Also the first thing I was thinking. The only reason I can see for using JSON Patch is for updating huge arrays. But I never really had such big arrays that I felt the necessity for something like this.\n \nreply",
      "Nice! I gotta say I didn't expect a thing called \"JSON Merge Patch\" to be simpler and more concise than a thing called \"JSON Patch\" :-)\n \nreply",
      "Same here, I actually made a tool for this called www.jsonmergepatch.com - give it a try\n \nreply",
      "I wonder if adding a merge op would be a viable option e.g.:    { \"op\": \"merge\", \"path\": \"/\", \"value\": { \"score\": null, \"author\": \"Frank Herbert\" } }\n\nIt's kind of nice to retain the terse and intuitive format while also gaining features like \"test\" and explicit nulls. It's of course not spec compliant anymore but for standard JSON Patch APIs the client could implement a simple Merge Patch->Patch compiler.\n \nreply",
      "> I quite like JSON Patch but I've always felt that it's so convoluted only because of its goal of being able to modify every possible JSON document under the sun.It seems like this is mainly a problem if you're implementing this _ad hoc_ on the client or server side -- is that right?I mean: presumably most of the time that you want to either of these, you already have both the old and new object, right?  Is it not straightforward to write a function (or library) that takes two plain objects and generates the JSON Patch from one to the other, and then use that everywhere and not think about this (but retain the advantage of \"being able to modify every possible JSON document under the sun\").If there are cases where you're making a delta without the original object (i.e., I know I always want to remove one field and add some other, whatever the original state was), it seems like you could have nice helpers like `JsonPatch::new().remove_field('field1').add_field('field2', value)`.I haven't actually done this so maybe I'm missing something about how you want to use these things in practice?edit to add my motivation: I'd much rather having something robust and predictable, even if it means writing tooling to make it convenient, than something that _seems_ easy but then can't handle some cases or does something different than expected (\"I wanted this to be null, not gone!\").\n \nreply",
      "Immer can generate patches from your changes: https://immerjs.github.io/immer/patchesI think I've seen this in other libraries too but I forget which.\n \nreply"
    ],
    "link": "https://zuplo.com/blog/2024/10/10/unlocking-the-power-of-json-patch",
    "first_paragraph": "JSON Patch is a standardized format defined in\nRFC 6902 for describing how to\nmodify a JSON document. It was created to address the need for a simple,\nefficient, and standardized way to apply partial updates to resources,\nespecially over HTTP. Unlike other partial update formats, JSON Patch is\nspecifically designed to work with JSON documents, which are often used in\nmodern web APIs.Initially, HTTP methods like PUT and POST were used to update resources.\nHowever, this often resulted in sending large amounts of data over the network,\neven when only a small part of the resource needed modification. The PATCH\nmethod was introduced to allow for partial updates, but there was no\nstandardized format for the patch document. JSON Patch fills this gap by\nproviding a clear, concise way to express changes to a JSON document, helping to\nreduce bandwidth, and improve the performance of web applications.JSON Patch operates on a JSON document as a sequence of atomic operations. An\noperation is an obj",
    "summary": "At last, the hero we didn't ask for but definitely didn't need: JSON Patch. Described in spine-tingling detail on zuplo.com, this riveting format is for those who dream in curly braces and find the thrill in minimizing HTTP payloads. Commenters wax poetic, discussing the epic complexities of JSON mutations like they're splitting the atom rather than modifying data. Because, you know, using PUT was just too straightforward. JSON Patch - perfect for when you feel your API just isn't convoluted enough! \ud83e\udd13\ud83d\udcc9"
  },
  {
    "title": "Jury awards American Airline $9.4M from website behind 'skiplagging' hack (courthousenews.com)",
    "points": 14,
    "submitter": "us0r",
    "submit_time": "2024-10-19T00:52:14.000000Z",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41884772",
    "comments": [
      "I'm surprised that this common enough that it's worthwhile for airlines to fight it. It requires both:- Flight being meaningfully cheaper using hidden cityand- Traveller is willing to deal with the restrictions (no carry-on, risk of route changing, no frequent flyer etc).\n \nreply",
      "Seems like this practice in general shows that a true free market doesn't exist in the airline industry. If it did exist then flights like these wouldn't be profitable to sell in the first place.\n \nreply",
      "I could imagine situations where airlines need to get pilots/crew/planes to some location for the next flight and somehow recoup costs there, and are willing to cut prices on such multileg flights to take business away from their competitors... but I generally agree with your statement.\n \nreply",
      "I don't understand the airline's argument that they couldn't resell the seat. Of course not, it was already paid for! Can someone make sense of their argument for me, as it seems prima facia illegitimate.\n \nreply"
    ],
    "link": "https://www.courthousenews.com/jury-awards-american-airlines-9-4-million-from-website-behind-skiplagging-hack/",
    "first_paragraph": "",
    "summary": "<b>Title:</b> Jury Thinks Skiplagging is Naughty, Gives American Airlines a Few Million for Snacks\n\nOnce again, the legal system swoops in to save a gigantic corporation a tiny percentage of its daily income by punishing the big bad website that helped thrifty travelers save a buck. Commenters, in a dazzling display of economic expertise, are flabbergasted that the airline industry doesn\u2019t represent a pristine free market utopia. \"Why can\u2019t unused seats just be free?!\" cry users, grappling with the unfathomable concept that airlines plan logistics better than a Reddit thread. Meanwhile, someone probably just booked another skiplagged flight because, let\u2019s face it, $9.4 million is just a slap on the wrist with a wad of cash. \ud83e\udd11\ud83d\udcb8\ud83d\udc4b"
  },
  {
    "title": "Running an open source app: Usage, costs and community donations (spliit.app)",
    "points": 149,
    "submitter": "scastiel",
    "submit_time": "2024-10-18T14:35:22.000000Z",
    "num_comments": 112,
    "comments_url": "https://news.ycombinator.com/item?id=41879845",
    "comments": [
      "I'm always curious what folks use for their database for things like this. Even though I like SQLite--a lot--my preference has become that the app is generally separate and mostly stateless. Almost always the data is the most important thing, so I like being able to expand/replace/trash the app infra at will with no worries.Thought about maybe running a Postgres VPS, but I've enjoyed using neon.tech more than I expected (esp. the GUI and branching). I guess the thing that has crept in: speed/ease is really beating out my ingrained cheapness as I've gotten older and have less time. A SaaS DB has sped things up. Still don't like the monthly bills & variability though.\n \nreply",
      ">Almost always the data is the most important thing, so I like being able to expand/replace/trash the app infra at will with no worries.Have you used SQLite with Litestream? That's the beauty of it. You can blow away the app and deploy it somewhere else, and Litestream will just pull down your data and continue along as if nothing happened.At the top of this post, I show a demo of attaching Litestream to my app, and then blowing away my Heroku instance and redeploying a clean instance on Fly.io, and Litestream ports all the data along with the new deployment:https://mtlynch.io/litestream/\n \nreply",
      "> No, my app never talks to a remote database server.> It\u2019s a simple, open-source tool that replicates a SQLite database to Amazon\u2019s S3 cloud storage.That was a very long walk to get to that second quote. And it makes the first quote feel deceptive.\n \nreply",
      "Thanks for the feedback!Can you share a bit more about why you feel it's deceptive?The point I was trying to make is that database servers are relatively complex and expensive. S3 is still a server, but it's static storage, which is about as simple and cheap as it gets for a remote service.Was it that I could have been clearer about the distinction? Or was the distinction clear but feels like not a big difference?\n \nreply",
      "S3 is acting as a backup, not a data store. The SQLite file is local to the app. This recent post discusses the tradeoffs in more detail and includes metrics.\nhttps://fractaledmind.github.io/2024/10/16/sqlite-supercharg...\n \nreply",
      "I'm currently using SQLite + Litestream with one app, though it's strictly Litestream as a backup/safety net and I'd be manually standing the thing back up if it came to building the server anew, as that's not automated.If anything, I'd probably end up looking at a dedicated PG VPS. I've started to get used to a few Postgres conveniences over SQLite, especially around datetimes, various extensions, and more sophisticated table alterations (without that infamous SQLite 12-step process), etc. So that's been an evolution, too, compared to my always-SQLite days.\n \nreply",
      "This is a well written guide, thanks!\n \nreply",
      "Cool, glad to hear it was useful!\n \nreply",
      "Spinning up a VPS for things like this is tempting to me too, but not having done significant backend work in over a decade my worry would be with administering it \u2014 namely keeping it up to date, secure, and configured correctly (initial setup is easy). What's the popular way of handling that these days?\n \nreply",
      "It's trivial to run mysql (or Perforce variant) or Postgres, with some minor caching for simple apps.I'm not sure what you are hitting that would go past the capacity of a small vps.Independent VPs for DB make sense, but if the requests are reasonably cached, you can get away with it (and beef up the backups) especially if it's something non-critical.\n \nreply"
    ],
    "link": "https://spliit.app/blog/spliit-by-the-stats-usage-costs-donations",
    "first_paragraph": "I created Spliit a few years ago, but the version you can use today is only one year old. As a user suggested on GitHub, it\u2019s a great opportunity to release some information about the project as a transparency exercise.In the last 12 months, Spliit received 152k visits, starting from ~200/week, and now regularly 5-6k/week. What is more interesting: the bounce rate is 33%, meaning that most people don\u2019t just visit the home page; they act on the website, either by switching groups, creating expenses, or reading a blog post.Among these 152k visitors, at least 29k used a shared link. Which confirms that the normal use case scenario is for someone to create a group, then share it with the participants. But many visitors also come from Reddit, thanks to the many posts where someone asks for a viable alternative to Splitwise.When looking where the visitors geographically come from, we can observe that countries where Spliit is the most popular are Germany, United States, and India. (To be hon",
    "summary": "**Little Database on the Prairie: How a Minor App Gorges on GitHub Glory**\n\nIn a heartwarming tale of humble bragging disguised as \"transparency,\" the creator of Spliit\u2014a poor man's Splitwise\u2014decided to <em>enlighten</em> the peons on GitHub with staggering metrics like \"152k visits\" and a bounce rate only a statistician could love. Commenters, in a thrilling display of missing the point, dive deep into the intricacies of databases, waxing poetic about SQLite and the existential dread of monthly bills from SaaS products. One brave soul even ventured into the thrilling world of SQLite replication with Litestream, as if pioneering uncharted territories of digital banality. Meanwhile, the jury is still out on whether any of this will help Spliit's users split a dinner bill without devolving into tribal warfare over who owes an extra dime."
  },
  {
    "title": "Code that helped end Apartheid (wired.com)",
    "points": 193,
    "submitter": "impish9208",
    "submit_time": "2024-10-18T13:05:21.000000Z",
    "num_comments": 121,
    "comments_url": "https://news.ycombinator.com/item?id=41879072",
    "comments": [
      "https://archive.is/yK8Jb",
      "So when he DM\u2019d me to say that he had \u201ca hell of a story\u201d\u2014promising \u201cone-time pads! 8-bit computers! Flight attendants smuggling floppies full of random numbers into South Africa!\u201d\u2014I responded.Ha ha ha. Yes, that was literally my very short pitch to Steven about Tim Jenkin's story!The actual DM: \"I think this has the makings of a hell of a story: https://blog.jgc.org/2024/09/cracking-old-zip-file-to-help-o... If you want I can connect you with Tim Jenkin. One time pads! 8-bit computers! Flights attendants smuggling floppies full of random numbers into South Africa!\"\n \nreply",
      "Did you end up discovering the original password to the zip file? (was it, as I'd hope, `TIMBOBIMBO` ?)\n \nreply",
      "No, I did not. I threw quite a lot of compute power at it using bkcrack (CPU) and hashcat (GPU) but never found out what it was. It was definitely not TIMBOBIMBO, sadly!I also ended up sponsoring the bkcrack project because the maintainer added a new option for me: https://github.com/kimci86/bkcrack/pull/126\n \nreply",
      "How much was \"quite a lot\"?\n \nreply",
      "I did a pass with bkcrack. The password is over 13 char.bkcrack.exe -k 98e0f009 48a0b11a c70f8499 -r 1..18 ?p\nbkcrack 1.7.0 - 2024-05-26\n[11:07:33] Recovering password\nlength 0-6...\nlength 7...\nlength 8...\nlength 9...\nlength 10...\nlength 11...\nlength 12...\nlength 13...\n \nreply",
      "I can tell you it's over 14 ?p, and over 16 ?u?d, and over 17 ?u.\n \nreply",
      "where's the original encrypted zip for this?\n \nreply",
      "This is such a fabulous story!! Thank you, good Sir, for bringing it to light!! <3The story reads like _The Cuckoo's Egg_ in a way. Spies, intrigue, covert comms, action, revolution!I loved that the code is still around, and works.Kudos!!\n \nreply",
      "Though, you could argue it was a 16 bit computer, of course :)(It was an 8088, essentially an 8086 with an 8 bit data bus, but 16bit registers and 20bit address bus).\n \nreply"
    ],
    "link": "https://www.wired.com/story/plaintext-you-can-now-see-the-code-that-ended-apartheid/",
    "first_paragraph": "To revisit this article, visit My Profile, then View saved stories.John Graham-Cumming doesn\u2019t ping me often, but when he does I pay attention. His day job is the CTO of the security giant Cloudflare, but he is also a lay historian of technology, guided by a righteous compass. He might be best known for successfully leading a campaign to force the UK government to apologize to the legendary computer scientist Alan Turing for prosecuting him for homosexuality and essentially harassing him to death. So when he DM\u2019d me to say that he had \u201ca hell of a story\u201d\u2014promising \u201cone-time pads! 8-bit computers! Flight attendants smuggling floppies full of random numbers into South Africa!\u201d\u2014I responded.The story he shared centers around Tim Jenkin, a former anti-apartheid activist. Jenkin grew up \u201cas a regular racist white South African,\u201d as he described it when I contacted him. But when Jenkin traveled abroad\u2014beyond the filters of the police-state government\u2014he learned about the brutal oppression in ",
    "summary": "Today in \"over-romanticized tech history,\" Wired brings us a story _so gripping_ you can almost hear the keyboard clicks and revolutionary coding from here! \ud83c\udf0d\u2728 John Graham-Cumming, apparently top guru of tech lore, DMs the author with a teaser that has everything from flight attendants smuggling floppy disks to \"one-time pads!\" Because nothing screams 'end of Apartheid' like 8-bit computers and secret codes. Meanwhile, our commenters embark on a profound quest missing the actual historical point, with one user lamenting over the true 16-bit identity of an 8088. Groundbreaking! \ud83d\udcbe\ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcac"
  },
  {
    "title": "Focus on decisions, not tasks (technicalwriting.dev)",
    "points": 72,
    "submitter": "kaycebasques",
    "submit_time": "2024-10-18T17:58:46.000000Z",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41881872",
    "comments": [
      "This is what I generally mean by taking an \"heuristic approach.\"I feel that we need to have a \"fuzzy logic\" approach to our work.However, that works best, when the engineer is somewhat experienced.If they are inexperienced (even if very skilled and intelligent), we need to be a lot more dictatorial.\n \nreply",
      "Thinking in Bets has been one of the most useful books to how I approach software engineering. It\u2019s not even about code, just how to make decisions effectively in limited information environments.\n \nreply",
      "Love that book. Such a powerful idea to phrase your predictions in terms of percentages rather than absolutes. Apparently the Super Bowl anecdote is controversial though? I.e. the conclusions to draw from that anecdote are very debatable.\n \nreply",
      "I don\u2019t remember the specific anecdotes too much, but the lessons make intuitive sense and feel useful.The one that sticks to mind most is that a good decision can have a bad outcome and that a good outcome doesn\u2019t always mean the decision was good.\n \nreply",
      "I have no idea what you mean by taking a \"fuzzy logic\" approach to work. Could you expand and explain that a bit please?\n \nreply",
      "This fits the way I like to use LLMs: I always ask them for options, then I decide myself which of those options makes the most sense.Essentially I'm using them as weird magical documentation that can spit out (incomplete but still useful) available options to guide my decision making at any turn.\n \nreply",
      "I like to think of it as the apprentices working for famous artists like Leonardo. The master would draw the outline/sketch, and then the students would fill in the blanks under supervision. Sometimes, the master would steal ideas from the students.\n \nreply",
      "Knowing how people use your system to make decisions is important. I think that knowledge is vital for maintaining, extending, or building a system.But the article suggests a higher responsibility: you should document your user's decision-making. You should tell them the context, the choices they have to make, and the consequences of their decisions.I've worked on a \"decision support system\" with that responsibility and it got really messy, really fast. Humans love to argue about consequences, even 100% absolutely known ones. They also despise automated emails bearing uncertainty, as well as docs demanding binary choices when many more choices are available in reality.I would hope the book beyond this article raises the concept of control. That is, to document a behavior, you need some guarantee (or enforcement) about that behavior so your documentation remains authoritative. IMO, the lack of authority/control is common, gaping blindspot of writing initiatives like https://www.plainlanguage.gov unfortunately.\n \nreply",
      "Quoting myself from r/technicalwriting discussion on this post:> Let me rephrase what I think is really important about Baker's idea. The dogma of technical writing education absolutely revolves around focusing on tasks. If we survey a lot of professional technical writers I will bet you that a majority of them believe that \"helping users achieve tasks\" is a primary goal of documentation, if not THE primary goal. This small quote from Baker is kinda radical (in the Latin sense of \"going to the roots\"), because it's suggesting that one of our fundamental assumptions is majorly lacking.For me personally, Baker's idea is fascinating simply because it sets the bar a lot higher than the current status quo of what's expected of technical writing. A lot of docs just assume that it's \"mission complete\" once a task is documented, and Baker (to me) is suggesting that it's simply not enough. Tasks of course still need to be documented, but tasks are a subset of the information that goes into decisions.I don't recall Baker's book discussing control in the way you mention. It's a new idea to me, thanks for sharing. One concrete example of control that comes to mind: if a lot of my docs rely on a page from another open source project, and that external page is not good (low quality), then it should probably be my responsibility to improve that doc. Many people might assume that docs external to their site are outside of their responsibility. But if you're really committed to supporting decisions then it doesn't really matter who is hosting the doc. Maybe there's a lot to learn from the ethos of being a good open source citizen in general\n \nreply",
      "\"Work is simply whatever we must do to get from one decision to the next.\" (Venkatesh Rao, Tempo)\n \nreply"
    ],
    "link": "https://technicalwriting.dev/strategy/decisions.html",
    "first_paragraph": "A quote from Every Page Is Page One\nthat has deeply changed how I approach technical writing:In technical communication, we don\u2019t talk much about decision support; we talk\nabout task support\u2026 In many cases, the information people need to complete their\ntasks is not information on how to operate machines, but information to support their\ndecision making\u2026 simply documenting the procedures is never enough\u2026 What I am talking\nabout is documenting the context, letting users know what decisions they must make,\nmaking them aware of the consequences, and, as far as possible, leading them to\nresources and references that will assist them in deciding what to do.\u00a9 2024 Kayce Basques",
    "summary": "\ud83d\udcda\ud83d\udca1 **The Focus Group Revolution: Task vs Decision in Tech Writing** \ud83d\udca1\ud83d\udcda\n\nCue the latest ideological kerfuffle in tech writing: \"Focus on decisions, not tasks!\" says a glib article that acts as if it has just unearthed the Dead Sea Scrolls of technical communication. The comment section quickly turns into a garden of half-baked philosophies where seasoned armchair pundits redefine procrastination as a strategic decision-making process. \ud83e\udde0\ud83c\udfad One brave soul tries to redefine \"fuzzy logic\" (spoiler: it still doesn't involve actual fuzz), while another likens their use of AI tools to consulting an oracle \u2014 albeit one that spat out semi-usable gibberish. Most entertaining is a gallant attempt to connect \"decision documentation\" with Renaissance artistry, because evidently, da Vinci was just trying to decide whether Mona Lisa should smile. Meanwhile, real decisions, like whether to read this article or not, remain as unaided as ever. \ud83d\ude02\ud83c\udfa8"
  },
  {
    "title": "Where the Digital Sidewalk Ends (smolways.com)",
    "points": 80,
    "submitter": "jmward01",
    "submit_time": "2024-10-17T20:42:44.000000Z",
    "num_comments": 45,
    "comments_url": "https://news.ycombinator.com/item?id=41873555",
    "comments": [
      "I don't know how solvable a problem this is.  There are two roads that would be labeled nearly identically that are an option for my commute.  Let's call them Road A and Road B.Road A: 45MPH, with a dedicated bike lane with no barrier.Road B: 40MPH with a dedicated bike lane with no barrier.The similarities end there though:Road A: Many turns with trees providing poor visibilityRoad B: Relatively straight, good visibilityRoad A: Typically has obstructions in the bike laneRoad B: Typically no obstructions in the bike laneRoad A: People speed rather massivelyRoad B: Many traffic signals; people who speed tend to take parallel Road A instead to avoid the signals.Road A: The bike lane is sometimes only 30cm wideRoad B: The bike lane is wide enough to ride 2 abreast, and there is usually an additional 30cm painted area between you and traffic.Road A: Car v. bike accidents are fairly regular, with multiple fatalities in the time I've lived here.Road B: Every car v. bike accident I'm aware of was at night and involved an intoxicated driver.I have 3 parallel options for going in this direction, in order of increasing total distance for traveled: Road A, Road B, and a MUP (i.e. combination bike/pedestrian path).For obvious reasons, I prefer Road B, but it's hard to label a map in such a way that it won't pick the two best choices as Road A (shortest) the MUP (avoid highways).\n \nreply",
      "The regional planning organization in my area uses a metric called Level of Traffic Stress: https://www.dvrpc.org/webmaps/bike-lts/And they made a routing tool based on this: https://www.dvrpc.org/ruti/\n \nreply",
      "I wonder if \u201cghost bikes\u201d belong in a bike map. Someone died on a bike here. If you\u2019re on a bike maybe be extra careful, maybe avoid this street next time.\n \nreply",
      "Deaths are the most clear-cut data, but close-calls will be more numerous and thus less noisy.  Either way it would need to be adjusted for cyclist volume.\n \nreply",
      "some cities/counties have (and publish) bike friendly commuting maps.Sort of like maps with the road B's highlighted.don't see why this data can't make it into maps, like A B C roads for cars.\n \nreply",
      "I think this is why Google preferentially wanted to take me down a 45 MPH state route that runs downhill and has curves and no separation between the bike lane and vehicle traffic instead of the nice, calm, traffic-less series of roads in residential neighborhoods that was accessible with a quarter-mile detour. Those roads didn't have sidewalks or bike lanes marked on the roads, so they were totally unsafe for a cyclist. As opposed to the 45 MPH road, which is perfectly safe because it has a bike lane.This is a great project.\n \nreply",
      "In my city the bike lanes are almost all death traps because they only exist to rake in federal funding with no care for making them useful or safe. I avoid them whenever possible. They routed one \"lane\" onto a narrow sidewalk under a wide railroad bridge and had to change a city ordinance prohibiting bikes on sidewalks in that region of the city.\n \nreply",
      "Do you, too, live in San Jose?My favorite feature of bike lanes there is how highway access ramps straight up cross them -- creating a flow of cars accelerating to highway speeds coming from behind and sideways from you, while you are in the perfect spot to be obscured by the car's A-frame.Or perhaps how I had to move away a sign saying \"SHARE THE ROAD\" (with bicycles, presumably) from the bicycle path. Multiple times.No, perhaps my favorite is how the Mayor decided to show off our bicycling infrastructure by taking a bicycle ride the first day of the year, and. . .. . . was promptly hit by an SUV, which broke his bones[1].If you don't live in San Jose, that's all you need to know about cycling in San Jose.[1] https://abc7news.com/san-jose-mayor-hit-by-car-bicyclist/500...\n \nreply",
      "No Google has definitely gotten progressively worse at navigating. The original satnav system in my 2004 Prius was very good at the time (probably still mostly is) - \"quick\" takes the presumed speed of a \"major\" road, and calculates with that. It led to mostly sensible routes following main roads. These were routes I would've picked myself from a map book.Google Maps these days...just doesn't seem to do that. They've overweighted their traffic management algorithm or something, but it seems to have no notion of the idea that you can't negotiate 50kmh residential roads with a turn every block at anything close to 50kmh, and such a drive is incredibly stressful. And then will do other things like conclude that a side-road against traffic turn into a main road (so right for me, equiv to a left in the US) is something that will easily be done any time of day rather then consist of waiting for a gap in 4 lanes of traffic.\n \nreply",
      "I suspect it's more than active traffic management, some of it is bound to be probing streets for which they don't have sufficient data.I've noticed weird little detours in areas where I'm familiar with but engaged navigation merely for an ETA. To me those little detours that don't save time or avoid construction suggest I'm being used to probe those side streets. It's like being a human ICMP packet.\n \nreply"
    ],
    "link": "https://www.smolways.com/post/where-the-digital-sidewalk-ends",
    "first_paragraph": "HomeBlogServicesLearn MoreAboutBikeIncubator, LLCContactMoreUpdated: 11 minutes ago(Direct link to data visualizer:  https://osm-viz.fly.dev )This application is meant to highlight what we perceive to be a problem in the digital map of the world \u2013 the map that all major map products (Google Maps, Apple Maps, Tesla onboard navigation, \u2026) as well as all boutique map products (Pointz, Komoot, \u2026) use as their underlying source of data. The problem is that, for any applications that are not car routing, the data used to achieve the task are usually abysmal \u2013 even where they are good in quality, the data are patchy and incomplete. If you are already very familiar with OpenStreetMap, its structure, how it\u2019s used, and all of the ways in which it falls short, you will probably see immediately what our web app (linked above) is all about and exactly what it shows. If not, the rest of this article will try to guide you through an orientation.A note: the data visualized in this app was pulled from",
    "summary": "<h3>Where the Digital Sidewalk Ends: More Like Where Sanity Ends</h3>\n<p>In a remarkable breakthrough in cartographic waffling, smolways.com introduces us to another nail-biting issue: digital maps might slightly misrepresent the utopia of urban biking. Enthusiastic commenters leap into action <em>deciphering</em> the academic catastrophe of cycling dangers, while engaging in a more perilous sport\u2014competitive anecdoting. Between gruesome tales of \"ghost bikes\" and San Jose's A-frame car ambushes, one marvels at the survival rate of cyclists following these handy-dandy death maps. Are these biking maps or a casino betting book on survival odds?</p>"
  },
  {
    "title": "TCAS averts possible head-on collision in Austin (flightradar24.com)",
    "points": 32,
    "submitter": "philip1209",
    "submit_time": "2024-10-18T19:37:24.000000Z",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41882742",
    "comments": [
      "The Cessna probably didn't have TCAS, right? So how did the system know whether to tell the Boeing pilots to raise or lower their altitude? I thought it was a system with a negotiation to avoid instructing both planes to turn the same way.\n \nreply",
      "> When a TA is issued, pilots are instructed to initiate a visual search for the traffic causing the TA. If the traffic is visually acquired, pilots are instructed to maintain visual separation from the traffic. Training programs also indicate that no horizontal maneuvers are to be made based solely on information shown on the traffic display. Slight adjustments in vertical speed while climbing or descending, or slight adjustments in airspeed while still complying with the ATC clearance are acceptablehttps://en.wikipedia.org/wiki/Traffic_collision_avoidance_sy...I am unsure what it would have told them to do in the RA case though.\n \nreply",
      "RA? Climb or descend. Or any of these more accurately: https://en.wikipedia.org/wiki/Traffic_collision_avoidance_sy...\n \nreply",
      "Sure, but which one? The Cessna doesn't have a TCAS to coordinate with, so how does it know which one is best? Does it just pick one and change it if the other aircraft reacts in the wrong way?\n \nreply",
      "It picks one.  If this does not improve the situation it issues a reversal.\n \nreply",
      "I\u2019m sorry, but in what world is it acceptable for an aircraft flying in an airport\u2019s atc controlled airspace to not be coordinating with ATC?I thought that was literally the entire reason we have controlled airspace rules?\n \nreply",
      "The Class C airspace at that location starts at 2,100 feet.  The Cessna was at 1,300 feet.  The airliner was at 1,700 feet.  The Cessna did not have to be coordinating with ATC because they were NOT in controlled airspace that requires radio communication with ATC, but it is optional and recommended.The real question here is, why does Austin Approach Control have airliners driving around lower than the ceiling of the Class C protected airspace, where EVERY pilot knows that there are little putt-putt planes pottering about?\n \nreply",
      "The site seems to be very focused on what happened in the air, I assume the site summarizes air traffic control or FAA reports for hobbyists, or that sort of thing, right? So we don\u2019t know what happened when the pilot of the Cessna got on the ground. Without knowing much about the rules of this sort of stuff, I assume they got a pretty stern talking to\u2026\n \nreply",
      "It's F1 weekend in Austin, so I'd assume that's part of the reason someone may think they don't need to care where they fly their private jet.\n \nreply",
      "Are you referring to the 45 year old Cessna when you say private jet? It looks like that plane flies a lot around Louisiana and Texas. Weird to jump to a conclusion about F1 money and an entitled pilot.\n \nreply"
    ],
    "link": "https://www.flightradar24.com/blog/tcas-averts-possible-collision-austin/",
    "first_paragraph": "",
    "summary": "Title: <em>TCAS Plays Sky Cop in Amateur Hour Over Austin</em>\n\nIn a thrilling display of airborne awkwardness, a Cessna that's probably older than your grandpa's toupee and a Boeing with more tech than a Silicon Valley startup played a game of \"chicken\" in the skies of Austin. Thankfully, the TCAS\u2014essentially an electronic hall monitor\u2014stepped in to avoid a midair mashup, deciding arbitrarily who gets to fly higher based on a high-tech eeny, meeny, miny, moe. \ud83d\udee9\ufe0f Commenters, in a spectacular show of missing the point, debate whether the Cessna\u2019s outdated tech got a stern talking-to on touchdown or if it was all because of the F1 crowd's VIP vibes. Meanwhile, the real question goes unanswered: Who lets these flying relics haphazardly wander into commuter aircraft lanes during peak hours? \ud83e\udd37\u200d\u2642\ufe0f"
  },
  {
    "title": "The feds are coming for John Deere over the right to repair (gizmodo.com)",
    "points": 308,
    "submitter": "rntn",
    "submit_time": "2024-10-18T16:32:13.000000Z",
    "num_comments": 149,
    "comments_url": "https://news.ycombinator.com/item?id=41880981",
    "comments": [
      "There was also a fatality in the last workplace strike.Deere seems to have bad relations with their employees, customers, and regulatory bodies.The shareholders should remove the board of directors.https://www.desmoinesregister.com/story/money/business/2021/...\n \nreply",
      "The shareholders don't care about any of that if they think the board did a decent job of propping up the stock price.Firing a board is generally risky, and the shareholders probably haven't fired them because even though the board has, almost objectively, not been good - firing them is likely even worse for the stock short term, and there aren't a lot of long-term, active investors left in the world.\n \nreply",
      "Tragedy of the commons is why short term is all that matters and will ever matter to non-ideological investors.If an action that hurts the stock short-term but will help int he long-term needs to be performed why would you as an investor enact it or even stay for the ride?You are better off either opposing it or selling your stock and then waiting to see if someone will enact the changes, then you have the \"insider\" information to know that the short-term stock drop was a good thing for the long-term and rebuy the shares cheaper.\n \nreply",
      "> Tragedy of the commons is why short term is all that matters and will ever matter to non-ideological investors.Tragedy of the commons was an ideological essay designed to justify privatization of public goods. It was disproven by data before it was published. I am sure there are some hyper specific examples where it has happened as described, but as a \u201cfact\u201d about the world and as a justification for any course of action, it\u2019s highly suspect.https://aeon.co/essays/the-tragedy-of-the-commons-is-a-false...\n \nreply",
      "I don't disagree that it was an ideological \"essay\" but I dunno why you're linking that when it's most associated with Aristotle, in particular:\"What is common to many is taken least care of, for all men have greater regard for what is their own than for what they possess in common with others.\"Maybe you're suggesting that that essay popularized the phrasing? But I'm pretty sure even as a coined \"term\" it was around before then\n \nreply",
      "This is the first I've heard that it's false; however it seems like many times in my life I've observed something suffering, seemingly from lack of ownership despite being a common good.What do you call that if you can't call it a tragedy of the commons?\n \nreply",
      "On HN I\u2019ve seen regular claims that \u201ctragedy of the commons has been disproven\u201d. I\u2019ve not yet identified which social bubble propagates this or what it is based on but there seems to be some niche in which people are being taught that it is categorically proven to be an invalid concept.",
      "Tragedy of the common is a concept dating back to ancient times that has extremely broad empirical support throughout history. The essay you mention took the name from all this real world experience, and even if the essay is bad, the concept is anything but.A simple search finds more examples and references to literature than you can likely read in years.I\u2019d recommend starting here https://en.wikipedia.org/wiki/Tragedy_of_the_commons\n \nreply",
      "When the UK enclosed their fields so that they were privately controlled crop yield immediately went up 20%. Theres overwhelming empiric evidence of tragedy of the commons being real.\n \nreply",
      "The board's goal was to lock-in maintenance with computer security, which failed catastrophically. All previous generations of Deere tractors have on-board electronics that can be jailbroken.https://www.theregister.com/2022/08/16/john_deere_doom/Just for that failure, they should all likely be gone.\n \nreply"
    ],
    "link": "https://gizmodo.com/the-feds-are-coming-for-john-deere-over-the-right-to-repair-2000513521",
    "first_paragraph": "",
    "summary": "**John Deere: Fumbled Right to Repair Warfares**\n\nIn an unsurprising twist, John Deere's stellar knack for antagonizing literally *everyone*\u2014workers, customers, and now the feds\u2014is showcased as the government swoops in over right-to-repair issues. The internet commentariat, in their infinite wisdom, morphs this corporate snafu into a sideline seminar about the tragedy of the commons, shareholder greed, and historical farm yields. Because when facing corporate overlords playing fast and loose with ethics, what better way to cope than airing grievances about century-old agricultural policies and ideological essays? Meanwhile, Deere tractors remain as locked down as Fort Knox, and just as likely to inspire a breakout. \ud83d\ude9c\ud83d\udc94"
  },
  {
    "title": "The Making of Micro Machines (readonlymemory.com)",
    "points": 69,
    "submitter": "Tomte",
    "submit_time": "2024-10-15T16:57:25.000000Z",
    "num_comments": 22,
    "comments_url": "https://news.ycombinator.com/item?id=41850570",
    "comments": [
      "> 'The company was on the verge of mass manufacturing its European cartridges when a serious bug was discovered in the code. \u2018It turned out that if you pressed brake or reverse on the starting grid the game just crashed,\u2019 says Graham. \u2018QA wasn\u2019t what it is now and nobody had ever tried that before. Codemasters had already manufactured tens of thousands of ROMs with this bug in, so in order to get it to work, they put some of the Game Genie hardware into the cartridge which would change the one byte of the code that contained the bug \u2013 that\u2019s how they fixed it.'Has anyone ever found a photo of the \"Game Genie Patch\" version of Micro Machines?  I can't find any photos of the cartridge board that uses that fix.\n \nreply",
      "I wonder if it was the actual game genie, or just the same concept. If you're just modifying a single byte at a single address, of a fixed ROM, you could easily throw something minimal together with a handful of transistors/resistors.\n \nreply",
      "When people are speaking about technology from 30+ years ago, they often get the details wrong...> \u2018I knew the C64 inside out and the NES has the same processor so, in some ways it was very similar,\u2019 says Graham. \u2018The NES was character based, it had sprites the same as the C64, it was a bit faster. You could only access 64k at a time, though, so we had to bank in and out of memory buffers \u2019Not 64K.  While that is the size of the address bus, a NES cartridge can only realistically use address range 6000-FFFF on the cartridge.  (Yes you can use 4020-5FFF as well, but that requires additional address decoding logic so it was seldom done).  For the mapper that Codemasters used, the 16KB bank 8000-BFFF was switchable, and the 16KB bank C000-FFFF was the fixed bank at the end of the ROM, and 4020-7FFF was unused (open bus)\n \nreply",
      "A story about reverse engineering the Micro Machines v3 format to create an editor\u2026 http://bradders.org/MMs/(not my work)\n \nreply",
      "That's mine! :-)I still play fairly frequently.Just this week we had a gathering of 7 people, playing MMv3 all on one big screen, with modified levels burned to a CDROM, using a PS2 with the sticky tape hack. It was awesome!AMA\n \nreply",
      "While I'm pretty sure that all versions of this game are pretty good, the N64 version in particular holds a special place in my heart. There was a way you could play it with 8 players on 4 controllers, with each person using half of a controller, and it was so much confusing fun! Often, at least at the start of a race, we'd only play a couple seconds before the game had to reset all the players to be back on the track, since things just went so fast that it was challenging to follow your car and keep it in bounds. So much fun!\n \nreply",
      "https://www.analogue.co/3d\n \nreply",
      "This is interesting. I wonder if this will get in hot waters with Nintendo. They're not the gentle kind when it comes to IP.I'm guessing they'll wait for them to start shipping products before they invest in the legal show stoppers.\n \nreply",
      "I had a blast playing that game when I was a kid - as I recall it struck a good balance between simple, accessible controls while also being challenging (and without being frustrating).  The gold cartridge rather than the typical NES cartridge probably also lent it an air of specialness.As I got older I realized that most kids with Nintendos had more games and I felt left out for never having played Zelda or Metroid.  But man, Micro Machines, Super Mario Bros 3 and Faxanadu was all I needed.\n \nreply",
      "It was such a great game - very simple to just sit down and play, and lots of fun. Sure, there were no powerups, vehicle customizations, no track designer, but it had amazing track/level design. Who as a kid didn't race their boats in the bathtub, cars in the kitchen...\n \nreply"
    ],
    "link": "https://readonlymemory.com/the-making-of-micro-machines/",
    "first_paragraph": "The dawn of the 1990s was a time of huge upheaval in the British games industry. Most of the well-known studios \u2013 the likes of Sensible Software, The Bitmap Brothers, Elite Systems, Gremlin Graphics and Hewson Consultants \u2013 had specialised in the home computer market, progressing through successive generations: the ZX81 and VIC-20; the Commodore 64 and Spectrum; the Amiga and Atari ST. But the irresistible rise of the Nintendo Entertainment System, and the promise of the SNES and Sega\u2019s Mega Drive, heralded a new era of console gaming, of closed systems replacing open architectures, and of global marketing and distribution. It would require a radical shift in approach, there would need to be entrepreneurism and chutzpah, but the outlook was simple: adapt or die.At the spring Consumer Electronics Show in 1989, Southam-based publisher Codemasters found itself on a stand next to the dominant force in the industry: Nintendo. The Japanese company was still showing off the NES, which by then",
    "summary": "Welcome to <em>The Making of Micro Machines</em>, where nostalgia-stricken tech bros and vintage game enthusiasts congregate to mythologize the strategic missteps of British gaming companies from the early '90s. In this thrilling episode of corporate ineptitude, learn how Codemasters bravely stood next to Nintendo without anyone noticing, crafted a game-breaking bug as a surprise feature, and deployed a miraculous one-byte \u201cGame Genie Patch\u201d that probably nobody tested either. The comment section is a delightful circus. Watch grown adults wax lyrical about the \u201cgood old days\u201d while mixing up technical details and reminiscing about games that were, on reflection, just alright. Come for the humorously recounted tech lore, stay for the arguments over which childhood game was *least* buggy. \ud83d\udd79\ufe0f\ud83d\udc1e"
  },
  {
    "title": "Using static websites for tiny archives (alexwlchan.net)",
    "points": 301,
    "submitter": "ingve",
    "submit_time": "2024-10-18T06:12:39.000000Z",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=41876750",
    "comments": [
      "I copy the images in my clipboard and save them in an HTML file to have single-file galleries:https://gist.github.com/egeozcan/b27e11a7e776972d18603222fa5...Live:https://gistpreview.github.io/?b27e11a7e776972d18603222fa523...Selecting via file-picker works too. Dragging usually does not. When all works, images are inserted inline as blobs.After adding images, if you save the page (literally file->save), the blobs are saved together. don't want a part when saving (for example, removing images)? inspect element, remove, save page.throw the page on some server or just double click on your computer/mobile.\n \nreply",
      "Love it! True to the original vision of the WWW. Tim Berners-Lee's WorldWideWeb browser was also an editor.https://github.com/cadars/john-doe gives me the same feels.\n \nreply",
      "That's really neat!You could add a \"Download this page\" button on the page which does some tricks to produce an HTML file with the images baked in that the user can download, which could work on mobile.Here's a quick prototype: https://gistpreview.github.io/?14a2c3ef508839f26377707dbf5dd... - code here: https://gist.github.com/simonw/14a2c3ef508839f26377707dbf5dd...\n \nreply",
      "Thank you! Great idea, I'll give it a try :)\n \nreply",
      "Very cool design! note if you can't get it to work, make sure you set file type to complete webpage when saving.\n \nreply",
      "That is slick. Offline first in the truest sense.\n \nreply",
      "This is very cool, thank you for sharing.\n \nreply",
      "Lots of folks mentioning Markdown in the comments. +1 to that. Plain text FTW. I think a lot about my own data hoarding / archiving, and plain text is such a key part of that. Very future-proof.Ever since WordPerfect I've preferred more deterministic, lightly-formatted documents with some way to see formatting characters directly. Markdown is brilliant, basically a DSL (domain-specific language) for HTML.The key to plain text is tooling! A couple Markdown tools I haven't seen mentioned here yet (even though they've come up on HN before) are:https://addons.mozilla.org/en-US/firefox/addon/markdown-view... - pretty-render Markdown right in the browserhttps://casual-effects.com/markdeep/ - standalone web-friendly Markdown formatter with many features\n \nreply",
      "I use GitHub to host my markdown files. A bit more information is in this article I wrote about it. I actually have 4 or 5 similar articles with various thoughts on this. I'm trying to find a way to make it simpler, maybe even for non-technical users, but I'm not there yet.https://joeldare.com/using-neat-css-on-github-pages\n \nreply",
      "I convert content to markdown and relevant images and then store them in an obsidian vault. I self-sync it with syncthing. It has quickly become a rather effective zettelkasten memory prosthetic on my laptop and phone.I also use google/facebook takeouts, reformat the results, and store+index all my human-facing correspondence in there. Text is cheap and I avoid most images. Its still under 200mb and instantly searchable with a nice UI and as a bunch of markdown files it is easily portable.\n \nreply"
    ],
    "link": "https://alexwlchan.net/2024/static-websites/",
    "first_paragraph": "In my previous post, I talked about how I\u2019m trying to be more intentional and deliberate with my digital data. I don\u2019t just want to keep everything \u2013 I want to keep stuff that I\u2019m actually going to look at again. As part of that process, I\u2019m trying to be better about organising my files. Keeping something is pointless if I can\u2019t find it later.Over the last year or so, I\u2019ve been creating static websites to browse my local archives. I\u2019ve done this for a variety of collections, including:I create one website per collection, each with a different design, suited to the files it describes. For example, my collection of screenshots is shown as a grid of images, my bookmarks are a series of text links, and my videos are a list with a mixture of thumbnails and text.These websites aren\u2019t complicated \u2013 they\u2019re just meant to be a slightly nicer way of browsing files than I get in the macOS Finder. I can put more metadata on the page, and build my own ways to search and organise the files.Each coll",
    "summary": "In an age where digital hoarding has become a societal norm, one brave soul on alexwlchan.net takes a stand by meticulously creating static websites to archive his imperatively unforgettable collection of... screenshots and bookmarks? Sure, it's *just a slightly nicer way* of using space that could otherwise store something less ephemeral\u2014like another copy of \"The Office\" in 4K. Meanwhile, the comment section transforms into a nostalgic tech utopia where everyone reminisces about WordPerfect and Markdown, competing for the most \"minimalist yet complex\" data-saving technique, because why keep things simple when you can make it a hobby? Look out, folks, we're one \"neat hack\" away from reinventing the filing cabinet\u2014all offline first, of course! \ud83d\ude1c"
  },
  {
    "title": "Subvert \u2013 Collectively owned music marketplace (subvert.fm)",
    "points": 149,
    "submitter": "cloudfudge",
    "submit_time": "2024-10-18T16:17:59.000000Z",
    "num_comments": 73,
    "comments_url": "https://news.ycombinator.com/item?id=41880829",
    "comments": [
      "It's a great start. Co-ops and non-profits can also be subverted and taken over. I hope you look ahead and plan very carefully.For example, according to an (unverified) story someone told me, a vendor to US east coast food cooperatives now controls many of them; they get their person in, pass bylaws empowering them and disempowering the board (the board usually lacking sophistication), and have deeper pockets for any legal struggle than any co-op member does.Also, I remember in the news that a non-profit or limited-profit company in the IT industry, founded for the public good, is going to be turned into a for-profit. The board actually fired the person behind this plan, but that person came back and fired the board members.\n \nreply",
      "> For example, according to an (unverified) story someone told me, a vendor to US east coast food cooperatives now controls many of them; they get their person in, pass bylaws empowering them and disempowering the board (the board usually lacking sophistication), and have deeper pockets for any legal struggle than any co-op member does.For anyone interested in learning more, this is a reference to the National Cooperative Grocers and the role of UNFI (a primary distributor for many food coops) and CDS (a cooperative grocery consulting firm). I've been pretty involved in my local grocery cooperative's governance over the past two decades. From my vantage point, there's some truth to this, but also some exaggeration (or more accurately, the pinning of other grievances, which themselves might be legitimate, on something that might not actually be related).I don't endorse or necessarily agree with the views expressed on these two websites, but they might help give some background:https://organizing.work/2019/04/why-do-coops-hate-unions/http://web.archive.org/web/20210213141044/http://www.takebac...\n \nreply",
      "The public document - PLAN FOR THE ARTIST-OWNED INTERNET - zine subvert published yesterday has quite a bit of detail on the complexities and possibilities in the governance and benefit of the co-op. The team supposedly includes lawyers and economists.https://drive.google.com/file/d/1znV0Q8_jjxFTiKeT_FETTZieiXl...\n \nreply",
      "The FAQ has:> Is this a crypto thing?>> No.I realize that crypto is a bad word for some people, but I think that the above answer has a corollary:> Does it have a single point of control that will attract corruption if enough of us start using it?>> YesCertainly plenty of poorly designed crypto things also have that point of control, but a well designed crypto thing at least has a shot at resilience.\n \nreply",
      "> Does it have a single point of control that will attract corruption if enough of us start using it?By opposition to crypto, which attracts distributed corruption when enough people use the project?I'm being glib, but complaining that a project not using crypto makes it inherently unsafe is pretty rich.\n \nreply",
      "It's not really a complaint about the project itself.  I'm actually considering paying the $100 to be a member because I think they're attempting to address an important problem and I want to see how it goes and it would probably be more fun to do so as an owner.But you've got to admit that its a peculiar rhetorical choice to explain at the landing page that your strategy doesn't involve coupling ownership/control of the platform with the ability to control tokens on a blockchain somewhere, without using the same space to explain what it does do instead.\n \nreply",
      "Smart contracts only affects on-chain stuff, and this deals with real-world things. No smart contract is going to help you if webmasters update website, or if a board decides to add a rule.See also: NFT delisting.\n \nreply",
      "When I imagine using crypto for this problem, the on-chain portion is a registry of hashes (for the songs) which maps them to addresses which I can send money to and then the contract multiplexes the money out to the appropriate people.  So if it's one dude with a guitar and a microphone, 100% goes to him. But if it's a remix of a remix of a remix, then maybe that money gets split 50 ways.I don't know if I need a website or a board for that.  Of course I'm not the one building this, so my imagined design doesn't matter.  But the question is: if not that, then what?  I'm curious, I'm just gonna sleep on it before I decide that I'm $100 curious.Edit: I see I can get the zine digitally first before deciding to be an owner. I guess I have some reading to do.\n \nreply",
      "But it possible to host a website almost on blockchain (see TON Sites as example) and withdraw certain modification permissions once the contract in launched.It seems to me that it is possible to implement fully decentralized bandcamp-like site.\n \nreply",
      "Musicians want to accept credit card payments (you lose a lot of potential sales if you only take crypto), which requires a central party to handle payment processing.\n \nreply"
    ],
    "link": "https://subvert.fm/",
    "first_paragraph": "\n\n      Bandcamp's corporate acquisitions threaten independent music. It's time\n      for a new model - one we collectively own and control.\n      \n      Our zine outlines how we'll turn our vision of a collectively owned\n      Bandcamp successor into a reality.\n      \n      Get your copy today to join us as a Founding Member.\n    \n        Physical zine\n        Free worldwide shipping [Supporters]\n        Founding Membership in the Subvert Co-op\n        Access to our members only forum\n        Co-ownership of Subvert\n        Unique member number\n        Membership certificate\n        Ability to influence platform policies and features\n      \n            Alex Durlak\n            Label Member\n            Id\u00e9e Fixe Records\n\n\n            Lindsey Mills\n            Artist Member\n            Surfer Blood\n\n\n            Charles Broskowski\n            Supporter Member\n            Founder, Are.na\n\n\n            Matt Werth\n            Label Member\n            RVNG Intl.\n\nBandcamp's\n        back-to-b",
    "summary": "**Subvert \u2013 Destroying the Indie Scene, One \"Co-op\" At A Time**\n\nSubvert.fm promises to save independent music artists from the cold clutches of corporate giants, by chaining them to a different, democratically-controlled anchor. Because nothing says \"musical freedom\" like a <em>membership certificate</em> and the thrilling bureaucratic drama of influencing platform policies alongside other bitter purists who also miss the old Bandcamp. Meanwhile, in the comments, armchair experts trade half-baked stories about co-op corruption with the zeal of conspiracy theorists claiming we'd all be saved by blockchain \u2013 if only we thought <i>really</i> hard about it. \ud83c\udf7f Grab your zine and your member number, and don't forget to witness the thrilling squabble over whether bad ideas sound better on forums or encrypted chat groups."
  },
  {
    "title": "Teaching old assert() new Tricks (ngs-lang.org)",
    "points": 4,
    "submitter": "todsacerdoti",
    "submit_time": "2024-10-16T08:24:07.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.ngs-lang.org/2024/10/06/teaching-old-assert-new-tricks/",
    "first_paragraph": "You deserve a better-than-telegraph shellMany languages have assert(). This post describes how assert() is implemented in Next Generation Shell and how these aspects/features are consistent with the rest of the language.Implied in the above is that the ideas below might not be a good fit for assert() in your favorite language or framework.Note that assert(EXPR, PATTERN) in NGS is same as EXPR.assert(PATTERN). It\u2019s called UFCS.Following the observation that in many cases predicate callbacks just compare the given value to a pattern, most if not all methods in NGS that were previously taking a predicate argument were upgraded to take a pattern argument instead. Following that development, assert(EXPR) got the assert(EXPR, PATTERN) flavor, which turned out to be used way more frequently than the \u201cregular\u201d assert(EXPR).Usage of patterns in assert() is completely aligned with the rest of the language, where many other methods support pattern arguments.Example 1:Example 2:assert(EXPR, PATTER",
    "summary": "In a groundbreaking move sure to revolutionize dozens of micro-basements worldwide, Next Generation Shell introduces bravery in coding: using patterns in assert(). Because regular assert statements are for plebs, NGS is here with assert(EXPR, PATTERN), turning your error checking fancier than a double espresso at a San Francisco start-up. Commenters, quickly pivoting from their usual rants about Emacs vs. Vim, dive into enlightening us about their personal adventures with assert statements, not realizing that the real assertion is assuming anyone else cares. \ud83d\ude44 Truly, NGS: making assert() not just a function, but a lifestyle statement."
  },
  {
    "title": "Why conventional wisdom on health care is wrong (a primer) (2020) (randomcriticalanalysis.com)",
    "points": 64,
    "submitter": "jeffreyrogers",
    "submit_time": "2024-10-17T15:57:54.000000Z",
    "num_comments": 91,
    "comments_url": "https://news.ycombinator.com/item?id=41870858",
    "comments": [
      "Well, that was interesting.What I'm unclear on is whether \"health spending\", in this analysis, is defined as money paid to care providers such as hospitals and dentists, or money paid by citizens for healthcare. Because you've got insurers and PBMs taking profit.The ratio of those two numbers is the efficiency of the American insurance model. How does it compare to the administration of a single-payer system such as the NHS?Until I see some data indicating otherwise, I'm going to look at my \u00a3200pcm national insurance and my \u00a39.90 prescriptions and my free ambulances, and Americans' $500pcm insurance and their unlimited prescription costs and their four-figure bills even when insured, and I'm going to continue to believe that Americans are punching themselves in the face.\n \nreply",
      "Anecdotally anyone can talk to a few doctors and find out just how much time they spend on updating charts/documenting information that's not directly relevant to the care, its just to protect against liability or work with insurance. Or how many hours they spend on phone calls fighting with insurance companies. The people who actually understand medicine wasting hours with some clueless rep with 0 understanding and a flowchart who's only job is to deny claims. Dr. Glaucomflecken on youtube has many videos about that too.So on the ground level, it's already clear some of our highest paid most valuable people spend 20-30% of their time on a flavor of administrative junk which isn't necessary in a single-payer system. I'm skeptical of claims that this waste doesn't translate into the higher level metrics.\n \nreply",
      "We spend even more money on healthcare administration than what\u2019s directly spent on it. HR departments screwing around with insurance. Various government benefits & other agencies having to mess with private health insurance issues. Attorneys general offices and state rep offices spending time to get insurers\u2019 and hospital billing departments\u2019 heads out of their asses (they do a lot of this).There are also untold hours lost in unpaid labor on the part of \u201cclients\u201d messing with insurance and hospital billing departments. It\u2019s not uncommon for someone who is, or is connected to a person who is, seriously sick for even a few days to spend a work-week or more of time that year messing with the billing from the incident. This can include uneventful pregnancies and births.\n \nreply",
      "> I'm unclear on is whether \"health spending\",They list their source as 2017 OECD data.  OECD seems to define this as:\"Health spending is the final consumption of health care goods and services including personal health care and collective services.\"Their charts are also drawn in a standard and more understandable way.[0]> Americans are punching themselves in the face.Hurtful,  but okay,  I do hope you realize it's the rampant monopolization of health care that is the problem in this country.  Yours solved it by simply creating a single publicly held monopoly.It's not as if either system is perfect and doesn't create it's own share and particular style of inhumane healthcare outcomes.  Prescription label prices are  noticeably different but are they meaningfully different where outcomes are concerned?[0]: https://www.oecd.org/en/data/indicators/health-spending.html\n \nreply",
      "NI does not pay for the NHS, it comes from general taxation, you\u2019re comparing the wrong figures.\n \nreply",
      "Well, there's also the rate of new drug and procedure discovery. I've heard it quipped that Americans are subsidizing the discovery of new medical techniques for the rest of the world. Whether that's worth a higher cost is arguable but I think the effect is there.\n \nreply",
      "When this gets brought up as a positive to our high healthcare spending (which you're not exactly doing, more just making note of the existence of the argument) it's such a head-scratcher for me.1) OK... maybe we should stop, then? Like, that seems like a terrible deal? How is that a justification at all? It seems like just a description of something very stupid we're doing.2) This would be a good deal if we were getting other countries to also pay high prices and bringing that money \"home\", but basically the exact opposite is happening. WTF.3) More often than not, the side of the issue that raises this as a good thing is also the side full of folks who think we should e.g. reduce spending on foreign aid, so it's especially weird that they're bringing it up.Plus, I'm very skeptical that the idea that drug development would dramatically slow down if the US stopped over-spending to the tune of 2x-100x on lots of drugs is even true. But setting that aside, it's still just a bizarre line of argument, to me.\n \nreply",
      "Not all subsidy is a bad thing. The money is used to fund real expertise and industrial capacity.\n \nreply",
      "> OK... maybe we should stop, then? Like, that seems like a terrible deal? How is that a justification at all? It seems like just a description of something very stupid we're doing.The US pays for drug development and then the rest of the world caps prices and gets the drugs cheaper. If the US stops then the money for drug development goes down, which is not great. What you really want is to get the other countries to pay their share, but how do you propose to do that?\n \nreply",
      "The reality is we have no evidence that other countries other countries wouldn't start developing drugs. Our fear that no one else would do is not grounded in the rational and we shouldn't let irrational fears decide what we do. This isn't something we can logic out ahead of time, we simply need to commit to not doing it.\n \nreply"
    ],
    "link": "https://randomcriticalanalysis.com/why-conventional-wisdom-on-health-care-is-wrong-a-primer/",
    "first_paragraph": "Critical analysis of topics that interest me.This is a short condensed summary of key facts and relevant evidence I have discovered in my extensive research on healthcare.\u00a0 Conventional wisdom on healthcare is far from the mark, both in general and specifically as pertains to the United States.\u00a0 I have written several lengthy blog posts over several years in which I have reviewed the evidence in painstaking detail. Still, many people lack the time, interest, or attention span to fully absorb, and much of the information necessary to understand the big picture is spread across several pages.\u00a0 \u00a0If you want to see further substantiation, please click through as I\u2019m mostly focused on covering the highlights in relatively broad strokes.Note (1): There are some new content and analysis on this page, but mostly I am aiming to summarize evidence I have already assembled and published on my blog.Note (2): I have made extensive use of log-log plots in this post wherein the value displayed on the",
    "summary": "In the groundbreaking expos\u00e9 \"Why Conventional Wisdom on Health Care is Wrong,\" a blogger with evidently limitless time and dedication to the *art* of rehashing healthcare data, takes us on a magical journey through what he describes as \"extensive research.\" Readers eager to nod along with graphs and debate the nuances of spending ratios find themselves in a thrilling echo chamber, questioning whether \"health spending\" means anything of actual substance or just another excuse for insurers to buy yachts. Commenters, with varying degrees of sarcasm and earnestness, juggle the glowing figures of the NHS against the dystopian American system, while debating high-minded concepts like whether subsidizing global medical innovation is America's charitable gift to the world, or just another way to overcharge for aspirin. Brace yourselves for the intellectual gymnastics \ud83e\udd38\u200d\u2640\ufe0f that promise to leave no misconception unturned, no chart unlogged, and no commenter unmocked."
  },
  {
    "title": "Distro (YC S24) is hiring a tech lead (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-10-18T17:01:16.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/distro/jobs/XGMjSPe-tech-lead",
    "first_paragraph": "",
    "summary": "Distro (YC S24), the latest sacrificial startup thrown into the Y Combinator volcano, proudly announces a job vacancy that everybody already forgot while clicking the \"apply\" button. In the comments, a gaggle of Hacker News aficionados engage in the sacred art of over-analyzing job descriptions and preaching the gospel of React and Kubernetes, because apparently those are mandatory buzzwords to include in any self-respecting tech sermon. The position itself promises the unique opportunity to become yet another cog in the ceaseless grind of Tech Lead machinery, with the added perk of endless pings from green founders. Join now, and witness first-hand how your soul becomes slowly intertwined with Slack notifications! \ud83d\ude80\ud83d\udcbb\ud83d\udd25"
  },
  {
    "title": "My solar-powered and self-hosted website (dri.es)",
    "points": 119,
    "submitter": "lightlyused",
    "submit_time": "2024-10-17T10:56:34.000000Z",
    "num_comments": 42,
    "comments_url": "https://news.ycombinator.com/item?id=41868353",
    "comments": [
      "Solar powered sites are cool and fun, but I find it ultimately lacking because so much of the rest of the networking infrastructure is reliant on the grid. It would be more energy efficient to just host the static site on cloudflare or whatever, and use the solar panel to charge some batteries, or something you would normally use the grid for. I suspect overall energy usage would be even lower if the site was hosted on a CDN, due to the CDN operators keeping their machines near full utilization, and fewer network hops required for an average request.\n \nreply",
      "You don't like that the rest of the infrastructure is on the grid, so you're proposing moving the site to the grid too?\n \nreply",
      "He's looking at it from a utilitarian point of view.From that perspective, you've got a finite amount of energy that's being consumed at any point of time and by that logic, you'd be better served to use the solar energy for something else in your household that would've needed the grid, because a cloud provider will almost certainly only need a fraction of the electricity you'd need to keep the static files served.So, by doing that you've effectively reduced your absolute energy footprint.But these projects aren't about reducing your energy footprint. They're about having a local webserver that's running on solar energy. Or in the authors words: At its heart, this project is about learning and contributing to a conversation on a greener, local-first future for the web\n \nreply",
      "Yeah, it\u2019s cool to just see what goes into it, and get a more intuitive understanding of the tradeoffs and what needs to be done to move more of this to green energy. It makes the (large) challenge of matching generation to consumption a whole lot more real. It\u2019s obviously not meant to be a maximized carbon reduction in any way commensurate to the effort.\n \nreply",
      "Yes it is not a contradiction here.Economies of scale make cloud more efficient for basic sites like this. It is like using street lights vs. every house putting out a few candles.In addition we love the grid! We want it to get greener. Put the effort into making the grid energy zero carbon, low pollution while still highly available.Furthermore using that solar to say charge your work laptop and then hosting the site in CF would net use less energy.\n \nreply",
      "the grid is going to continue to get efficiency and renewable energy improvements that help at a much more massive scale.it's 1000x more helpful to just like only run your big appliances when your grid is using more renewables.\n \nreply",
      "Have you heard of https://meshtastic.org/ ?\n \nreply",
      "Come check out my solar site. it's hosted in my basement and I paid a premium to my utility company so my power comes from solar credits (xcel)It's be neat to have a digital ocean or equivalent that allows locating in data centers by energy source.\n \nreply",
      "Agreed. When I see this type of thing, I am always turned off by people describing it as \"greener\" or \u201cmore sustainable\u201d. Every small website having its own solar panel and hardware is not greener. People frequently think only of the carbon emissions of the energy used by the hardware once it's running, ignoring the carbon (and raw material) cost of building that hardware.Serving websites is an area where capitalism\u2019s promise of achieving efficiency of resource utilization through economic incentives probably actually works, via shared hardware.This is a hobby and aesthetic thing, which is valid and interesting.If anyone has some good data about carbon emissions of self-hosted vs shared hardware I\u2019d love to see it.\n \nreply",
      "> People frequently think only of the carbon emissions of the energy used by the hardware once it's running, ignoring the carbon (and raw material) cost of building that hardware.The carbon is hard to account for in manufacturing.  Solar, for example, is pretty close to being produced entirely with electric consumption and very little required CO2 output (except perhaps in the transport of silicon and other raw materials).  The big energy draw for solar and battery is a kiln stage in both.  Solar has to melt down the silicon which requires a high temperature furnace and batteries are basically \"cooking\" the raw materials onto their foil.The math for solar is something like 1 to 4 years of generation before it pays back the manufacturing power debt.  Batteries tend to be much shorter as they take less energy in their manufacturing process (with some hopeful techniques in the future significantly reducing that number).Now, none of this is to contradict you, just putting the numbers out there.  I completely agree that a server farm is likely to be far more efficient for hosting a website than home built solar powered pi.  The CO2 emissions will be hard to beat, particularly if your cloud host resides in the PNW where power is nearly entirely renewable already.\n \nreply"
    ],
    "link": "https://dri.es/my-solar-powered-and-self-hosted-website",
    "first_paragraph": "",
    "summary": "Welcome to yet another technophile's dream (or delusion), where \"saving the planet\" involves hosting a solitary website on a cluster of solar-powered Raspberry Pis rather than on the outsourced behemoth grids. Our brave blogger has put down the gauntlet, daring to defy the energy-hoarding corporate overlords by...still using parts produced by them. Meanwhile, the comment section blossoms with deep, profound, eco-efficient insights like suggesting that solar power should be \"better utilized\" for charging your work laptop (because that\u2019s what's really important, right?). Others swoon over a utopian future where every basement-powered website will save the world, overshadowed only by the classic debate of whether a single solar cell can really offset the apocalyptic manufacturing footprint of their tech gadgets. Get your popcorn ready\u2014the grid wars have begun! \ud83c\udf7f\ud83d\udca1\ud83c\udf0d"
  }
]