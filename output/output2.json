[
  {
    "title": "The secret inside One Million Checkboxes (eieio.games)",
    "points": 386,
    "submitter": "todsacerdoti",
    "submit_time": "2024-08-29T21:20:12",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=41395413",
    "comments": [
      "This part is so important:> The adults in my life were largely not mad at me. They asked me to knock it off, but also made me a t-shirt. I don\u2019t think I\u2019d be doing what I do now without the encouragement that I received then.Teena need a place to be moderately mischievous, with semi-real social outcomes, but also some boundaries and help to not take it too far.And adults who aren\u2019t authorities over them except insofar that they have cool talents the kids want to learn.\n \nreply",
      "author here :)this is my favorite story from running the site, and possibly the best story I've ever been a part of. I'm not a big crier but I have cried so many times thinking and trying to write about it over the past 2 months. And of course, the process of discovery (and going from panic to excitement) was pretty crazy too.One of my favorite things about this is that it validated one of the core beliefs I have when making these things - that you need constraints for the small group of people that are jerks, but that for the most part those constraints are fodder for the largely-good and very creative folks that play around on the internet.Happy to answer any questions folks have!\n \nreply",
      "I loved this. It's really.. endearing and adorable? And it's what I wish the internet was used for more.Thanks for the writeup!\n \nreply",
      "Fantastic telling of it in both text and video form. Great to celebrate these people doing the kinds of things we learned so much from! Thanks for sharing.\n \nreply",
      "Perhaps you have said this elsewhere, but why shut the site down?\n \nreply",
      "A few reasons!  * More than anything, I think it's good for things to end! I figured interest in the site would die off over time (and it started to), and I thought it was better to close things out providing a special experience for the people that used it than to keep it up to get a few more users\n  * Costs started adding up; donations stopped matching them. I coulda figured out how to lower my costs but I wasn't excited about it.\n  * While the site was up I felt an obligation to make sure someone hadn't found some trivial workaround to deface the thing and I didn't want to do that anymore.\n\nI'm very pro ephemeral stuff! So I feel good about the decision. But it's a good question.\n \nreply",
      "I enthusiastically agree - and really all that matters is that you feel good about it. As a software engineer who's built (and shut down) many projects, I have always been envious of art forms in which the artist gets to create a piece of work and then \"finish\" it. We are often at the mercy of perpetual maintenance.Well done and nice execution.\n \nreply",
      "Hi! I missed the site but I have enjoyed reading about it after. As much as I enjoyed the last article, I enjoyed this one even more.Thank you for both the site and the articles.\n \nreply",
      "Thanks for the great writeups, this one, the one about scaling, and your work in general. It's been very inspiring.Also, you're absolutely right about largely good and creative people.I built a OMCB clone because the concept possessed me; i threw it online, a day later, okay a couple of dicks, whatever. Holy shit someone put a huge Hokusai's The Great Wave in there! (My version uses a fixed width/height, a big scrollable canvas, so that was easy to spot)Seeing that felt so good, so joyful :)\n \nreply",
      "This sounds like a perfect recipe for an episode of the Corecursive Podcast!\n \nreply"
    ],
    "link": "https://eieio.games/essays/the-secret-in-one-million-checkboxes/",
    "first_paragraph": "A few days into making One Million Checkboxes I thought I\u2019d been hacked. What was that doing in my database?A few hours later I was tearing up, proud of some brilliant teens.But let\u2019s back up.Note - I\u2019m trying something new; I\u2019ve also made a YouTube video that tells this story. I\u2019m trying to decide whether I\u2019m interested in making videos; check it out if you\u2019d like!\nOn June 26th 2024, I launched a website called One Million Checkboxes (OMCB). It had one million global checkboxes on it - checking (or unchecking) a box changed it for everyone on the site, instantly. The site, 30 minutes after launch My expectations for the site were very low and very wrong. I thought hundreds of players would check thousands of boxes - instead, 500,000 players checked over 650,000,000 boxes in the two weeks that I kept the site online. The site made it into the New York Times and the Washington Post; it\u2019s on Know Your Meme and Wikipedia. The whole thing was a wild ride.A separate blog of mine covers the ",
    "summary": "**One Million Tears For One Million Checkboxes**\n\nIn a baffling attempt to hop onto the experiential digital bandwagon, some guy launches \"One Million Checkboxes,\" a website that literally lets you check boxes. Half a million bored souls eagerly commit to altering a bit's fate, with over 650 million boxes checked, proving people really will click on anything. The creator puzzles through a rollercoaster of irrelevant emotions, gets misty over \"brilliant teens\" playing checkbox messiah, and shuts down his creation ceremonially\u2014because apparently, digital checkboxes also need a Viking funeral. Commenters, equally sniffly, reminisce about their lost checkbox paradise, with the most articulate echoing somber eulogies for a website that, let's be honest, even your cat wouldn't miss. \ud83d\udce6\u2705\ud83d\ude3f"
  },
  {
    "title": "Bypassing airport security via SQL injection (ian.sh)",
    "points": 1139,
    "submitter": "iancarroll",
    "submit_time": "2024-08-29T15:53:08",
    "num_comments": 268,
    "comments_url": "https://news.ycombinator.com/item?id=41392128",
    "comments": [
      "Hilarious that the entire TSA system is vulnerable to the most basic web programming error that you generally learn to avoid 10 minutes into reading about web programming- and that every decent quality web framework automatically prevents.It is really telling that they try to cover up and deny instead of fix it, but not surprising. That is a natural consequence of authoritarian thinking, which is the entire premise and culture of the TSA. Any institution that covers up and ignores existential risks instead of confronting them head on will eventually implode by consequences of its own negligence- which hopefully will happen to the TSA.\n \nreply",
      "> Hilarious that the entire TSA system is vulnerable to the most basic web programming error that you generally learn to avoid 10 minutesThe article mentions that FlyCASS seems to be run by one person. This isn't a matter of technical chops, this is a matter of someone who is good at navigating bureaucracy convincing the powers that be that they should have a special hook into the system.What should really be investigated is who on the government side approved and vetted the initial FlyCASS proposal and subsequent development? And why, as something with a special hook into airline security infrastructure, was it never security audited?\n \nreply",
      "Based on the language on their site about requiring an existing CASS subscription, my guess is there was no approval at all. It appears this person has knowledge of the CASS/KCM systems and APIs, and built a web interface for them that uses the airline's credentials to access the central system. My speculation is that ARINC doesn't restrict access by network/IP, so they wouldn't directly know this tool even exists.Some quick googling shows the FlyCASS author used to work for a small airline, so this may piggyback off of his prior experience working with these systems for that job. He just turned it into a separate product and started selling it.The biggest failure here is with ARINC for not properly securing such a critical system for flight safety.\n \nreply",
      "This right here people need to pay attention to gut the following reason:One person can make a lot of impactThe most common thing I hear people say with respect to their jobs is: \u201cI\u2019m just one person, I can\u2019t actually do anything to make things better/worse\u2026\u201dBut it\u2019s just wrong and there\u2019s thousands of examples of exactly that over and over and overIn this case, if this is true, it\u2019s both amazing that:One person, or a small number of people, could build something into the critical path as a sidecar and have it work for a long time andAnd second, the consequences of \u201chero\u201d systems that are not architecturally sound, prove that observability has to cover all possible couplings\n \nreply",
      "Oh, everyone knows that one single person can make things a lot worse. That's all that's happening here. That doesn't say anything about how much one single person can make things better. In the former case, your powers are amplified by the incompetence of everyone else involved; in the latter case, they are diminished.\n \nreply",
      "Better / worse for whom?Given the nature of these systems, this 1 person likely made the day to day lives of a lot of people better, providing an (arguably) snappier web interface to existing systems.Granted, they've probably made someone's day a lot worse with this discovery, but..\n \nreply",
      "This is exactly itIt was done for a reason and the fact that it persists despite all odds, means it\u2019s doing something useful\n \nreply",
      "Yeah but this is not very actionable. It is like saying that one person can win the lottery.You have to be in the right place at the right time.\n \nreply",
      "If this were the case, then it seems quite plausible that the website itself was just a passthrough, and the APIs provided by ARINC would be exposed.THis then begs the question of how ARINC passed security audit.\n \nreply",
      "Someting I\u2019ve been thinking about, esp since that crowdstrike debacle. Why do major distributors of infrastructure (msft in case of crowdstrike, DHS/TSA here) not require that vendors with privileged software access have passed some sort of software distribution/security audit? If FlyCASS had been required to undergo basic security testing, this (specific) issue would not exist\n \nreply"
    ],
    "link": "https://ian.sh/tsa",
    "first_paragraph": "08/29/2024Like many, Sam Curry and I spend a lot of time waiting in airport security lines. If you do this enough, you might sometimes see a special lane at airport security called Known Crewmember (KCM). KCM is a TSA program that allows pilots and flight attendants to bypass security screening, even when flying on domestic personal trips.The KCM process is fairly simple: the employee uses the dedicated lane and presents their KCM barcode or provides the TSA agent their employee number and airline. Various forms of ID need to be presented while the TSA agent\u2019s laptop verifies the employment status with the airline. If successful, the employee can access the sterile area without any screening at all. A similar system also exists for cockpit access, called the Cockpit Access Security System (CASS). Most aircraft have at least one jumpseat inside the cockpit sitting behind the flying pilots. When pilots need to commute or travel, it is not always possible for them to occupy a revenue seat",
    "summary": "**Bypassing airport security via SQL injection (ian.sh)**\n\nToday in \"How is this still a thing?\", we learn that a crucial part of airport security can be duped by SQL injection, a bug so ancient it's practically got its own exhibit in the cybersecurity museum. Commenters are split between marveling at this Jurassic tech blunder and philosophizing about the metaphysical significance of one under-caffeinated coder effortlessly bypassing what millions of dollars of TSA funding couldn't fortify. Meanwhile, somewhere an intern gets a tragic flashback to their first \"Hello, World!\" program, thinking, \"Wait, didn\u2019t I fix that bug?\" \ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "Anthropic's Prompt Engineering Interactive Tutorial (github.com/anthropics)",
    "points": 86,
    "submitter": "sebg",
    "submit_time": "2024-08-29T22:21:55",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=41395921",
    "comments": [
      "Bits of this tutorial feel a little bit out-dated. The appendix on function calling for example - https://github.com/anthropics/courses/blob/master/prompt_eng... - shows how to implement function calling from scratch using XML-style tags - but Claude's API has had JSON schema-based function calling for a few months now: https://docs.anthropic.com/en/docs/build-with-claude/tool-us...Oh, never mind - they have an entire separate course about tool use via the API here: https://github.com/anthropics/courses/blob/master/tool_use/0...So they're using tools in that appendix purely to demonstrate how sophisticated you can get with raw prompting.\n \nreply",
      "The documentation still advocates using XML tags as a subset of prompt engineering despite the push for JSON-based structured I/O, confusingly: https://docs.anthropic.com/en/docs/build-with-claude/prompt-...\n \nreply",
      "If you\u2019re not a total beginner the last two chapters and appendixes might be the only ones worth reading.> Chapter 8: Avoiding Hallucinations...Give Claude the option to say it doesn't know the answer to a question ...Ask Claude to find evidence before answering...it's best practice to have the question at the bottom after any text or document...Bonus lesson: Sometimes, Claude's hallucinations can be solved by lowering the temperatureChapter 9 says the order of prompt elements should be: task, tone, examples, input data, precognition, output formatting, prefill. Where prefill simply means you can give a prefix that the answer starts with by providing a partially completed assistant message, but tbh I haven\u2019t ever reached for that in my use cases.Appendixes include some examples of function calling and RAG.IMO the official Anthropic docs are a more accessible starting point and they contain essentially the same info.\n \nreply",
      "I talk to ai like a caveman mostly. Instead of over optimizing my prompt I just try to find the minimal amount of representation to get the llm to understand my problem and solve it for me and I have been very productive with this strategy. What would someone like me get out of prompt engineering? Or is it more for things like agents, bots, and assistants?\n \nreply",
      "I'm starting to use LLMs more and more for technical/programming questions, but my success rate so far is only about 50/50. I haven't tried a lot of them, but so far Copilot is my least favorite. When I ask it a technical question, it seems to give me answers to a question /sort of like/ but not exactly like what I was asking. Essentially treating every prompt like an X/Y problem. Which is much more frustrating than just telling me my question is rare enough that it doesn't know the answer.\n \nreply",
      "If you're trying to ship something to production that has consistent behavior within bounds and handles edge cases you'll need to do quite a bit of work. For ChatGPT use your strategy works fine\n \nreply",
      "I'd guess the latter.My SO wanted Google Assistant at home after gotten used to it in our Android Automotive-based car. So I've been dabbling with local LLMs, as a learning experience.I got one prompt which classifies the question, ie asking for weather, math question or knowledge etc. There I ask it to only output the category, so I can easily do different things based on that.For knowledge-based stuff I include our town and country, tell it to use metric units and be brief. I tell it to ask clarifying questions if needed. If I don't it'll use miles, or both miles and km, and be too long-winded and assumes too much.For calculations, I've been asking it to output Octave code that computes the answer, giving the result in a specific variable name, and without explanation. If it can't then output a special sequence. Without it'll include explanations of what the code does and not be consistent with variable naming.Been using Gemma 9B so far, which performs well on my aging 2080Ti, and haven't actually put all the pieces together yet (my SO asked last weekend). But seems very promising, and adding the extra instructions for each task radically changes the output and makes this approach viable.Btw, I know there are probably tons of these assistants out there. I just enjoy figuring out how things work.\n \nreply",
      "I imagine you might do well to write your prompt with similar language (diction, sentence construction, etc.) to what you'd find in the output/answer you are attempting to evoke from the LLM. These are prediction machines, after all.\n \nreply",
      "I do similar to you. I was commenting in another thread of similar:From the visual codeViz thread\n---https://news.ycombinator.com/item?id=41393458...I've been wanting to have a GPT directly inside Blender to Talk Geometry Nodes - because I want to tie geometry nodes to external data to external data which runs as python inside blender that draws the object geometry that suitabley shows/diagrams out the nodes of my game I am slowly piecing together 'The Oligarchs' which is an updated Illuminati style game - but with updates using AI to creat nodes directly from Oligarch IRL files, such as their SEC Filings, Panama Papers, and all the tools on HN are suited to creating.\nI went to school for Softimage & Alias|WAVEFRONT (which became MAYA) Animation in 1995 :-)So I like your DNA.I want to unpack the relationships of the Oligarch, programmatically, with hexagonal nodes, similar to this[0]- but driven by Node-based-python-blocks-GraphQL-hierachy. And I am slowly learning how to get GPTBots to spit out the appropriate Elements for me to get there.[0] - https://www.youtube.com/watch?v=vSr6yUBs8tY(ive posted a bunch of disjointed information on this on HN - more specifically about how to compartmentalize GPT responses and code and how to drive them to write code using Style-Guide, and gather data using structures rules for how the outputs need to be presented..)EDIT:I throw commands at lit like this, where I tell it to \"give me a ps1 that sets a fastAPI directory structure, creates the venv, touches the correct files give me a readme and follow the best practice for fastAPI from [this github repo from netflix]And it gave me that script...Then, here is the following when I want to document it. Then, Ill take that script and tell it to give me a webUI to run it and invoke it and add logging and dashboards.I do this to practice making tooling logic doo-dads on the fly, and then iterate through them.https://i.imgur.com/7YOjJf8.pnghttps://i.imgur.com/KecrvfZ.pnghttps://i.imgur.com/tKYsmb9.pnghttps://i.imgur.com/nCGOfSU.pnghttps://i.imgur.com/ayDrXZA.pngEtc -- I always make it diagram. Now I can throw a bunch of blocks in a directory and tell it to grab the components from the directory and build [THIS INTENT].app for my.\n \nreply",
      "Anyone else find it funny that the entire concept of prompt engineering is probably very close to good management and communication principles?It's kinda hilarious when you think about it.My mental model for LLM chatbots is to treat them like a junior intern that has access to google. Sure they can get things right but realistically I have to check their work to prevent any show stopping issues.\n \nreply"
    ],
    "link": "https://github.com/anthropics/courses/tree/master/prompt_engineering_interactive_tutorial",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          ",
    "summary": "**Anthropic Attempts Tutorial Triumph, Teaches Tired Tech**\n\nAnthropic swings again, producing a GitHub repository aiming to sculpt the sloppy art of \"prompt engineering\" into a discernible form. Unfortunately, it's like watching a toddler perform surgery with a crayon. The comment section becomes a tragicomic scene where enthusiasts debate obsolete instructions on XML like medieval scholars discussing the importance of leeches in brain surgery. Meanwhile, one brave soul admits to conversing with AI as if it's a caveman, likely achieving better results than those drowning in documentation designed to turn simple tasks into labyrinthine puzzles. Good luck, prompt engineers, you'll need it!"
  },
  {
    "title": "Google Closure Library has been archived (github.com/google)",
    "points": 63,
    "submitter": "benatkin",
    "submit_time": "2024-08-29T22:22:12",
    "num_comments": 28,
    "comments_url": "https://news.ycombinator.com/item?id=41395925",
    "comments": [
      "Note that this doesn't include the Closure Compiler[0], and \ndoesn't affect Clojurescript[1], which has traditionally used the compiler and related bits of the Google Closure library.[0] https://github.com/google/closure-library/issues/1214[1] https://clojurescript.org/news/2024-01-24-release\n \nreply",
      "it does affect clojurescript, if they don\u2019t have a plan to remove it they will get in big trouble later\n \nreply",
      "Can you clarify what you mean for someone unfamiliar? If Closure Compiler is sticking around (and the linked issue is very clear that it will), what is it that ClojureScript needs to have a plan to remove?\n \nreply",
      "Do you really believe  that google discontinuing most of a project but one part really means that they will support that part for a long time? If so I have a bridge to sell you\n \nreply",
      "So this is just the usual \"Google kills projects lol\" comment and not something based in any information about Closure Compiler specifically. Understood.\n \nreply",
      "All of Google's JavaScript and TypeScript is compiled with Closure compiler. It's not going anywhere.You're only relating the two because they share a name from being kind of sort of part of the same project 20 years ago. Closure library hasn't been used internally for a new code for a very long time.\n \nreply",
      "I have a causeway (going with something different for variety) to sell you because you don\u2019t take into account that the ClojureScript project can just fork because they have sufficient access to expertise (most projects don\u2019t, but they do, partly because they\u2019re big enough and partly because they\u2019re very strong with computer science). You raise a good point but don\u2019t allow for solutions to be presented and assume it\u2019s a distaster scenario.\n \nreply",
      "Closure is two separate projects, the Compiler and the Library, and this is the Library that's being archived. Might be good to note that in title, if @dang or another mod is around.\n \nreply",
      "I\u2019m here and I made the update. I think it will have an effect on both and I considered putting library in the title but decided to have a shorter title, however by popular demand I changed it.Edit: looked at the closure compiler repo and maybe it\u2019s good, though I always thought of it as partly depending on the library\u2026Edit 2: This explains the situation. Sounds like Closure Compiler has a bright future! I could see ClojureScript eventually taking over maintenance though. https://news.ycombinator.com/item?id=41396522\n \nreply",
      "Notably not the Closure Compilerhttps://github.com/google/closure-compiler\n \nreply"
    ],
    "link": "https://github.com/google/closure-library",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Google's common JavaScript library\n      Closure Library has been archived. We no longer see it as meeting the\nneeds of modern JavaScript development, and we recommend that users look\nfor alternative solutions.Please see #1214 for\nmore details.Previous version of this README can be found here.\n        Google's common JavaScript library\n      ",
    "summary": "**Google Closure Library R.I.P. (Not Really Though!)**\n\nIn a shocking display of routine housekeeping masquerading as innovation, Google has finally thrown the Closure Library into the dusty archives of GitHub, essentially telling developers to \"go find someone else's code to play with.\" \ud83d\udce6\ud83d\uddd1\ufe0f As expected, the comment section instantly transformed into a delightful circus of confusion and outrage. Clever souls rushed to remind others that, no, this doesn't spell doom for the Closure Compiler\u2014because obviously keeping one leg of a chair while tossing the other ensures perfect stability, right? Meanwhile, a wise bridge seller capitalizes on the chaos, suggesting that this is merely a prelude to Google's traditional hobby of killing beloved projects. Can't wait for the next thrilling episode of *What Will Google Cancel Next?* \ud83c\udfad\ud83c\udf7f"
  },
  {
    "title": "Elasticsearch is open source, again (elastic.co)",
    "points": 289,
    "submitter": "dakrone",
    "submit_time": "2024-08-29T20:10:02",
    "num_comments": 168,
    "comments_url": "https://news.ycombinator.com/item?id=41394797",
    "comments": [
      "> The good news is that while it was painful, it worked. 3 years later, Amazon is fully invested in their fork, the market confusion has been (mostly) resolved, and our partnership with AWS is stronger than ever. We were even named AWS partner of the year.I don't entirely understand this bit.\n \nreply",
      "It was so that AWS would create their own name for their fork:> we changed the license, knowing it would result in a fork of Elasticsearch with a different name and a different trajectory. It\u2019s a long story.I think the name of the fork is now OpenSearch.\n \nreply",
      "Am I the only one not buying this reasoning? Seems like there's more than is being said, otherwise they would have said this by now. I'd reckon that ELv2 had friction that couldn't be easily overcome without OSS or at the very least DOSP [0]. I personally experienced said friction with ELv2, so makes me curious.[0]: https://opensource.org/dosp\n \nreply",
      "Sounds to me like they're trying to cover up a bad case of regret. At our company we've fully shifted to OpenSearch, so there's no going back to ElasticSearch even if we wanted (not that we would want to). Also there is a lot of engineering contribution from Amazon that's seemingly gone now from Elasticsearch, right?\n \nreply",
      "Sounds like you've fully shifted to exploiting (without contributing anything) to OpenSearch. I think Elastic will be just fine without your company or Amazon.\n \nreply",
      "They are paying Amazon for\nOpensearch resources. To Amazon, that\u2019s enough contribution to continue supporting the source code.\n \nreply",
      "How truly far we've fallen that people refer to using open source software\u2014 literally software given away for other people to use, exploitation.Gonna go tell those freeloading kids who take my candy on Halloween that they're exploiting me unless they contribute to next year's candy bowl.\n \nreply",
      "I'm not the GP, and I don't have a dog in this fight, but I think what they are complaining about is this sequence of events:  - ElasticSearch was open-source\n  - Amazon offered ElasticSearch open-source as a paid service\n  - ElasticSearch was not happy about this and changed their license\n  - Amazon forked ElasticSearch (the open-source version) and created OpenSearch based on that, continuing to serve OpenSearch\n  - (Few years pass)\n  - Amazon and ElasticSearch are now buddies\n\nI think GP is talking about the events that transpired a while back before Amazon and ElasticSearch made up.\n \nreply",
      "What, when Amazon forked an open source project, as was allowed by the license, and continued to support that open source license even when the original company abandoned theirs? I\u2019m not an Amazon fan, but they did the same thing (forking) that many others do, and they did it perfectly legally according to the license.\n \nreply",
      "Yes, doesnt Elasticsearch do the same thing with Lucene?https://stackoverflow.com/questions/27793721/what-is-the-dif...AWS just offered a convenience layer over ElasticSearch\n \nreply"
    ],
    "link": "https://www.elastic.co/blog/elasticsearch-is-open-source-again",
    "first_paragraph": "Build tailored experiences with Elastic.Scale your business with Elastic PartnersSearch and analytics, data ingestion, and visualization \u2013 all at your fingertips.By developers, for developersUnlock the power of real-time insights with Elastic on your preferred cloud provider.Prototype and integrate with LLMs faster using search AI.Discover a world of AI possibilities \u2014 built with the power of search.Protect, investigate, and respond to cyber threats with AI-driven security analytics.Unify app and infrastructure visibility to proactively resolve issues.See how customers search, solve, and succeed \u2014 all on one Search AI Platform.Exceed customer expectations and go to market faster.Cisco saves 5,000 support engineer hours per monthSitecore automates 96 percent of security workflows with ElasticComcast transforms customer experiences with Elastic ObservabilityStay at the forefront of innovation with technical tips from the experts.Code with other developers to create a better Elastic, toge",
    "summary": "Title: <em>Elasticsearch goes full circle and \"re-liberates\" itself?</em>\n\nGet excited, developers! Elasticsearch is \"open source\" again\u2014or at least that's what their shiny new marketing spiel on elastic.co claims. In a spectacular display of buzzword bingo, they promise everything from AI-driven unicorn dreams to a staggering overuse of the word \"elastic.\" Meanwhile, bewildered commenters try to wrap their heads around corporate chess games, sling around accusations of exploitation like drunkards at a dart board, and miss the point entirely\u2014because really, it's all about who gets the fanciest badge from AWS. \ud83c\udfaf\u265f\ufe0f\ud83e\udd39\u200d\u2642\ufe0f Blind loyalty in technology: causing amnesia and d\u00e9j\u00e0 vu since the inception of open source."
  },
  {
    "title": "Imbue (Formerly Generally Intelligent) (YC S17) Is Hiring a Research Scientist",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-30T01:00:26",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "item?id=41396868",
    "first_paragraph": "",
    "summary": "At Imbue (ne\u00e9 Generally Intelligent, as if a rebrand could scrub clean the stink of past failures), the startup wizards are on a quest for a *Research Scientist* \ud83e\uddd9\u200d\u2642\ufe0f\u2728 who can miraculously convert venture capital into \"groundbreaking\" AI research. In the comments section, the usual suspects\u2014self-proclaimed tech prophets and garage-dwelling \"founders\"\u2014vie for the top prize in the Hyperbole Olympics, each dismissing years of academic study for the chance to be overlooked by a bot recruiter. This job post proves that in Silicon Valley, optimism is undying, and memory is, conveniently, very short."
  },
  {
    "title": "Visit Bletchley Park (bletchleypark.org.uk)",
    "points": 22,
    "submitter": "bookofjoe",
    "submit_time": "2024-08-29T23:46:25",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41396501",
    "comments": [
      "If you're interested in historical buildings, go ahead, visit Bletchley Park. But expect most of the exhibits to be buildings, living quarters, offices etc. and mundane descriptions of day-to-day life there, not exhibits about the technical/cryptological aspects (although they do have some of those too).If you're interested in computer history and Bombe/Colossus, visit the much lesser known National Museum of Computing at the same site. https://www.tnmoc.org/I spent way too much time at the the boring parts of Bletchley Park and as a result didn't have enough time for the Museum of Computing.I'd recommend starting at the Museum of Computing, then if you have time left, the 1-2 buildings inside the official Bletchley Park museum that have the crypto exhibits.\n \nreply",
      "I visited both, on a whim, while I was by accident in the area, prodded by a comment just like this. It was a fantastic experice! And I did enjoy the National Museum of Computing more! There was an excellent elderly guide there that seemed to have been in computing since forever and told us stories of when he worked on machines just like the ones in the exhibition. A small bunch of us nerdy guys hung around him and it was great interacting with them and the tour guide. It was really really an excelent experice!If I'm ever in the area again, I would love to revisit and I would love to have my young son with me as well.\n \nreply",
      "Note also that the two museums have different schedules, so you'll want to make sure you go when both are open.\n \nreply",
      "This caught me out when I was a teenager. Luckily, there was a chap in the Museum of Computing who was spending his Sunday working on the Colossus, and he was happy to let me in and show me around! I'll never forget that kindness, it was truly a fascinating trip.\n \nreply",
      "I found chatting to the people working on stuff was the best bit.\nAlmost the level of ignoring the exhibits and look at the bits of stuff being worked on around.\n \nreply",
      "The people that work at the Museum of Computing are truly amazing! I had such a blast there!\n \nreply",
      "Bletchley Park is excellent - I went in the early 2000s when it was just some huts and a country house. I return in 2024 and they now have full exhibitions, including one that goes through the full history and workings of the Bombe. I thought I was a bit of an Enigma nerd but it turns out I hadn't heard of Lorenz/Tunny at all and so it really added another layer to my knowledge of the work at Bletchley.Also visiting Bletchley and then watching the Imitation Game makes it seem like the rushed medical drama from Mitchell and Webb [0][0] https://www.youtube.com/watch?v=C_AmdvxbPT8\n \nreply"
    ],
    "link": "https://bletchleypark.org.uk/",
    "first_paragraph": "",
    "summary": "<h2>Welcome to Bletchley Park: Where Even History Gets Bored</h2>\n\nOh, joy! If you\u2019ve ever wanted to meticulously explore every mundane corner of a historical site while zealously ignoring the groundbreaking cryptographic work that actually happened there, Bletchley Park is the place to be. Surprisingly, commenters on the site suggest skipping this snoozefest in favor of the National Museum of Computing, because nothing complements a historic site like telling visitors to go somewhere else. As ecstatic nerds swap long-winded stories with grandpa-grade guides, remember that getting locked out because you didn\u2019t check the opening times is almost as fun as watching paint dry on Enigma machines. Plan your weekend accordingly if you crave *excitement* - by hoping you encounter that one lovely person working on their day off, so you can actually see something cool behind-the-scenes instead of just the regular old exhibits. \ud83d\ude44"
  },
  {
    "title": "SDL3 new GPU API merged (github.com/libsdl-org)",
    "points": 45,
    "submitter": "caspar",
    "submit_time": "2024-08-29T23:04:32",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=41396260",
    "comments": [
      "SDL3 is still in preview, but the new GPU API is now merged into the main branch while SDL3 maintainers apply some final tweaks.As far as I understand: the new GPU API is notable because it should allow writing graphics code & shaders once and have it all work cross-platform (including on consoles) with minimal hassle - and previously that required Unity or Unreal, or your own custom solution.WebGPU/WGSL is a similar \"cross-platform graphics stack\" effort but as far as I know nobody has written console backends for it. (Meanwhile the SDL3 GPU API currently doesn't seem to support WebGPU as a backend.)\n \nreply",
      "Why is SDL API needed vs gfx-rs / wgpu though? I.e. was there a need to make yet another one?\n \nreply",
      "The more the merrier if you ask me. Eventually one will win but we need more experimentation in this space. The existing GPU APIs are too hard to use and/or vendor-specific.\n \nreply",
      "Writing bits of Vulkan or D3D12 really isn't that bad if you're working within an engine which does most of the setup for you, which is nearly always the case for practical work. If you're doing everything yourself from scratch, you're probably either a hobbyist tinkering or a well-compensated expert working for a AAA game developer.\n \nreply",
      "Having a C API like that is always nice. I don't wanna fight Rust.\n \nreply",
      "WebGPU has a (mostly) standardized C API: https://github.com/webgpu-native/webgpu-headers\n \nreply",
      "More context here: https://icculus.org/finger/flibitijibibo?date=2024-06-15&tim...\n \nreply",
      "This article from the main author of the API describes how the buffer cycling works: https://moonside.games/posts/sdl-gpu-concepts-cycling/\n \nreply",
      "Are there any examples?\n \nreply",
      "Examples will be based on this repo, I believe: https://github.com/TheSpydog/SDL_gpu_examples\n \nreply"
    ],
    "link": "https://github.com/libsdl-org/SDL/pull/9312",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \nHave a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.\n  By clicking \u201cSign up for GitHub\u201d, you agree to our terms of service and\n  privacy statement. We\u2019ll occasionally send you account related emails.\n    Already on GitHub?\n    Sign in\n    to your account\n  With 3.0 quickly on the horizon, we've decided to fast-track this so we can get more eyeballs sooner:A close relative to the FNA project is the MoonWorks project, which intends to be more of a successor to XNA rather than a reimplementation like FNA. The graphics component is called Refresh, which is like FNA3D but targets modern APIs like Vulkan. With SDL_gpu still in the early stages, and with Refresh in the process of revamping its API for a 2.0 release anyway, we've decided to just turn Refresh into a pos",
    "summary": "In a stunning turn of inefficiency that could only be orchestrated by the brightest minds in graphics programming, the SDL team courageously merges yet another GPU API into the heated soup of existing solutions. Because, <i>obviously</i>, what the world lacks is <em>another</em> way to write shaders that say \"Hello, World!\" across fifteen platforms. The GitHub warriors dive in with their usual sagacity, querying the reincarnation of wheel-design and joyously conjecturing which API will reign supreme in this crucial battle of obscurity. Meanwhile, somewhere a hobbyist tweets #StillNotUsingRust and dreams of an easy life, free from the tyranny of graphics APIs."
  },
  {
    "title": "JPMorgan's Python training for business analysts and traders (github.com/jpmorganchase)",
    "points": 42,
    "submitter": "sebg",
    "submit_time": "2024-08-29T22:18:39",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=41395884",
    "comments": [
      "I prefer the article title that my eye's read when I first opened the page:\"Monty Python's training for business analysts and traders\"I'm sure that that was required reading at the Crimson Permanent Assurance corporation.\n \nreply",
      "If this is what they are using as \u201ctraining\u201d, no wonder my portfolio with them doesn\u2019t perform. Rookies\n \nreply",
      "Knowing life, this is not the actual \"training\" but just a course that was allowed to squeeze through multiple layers of reviews and approvals and was deemed benign enough and useless enough that it can be disclosed. Not that the actual training is any better.. hell, our company recently made a big show of \"giving\" us access to a training platform, which, among other things, had basically a pdf book, but restricted with ridiculous controls. I sincerely doubt anyone learns that way... sadly, I am not helping, because now to show I am doing stuff, I up those stats artificially. I suck.\n \nreply",
      "Former JPM quant here. This sounds right. Maybe this is something an intern was paid to write.It says training for \"business analysts\" (I think fresh college grads?) and \"traders,\" not quant researchers.\n \nreply",
      "A quick review of the notebooks shows its more just a recipe cookbook for some basic tasks.. I don't really see how this is \"training\".\n \nreply",
      "\"This course is designed to be an introduction to numerical computing and data visualization in Python. It is not designed to be a complete course in Computer Science or programming, but rather a motivational demonstration of how relatively complex topics can be accessible even to those without formal progamming backgrounds.This training is designed to be conducted in-person, led by J.P. Morgan technologists and traders.\"\n \nreply",
      "A quick review of the lecture notes shows its more just a recipe cookbook for some basic tasks...I don't really see how this is a university course.\n \nreply",
      "Some comments in here already touching on this being dated.Any suggested resources that are up-to-date? Paid or free.\n \nreply",
      "Does JPM have a global python object system like GS?\n \nreply",
      "It\u2019s quite out of date isn\u2019t it.\n \nreply"
    ],
    "link": "https://github.com/jpmorganchase/python-training",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Python training for business analysts and traders\n      This Python training is for JPMorgan business analysts and traders, as well as select clients.\nThis course is designed to be an introduction to numerical computing and data visualization in Python. It is not designed to be a complete course in Computer Science or programming, but rather a motivational demonstration of how relatively complex topics can be accessible even to those without formal progamming backgrounds.This training is designed to be conducted in-person, led by J.P. Morgan technologists and traders. For interested institutional clients, please contact your J.P. Morgan team.This repository relies on the Binder project, which is generously supported by Google Cloud Platform, OVH, GESIS Notebooks and the Turing Institute.This software is licensed under the Apache 2 l",
    "summary": "**JPMorgan's Attempts at \"Python Training\": A Sitcom Script?**\n\nIn a stunning display of modern cluelessness, JPMorgan has decided to teach Python to their business analysts and traders, because clearly, handling economic crises wasn't enough of a challenge. The course content, generously described as an \"introduction\" rather than an actual education, is essentially a few Python print statements masquerading as financial insights. Comment sections quickly turned into a tragicomic theater with reminiscences of when \"Monty Python\" meant a fun movie night, not a crash course in how to mistakenly ruin your finance career with misplaced code. Former employees and bitter traders mingle in the chaos, sharing tales of similar corporate \"educational\" farces, while collectively mourning their portfolios. Welcome to the blind leading the blind in the digital age where JPMorgan not only manages your money but also mismanages programming pedagogy."
  },
  {
    "title": "Chrome is entrenching third-party cookies that will mislead users (brave.com)",
    "points": 282,
    "submitter": "NayamAmarshe",
    "submit_time": "2024-08-29T14:53:15",
    "num_comments": 131,
    "comments_url": "https://news.ycombinator.com/item?id=41391412",
    "comments": [
      "Have been using Firefox for a long time, no issues, though long ago when I had little memory, Chrome was using less of it. Firefox also has HTTPS-only mode, encrypted DNS without fallbacks, supports SOCKS and Encrypted Client Hello (although almost no website support it). However, it is better to just buy more memory (unless you are lucky to use Apple products).Regarding analytics, I believe browsers should take user's side and do not cooperate with marketing companies; even better, they should implement measures to make user tracking and fingerprinting more difficult. There is no need to track user's browsing history; just make a product better than competitors (so that it gets first place in reviews and comparisons) and buy ads from influencers.It would be great if browsers made fingerprinting more difficult, i.e.: not allowed to read canvas data, not allowed to read GPU name, enumerate audio cards, probe for installed extensions etc. Every new web API should guarantee that it doesn't provide more fingerprinting data or hides the data behind a permission.Regarding 3rd party cookies: instead of shady lists like RWS browsers should just add a button that allows 3rd party cookies as an exception on a legacy website relying on them (which is probably not very secure). Although, there is a risk that newspaper websites, blog websites and question-answers websites will force users to press the button to see the content.\n \nreply",
      "> Every new web API should guarantee that it doesn't provide more fingerprinting data or hides the data behind a permission.FWIW, it's practically impossible to provide that guarantee because the API necessarily provides at least the data point of, \"Did they select an option in the permission notification?\" (\"If yes, what option was selected?\" etc.)It's often said that the only solution to this is regulation and there seems to be a good case for that perspective.\n \nreply",
      "> FWIW, it's practically impossible to provide that guarantee because the API necessarily provides at least the data point of, \"Did they select an option in the permission notification?\" (\"If yes, what option was selected?\" etc.)Wrong. The status of permissions should not be visible to the page in most cases. Instead, fake data should be returned from them. That would be practical.\n \nreply",
      "It's always better to give no data (aside from leaving them with \"we couldn't collect that data\") than it is to give fake data because that fake data will be used against you just as often as real data would. Don't hand companies extra ammo to use against you, or think that you're safe just because they've written an incorrect assumption about you on the bullet. You're still going to be taking the hit.\n \nreply",
      "I've heard that fake data, like from AdNausium, just becomes noise as the advertisers know the patterns to filter them out.Assuming that's true, it seems to waste everyone's time and bits to fake it instead of just not answering or a minimal denial.\n \nreply",
      "> I've heard that fake data, like from AdNausium, just becomes noise as the advertisers know the patterns to filter them out.It's actually much worse. That fake data is dangerous because data brokers don't really care how accurate their data is. Even the fake data AdNausium stuffs into your dossier will be used against you eventually, just like the real data will be. If you get turned down for a job, or your health insurance rates go up, or you have to pay more for something than you would have otherwise, you won't even be told that it was because of data someone collected/sold/bought. You sure won't be told if it was fake or real data and you won't be given any opportunity to correct it.\n \nreply",
      "> If you get turned down for a job, or your health insurance rates go up, or you have to pay more for something than you would have otherwiseIt must suck to live in a capitalist dystopia. Dunno why Americans put up with it.\n \nreply",
      "It's the democracy. The big capital one./s\n \nreply",
      "> API necessarily provides at least the data point of, \"Did they select an option in the permission notification?\"If a bird app (or, heck, pancake recipe site) asked for WebRTC or GPU access I would be rightfully suspicious. It's a shame these things don't happen.\n \nreply",
      "One solution to this is to have the option to feed the application fake but plausible data. Android (or maybe some Android fork I was using) used to have this option for dealing with apps that insist on asking for location permission for no reason.\n \nreply"
    ],
    "link": "https://brave.com/blog/related-website-sets/",
    "first_paragraph": "\n                                              Privacy, extensions, and the best option for every platform.\n                                            \n                                              Crypto, NFTs, and all things blockchain. Learn the basics of Web3.\n                                            \n                                              Short, plain-language intros to common Internet and computer terms.\n                                            \n                                              See how Brave stacks up against other browsers.\n                                            \n                                              LLMs, machine learning, and the foundations of AI, for both users and devs.\n                                            \nPublished Aug 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis post presents research on the privacy harms and risks of Google\u2019s recent Related Website Sets feature, to be presented at the 2024 Internet Measurement Conference. The research finds ",
    "summary": "In the thrilling world of browsers battling *evil* cookies, Brave steps up with a groundbreaking piece, revealing Google Chrome's dastardly plan to fool us all with their \"Related Website Sets\". \ud83c\udf6a\ud83d\udc94 Commenters burst into a chaotically intellectual frenzy, debating the ethics of browser privacy like it's the fall of Rome. Some rally for fake data as a shield against corporate overlords, while others ruefully reminisce about Firefox's golden days. Meanwhile, practical advice floats around about living in a \"capitalist dystopia\", because apparently, having a choice in browser extensions is the hill we\u2019ll all die on. \ud83c\udfad\ud83d\udee1\ufe0f"
  },
  {
    "title": "I'm blocking connections from AWS to my on-prem services (m3047.net)",
    "points": 18,
    "submitter": "m3047",
    "submit_time": "2024-08-30T00:15:51",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=41396641",
    "comments": [
      "All the major cloud providers publish machine-readable lists of their ranges, e.g.https://docs.aws.amazon.com/vpc/latest/userguide/aws-ip-rang...https://www.microsoft.com/en-us/download/details.aspx?id=565...https://support.google.com/a/answer/10026322?product_name=Un...etc etc...\n \nreply",
      "I haven't checked today, but e.g. Amazon's JSON file belies the fact that they own 3.0.0.0/8 in practice if not in fact. I'm not going to play \"let's block Amazon's huge JSON list\". I've got 53 rules. I'm willing to step on some fingers  / toes. I can get to sites in AWS just fine.\n \nreply",
      "That's a shame, really. You are risking that none of your stuff will show up on search engines, not even Marginalia. You are risking that none of your stuff will be saved in the Wayback Machine. Maybe you want that, in which case you should block all the clouds and data centers, just to be sure. You might even be blocking your site from some small ISPs  based on where they run their CGNAT gateway (I doubt this, but it's possible).As far as I noticed, ping with a spoofed source address is the only actual abuse mentioned in the article. It should go without saying that you can't tell if a spoofed ping packet came from AWS, because the source address is the address the spoofer wants you to send a reply to, not the spoofer's address. And a much less invasive mitigation would be rate-limiting pings to, say, 10 per second.While the Internet is becoming balkanized this is mostly because of social media siloing itself to generate advertising and data revenue and to extract profit from AI training data (e.g. the Reddit/Google exclusivity deal) rather than because of providers blocking IP ranges.I certainly don't understand the rational mindset behind blocking certain providers over some pings and then complaining about IP connectivity becoming balkanized. The balkanization is caused by the ones doing the blocking.\n \nreply"
    ],
    "link": "http://consulting.m3047.net/dubai-letters/balkanized-internet.html",
    "first_paragraph": "I apologize for yet another digression.The direct result of bulletproof infrastructure / cloud providers which are \"too big to fail\" is the\n         balkanized internet. I don't think this has fully come to pass, but it might be starting\n         to happen or might come to pass soon.\n      Back in 1995 NSFNet\n         shut off their nationwide backbone (at least for public access). Around 2000 there was a great\n         dieoff, and since that time the dominant model has been one of advertising and unabashedly selling user behavioral data (surveillance capitalism); with the advent of AI that monetization has been extended to users' work product (their pictures, their posts, etc.) as well.\n      Prior to about 1989 there was no public access to the internet. Which is to say, there was one\n         large network which ran internet protocols, and it was private to government, military, and research /\n         educational institutions. The core of the backbone was administered by the\n      ",
    "summary": "**Title: Cyber-Hermit Blocks AWS, Internet Barely Notices**\n\nIn an *epic* display of digital futility, a solipsistic sysadmin has heroically decided to barricade their on-premises data forts from the marauding cloud hordes of AWS, likening their network isolation tactics to some grand geopolitical chess move. Commenters, in act of impressive mental gymnastics, navigate through the \"morass of lateral thinking,\" advising on the nuanced craft of cloud provider ostracism while casually ignoring the irony of discussing internet balkanization on a publicly accessible blog. One lone voice of reason suggests a simpler solution\u2014rate-limiting\u2014but is promptly drowned in a sea of IP range lists, proving that common sense is the rarest data packet on the cyber battlefield. \ud83d\ude02"
  },
  {
    "title": "Firewall rules: not as secure as you think (haskellforall.com)",
    "points": 33,
    "submitter": "jnord",
    "submit_time": "2024-08-29T22:56:27",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=41396206",
    "comments": [
      "This methods discussed in the article are how I have seen some hardware appliance vendors SSH into their devices despite the customer only allowing outbound connections to a cloud provider.  I would call these out in security reviews and it would get political really fast as the team buying the device wanted this feature but the compliance team wanted the flows documented that would conflict with security policies and ultimately would cause an audit failure.  It got even more interesting when one of the vendors was also a B2B customer.  A firewall vendor claimed they could block anything inside that outbound HTTPS connection that was not HTTP but they could not.  I will not be permitted to add specific details such as the appliance vendor or firewall vendor.I would wager some companies don't even know that a vendor can SSH into the customer datacenter not just the device despite only allowing outbound HTTPS flows to a cloud provider.\n \nreply",
      "I understand the technical issue but on a broader sense, the instant that a vendor supplied black box is installed behind your firewall and allowed to make any sort of communication towards a vendor controlled endpoint, doesn't it immediately technically allows full remote control?Lots of talk about tunneling and wrapping/disguising ssh but a vendor does not need any of that to control its machine.For example you could have the on-prem host poll a \"licensing\" or \"software update\" server that also happens to reply with ad-hoc commands to execute on demand. Could be straight up shell commands and the result can be sent back. No need for ssh, long lived connections, reverse tunnels or anything.The only way to mitigate this is to fully trust the vendor, have a strong legal framework to protect against wrongdoings or fully block all internet access to endpoints you don't fully control.\n \nreply",
      "Seriously, there were fights over that?In any remotely reasonable organization, that should be an instant, permanent blacklisting for the vendor, and termination of the \"we will escort you to clean out your desk right now\" variety for any internal employee who knew about it or enabled it. Probably with a line dropped to law enforcement.\n \nreply",
      "Which is why you need a modern firewall that MitMs both TLS and SSH. Not hard to do these days.\n \nreply",
      "I don't think a modern firewall can MiTM HTTPS TLS without triggering a \"Warning: Potential Security Risk Ahead\" (Firefox) or \"Your connection is not private\" (Chrome).Edit: typo\n \nreply",
      "I don't think _any_ firewall can MITM traffic without this happening unless you install the appropriate certificate in each client machine's trust store. I bet that with the advent of such \"all-in-one\" solutions as Fortinet appliances or Cisco's VPN applications that this would be handled automatically. If not I'm sure an endpoint management solution could be coaxed into doing this via some glue scripts. I haven't been an \"IT guy\" in a decade-plus but I'd be surprised if this wasn't within reach fairly easily these days.\n \nreply",
      "Pretty sure you still can, it just requires that the client system trusts the CA being used to sign the MITM certs. That obviously limits the cases where it works, but not to zero.\n \nreply",
      "That\u2019s been my experience. The difference being in a corporate environment they can push policies to all employee endpoints that make this happen with no scary warning (trust the internal CA, etc).\n \nreply",
      "Do they always work?  Can't they pin certs?\n \nreply",
      "They can pin certs, but at least you know that you can't see that traffic and make a policy decision about allowing it anyways or trying to force the vendor to drop it.\n \nreply"
    ],
    "link": "https://www.haskellforall.com/2024/08/firewall-rules-not-as-secure-as-you.html",
    "first_paragraph": "\nThis post introduces some tricks for jailbreaking hosts behind\n\u201csecure\u201d enterprise firewalls in order to enable arbitrary inbound and\noutbound requests over any protocol. You\u2019ll probably find the tricks\noutlined in the post useful if you need to deploy software in a hostile\nnetworking environment.The motivation for these tricks is that you might be a vendor that\nsells software that runs in a customer\u2019s datacenter (a.k.a. on-premises\nsoftware), so your software has to run inside of a restricted\nnetwork environment. You (the vendor) can ask the customer to open their\nfirewall for your software to communicate with the outside world\n(e.g.\u00a0your own datacenter or third party services), but customers will\nusually be reluctant to open their firewall more than necessary.For example, you might want to ssh into your host so\nthat you can service, maintain, or upgrade the host, but if you ask the\ncustomer to open their firewall to let you ssh in they\u2019ll\nusually push back on or outright reject the ",
    "summary": "Today on \"How to Circumvent Corporate Security with a Smirk,\" we stroll through yet another enlightening blog entry where hackers and corporate vendors unite in the joyous dance of bypassing firewalls. The post serves a delectable buffet of tricks for SSH-ing into, what are supposed to be, Fort Knox-level secure networks because, who cares about protocols when you have software to sell, right? Meanwhile, the comment section transforms into a delightful circus with IT aficionados reminiscing on the good ol' days of \"unpermitted\" SSH tunneling, heated debates on modern firewall capabilities, and at least one brave soul questioning if firewalls do anything at all. Prepare for a rollercoaster ride of deftly justified security loopholes and the nostalgia of the wild west era of internet security. \ud83e\udd20\ud83d\udcbb\ud83d\udd13"
  },
  {
    "title": "Build an Infinite Canvas (infinitecanvas.cc)",
    "points": 26,
    "submitter": "hotfixguru",
    "submit_time": "2024-08-25T17:59:50",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://infinitecanvas.cc/",
    "first_paragraph": "",
    "summary": "In another earth-shattering display of innovation, the web\u2019s brightest minds converge on infinitecanvas.cc, a site that promises the digital equivalent of never-ending paper to scribble on. Finally, tech enthusiasts and aspiring Da Vincis can unleash their masterpieces\u2014except most are just drawing stick figures and arguing over JavaScript libraries. Comments on the site fluctuate between unbridled confusion and misplaced enthusiasm, revealing that the most infinite thing about the canvas might just be the users' capacity for missing the point. Will humanity ever recover from such an unprecedented expansion of both canvas and ego? Only time (and bandwidth) will tell."
  },
  {
    "title": "Launch HN: CodeViz (YC S24) \u2013 Visual maps of your codebase in VS Code",
    "points": 103,
    "submitter": "LiamPrevelige",
    "submit_time": "2024-08-29T17:50:23",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=41393458",
    "comments": [
      "Codeviz is such a cool project. Love having it in VSCode.\n \nreply",
      "Congratulations on the launch - this looks great and I've been waiting for years for something like this! As a researcher who mostly uses Python, and explores/navigates a large number of repos for a short time, often written by other researchers not necessarily trained in software best practices, I was always frustrated (and surprised) that there was no VS Code extension or tool that gave me a quick overview/visualization to get a high level gist of different modules and code/data flow!I tried this with a bunch of small open-source repos and it works great! I imagine using LLM might be a hard no for some people/enterprises - any plans to use stand-alone licenses with small local models? It seems like for what LLM is doing here (if I understand it right, help  label the modules in natural language and perhaps help organize them into this hierarchy/modules) you don't necessarily need a SoTA model, right?Also, this could be coming from LLMs, but I see that the visualizations are more biased towards terminology used in web-dev? (for example, one of my robot related repo was organized into front-end, back-end, etc. with I guess is kinda right but not exactly lol). It would be nice to see an interactive visualization where I can iterate on the initial viz with information I know, e.g., I drag and drop a module  or rename it and then you probably do another pass with this feedback and LLMs and update my overall visualization with more domain specific labels and partitions?Edit: Exploring CodeViz on a few more repos, and it seems like you have a set of hardcoded labels for the highest hierarchy in the architecture diagram? (so far, I've only seen Users, Databases, Backend, Frontend, and Shared Components). I am guessing this is something passed on in the prompts? It'll be nice to allow user to define their own set of labels/partitions at one or more levels and then try to create an architecture visualization that fits into these labels/constraints (although I am guessing at some point you have to be wary of hallucinations?)\n \nreply",
      "We'd love to use local models and have played around with them a bit. Exactly right about the labeling - we didn't stick with local models because 3.5 sonnet is exceptionally good at finding niche architecture labels and merging similar modules (since code analysis is chunked). Copilot tools are becoming very popular, so companies are getting less strict around LLMs and code, but ultimately we do think everyone is better off if CodeViz is self-contained.There is some hard coded bias for web dev. Diagram modification is definitely high on our todo, and we've been finding ways to reduce pre-defined structure in our prompts to LLMs so they work with broader tech stacks. When we sell licenses to teams, we do some manual checks for accuracy and detail, which helps us improve the public extension.What's the name of the robotics repo? And any preference for modifying the diagram directly vs instructing changes by text?\n \nreply",
      "Re: EditThe top level categorizations are indeed fixed, however the nodes themselves can be arbitrary.  We've found this helps with grouping and organization while still allowing for the flexibility required to accommodate different systems.  I'm curious, are there any categories missing here that could be added?Currently, we categorize by: Frontend (UI/UX elements), Backend (API/Business/Data Access), DB(persisting storage), External Services (Backends maintained outside codebase), Shared Components (internally maintained libraries and helpers)\n \nreply",
      "Twice the monthly cost of Copilot seems crazy to me, and I\u2019m nowhere near as subscription-hostile as most folks here.\n \nreply",
      "I get where you're coming from - Copilot is very cheap and useful! We want the free version of CodeViz to be as useful as possible for you. Some features, like the second architecture layer, are just expensive for us.Personally, I spend a small fraction of my time actually typing code and much more time gathering context & building a mental map. CodeViz speeds up the mental map part, so we hope to deliver much more value than $19/moLet me know if there's anything missing from CodeViz that would change your mind!\n \nreply",
      "\u201cOn mobile? Add a calendar reminder to install CodeViz\u201dThis is genius\n \nreply",
      "Perfect. Now I have a calendar event to \u201cInstall Covid\u201d. Thanks!\n \nreply",
      "The name association is a genuine concern of ours :)\n \nreply",
      "Let's hope it spreads as well :p\n \nreply"
    ],
    "link": "item?id=41393458",
    "first_paragraph": "",
    "summary": "**Launch HN: CodeViz (YC S24) \u2013 Visual maps of your codebase in VS Code**\n\nThe Hacker News commentariat unites in rare, dribbling ecstasy over CodeViz, a VS Code extension that ostensibly saves beleaguered developers from the impossible task of reading their own code. One eager fan highlights its cost: twice that of GitHub Copilot, because why not pay more for the privilege of visual crutches? Meanwhile, an optimistic soul dreams of dragging, dropping, and renaming his way through buggy spaghetti code, apparently unaware that no amount of AI-powered eye candy can fix that mess. Predictably, everyone misses the feature to rename \"CodeViz\" to \"CodeSalvation\" in hopes of dodging the inevitable PR disaster when users accidentally evangelize \"Covid\" at their next stand-ups. \ud83d\ude43"
  },
  {
    "title": "Raspberry Pi Pico does line rate 100M Ethernet (github.com/rscott2049)",
    "points": 180,
    "submitter": "rscott2049",
    "submit_time": "2024-08-29T15:29:13",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41391822",
    "comments": [
      "> receive side uses a per-packet interrupt to finalize a received packetThis has made much faster systems not being able to process packets at line speed. A classic was that standard Gigabit network cards and contemporary CPUs were not able to process VoIP packets (which are tiny) at line speed, while they could easily download files (which are basically MTU-sized packets) at line speed.\n \nreply",
      "Fortunately, the receive ISR isn't cracking packets, just calculating a checksum and passing the packet on to LWIP. I wish there were two DMA sniffers, so that the checksum could be calculated by the DMA engine(s), as that's where a lot of processor time is spent (event with a table driven CRC routine).\n \nreply",
      "Luckily the RP2040 has a dualcore CPU so one core can be dedicated entirely to receiving the interrupts, passing it to user code on the other core via a FIFO or whatever else you fancy.\n \nreply",
      "I just started playing around with PIO and DMA on a Pico, and it\u2019s really fun just how much you can do on the chip without invoking the main CPU. For context, PIO is a mini-language you can program at the edge of the chip that can directly respond to and write to external IO. DMA allows you to tell the chip to send a signal based on data in memory, and can be programmed to loop or interrupt to limit re-invoking. The linked repo uses these heavily for its fast Ethernet communication.\n \nreply",
      "For added clarity, the Pico includes an RP2040 which is where the PIO runs.\n \nreply",
      "Thanks, and you're correct; not sure why you got downvoted for this. For anyone curious here are the data sheets for RP2040 [for original Pico] and RP2350 [for Pico 2], which describe the systems in detail.RP2040: https://datasheets.raspberrypi.com/rp2040/rp2040-datasheet.p...RP2350: https://datasheets.raspberrypi.com/rp2350/rp2350-datasheet.p...\n \nreply",
      "\"the Pico includes an RP2040 which is where the PIO runs\" to me sounds like it implies either- The original Pico was not built around the RP2040 as its central part (\"includes\" sounds to me like it was an addition)- The Pico 2 includes a RP2040 (in addition to the RP2350) which runs PIONeither of which are true. I'm guessing some other people had a similar reaction.\n \nreply",
      "Why is the transfer rate non-linear with respect to the system clock? At 100 MHz the rate is 1.38 Mbit/s and at 200 Mhz it is 65.4 Mbit/s.\n \nreply",
      "Latency kills...and Ethernet uses exponential backoff.\n \nreply",
      "More specifically TCP uses exponential backoff. Ethernet will happily keep drowning you in packages at line rate, if I'm not mistaken.\n \nreply"
    ],
    "link": "https://github.com/rscott2049/pico-rmii-ethernet_nce",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          This is an update to the existing pico-rmii-ethernet library by Sandeep Mistry.\nPlease see README_orig.md for a description of that library. Note that the\nRMII board modifications to use an external 50 MHz clock are still necessary.This library uses DMA driven ring buffers for both transmit and receive. The\ntransmit side is entirely DMA driven, while the receive side uses a per-packet\ninterrupt to finalize a received packet. Performance does vary with system\nclock speed, and the memory region the executable is placed:The MDIO interface was changed from polled to interrupt driven, thus freeing\nup more processor time for packet processing.The library uses:At 300 MHz, almost all of core 1 is used. It is possible to use about 6\nusec per packet poll, verified by placing a sleep_us(6) call in\nnetif_rmii_ethernet_loop() and running iperf. Core 0, o",
    "summary": "In a desperate attempt to make the Raspberry Pi Pico do something \"cool,\" a brave soul updates an Ethernet library finally allowing this glorified microcontroller to wheeze through 100M Ethernet without imploding. The creator slaps together DMA driven ring buffers, a mix of interrupts, and just enough techno-jargon to make sure the lone core left functioning begs for mercy after handling a single packet. Aside from performance tweaks that read like a poor man's overclocking guide, devout followers in the comments theorize on how multicore architectures could, should, and might salvage this setup, while simultaneously missing the point entirely. In true internet fashion, arguments break out, downvotes fly, and no one is entirely sure what the \"PIO\" actually does, but it sure sounds impressive at parties. \ud83c\udf89"
  },
  {
    "title": "Can solar costs keep shrinking? (tomaspueyo.com)",
    "points": 213,
    "submitter": "GoRudy",
    "submit_time": "2024-08-29T13:57:51",
    "num_comments": 349,
    "comments_url": "https://news.ycombinator.com/item?id=41390884",
    "comments": [
      "I spend four months of the year traveling in an RV. Two years ago, I budgeted over $6000 for my desired solar setup (at least 1200Ah of battery, at least 2400W of solar, plus various controllers and other components). It was expensive enough for me to hold off, and more importantly, I didn't have the space on my current RV for the solar panels.In just the two years since then, prices on batteries and panels have dropped 25% or more, and solar power per square foot at a good price point has gone up significantly (400W monocrystalline panels can be gotten for $200, in the same form factor as the 200W panels I had been budgeting for). I've now lowered my budget to $4000 for the same setup I was planning to spend $6000 on two years ago, and with 400W panels, I no longer need to upgrade to a larger RV to begin the project.This summer is almost over, so I'm going to wait until spring to start assembling my system in earnest. Anecdotally, this is a game-changer for me. I'm looking toward year-round full-timing starting next summer, because I can now afford the power I need and don't need a larger RV as soon as I thought I would.I intend to buy undeveloped land far from civilization in the next few years, and I'm now confident that I can DIY a whole-house solar and battery setup so cheaply that access to mains power won't be a factor in deciding where I settle. Even with seasonal variation in power production, I'll manage just fine, and the system will pay for itself in well under five years. In fact it'll pay for itself instantly if you discount the five-figure cost I would otherwise have had to pay for running a new mains power line far into the woods. And by the time I pick some land to settle on, I'll already have enough solar on my RV that I won't even need to augment the system initially; I'll be able to power a small house in a temperate climate directly off the RV itself, while I build a larger solar array (likely ground-mounted to avoid regulations and insurance complications related to roof-mounted setups).I know my situation is unusual, but the fact that any of this is possible for well under $10k is a huge change from even a decade ago.\n \nreply",
      "If you're looking for some inexpensive land that can be fairly remote you might do worse than Cochise county AZ. The county has an opt out permit program as long as you have at least 4 rural acres. You can build whatever you want, no permits, no inspections. There are some restrictions. No bank will give you a mortgage, can't sell or rent it out within 2 years. You can live in your RV while you build, for 2 years I think, with a 3 year extension possible. Lots of sun for solar. Rooftop water collection for water. There are some septic options, composting, or traditional. All legal. A few dozen people on youtube building all kinds of off grid natural buildings. They get together once in a while, let you tour their build, ask questions. A little snow in the winter, not many days above 100F. 4000+ feet elevation. Fiber laid in some surprisingly rural areas. No paved roads type areas. Weird, but cool.I did the full time RV bit in a class A. Hated it. Too small to live in, too big to travel in. Hate that black tank. Had to leave great camping spots once a week to dump and get more water, or hook up to some sort of developed campgrounds. Sucked. Regret not going for a small schoolie to travel in, large house on 5 acres to live in. That's the new plan, anyway. Best of luck.\n \nreply",
      "Average rainfall 11 to 41 inches per year -- more than I expected in AZ. Still, to have reasonable water year-round for 2 people you'd need to collect across maybe 10,000 square feet? That's not a small installation.\n \nreply",
      "> I intend to buy undeveloped land far from civilizationI'm always intrigued by this notion, I know plenty of Americans have this kind of plan, but it's never quite as remote as they think, because, y'know you're still in America somewhere.Unless you're thinking of somewhere in northern Canada.I'd love to know where is considered 'far from civilization' on the continental US.\n \nreply",
      "America can get extremely remote and so you can choose how remote you want to go. Here is a journalist from Manhattan meeting folks living off-grid in Colorado: https://www.newyorker.com/magazine/2022/11/28/what-going-off...Not to mention Slab City: https://en.wikipedia.org/wiki/Slab_City,_CaliforniaInfrastructure-wise, roads come before water or electricity, and so plenty of the United States has a road but no power or water. This can even be standard for those who are near civilization.\n \nreply",
      "Slab City is 100 miles from San Diego and only 160 miles from Los Angeles! That's not remote - remote should be classified using a decreasing population density over distance function.Remote is somewhere like Nullarbor, which is 183 miles from Ceduna (population < 6000). It's over 1000km to Adelaide (population 1.3m) or 1600km to Perth (population 1.9m).https://en.wikipedia.org/wiki/Nullarbor,_South_Australia\n \nreply",
      "That's not remote: there's a paved road that goes there! Remote should be somewhere that requires weeks of travel by sailboat or camel to get to.\n \nreply",
      "> I'd love to know where is considered 'far from civilization' on the continental US.There are many interpretations and levels of remoteness.In my case, I want to be away from the sights and sounds and crowds of anything that would be considered urban or suburban. My prime criteria is that I don't want to see another person unless I choose to. I don't want to see a road with cars on it, I don't want to see another house or any other man-made structure that isn't mine. Ideally I don't want to hear anyone else either, but I accept that I may hear things in the distance.I don't want to be a complete hermit, and I'm not a survivalist looking to be 100% self-sufficient. I want a small-to-midsized town about 30-60 minutes away. Something with a grocery store, gas station, and a post office or other place to receive deliveries. I don't want to be more than an hour away from doctors' offices or a hospital or urgent care. I don't want to be trapped by snow for weeks at a time.Saying \"far from civilization\" was a stretch. What I really mean is I don't want to have people all around me. And I don't want to be anywhere near cities and suburban sprawl. I don't want neighbors in any meaningful sense.Places I'm considering are Maine, Montana, northern Wisconsin, Upper Peninsula Michigan. I would absolutely consider parts of Canada, particularly northern BC. I don't have an easy path to Canadian citizenship, though. Before my Canadian girlfriend passed away last year, we had been planning to look for a secluded lakeside cabin or undeveloped land in BC.My requirements are dense forest (desert/plains states are right out) and water (lake or canoeable river) on the property itself. I can live with other people using the water, so long as it's not motorboats and a party scene.\n \nreply",
      "I find this fascinating. It really does take a variety of people to make up a culture. My ideal living situation includes being able to see someone I know every time I walk out side my home, being able to walk to a bagel shop, a grocery store, a post office, a train station, and ideally a library and bike shop and parks.It would be boring if we were all like me and clumped together.\n \nreply",
      "I've lived like that. I lived in the Mission in San Francisco with four of my closest friends (all from NJ) within walking distance. It was great being able to meet up for lunch or drinks or go out clubbing, it was always a pleasant surprise to run into them randomly on the street or in a park.I lived in midtown Manhattan and I loved being able to go out at 3am in the middle of the winter and find a fresh produce stand on the corner outside my apartment, get falafel wraps and Ethiopian and Thai and sushi and all the other great food during my lunch break, and have museums and concerts and Broadway shows within walking distance.I lived in the Cayman Islands and had a roommate, could walk to the beach and to my favorite bars, my house was the primary hangout spot for all my friends. I was socializing daily there, and it was a small community where I knew just about everyone everywhere I went.I'm old now. I don't drink anymore. I have no interest in parties. Even when I live in or near a city, I don't take advantage of much of anything it has to offer. I'm sick of the noise and filth and crowds, the crime and homelessness, the lack of privacy that comes with urban living. All my friends are older, have kids, live in the suburbs and are scattered all over the country. The eight months of the year that I'm not on the road in my RV, I'm living in a cookie-cutter suburban house and have no local friends at all. I exchange pleasantries with the neighbors.My entire social life is online now, and when I can, I'm traveling. I want maximum peace and quiet. I'll go visit friends and family every couple months if I want to socialize. If/when I do settle down at my far-from-civilization objective, I may very well start feeling lonely and seek out social clubs or social hobbies. But I'll be glad to have my seclusion to return to.\n \nreply"
    ],
    "link": "https://unchartedterritories.tomaspueyo.com/p/can-solar-costs-keep-shrinking",
    "first_paragraph": "",
    "summary": "<strong>Can Solar Costs Keep Shrinking Forever?</strong> - A riveting expos\u00e9 unfolds on tomaspueyo.com, wherein a tech blogger outlines the seemingly endless descent of solar panel costs much to the chagrin of their savings account. Commenters chime in with their off-grid dreams, turning the discussion into a masterclass on discount engineering degrees and Google-learned electrical work. In a daring escape from societal norms (and possibly common sense), one ambitious commenter plans their battery-powered ascent to Walden Pond 2.0, with only YouTube tutorials as their guide. Meanwhile, another enthusiast calculates rainwater catchment needs with the precision of a toddler stacking blocks. Break out the popcorn, the solar-powered future is upon us, and it's hilariously underprepared. \ud83c\udf1e\ud83d\udd0b\u2728"
  },
  {
    "title": "Show HN: MinutesLink \u2013 AI note taker for online calls (minuteslink.com)",
    "points": 8,
    "submitter": "elisegr",
    "submit_time": "2024-08-29T23:22:32",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://minuteslink.com/",
    "first_paragraph": "",
    "summary": "The web-based echo chamber proudly presents its latest solution in search of a problem: <em>MinutesLink</em>. This groundbreaking AI note taker vows to capture the riveting discourse of your average online call, ensuring no gem like \"can you see my screen?\" goes unrecorded. The commenters, basking in the glow of Silicon Valley's relentless innovation, trip over themselves to predict global productivity spikes as if writing efficient meeting minutes equates to landing on Mars. Meanwhile, the real miracle is that nobody has yet suggested integrating blockchain to further \"enhance\" this indispensable tool. \ud83d\ude80\ud83e\udd13"
  },
  {
    "title": "The Imperial Origins of Big Data (yale.edu)",
    "points": 9,
    "submitter": "drdee",
    "submit_time": "2024-08-29T23:08:23",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41396279",
    "comments": [
      "also where the word \"statistics\" comes from:> The term statistics is ultimately derived from the Neo-Latin statisticum collegium (\"council of state\") and the Italian word statista (\"statesman\" or \"politician\"). The German Statistik, first introduced by Gottfried Achenwall (1749), originally designated the analysis of data about the state, signifying the \"science of state\" (then called political arithmetic in English).https://en.wikipedia.org/wiki/History_of_statistics#Etymolog...\n \nreply",
      "Ralph Van Deman in 1901-1903 in the Philiphines-American War, compiling huge files on the population to try to root out rebels, was imo one of the first big data people.He brought that home to America. And started keeping copious notes on anything the capitalist fatcats dubbed un-American. His labor, leftist, civil rights suppression activities got entrenched as quasi-legal vigilantism & endless paperwork & survelliance as the American Protection League. He was an O.G. keeper of big data.\n \nreply"
    ],
    "link": "https://yalebooks.yale.edu/2024/08/28/the-imperial-origins-of-big-data/",
    "first_paragraph": "As we transition our order fulfillment and warehousing to W. W. Norton, select titles may temporarily appear as out of stock. We appreciate your patience. Map by Emanuel Bowen on Wikimedia\n            August 28, 2024 | ceb95 | East Asian Studies, Essays, European History, History, Technology  Asheesh Kapur Siddique\u2014We live in a moment of massive transformation in the nature of information. In 2020, according to one report, users of the Internet created 64.2 zetabytes of data, a quantity greater than the \u201cnumber of detectable stars in the cosmos,\u201d a colossal increase whose origins can be traced to the emergence of the World Wide Web in 1993.1 Facilitated by technologies like satellites, smartphones, and artificial intelligence, the scale and speed of data creation seems like it may only balloon over the rest of our lifetimes\u2014and with it, the problem of how to govern ourselves in relation to the inequalities and opportunities that the explosion of data creates.But while much about our er",
    "summary": "**Colonial Byte Power: The Pomposity of Data Empire**\n\nIn a breathless display of historical dot-connecting, a recent piece from Yale dusts off the cobweb-laden term \"big data\" and magically traces its lineage back to the cradle of imperialism. Clearly, 64.2 zetabytes of digital fluff must obviously be blamed on dusty bureaucrats and starry-eyed colonials armed with abacuses and oppression. The commenters, seizing on a solitary mention of \"statistics,\" launch a crusade to pin even this modern technological behemoth squarely onto the shoulders of 18th-century statists, because every digital woe, obviously, was seeded in the days of the quill. Another insightful dive into how everything wrong with the internet can be traced back to people in funny hats deciding they knew best. \ud83c\udf0d\ud83d\udcbe"
  },
  {
    "title": "Show HN: A discovery-focused search engine for Hacker News (trieve.ai)",
    "points": 110,
    "submitter": "skeptrune",
    "submit_time": "2024-08-29T17:08:20",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=41393005",
    "comments": [
      "Congrats on building and shipping! I love how \"at home\" the styling feels.I searched for `Excel` since that's a topic I care about and tend to follow frequently.The first (\"most relevant\") link is a post with 1 point and 0 comments from 2020. The second link has 1 comment  and 2 points, from 2017.The top two links on Algolia's search have about 1000 points each and are way more topical.I tried to hit \"Back\" to make this comment, and saw the site broke my browser's navigation. I was forced to right click on \"Back\" (or spam the back button) to get back here... so not a great experience, overall\n \nreply",
      "- the version without search as you type and JS at https://hnnojs.trieve.ai/ has a more normal back button- Algolia ranks by points by default while we are ranking by relevance score which is the difference. You can order our results by points if you want to with the order by select component that says \"relevance\".We went back and forth on making points sorting default and ended up deciding against it, but maybe we should have. Our thinking was that since it's focused on \"discovery\" it was worth prioritizing relevance, but I can see how it can feel the result quality isn't as great. HN is really good at highlighting interesting links.Best fix would have been LTR, but we made incorrect decisions early on which made the rewrite a bit too hard - https://trieve.ai/launching-trieve-hn-discovery/#relevance-q...\n \nreply",
      "I like the layout. I like the Try with Angolia button. I like the search recommendations.But when I search for \"FreebBSD\" I get:* FreeBSD is an amazing operating system* FreeBSD Is an Operating System* 9x FreeBSD \u2013 a lesson in poor defaultsIn the top 15 results, 10 are duplicates. And none of the articles are interesting.If I sort by points instead of relevance, nothing has to do with FreeBSD.\n \nreply",
      "Yeah. We didn't dedup when we ingested, but probably should have. The hack for this right now would be putting \"FreeBSD\" in quotes when sorting by points or adjusting the score threshold up in options. We have it set to a very low value by default.\n \nreply",
      "Oh thanks! I had one heck of a difficult time trying to find the author and title of 'the glass bead game' by Herman Hesse. It's pretty hard to find with simple keyword based search.Though short comments seemed to score a bit too highly IMHO. It took a while to find a query that found the long rambly comment I needed.\n \nreply",
      "We spent quite a bit of time trying to make length normalization better, but there's still a lot of room for improvement. The default behavior was super biased towards longer rambly comments and we may have over-corrected. I appreciate the note.\n \nreply",
      "This is impressive! I've frequently encountered challenges with Algolia search not locating specific items, but this appears to offer a much more detailed search capability.I've bookmarked this site and hope it remains available when I need it, unlike many great Show HN posts that vanish after six months or so.\n \nreply",
      "Glad you found it useful! Fulltext search will almost certainly be up in perpetuity, however, we may drop the semantic index if it doesn't get much usage as that's significantly more expensive to host.\n \nreply",
      "> $6835.39/monthThis seems way higher than I expected. Cloud pricing is out of control when Postgres is already > $500 for a small instance that could be run for a fraction if it wouldn\u2019t be a cloud provider.\n \nreply",
      "Pretty sure we will have to co-locate soon in general. Cloud provider costs are near unsustainable for our business at least.\n \nreply"
    ],
    "link": "https://hn.trieve.ai/",
    "first_paragraph": "",
    "summary": "**Show HN: Discovery Goes Haywire, Users Confused and Trepidatious**\n\nIn yet another stunning display of \"discovery,\" a bold HN user unveils <i>trieve.ai</i>, a search engine designed to navigate the dank crevices of Hacker News's least commented and least liked posts. \"Congrats on recreating Google from 1998,\" cheers one user, somehow happy about a search result yielding a lonely Excel post without comments or clout. Meanwhile, another adventurer can't escape the Bermuda Triangle of buggy back buttons without a right-click s\u00e9ance. The grand finale involves users discovering that they can, indeed, sort by points, thus completely undermining the whole premise of 'relevance'\u2014because nothing says \"I've found what I was looking for!\" like a search engine that needs a manual on how to work around its 'features'. \ud83c\udf89\ud83d\udcbb"
  },
  {
    "title": "Mastering ISO 8583 Message Networking with Golang (alovak.com)",
    "points": 8,
    "submitter": "alovak",
    "submit_time": "2024-08-28T13:16:54",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://alovak.com/2024/08/27/mastering-iso-8583-message-networking-with-golang/",
    "first_paragraph": "In the previous post, Mastering ISO 8583 Messages, we looked at how to create specifications in Go, set data to messages, and finally, how to get binary values from them. This post will explore how messages are sent and received by card network participants such as acquirers and issuers. For the coding part to work with messages, we will use the moov-io/iso8583 Go package. The source code for the Go client we will be creating in this post can be found at the following link: https://github.com/alovak/iso8583-client-demo.ISO 8583 is a standard for financial transactions with payment cards. There are different types of financial transactions, and each transaction consists of request and response messages. Let\u2019s take a look at payment transactions. While the process of handling payment itself is more complex and includes clearing and settlement, in this document we focus only on the messages and the networking.Here is a transaction flow for card authorization (this is when you pay for some",
    "summary": "Today on the enlightening niche blog circuit, a stirring sequel emerges: \"Mastering ISO 8583 Message Networking with Golang.\" Once again, the fin-tech gladiators equip their GoLang armor, ready to charge the formidable bastions of card network communications. For those unaware, ISO 8583 is, shockingly, <em>not</em> a new Star Wars droid, but a thrilling standard for managing the zesty tales of financial transactions. Dive deep into the riveting world of 'acquirers' and 'issuers' as we scribble Go code that could, with any luck, revolutionize buying half a latte. Comments section below is a delightful warzone where seasoned coders battle rookies over the nuances of binary message parsing, and a misplaced semicolon can ignite World War III. \ud83d\udc7e\ud83d\ude80\ud83d\udcb3"
  }
]