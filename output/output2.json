[
  {
    "title": "New exponent functions that make SiLU and SoftMax 2x faster, at full accuracy (github.com/ggerganov)",
    "points": 180,
    "submitter": "weinzierl",
    "submit_time": "2024-05-15T19:57:01",
    "num_comments": 40,
    "comments_url": "https://news.ycombinator.com/item?id=40371612",
    "comments": [
      "About 20 years ago, I was programming for the Hughes radar signal processor, a highly parallel pipelined machine which accounted for much of Hughes success in radar processing. Anyway, I needed to compute e^x for 0 < x < 1. The processor had a multiply, so I used four 256 long tables of e^x for each possible 8-bit values in the 4 blocks in the 32-bit word, multiplied them to get the final value. It was about 5 times as fast as the previous best e^x routine.\nThat machine was fun! It is obsolete now, but for many years is could process radar signals faster that processors that were nominally many times faster.",
      "In case anyone else wasn't quite following this, I think the idea ise^x= e^(a+b+c+d)  (where abcd are the bytes of x)= e^a * e^b * e^d * e*dand then you make a lookup table for each of those e^a, e^b values.(I fudged \"a\" here, where it's really more like \"high byte << 24\", but I think you just make your lookup table for e^a be a map from a => e^(a<<24), and similar for the other bytes.)",
      "How much do these silu and softmax improvements affect the LLM inference speed as a whole? Correct me if I'm wrong but I feel that this change will only have a small effect as the majority of the time is spent doing matrix multiplications.",
      "Overwhelming majority of flops is indeed spent on matmuls, but softmax disproportionately uses memory bandwidth, so it generally takes much longer than you'd expect from just looking at flops.",
      "If cpu softmax were limited by memory bandwidth, then these vectorization optimizations wouldn't improve performance.",
      "Why does it disproportionately use bandwidth?",
      "In transformers the attention matrix is N*N, so there are a lot of values to go over. Typically makes it memory bandwidth bound, not compute bound.",
      "Oooooh, I forgot that the self attention layer has a softmax. I thought this was referring to a softmax on the dense forward layer. Thanks!Next question: does the softmax in the SA block cause it to be bandwidth bound\u2014won\u2019t it have to materialize all the parameters of the N^2 matrix either way? Does SM cause redundant data reads?",
      "> replaces short[65536] look up tableIs that not quite dim to begin with (having a LUT the size of the whole L1 cache?) or does it work surprisingly well because of some probabilistic fudging?",
      "The lookup table does surprisingly well because the workload is otherwise extremely cache-hostile, and it doesn't really matter if you blow up your L1 cache, none of the data you evicted because you needed to fit the LUT was ever going to be reused anyway.ML loads in general are streaming loads that linearly load the entire dataset for every iteration."
    ],
    "link": "https://github.com/ggerganov/llama.cpp/pull/7154",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "In a world where every millisecond counts and every programmer believes they're just one optimization away from revolutionizing computing, one GitHub warrior unleashes <i>New Exponent Functions that Make SiLU and SoftMax 2x Faster, at Full Accuracy</i>. Our hero decisively updates the ancient art of exponentiation, promising a brave new world tuned to double precision! Meanwhile, the comment section becomes a geriatric ward of one-upmanship, where dusty tales of 1970s radar systems morph into a humblebrag Olympics. \"Back in my day,\" boasts one, scribbling on the hazy line between relevance and quaint nostalgia. Everyone else in the thread interrupts their potential learning moment to flex their half-digested concepts, obviously not grasping that most improvements in machine learning boil down to keeping the GPU lights blinking a bit longer. \ud83d\ude80\ud83d\udcbe"
  },
  {
    "title": "LLMs are not suitable for brainstorming (piaoyang0.wordpress.com)",
    "points": 18,
    "submitter": "bcstyle",
    "submit_time": "2024-05-15T23:47:55",
    "num_comments": 10,
    "comments_url": "https://news.ycombinator.com/item?id=40373709",
    "comments": [
      "> The reason [LLMs are not a good tool to do truly effective brainstorming] is LLMs are trained to follow existing patterns in the human-produced corpus, and not natively taught to \u201cbrainstorm\u201d.The problem with this argument is that people do the same thing, we\u2019re not that great at brainstorming either. When we brainstorm in groups, we\u2019re just bringing multiple points of view together. The more data LLMs are trained on, the more viewpoints it might be able to bring that you haven\u2019t considered.That said, LLMs and all NNs so far are built to interpolate, and they are bad and have unbounded error when extrapolating outside their training examples. That is a good reason to not expect today\u2019s AI to come up with new ideas.",
      "I\u2019ll add this here for the fake internet points but also as a general PSA. Most people are not aware that \u201cBrainstorming\u201d although popularized as a process since its origins in the 1960s\u2026  is actually Step 2. Most people are unaware that Step 1 is called \u201cQuestionstorming\u201d. At the heart of the process is leveraging divergent and convergent modes of thinking which is done both to generate questions (and select the most promising ones) and answers\u2026 you\u2019re welcome :D",
      "Secret prompt - add 'using TRIZ methodology' to your brainstorming prompts",
      "Idk. In the vast majority of topics that intellectually stimulate me, I'm far below the average specialist. That means LLMs have a lot to offer me.",
      "I find LLMs to be useful tools for brainstorming.I have often inadvertently found myself brainstorming just by explaining something challenging I am working on to a colleague with no expertise in my area. Just explaining forces new perspectives and new ideas.Similarly, LLMs are captive audiences for throwing out ideas and playing with them. The interaction creates something more than just talking to myself or a whiteboard.Not all brainstorming partners need to be first order contributors to have a very positive impact. Different kinds of interactions can spark entirely different kinds of ideas.",
      "I agree with this, but also get  so irritated with the endless praise. Like, tell me I'm wrong and tell me why. If I present a bad idea, tell me that.Or even like \"huh? explain\"Looking forward to ramping up the honesty parameter. Hoping OpenAI's new voice model trivializes this so I don't need to prompt engineer.",
      "This is utterly wrong. The strength of LLMs is that they have very wide knowledge, so while your specialist knowledge might surpass them in a particular area it will know more than you across other topics.When brainstorming this wide knowledge is what you want. The trick is (as always) better prompting to push it hard - so things like \"consider parallels in similar situations in other fields\" are useful.",
      "GPT4 is great for brainstorming. It helped me come up with an idea for my last paper.",
      "The author might accuse you of merely \"better-than-average level\" thinking",
      ">However, here I would like to argue that (especially in cutting edge scenarios) LLMs are not a good tool to do truly effective brainstorming.Great title, they baited and switched.95% of the time I don't need effective brainstorming, I need a bunch of ideas, let me pick the best, and move on.If its a real engineering problem, then I need>truly effective brainstorming."
    ],
    "link": "https://piaoyang0.wordpress.com/2024/05/15/llms-are-not-suitable-for-brainstorming/",
    "first_paragraph": "This may be obvious to many people already, but I recently thought about this in a few scenarios and figured it may be valuable to articulate it clearly.",
    "summary": "Title: AI Can't Think Outside the Box, Cry Humans Stuck in Circles\n\nIn a stunning revelation on piaoyang0.wordpress.com, a blogger reminds the world that language models, like human toddlers at a brainstorming session, follow patterns rather than create genuine innovation. Commenters, in a desperate bid to validate their existence in the algorithmic shadow of the LLMs, argue both sides of a coin that\u2019s already been spent. One genius suggests that if we just *ask* the AI nicely (using TRIZ, of course), it might just spit out a Mona Lisa of ideas, ignoring that true inspiration often skips the predictable pathways. Meanwhile, another commenter advocates for AI as the ultimate brainstorming buddy, proving once again that low expectations are the key to eternal happiness. \ud83d\ude02\ud83e\udd16\ud83d\udca1"
  },
  {
    "title": "Adobe Photoshop Source Code (2013) (computerhistory.org)",
    "points": 409,
    "submitter": "PaulHoule",
    "submit_time": "2024-05-15T15:12:15",
    "num_comments": 161,
    "comments_url": "https://news.ycombinator.com/item?id=40368016",
    "comments": [
      "Early Photoshop was junk compared to Deluxe Paint on the Amiga. History only remembers the winners so it\u2019s unfortunate DPaint gets lost the midst time.",
      "In the aughts I worked at Adobe and spent time trying to archive the source code for Photoshop, Illustrator, PostScript, and other apps. Thomas Knoll's original Mac floppy disk backups were available, so I brought in my Mac Plus, with a serial cable to transfer the files to a laptop via Kermit. The first version was 0.54, dated 6 July 1988. The files on the floppies were in various ancient compressed archive formats, but most were readable. I created an archive on a special Perforce server of all the code that I found. Sadly, the earliest Illustrator backups were on a single external disk drive that had gone bad.",
      "Thank you for your service. Super cool project. Hopefully they make their way to archive.org or Github someday.",
      "Adobe has the only copy, and they have donated early versions of PostScript (https://computerhistory.org/blog/postscript-a-digital-printi...) and Photoshop; people should ask Adobe to release more. Everything I find in the public domain I post at https://www.softwarepreservation.org/projects .",
      "Wow are you the one that posted the original LISP 1.5 source code? I colorized that and used it to good effect in my blog posts. https://justine.lol/sectorlisp/#listing",
      "I beat the bushes for the source code, documenting my finds (https://mcjones.org/dustydecks/archives/category/lisp/) and posting them (https://www.softwarepreservation.org/projects/LISP/lisp15_fa...), but the early work was done by Jack Harper, Pascal Bourguignon, Rich Cornwell and Bob Abeles, Andru Luvisi, Angelo Papenhoff, Al Kossow, and others.",
      "isn't the topic the Patents, not the code?  The code is mired in Mac toolbox details, no?",
      "Patents expire after 20 years at most, I believe. Everything from before 2004 has expired already.",
      "Can you patent open-source code?",
      "Even the parents from 1988-2000 would be well expired now"
    ],
    "link": "https://computerhistory.org/blog/adobe-photoshop-source-code/",
    "first_paragraph": "pho\u00b7to\u00b7shop, transitive verb, often capitalized \u02c8f\u014d-(\u02cc)t\u014d-\u02ccsh\u00e4p to alter (a digital image) with Photoshop software or other image-editing software especially in a way that distorts reality (as for deliberately deceptive purposes) \u2014 Merriam-Webster online dictionary, 2012",
    "summary": "Title: Adobe Photoshop Source Code (2013) (computerhistory.org)\n\nIn a dazzling display of nostalgic technobabble, computerhistory.org drops the source code for Adobe Photoshop like it\u2019s hot (spoiler: it\u2019s not 1988 anymore). Commenters embark on a digitized odyssey, comparing ancient software relics and flexing their retro-tech muscles with tales of \"back in my day\" bravado, where floppies ruled and \"Deluxe Paint\" wasn't just a hipster cocktail. One heroic soul recounts their epic quest to rescue this sacred code from the dusty depths of obsolete Macintosh drives, prompting a chorus of back-pats and coulda-shoulda-wouldas about making this digital antiquity public. Meanwhile, the real MVP queries if the chatter is about patents or code, subtly reminding everyone that they're geeking out over techno-ghosts that can't even haunt a patent office anymore. \ud83e\udd13\ud83d\udcbe"
  },
  {
    "title": "In medicine what's the difference between an -ectomy, an -ostomy, and an -otomy? (1986) (straightdope.com)",
    "points": 92,
    "submitter": "tzs",
    "submit_time": "2024-05-15T20:01:37",
    "num_comments": 36,
    "comments_url": "https://news.ycombinator.com/item?id=40371650",
    "comments": [
      "It's fascinating how surgical terminology, much like programming languages, uses precise syntax to convey complex operations in a compact form.Just as in coding, where function names like append(), open(), or close() might describe operations on data, surgical terms like -ectomy, -ostomy, and -otomy encapsulate detailed medical procedures on the human body.This linguistic efficiency not only facilitates clear communication among professionals but also mirrors the procedural thinking found in technical fields.",
      "Similarly, -itis vs. -osis.-itis means the part of the body is swollen, while -osis means the part of the body is damaged.I found this out back in 2012, when I had a very nasty case of tendonitis in one of my feet, and I found out that the vast majority of cases of tendonitis should be called tendinosis instead, as it's pretty rare for the tendons themselves to swell; rather, the tendons themselves deteriorate from overuse.",
      "> -itis means the part of the body is swollen, while -osis means the part of the body is damagedWhat if the part of the body is swollen and damaged?",
      "-opathy would generally be suitable. It implied a pathological condition involving some form of tissue damage or dysfunction.Minor clarification, swelling does not necessarily always mean inflammation. -itis refers to inflammation of an organ or area of the both. Swelling, particularly when from fluid retention, is referred to as -edema. There is overlap though. :)",
      "> -itis means the part of the body is swollen, while -osis means the part of the body is damaged.Does the suffix have any similar meaning outside of medicine? Like, osmosis?",
      "itis -> infection.",
      "itis = inflammationInfection can cause an itis, but not all itis is due to infection. Such as inflammatory arthritis (noninfectious) vs septic arthritis (infectious).",
      "Also, hyper- is more, hypo- is less.",
      "In pop culture, there's a very common mixup between tracheotomy (the actual incision or cut made into the trachea) and tracheostomy (the overall procedure involving the creation of the hole, aka stoma).",
      "So I guess tracheology is the study of throats, and tracheaectomy is what werewolves would do."
    ],
    "link": "https://www.straightdope.com/21341781/in-medicine-what-s-the-difference-between-an-ectomy-an-ostomy-and-an-otomy",
    "first_paragraph": "Dear Cecil: In medicine, what\u2019s the difference between an \"-ectomy,\u201d an \"-ostomy,\u201d and an \"-otomy\u201d? My wife believes they mean \u201chack it off,\u201d \u201cbite it off,\u201d and \u201cpinch it till it drops off.\u201d J.W., Chicago",
    "summary": "Welcome to another day of embarrassing misunderstandings presented as education on *straightdope.com*. Today, we dive headfirst into the oh-so-mystical world of surgical suffixes where we pretend that equating complex medical procedures with syntax in programming makes us look intellectual. The comments section valiantly continues this fa\u00e7ade, bringing linguistics into the mix with the charm of a first-year med student explaining surgery to a teddy bear. Expect bloated comparisons, misplaced self-assuredness, and the inevitable derail into random medical anecdotes. Brace yourselves for a ride on the rhetoric rollercoaster, where everyone leaves more confused but convinced they've just had a profound learning experience! \ud83c\udfa2\ud83d\udc89\ud83e\udd37\u200d\u2642\ufe0f"
  },
  {
    "title": "Project Gameface launches on Android (googleblog.com)",
    "points": 84,
    "submitter": "xnx",
    "submit_time": "2024-05-15T19:39:28",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=40371401",
    "comments": [
      "Was instantly confused as this is already a name of a product (Coherent Gameface for making a game UI in HTMl5 for multiple engines).Seems this might well be useful, possibly even for normal people. But if I was suddenly full-body paralyzed, I would do everything and anything to get a Neuralink after seeing the first human user of that get ~80% of my score in a test measuring fast and accurate mouse movements.",
      "Even as a currently able-bodied person it's interesting to think about applications for this. I downloaded the Windows app and did a quick binding of mouth left and mouth right to my arrow keys so I can effortlessly flip through image galleries. (Couldn't figure out how to turn off cursor movement entirely though.)",
      "Absolutely. DCS is a plane combat simulator, hevely relies on head tracking for immersion, and the UX is literally you clicking buttons and some are below the HUD and left and right so you need all the head degree of freedom just to look aroundThat doesn't leave much space to zoom in and out which is a issue because visual acuity is greater than monitor fidelity so some degree of zoom is required to keep tab of adversariesIt'd be awesome to have a \"squint\" gesture to zoom in",
      "while I'm hopeful google will be responsible long-term maintainers of this accessibility feature, given their \"ship and drop\" attitude (music, reader, podcasts, stadia...) I'm glad they've chosen to open source this. In any case, an exciting project!",
      "Yea given that it's open source, I'm hopeful that it'll keep going as an accessibility tool for a long time.  https://github.com/google/project-gameface",
      "macOS has a lightweight version of something similar built-in: https://eshop.macsales.com/blog/64948-control-mac-with-head-...You can move your head around to move the cursor, smile to click, raise your eyebrows to right-click, and fart to make it to type the embarrassed emoji (not really, lol).But it's pretty cool.Also reminds me of the Dasher app, an eye-tracking typing app with text prediction. You stare at the a letter, one at a time in an ever-zooming tree, in order to form words: https://www.inference.org.uk/dasher/DasherSummary2.html",
      "It\u2019s a good day for accessibility between this and the stuff apple announced.",
      "Tomorrow is Global Accessibility Awareness Day: https://accessibility.day/Hence the announcements, I assume.",
      "Can this be used without internet connections or is it just continuously streaming your face video back to Google?",
      "The model runs locally on device, you can find the mediapipe model in the repo."
    ],
    "link": "https://developers.googleblog.com/en/project-gameface-launches-on-android/",
    "first_paragraph": "At I/O 2023, we launched Project Gameface, an open-source, hands-free gaming \u2018mouse\u2019 enabling people to control a computer\u2019s cursor using their head movement and facial gestures. People can raise their eyebrows to click and drag, or open their mouth to move the cursor, making gaming more accessible.",
    "summary": "Google, pioneer of perpetual beta software, unveils its latest gimmick: <em>Project Gameface</em>. Now you can twitch and gurn your way through your favorite games, because nothing says accessibility like eyebrow-raising UI controls that seem lifted from a dystopian sci-fi novel. Tech enthusiasts and confusion hobbyists in the blog's comments juggle skepticism and mild excitement, dishing out a mix of technical modesty and the usual resignation over Google's infamous commitment issues. Meanwhile, someone's already mistaking the tech for telepathy training, proving that no matter how straight you make the face-control feature, some users will always squint things up."
  },
  {
    "title": "Jepsen: Datomic Pro 1.0.7075 (jepsen.io)",
    "points": 178,
    "submitter": "aphyr",
    "submit_time": "2024-05-15T16:57:30",
    "num_comments": 25,
    "comments_url": "https://news.ycombinator.com/item?id=40369467",
    "comments": [
      "I was a fly on the wall as this work was being done and it was super interesting to see the discussions. I was also surprised that Jepsen didn\u2019t find critical bugs. Clarifying the docs and unusual (intentional) behaviors was a very useful outcome. It was a very worthwhile confidence building exercise given that we\u2019re running a bank on Datomic\u2026",
      "> I was also surprised that Jepsen didn\u2019t find critical bugs.From the report...\"...we can prove the presence of bugs, but not their absence...\"",
      "That's consistent with the usual definition of \"finding\" anything.",
      "\"Absence of evidence is not evidence of absence.\"",
      "Did you not do this work yourself before you started running the bank on it?",
      "I doubt any organization that isn't directly putting lives on the line are testing database technology as thoroughly and competently as Jepsen.  Banks jobs are to be banks, not be Jepsen.",
      "This is the first time I try reading a Jepsen report in-depth, but I really like the clear description of Datomic's intra-transaction behavior. I didn't realize how little I understood the difference between Datomic's transactions and those of SQL databases.One thing that stands out to me is this paragraph  Datomic used to refer to the data structure passed to d/transact as a \u201ctransaction\u201d, and to its elements as \u201cstatements\u201d or \u201coperations\u201d. Going forward, Datomic intends to refer to this structure as a \u201ctransaction request\u201d, and to its elements as \u201cdata\u201d.\n\nWhat does this mean for d/transact-async and related functionality from the datomic.api namespace? I haven't used Datomic in nearly a year. A lot seems to have changed.",
      "Datomic software needed no changes as a result of Jepsen testing. All functionality in datomic.api is unchanged.",
      "Congrats, that is a rare outcome!",
      "It struck me that Jepsen has identified clear situations leading to invariant violations but Datomic\u2019s approach seems to have been purely to clarify their documentation.  Does this essentially mean the Datomic team accepts that the violations will happen, but don\u2019t care?From the article:> From Datomic\u2019s point of view, the grant workload\u2019s invariant violation is a matter of user error. Transaction functions do not execute atomically in sequence. Checking that a precondition holds in a transaction function is unsafe when some other operation in the transaction could invalidate that precondition!"
    ],
    "link": "https://jepsen.io/analyses/datomic-pro-1.0.7075",
    "first_paragraph": "Datomic is a temporal Entity-Attribute-Value OLTP database which supports non-interactive transactions on top of pluggable storage engines. It offers a variety of query mechanisms across thick and thin clients, including Datalog, graph traversal, and an ODM-style API. We evaluated Datomic Pro 1.0.7075 and found its inter-transaction safety properties appear stronger than claimed. Not only was every history Serializable, but sessions bound to a single peer appear Strong Session Serializable, and histories restricted to write transactions and reads using d/sync appear Strong Serializable. However, inside of a transaction Datomic behaves as if operations were evaluated concurrently. Depending on how one interprets those operations, this might violate three of the most widely accepted formalizations of Serializability, each of which specify serial intra-transaction semantics. It also creates the potential for invariant violations when composing transaction functions. Datomic has published ",
    "summary": "<h1>Database Drama Delights: Datomic Decoded!</h1>\n<p>In an epic clash of technical verbosity, Jepsen dives deep into Datomic Pro 1.0.7075, discovering that\u2014surprise\u2014it behaves exactly as a complex, temporal database should! Expecting catastrophic failures and finding none, commenters oscillate between <em>awe</em> and <em>existential dread</em>, as they contemplate using this byzantine system to operate something as trivial as a bank. Meanwhile, Datomic's team celebrates their success in documentation gymnastics, skillfully dodging any real changes post the Jepsen gauntlet. Because, who needs fixes when you can redefine your problems away? \ud83c\udf89\ud83d\udc1b</p>"
  },
  {
    "title": "A 'plague' comes before the fall: lessons from Roman history (thebulletin.org)",
    "points": 64,
    "submitter": "diodorus",
    "submit_time": "2024-05-15T20:11:55",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=40371785",
    "comments": [
      "A fascinating insight for me came via Kyle Harper's book The Fate of Rome:  plagues co-evolve to match civilisations.<https://press.princeton.edu/books/hardcover/9780691166834/th...>That is, the Antonine, and other plauges of Rome relied on the dense urbanisation, highly-developed transportation, and long-range trading patterns, all of which served to provide the conditions in which pathogens which were either benign or simply too virulent to establish themselves in smaller, less-interconnected communities --- in such cases the epidemics would simply burn themselves out, perhaps decimating a village or small trading group of same, but not spreading further.The fact that Yersinia pestis seems to have evolved first in rodent populations (themselves often microcosms of human civilisations with large and interconnected population clusters) seems relevant.  IIRC Harper cites hamsters or gerbils as the prior host species to humans.James Burke, of Connections fame, stated in a later interview \"ReConnections* of how he'd extend the episodes of the original series that a key consequence of jet air travel would be an increased spread of global pandemics.  This occurs toward the end of this video:<https://archive.org/details/JamesBurkeReConnections_0>More broadly, it seems to me that any network will co-evolve parasite or pathogenic entities, whether we're talking human cities (look at the long list of urban vices, online abuse, network issues, and the like).  One factor contributing to the \"golden age\" effect is that such ages exist before the parasites / pathogens evolve.  As such, the sense that \"something went wrong\" seems to me misled, and a better formulation would be that such golden ages were living on borrowed time in the first place.Podcast episode of Harper addressing a similar theme from Plagues Upon the Earth:  <https://traffic.megaphone.fm/NBN2511895438.mp3>.",
      "They say you have to be lucky to get cancer statistically because you actually have to live a long time to even get it.I guess if you are several hundred year old empire you will have run into just about everything. The plague was just the last thing they saw, not the ultimate thing that ended them.We\u2019d be lucky if all this crap lasts long enough to watch a plague run through it.",
      "You sound just like a Roman.",
      "American, close enough. Our empire hopes at least.",
      "kids can get cancer though",
      "The Roman empire was based on slavery and had to die so that humanity could progress. Slave labor is vastly less efficient than having serfs that can keep (part of) what they produce and have incentives to +-work harder. That allowed producing more food with less workforce so many people were free to do other things and bigger cities could develop which in turn would set the foundation for making the industrial revolution possible.Everything else is just some weird romanticism of the antique when in reality many things got better in the medieval period. There wasn't a sudden fall of the Roman Empire. It was a gradual process and the structures coming after it saw themselves not as a replacement but as a continuation of Roman traditions.Edit: As people misunderstand. The important difference between serfdom and slavery is not freedom. The important part is incentives to work harder. Slaves only have the incentives to avoid the whip. They do not own what they produce. They need to be closely supervised and micromanaged. In contrast many forms of serfdom allowed to the serfs to keep some of the stuff they produced so they had incentives to work more efficiently.",
      "1. I don't think it's accurate to call Rome a slave based economy. Most people in Rome were freemen. Slaves served certain functions that would have been hard to replace with freemen, but I wouldn't say that makes Rome a slave based economy. For example, migrant laborers can't be easily replaced in the modern US economy, but I wouldn't say that the US economy based on migrant labor.2. Relating incentives to productivity is an anachronistic grafting of industrial and post-industrial economic thought onto a subsistence farming economy where it doesn't apply. If a serf works harder and produces more crop, what are they going to do with it? Sell it to their neighbor, who is also a farmer and grows all the same things? To the best of my knowledge, there was not a significant cash crop economy in medieval Europe.3. As others have pointed out, slavery continued long after the Roman Empire.  It was much more prevalent in the encomienda system and the antebellum South than it ever was in Rome. The proportion of enslaved people in mid 1700s South Carolina about 3 to 5x that of ancient Rome. And even that's nothing compared to the Caribbean.",
      "1. Rome is commonly characterized as a slave holding society. I think it is fair to say that it was based on slavery as getting new slaves was a big enough concern that it was willing to wage wars over it. It is hard to get estimates but something like 15% percent might have been slaves which is pretty huge. Of course saying it was only based on slavery would be too far.2. \nCities can not sustain themselves but need the existence of villages that would sell them the food.In the late medieval period farmers absolute would sell the surplus for money on the market. We also see also traveling merchants that would go from village to village to offer wares.> Sell it to their neighbor, who is also a farmer and grows all the same things?Some people in a village did not have land but were craftsmen like the iconic blacksmith.An yes it was a development and early medieval period ages the serf was probably just happy to have slightly more to eat. It didn't all happen over night. Villages started very self-sufficient. Basically producing everything they needed themselves while we see much more diversification of labor later on.3. I made a comment about the US South as well. An yes, history is not linear. There are just general tendencies.",
      "Slavery still exists today and I would even go as far as to say it is still widespread (random examples: in Chinese concentration camps/work camps, Chinese prisoners on fishing vessels, sex slaves globally, worker slaves in the Middle East but also certain parts of latin America for example).",
      "> The Roman empire was based on slavery and had to die so that humanity could progress.Making a statement with confidence doesn\u2019t make it true. You might want to provide substance for this statement as the rest of your comment is based on it."
    ],
    "link": "https://thebulletin.org/2024/05/a-plague-comes-before-the-fall-lessons-from-roman-history/",
    "first_paragraph": "",
    "summary": "In the never-ending quest to shoehorn modern buzzwords into ancient history, <em>The Bulletin</em> captures the imagination of pseudo-intellectuals by equating the fall of Rome with, wait for it... plagues! \ud83d\ude44 Readers and commenters alike leap at this \"revelation,\" wielding their half-digested book excerpts like gladiators with foam swords, breathlessly connecting pandemics to the fall of every civilization from Atlantis to Zanzibar. No historical stone is left unturned or untheorized, much to the delight of armchair epidemiologists and wannabe historians pontificating from the depths of their dizzying erudition. We all just can't wait for the next empire-breaking sniffle to prove us right once again! Because, you know, correlation is *totally* causation."
  },
  {
    "title": "SSD death, tricky read-only filesystems, and systemd magic? (rachelbythebay.com)",
    "points": 53,
    "submitter": "ingve",
    "submit_time": "2024-05-15T21:02:50",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=40372317",
    "comments": [
      "I know they mean well, but if you have a filesystem you want to preserve on questionable media... don't interact with (mount/repair) it until you've made your copy.With journaling and the like (soft-RAID)... mounting at all can induce more writes than you expect; yes, even read-only. That's probably not what you want. Consistency is key.'Just' (I know) use \"dd\", \"ddrescue\", or simply pv/cat/shell in+out redirects (some limitations apply) to evacuate it first. This gets you two things:    1) infinite tries: files/loop devices are cheap and can be copied for free\n    2) better odds: the new home isn't questionable, right?\n\nStop thinking in terms of devices. You can write this drive to a file. You can write files to drives. Same for reading/checksums. It's all made up, yet it works. Magic.By skipping past the block level to the filesystem, one is limited in what they can recover. You're at the mercy of it being more sane than truly necessary.By mounting it, and potentially/indirectly writing [with bad media/connectors/whatever], you may be irrevocably breaking the filesystem.Perhaps they get into this, but wow. I know I'm not 'The One'. I hate to pretend to be; but this is day one data-preservation stuff. I'm almost certain we've all taken compliance training that covers this.Cutting corners like this is pretending to be The One; you don't know better. Nobody is perfect, I'm on my soap box because I've done the same thing - and want to save others. It hurts.I had to post when I got to \"LOL it went RO, it's probably fine\" (my words)... when no, the kernel interjecting to protect the data is not a reasonable first line of defense.Aside from what they chose to do... damage well-and-truly could already be done by their inaction. Having left it running/hoping for re-allocation.I wish I could have as much faith as they've shown in what little I've read so far. I close my rant with this: a little mechanical sympathy goes a long way. Now back to reading.",
      "Right, you need at least some dm-linear device mapper device in between to ensure a device is RO I think. Also, ext4 has separate \"noload\" option that actually instructs it not to touch the disk (ofc relying solely on \"ro,noload\" to not mess up with your broken FS is a bad idea)",
      "> mounting read only can induce more writes than you expect.Have you got a source for that? I can only find speculation on various sites, but nothing that explicitly confirms that outstanding journal entries would be written at RO mount time.",
      "From the util-linux \"mount\" man page:\"Note that, depending on the filesystem type, state and kernel behavior, the system may still write to the device. For example, ext3 and ext4 will replay the journal if the filesystem is dirty. To prevent this kind of write access, you may want to mount an ext3 or ext4 filesystem with the ro,noload mount options or set the block device itself to read-only mode, see the blockdev(8) command.\"",
      "This seems to be the relevant behaviour from ext4: https://github.com/torvalds/linux/blob/8c06da67d0bd3139a97f3...",
      "Nothing readily available, can I be a source?The remaining nugget/memory on this I have is a fairly open ended... \"it depends [on the filesystem]\". It's been a while since I dove into this earnestly.IIRC there's no single journal mechanism - they all choose to do it (or not) in their own way.Journaling was just one example, too.Let's not ignore other quirks, like how BTRFS conflates volume/filesystem management... and may exhibit behavior impossible with XFS/ext{2,3,4} [under N failure conditions] WRT soft-RAID.By not mounting it, you're avoiding the \"kernel/software filter\" that adds entropy to an already-rich process.Side note: you caught me in a series of edits for phrasing :D",
      "> I know they mean well, but if you have a filesystem you want to preserve on questionable media... don't interact with (mount) it until you've made your copy.A further clarification of your point...If you are a wearing a forensics hat and engaged in formal recovery effort, your point is significant good advice.But when you are a regular joe trying to deal with a failing device, it's usually not clear when you've put on the forensics hat; you just find yourself struggling with a device that is not returning your data.And while you are in this moment of discovery the failure mode, the system is trying to do all its normal stuff, including maintaining the journal.So if playing the journal is a hazard to your data, it has likely already been encountered and any corruption as a result of journal processing been done.At some point the regular joe may decide to put on the forensic hat and formally rescue the device.In my experience, this is not necessarily the most productive next step. It depends on the device, its failure mode, the data and the usage scenario. Too many variables to generalize about tactics.No matter the value of your data, it's wise to begin a disciplined recovery as soon as possible. Where backups are the best place to start any recovery effort.However, once you decide to rescue the device \u2014 as opposed to nurse it along to gain access to something \u2014 that's the clear threshold to become scrupulous about read-only access: It's important to keep both the source and the destination devices in a read-only state from the beginning to end of the rescue to avoid further corruption of the copy.In this light, any journal processing that occurs for read-only mounts, presents a possibility hazardous condition with implications beyond the scope of casual system operation. This is the province of a proper forensics regime, with a rescue system configuration and protocol designed to prevent tainting the target, and also beyond the scope of a user comments on a help thread.Given all the other uncertainties, getting to the bottom of what a read-only mount actually does with the journal on your system at hand is probably not productive. But arranging for read-only mounts is given the uncertainty is certainly productive, so prevent auto-mounts while rescuing.If you want to use GNU ddescue,\nI've written a bash script to help with preventing auto-mount for Linux and macOS.github @c-o-pr ddrescue-helperBasically the script just writes /etc/fstab with the UUID of the filesystem and \"noauto\" option and unmounts the device. But there's other logic in the script that may be useful.",
      "There's nothing special about dd. You can, and should, simply use `cp`",
      "That's the least important part about my post. I mentioned several things, one being the shell you probably already have running, that all will do a perfectly fine job - to hopefully demonstrate that.'cp' is sparse aware. Yay. Is that a good thing, actually? If the source has zeroes, I want to write zeroes. Not sure... probably fine. I've been awake too long.'ddrescue' is special, but again, consistency is key. I don't really care how it's copied. I just question 'should' - there are advantages elsewhere.edit: My main gist is this - living off the land applies here, too! Whatever option is available. Make the copy, check the checksums. Pull out the bigger gun if necessary.Never know when you have to rely on some binary that's surviving only in caches. Stuck with a broken system and no live environment in sight.",
      "There is, however, something special about `ddrescue` (note that there are 2 different projects named thus; I don't remember the difference) - it does all the okay sectors, then goes back and retries bad sectors.Just be sure to store the ddrescue state file somewhere persistent."
    ],
    "link": "https://rachelbythebay.com/w/2024/05/15/ro/",
    "first_paragraph": "Software, technology, sysadmin war stories, and more.",
    "summary": "Once again, the tech wizards and armchair sysadmins convene on rachelbythebay to magically solve ancient IT curses like SSD death and filesystem calamities with the highly accredited \"poke it with a stick\" method. \ud83d\udcbb\ud83d\udd2e Watch as the masses enlighten us with their epic tales from the frontlines of badly beaten block devices, and debate whether to 'dd' or not to 'dd', as if the mere utterance of \"ddrescue\" can resurrect Lazarus from his data grave. Meanwhile, the comments section devolves into a hilarious mixture of tech elitism and unsolicited advice on read-only settings, sprinkled generously with the kind of textbook paranoia you\u2019d expect when someone accidentally opens a system32 folder. \ud83d\udcc2\ud83d\udca5 Amidst this existential pondering, remember it\u2019s all futile, as your data was probably doomed from the moment you bought that suspiciously cheap SSD from a pop-up ad. \ud83c\udf2a\ufe0f\ud83d\uddd1\ufe0f"
  },
  {
    "title": "Dragonfly: An optical telescope built from an array of off-the-shelf Canon lens (utoronto.ca)",
    "points": 141,
    "submitter": "fanf2",
    "submit_time": "2024-05-15T16:24:03",
    "num_comments": 60,
    "comments_url": "https://news.ycombinator.com/item?id=40369021",
    "comments": [
      "This kind of a setup of a large number of small, cheap detectors works well for observing diffuse objects with low surface brightness.  Generally speaking telescopes improve with size because larger telescopes can resolve smaller objects, so you can concentrate the light from your source into a smaller patch and increase the signal-to-noise with respect to the background. But once you have resolved the object (which doesn't require a very large diameter for a diffuse object) you no longer get any benefit from a larger telescope except for the greater light collecting power. So there's no benefit to having a single large mirror vs a large number of smaller detectors. Since it's a lot easier and cheaper to just buy a bunch of off the shelf components rather than build a large mirror from scratch that is what they did here.A friend of mine in grad school worked on a project that is similar in spirit called ASAS-SN. It also used off the shelf cameras but distributed them around the world so that they could detect supernovae and other transients. Because everything was off the shelf they could build out their network on a shoestring budget. I think they're the first to discover the vast majority of all bright supernovae these days.",
      "- \"diffuse objects with low surface brightness\"Are things amateur photographers with small telescopes (and lots of patience) sometimes discover,https://old.reddit.com/r/space/comments/13uco46/i_discovered... (\"I discovered this planetary nebula using a $500 camera lens, now it carries my name\")https://www.astrobin.com/i9yy6f/ (18 hours!)",
      "I was struggling to see the planetary nebula inside that blue circle before I realized that is the planetary nebula. cool!",
      "Somewhere there's a large, bright nebula in the shape of a red arrow no astronomer's ever noticed.",
      "https://en.wikipedia.org/wiki/Redshift-space_distortions",
      "I still don\u2019t understand how objects in space of that angular diameter are still being discovered. I would have to imagine lots of people have seen it, but just never chose to document or catalog it?",
      "This is an extremely long (18 hour) exposure in specialized narrowband spectral filters that have no usefulness for anything other than these particular targets.",
      "Oh nice!!!  I looked at a couple sky surveys in different, and it was nowhere to be found in their data, that\u2019s so cool!",
      "Can one use millions of smaller detectors if one finds a way to point them in one direction and synchronize them to take pictures at the same exact moment?I mean, can millions of phone cameras make one giant virtual telescope?",
      "Took me a while to understand what you meant. A phone camera already is millions of smaller detectors .... But I think you mean coordinating millions of people to all take photos of the same direction in the sky and then combining all the photos? I'm sure it can be done with an app and a way to build that crowd of users! But the field of view will still be huge because they're not telescopes/telephoto lenses."
    ],
    "link": "https://www.dunlap.utoronto.ca/instrumentation/dragonfly/",
    "first_paragraph": "main content begins",
    "summary": "<b>Dragonfly: A Tribute to Bargain Bin Astronomy</b>\nYou ever wonder what happens when you strap together a metric ton of Canon lenses and call it a day? Well, the University of Toronto sure did, crafting an \"optical telescope\" that's nothing more than a franken-array of off-the-shelf glass. Armchair astronomers in the comment section are waxing poetic about \"diffuse objects with low surface brightness,\" because if there\u2019s one thing more thrilling than using cheap tech, it\u2019s making it sound like rocket science. Between suggestions of turning a zillion iPhone cameras into the next Hubble and finding long-lost nebulae shaped like directions, it's a real showcase of making much ado about very little.\ud83d\udd2d\ud83d\udcab"
  },
  {
    "title": "Show HN: Open-source BI and analytics for engineers (github.com/quarylabs)",
    "points": 166,
    "submitter": "louisjoejordan",
    "submit_time": "2024-05-15T14:02:35",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=40367090",
    "comments": [
      "Side comment: what an interesting landing page it has. That Slack CAT button right within the fold is a good idea. A walkthrough and a way to schedule a meeting with the founders. This is very straightforward. Good luck!",
      "Hey! OP here. This made my day, thank you!",
      "All these comments ask for comparisons. It might be worth creating some alternative pages like podia do [1]. It could be helpful for your growth.Seems like a cool project![1] https://www.podia.com/podia-alternatives",
      "Hey! OP here. This is really good feedback thank you.",
      "Seems similar to plotly dash, no?",
      "How does it differ from OpenDashboard?",
      "Does it support datasource merges like redash do? I had hard time looking for simple solution where I could easily join data from multiple sources and provide simple charts from engineering to support teams.",
      "We do if you use DuckDB and you pull data from your data sources through DuckDB. DuckDB can act as a single interface between multiple data source types. Feel free to DM me with any more questions. around your specific use-case and I can help.",
      "This would make a good blog tutorial, I think.",
      "From an external look, that sounds a lot like what dbt is meant to be. Why would one choose quary over dbt?"
    ],
    "link": "https://github.com/quarylabs/quary",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "Welcome to the latest episode of Hacker News Theater, where today's spectacle showcases an open-source BI tool that's as \"revolutionary\" as a new Slack notification sound. \ud83d\udc68\u200d\ud83d\udcbb\ud83c\udf89 The comment section, a veritable bastion of original thought, bursts with enthusiasm over a shiny Slack CAT button and strategies blatantly ripped from every startup guide ever written. As commenters trip over each other to offer comparisons with every BI tool of the past decade, the project's creator nods vigorously at anything resembling praise. Strap in as we witness the fierce battle for validation, with bonus points for using the most buzzwords in a single feedback post! \ud83d\udcca\ud83d\ude80"
  },
  {
    "title": "Starting emails with \"BEGIN PGP MESSAGE\" will fool the filter (nondeterministic.computer)",
    "points": 96,
    "submitter": "ColinWright",
    "submit_time": "2024-05-15T16:32:08",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=40369119",
    "comments": [
      "There is a special place in Hell for people who \"helpfully\" rewrite email bodies in transit.",
      "In my experience, the vast majority of corporate mail filters ban certain file types based on name extensions.Fewer, but some, inspect files to deduce their type.None care about encrypted zips with the file renamed to a common extension (encrypted zip manifests are unencrypted, so the file names are still visible).",
      "> Fewer, but some, inspect files to deduce their type.I don\u2019t know. That\u2019s extremely error-prone, often easy to fool, and in some cases hardly possible in the first place.I think the only thing that\u2019s really feasible is filtering out known bad things as some mild form of damage reduction.",
      "And many people have learned to bypass these filters by renaming extensions. You can always zip things up or just rename foo.py to foo.py.pdfBut I understand that there is still reason to filter filetypes. Apparently some programs will run programs if they see certain filetypes... Here's a recent telegram exploit where the user did have to click on the file https://www.bitdefender.com/blog/hotforsecurity/telegram-pat...",
      "Yes, it isn't just voodoo, \"properly\" labelled file types can carry dangers that \"improperly\" labelled ones do not. For example, if someone wants you to open a Word document with a bad macro, getting you to open it may be no big deal, but getting someone to \"OK, first save it, then navigate to it with Explorer, then change Explorer to 'Show Extensions', then rename it to this, then open it\" is likely to either set off some alarm bells, or simply be impossible for the technically-unsophisticated target.Even if it is the same bytes nominally behind the \"improper\" and \"proper\" metadata labels the security profile of the two bits of content can still be very different.Also, obviously, you'll always be able to get things \"through\" a filter like this. But the value of raising the bar of the exploit is still quite substantial; the \"conversion funnel\" for such exploits has a very sharp dropoff at every step, including even the first (most such attempts at an exploit even if delivered would not be unpacked by the target user).Systems can generally block encrypted archives, though I suspect that many admins end up leaving that \"off\". I'm not sure it's a huge vector in the real world. My impression is that at the moment the most dangerous emails are the social attacks. Though the technical attacks are still non-trivial, still hit people, and technical folks can underestimate the need for non-technical folks to be protected from them.",
      "> obviously, you'll always be able to get things \"through\" a filter like this. But the value of raising the bar of the exploit is still quite substantialI just want to stress this part.So many people I talk to will just dismiss things because something isn't bullet proof. Like there's a binary option. But in reality there's a continuum. I'm the annoying person that tries to get my friends to use Signal, but then say if you won't install, that WhatsApp is my next preference. People on Signal forums will say that you shouldn't have the ability to delete or nuke conversations (now you can delete some, but only if <3hrs old) BECAUSE you can't guarantee the message content wasn't copied. Which is just fucking insane. It's not incorrect, but you have to think of things probabilistically and security is about creating speedbumps, not bullet proof vests. It is standard practice in many industry settings to remotely wipe a device (and then operate under the assumption that the data was leaked) because if you don't, adversaries have infinite time to copy that data rather than finite.In most things, there are no perfect solutions. We have to think probabilistically and the tradeoffs for different environments (which are dynamic). Trying to make perfect solutions are not only unachievable, but even if they were they wouldn't last for long.",
      "It\u2019s wild to me that this has been the eventual consequence of file extensions.MS decided that they were too advanced and hid them by default, thousands of companies tried to do automagic things instead of pushing for people to understand extentions, and inevitably the automagic stuff introduced exploits that were far worse than that education.",
      "\u201cPushing for people to understand extensions\u201d only does so much. So many file types are turing complete, and whatever runs them has access to a varying set of resources. That may be intentional, unintentional, by design, or through vulnerabilities.What you need is proper sandboxing of the consuming applications, allowlisting of those applications (instead of \u201cfile types\u201d with unspecified client applications), and ideally some type of trust system on top (but we all know how little acceptance stuff like PGP or even S/MIME has).In other words: It should be safe for people to open any attachment they get. Think about web browsers: Heavy-hitting vulnerabilities aside, almost every web page you visit is safe for consumption by your computer, because of the browser\u2019s security model. Same with iOS apps.The remaining risk is addressed by provenance/trust.",
      "There's always just prefacing with magic bytes. :)https://en.m.wikipedia.org/wiki/List_of_file_signatures",
      "This reminds me of working at a company in Brussels during eBays heydays. Their URLs looked like http://offer.ebay.com/ws/eBayISAPI.dll?...And the filter saw .dll and denied my request."
    ],
    "link": "https://nondeterministic.computer/@martin/112444389342113780",
    "first_paragraph": "",
    "summary": "**Title:** How to Sneak Past Email Filters with Ancient Cryptographic Voodoo\n\nIn yet another groundbreaking discovery from the halls of \"nondeterministic.computer,\" tech wizards have unearthed the secret spell: start your emails with \"BEGIN PGP MESSAGE\" to make the corporate mail filters surrender. Watch in awe as code enthusiasts and weekend hackers banter about on forums, competing for the most convoluted way to bypass a system function that... checks file names? That's right, daring adventurers: by renaming your deadly \".exe\" files to the more innocuous \".txt,\" you, too, can teach an old mail filter new tricks! Meanwhile, the comment section erupts in a catastrophic blend of paranoia and technical righteousness, a digital Tower of Babel where everyone speaks in code and nobody agrees. \ud83c\udfa9\u2728 Just remember, if something breaks, it was probably just a typo in your incantation."
  },
  {
    "title": "Apple announces new accessibility features, including eye tracking (apple.com)",
    "points": 284,
    "submitter": "dmd",
    "submit_time": "2024-05-15T14:22:05",
    "num_comments": 160,
    "comments_url": "https://news.ycombinator.com/item?id=40367331",
    "comments": [
      "Imagine an eye-tracking loupe function on mobile: fit the same amount of text but bubble up the part under foveolar gaze. Save on readers* everywhere.* readers are those glasses you can pick up at the drug store for $17.99.",
      "Accessibility is for everyone, including you, if you live long enough. And the alternative is worse. So your choice is death or you are going to use accessibility features. \u2013 Siracusa",
      "See also https://en.wikipedia.org/wiki/Curb_cut_effect",
      "I aimed for the upvote button but they\u2019re so tiny that my fat finger hit the downvote button by accident and then I had to retry the action. This is what people mean by accessibility is for everyone all of the time.",
      "I have a tremor and I run into this issue on HN all the time. I need to zoom in a lot to be sure I'll hit it.",
      "I don\u2019t (yet) have accessibility challenges beyond glasses but hitting the tiny arrows is incredibly difficult. How come HN doesn\u2019t update to be more accessible? It\u2019s been a long time\u2026 I\u2019m surprised it hasn\u2019t been talked about by the team there.",
      "As someone who has carried out accessibility audits, I can unfortunately attest to this topic being a blindspot in tech circles. I remember hanging out with fairly senior frontend devs from a FAANG company who didn't know what purpose skip links served on websites. It can also be an uphill battle to advocate for remediation once design and development work is already baked in.",
      "Do you think Paul Graham or Garry Tan give a shit about accessibility?",
      "No.",
      "Try https://www.modernhn.com if you haven't already. UI elements have more spacing around them, especially if zoomed in."
    ],
    "link": "https://www.apple.com/newsroom/2024/05/apple-announces-new-accessibility-features-including-eye-tracking/",
    "first_paragraph": "Text of this article",
    "summary": "In a thrilling display of benevolence, Apple graces us mortals with new \"eye-tracking\" goodies and other magical widgets that allow the plebs to operate their iDevices with marginally less inconvenience. In an eye-opening expos\u00e9 of tech illiteracy, commenters stumble over themselves in a humblebrag-athon about who can misinterpret accessibility features most egregiously. One visionary soul wistfully imagines a utopia where the effort needed to click an upvote button doesn't equate to scaling Mount Everest. Meanwhile, tech\u2019s luminaries presumably nod sagely from their inaccessible ivory towers, blissfully unaware of what \"skip links\" are\u2014or simply not giving a flying fig. And thus, the circle of tech \"progress\" rolls on, ever inclusive, ever elusive. \ud83d\ude44\ud83d\udc94"
  },
  {
    "title": "Show HN: Tarsier \u00e2\u20ac\u201c Vision utilities for web interaction agents (github.com/reworkd)",
    "points": 136,
    "submitter": "KhoomeiK",
    "submit_time": "2024-05-15T16:46:15",
    "num_comments": 50,
    "comments_url": "https://news.ycombinator.com/item?id=40369319",
    "comments": [
      "Reminds me of [Language as Intermediate Representation](https://chrisvoncsefalvay.com/posts/lair/) - LLMs are optimized for language, so translate an image into language and they'll do better at modeling it.",
      "Cool connection, hadn't seen this before but feels intuitively correct! I also formulate similar (but a bit more out-there) philosophical thoughts on word-meaning as being described by the topological structure of its corresponding images in embedding space, in Section 5.3 of my undergrad thesis [1].[1] https://arxiv.org/abs/2305.16328",
      "Congratulations on shipping!In https://github.com/OpenAdaptAI/OpenAdapt/blob/main/openadapt... we use FastSAM to first segment the UI elements, then have the LLM describe each segment individually. This seems to work quite well; see https://twitter.com/OpenAdaptAI/status/1789430587314336212 for a demo.More coming soon!",
      "Looking at OpenAdapt, I'm wondering why they didn't integrate Tarsier into AgentGPT, which is their flagship github repo but doesn't seem to be under active development anymore.",
      "We have a lot more powerful use-cases for Tarsier in web data extraction at the moment. Stay tuned for a broader launch soon!",
      "A few questions:Does this work in headless mode?Are you getting a screenshot of the whole webpage including scrolling? Or just the visible part. The whole page, like singlepage.js would be great and is much more useful in many circumstances, although I'm not sure sure how to handle infinite scrolling. (If not, clean simple APIs for scrolling that don't require fiddling and experimentation would be great.)Instead of Google OCR (the only OCR), what about Apple's native OCR? That would be amazing.",
      "Great work guys! How did you benchmark traiser's 10-20% better? Would love to see exactly how each method scored",
      "How do you make sure the tagging of elements is robust? With regular browser automation it's quite hard to write selectors that will keep working after webpages get updated; often when writing E2E testing teams end up putting [data] attributes into the elements to aid with selection. Using a numerical identifier seems quite fragile.",
      "Totally agreed\u2014this is a design choice that basically comes from our agent architecture, and the codegen-based architecture that we think will likely proliferate for web agent tasks in the future. We provide Tarsier's text/screenshot to an LLM and have it write code with generically written selectors rather than the naive selectors that Tarsier assigns to each element.It's sort of like when you (as a human) write a web scraper and visually click on individual elements to look at the surrounding HTML structure / their selectors, but then end up writing code with more general selectors\u2014not copypasting the selectors of the elements you clicked.",
      "Ooh that's a very neat approach, great idea! Chains of thought across abstraction layers. Definitely worth a blog post I reckon.Good luck!"
    ],
    "link": "https://github.com/reworkd/tarsier",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.",
    "summary": "**HN Launches Yet Another Tool for the Tiny Overlap of Linguists and Web Developers**\n\nAh, the constant garden of Eden for the self-congratulatory tech elite - Hacker News unveils <em>Tarsier</em>: the latest in squinting really hard at web pages until they turn into semi-useful code. A forum member quickly ties it to some obscure thesis about language and images, inadvertently revealing more about his thesis advisor's patience than about the tool itself. Others suggest integrating this tool with every other half-baked project under the sun while hypothesizing about \"broader launches\" and other euphemisms for \"please notice us.\" Meanwhile, commonplace questions about functionality get lost amid boasts of how this tool teeters on the edge of reinventing how we poorly automate web interactions. \ud83d\ude02\ud83d\udc53\ud83d\udd78\ufe0f"
  },
  {
    "title": "Show HN: I made a Mac app to search my images and videos locally with ML (desktopdocs.com)",
    "points": 66,
    "submitter": "correa_brian",
    "submit_time": "2024-05-15T19:44:46",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=40371467",
    "comments": [
      "Just a heads up, im in the US, and when I click the link to buy, the price is $24.99I'd personally love it if this also indexed text files and pdfs.",
      "Can this recognize individuals? I really need something that can index images by which ones have prepend vs prepend\u2019s kid or whatever.",
      "Would you consider a trial that can ingest 100 images? I\u2019d like to try it before buying.",
      "Just a legal question. Can it really be non refundable? Aren\u2019t there markets where regulators would require for you to refund?",
      "Is OP in those markets?I think practically it doesn\u2019t matter. This is two people selling something for $20 (or maybe $50, I can\u2019t tell). They tell you beforehand, so you know. It\u2019s unlikely they operate in jurisdictions that force software to be refundable. So I guess you can sue them to get your money back. Or chargeback through your credit card.This is why we can\u2019t have nice things. I miss the old internet where it was just people sending small amounts of money to other people for cool things (people mailed me checks in 1996). And users didn\u2019t have the expectation of legal expenses to account for unlikely edge cases.",
      "I'd love it if someone paid me with a check.Just trying to build cool stuff and put it out there into the world.",
      "Not sure how much low-level filesystem access you'd need, but it would be cool to support adding metadata via tags, a lahttps://github.com/jdberry/tag/",
      "Adding on to this, it would be cool if it operated directly on the filesystem rather than having to first add everything to an intermediate \"library\". It's kind of a minor thing but definitely a pet peeve of mine when apps graft their own concept of a \"library\" onto my own perfectly working filesystem.Demo looks neat though. I wonder if it can tell me which of my video files are SD, HD, and 4K. I've ripped so much media that I've lost track and did not name my files in such a way that it's obvious what resolution each one is. Something like that probably doesn't even need AI, just a peek at the existing metadata.",
      "I use get-video-properties python library for metadata harvesting. Works great",
      "Some feedback - for me (and probably for the HN crowd) saying it's powered by \"AI\" takes away credibility from what otherwise seems like a reasonable project.My first impression was that you'd \"just\" upload my pictures to OpenAI with a prompt and call it a day.Maybe highlight that it uses ML running locally? (I see that it's in the FAQ,  but in the title)"
    ],
    "link": "https://desktopdocs.com",
    "first_paragraph": "Desktop Docs is the all-in-one platform to browse, edit, and export your media files.",
    "summary": "**Show HN: I Reinvented File Explorer With AI Glitter**\n\nIn what can only be described as a groundbreaking rehash of existing technology, a brave Hacker News solo-preneur has unveiled yet another \"revolutionary\" app, <em>DesktopDocs</em>, that promises to change how we browse media files - now with extra unnecessary machine learning! HN commenters fall over themselves hypothesizing edge cases and proposing \u2018vital\u2019 features like indexing their extensive PDF collections of forgotten academic papers. One grizzled vet longs for the simpler times of \u201896, when men were men and software refunds didn\u2019t exist. Meanwhile, the skeptical techie laments the misuse of the term AI, deeply concerned that it might actually just be ordinary programming dressed up for the VC ball. \ud83c\udfa9\ud83e\udd16\ud83d\udcbe"
  },
  {
    "title": "Prototypal Inheritance (2008) (crockford.com)",
    "points": 4,
    "submitter": "Tomte",
    "submit_time": "2024-05-13T08:29:02",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "https://crockford.com/javascript/prototypal.html",
    "first_paragraph": "",
    "summary": "In a breathtaking display of intellectual hubris, Crockford once again attempts to enlighten the teeming hordes of JavaScript developers about prototypal inheritance, a concept that every CompSci dropout insists they mastered right after \"Hello, World.\" As per usual, the comment section morphs into a battleground where self-proclaimed experts compete to see who can misunderstand the material with the most <em>panache</em>. The sheer irony of programmers trying to inherit knowledge, without cloning the fundamental understanding, is evidently lost on all involved. If there\u2019s ever an apocalypse for common sense in programming, it\u2019s clear these commenters will be the first to go. \ud83d\ude44"
  },
  {
    "title": "It's 2024 and drought is optional (asteriskmag.com)",
    "points": 30,
    "submitter": "vwoolf",
    "submit_time": "2024-05-15T20:36:39",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=40372071",
    "comments": [
      "Leftover brine is really a problem thoughhttps://www.wired.com/story/desalination-is-booming-but-what...",
      "Yeah I don't think you can just dump it in the top end of the gulf of california, especially if that's where you're sourcing feedwater.",
      "Pump it into the Salton Sea.",
      "Huh; so desalinated water costs between $0.001 to $0.008 depending on the cost of the power.I pay about $0.0136 / gallon so almost 10x the cost of producing.Seems like (for non-agricultural use) we will be able to afford desalination -- I guess the real issue will be agricultural uses which are predicated on free water.",
      "Making desalinated water is relatively cheap. You can do it with a clear tarp and sunlight. Desalination at scale is less cheap. Almost all water is cheap.One (there are several) problem with desalination is that it often has to compete with free.In many parts of North America the water is actually free. Like New York City-- their water falls from the sky in upstate NY, for free, into reservoirs that are not free to maintain to be distributed by colossal public works projects which are 100% not free.Pumping water, desalinated or not, to your house is not free.Whoever is charging you $0.0136/gal for water may indeed (especially if you don't live in an arid region with strange water rights laws) be charging you $0.0136/gal to pump water to your house and $0.0000/gal for the actual water.If they switch to a desalinated source it is highly likely they will charge you $0.0136/gal + \u2265$0.0010/gal = \u2265$0.0146/gal.Of course, if you live near a source of salt water you can cut out the middleman and, after massive up-front capital outlays, desalinate your own water for \u2265$0.001/gal assuming you live long enough to recoup the cost of the initial expense.",
      "Love the vision. Feels like something that can\u2019t get done and isn\u2019t proposed as viable- if a RE developer tried to do it for profit, they\u2019d get shut down - ironically probably by environmental groups- govt doesn\u2019t actually build anything audacious anymoreUnless I\u2019m missing something?",
      "> Los Angeles, a thriving metropolis with a population and economy larger than Australia\u2019sWhen did LA become a city of 26 million?",
      "Australia's population is now meant to be 27 million.https://www.abc.net.au/news/2024-01-24/australias-population...",
      "California's population is larger than Australia's. But yeah, LA's specifically is about half (give or take)",
      "This gives Southern California including LA a listing as a single supercity population 24.4 million which is close but not quite there.https://en.wikipedia.org/wiki/List_of_megalopolises"
    ],
    "link": "https://asteriskmag.com/issues/06/its-2024-and-drought-is-optional",
    "first_paragraph": "In the early 20th century, the United States diverted and dammed nearly every major river that runs through the West, ushering in an era of unparalleled dominion of water. Today, California once again struggles with water scarcity \u2014 but solar energy could change all that.",
    "summary": "The usual gang of <em>self-appointed futurists</em> at asteriskmag.com have decreed droughts \"optional\" now, thanks to the magic of solar-powered desalination. It\u2019s 2024, and rather than considering sustainable usage or climate action, apparently, we can just engineer our way out of environmental crises! In the comments, armchair experts juggle the ethics of dumping brine into the Gulf of California and ponder the grave issue of paying 1.36 cents per gallon for water. Meanwhile, a delightful side debate rages about whether LA is as big as Australia, because population figures are clearly as easy to manipulate as our precious natural resources. \ud83d\ude02"
  },
  {
    "title": "Making a Postgres query 1k times faster (mattermost.com)",
    "points": 106,
    "submitter": "d0mine",
    "submit_time": "2024-05-15T21:00:02",
    "num_comments": 34,
    "comments_url": "https://news.ycombinator.com/item?id=40372296",
    "comments": [
      "OR Queries are performance killers because they often force a full table scan. Another alternative to the Or is actually a UNION.A UNION allows you to use two separate indexes and can speed up queries.",
      "This seems like the type of thing that a sophisticated query planner in 2024 would be able to figure out on its own? Maybe as a non database expert I'm expecting too much?",
      "If at all possible, use a union all rather than a plain union to avoid an extra sort / unique node.I've used that OR > UNION ALL trick a number of times to vastly improve performance on specific queries. It's crazy how much of an effect it can have.I wish Postgres would implement a planner optimization to automatically run queries with an OR more efficiently (e.g. use the same plan as with a UNION/ALL where possible).",
      "This can be mitigated somewhat by not having ORs at top level.Of course UNION [ALL] does often yield better performance, yes.",
      "I generate queries like this in a paging library I wrote at work, I did not use the tuple comparison because sometimes the sorting order is different between fields, each field in the tuple comparison would need a different operator, such asWHERE Posts.CreateAt > ?1 OR (Posts.CreateAt = ?1 AND Posts.Id < ?2) ORDER BY Posts.CreateAt ASC, Posts.Id DESCI now wonder if it might be possible to use a negation operator to invert the sorting order, like:(Posts.Id, -Posts.CreateAt) < (?1, ?2) ORDER BY Posts.CreateAt ASC, Posts.ID DESCand keep the performance improvement.",
      "Not at all surprised that Laurenz Albe played a key role in figuring this out -- I've learned so much about postgres from his Stack Overflow answers over the years.",
      "FTA: \u201cWhy the query planner doesn\u2019t automatically convert a condition like x > a OR (x == a AND y > b) to (x, y) > (a, b) is something I still don\u2019t understand to this day, though.\u201dThere\u2019s a difference between the two. https://www.postgresql.org/docs/current/sql-expressions.html...:\u201cThe order of evaluation of subexpressions is not defined. In particular, the inputs of an operator or function are not necessarily evaluated left-to-right or in any other fixed order.Furthermore, if the result of an expression can be determined by evaluating only some parts of it, then other subexpressions might not be evaluated at all. For instance, if one wrote:    SELECT true OR somefunc();\n\nthen somefunc() would (probably) not be called at all. The same would be the case if one wrote:    SELECT somefunc() OR true;\n\nNote that this is not the same as the left-to-right \u201cshort-circuiting\u201d of Boolean operators that is found in some programming languages.\u201dhttps://www.postgresql.org/docs/current/functions-comparison...:\u201cFor the <, <=, > and >= cases, the row elements are compared left-to-right, stopping as soon as an unequal or null pair of elements is found.\u201dSo, this is using something akin to short-circuiting.I think that can mean the two give different results in the presence of null values.\u21d2 I would guess the optimizer isn\u2019t smart enough to detect when the second, stricter query will be equivalent and faster.",
      "My question after a brief look - why not just use ID instead of CreateAt?",
      "Maybe I'm sleepy, but what is doing elasticsearch in that post?",
      "consuming the data returned by the query"
    ],
    "link": "https://mattermost.com/blog/making-a-postgres-query-1000-times-faster/",
    "first_paragraph": "",
    "summary": "In an epoch-defining revolution of SQL management, a Mattermost miracle worker has heroically managed to accelerate a Postgres query from glacial to merely sluggish. Commenters, erupting in ecstasy, exchange their bewildering \"OR to UNION\" spells like Hogwarts students in the Forbidden Section. Meanwhile, one brave soul dares to dream the impossible: a world where Postgres's query planner isn't outsmarted by a mid-level software developer's hastily drafted script. \ud83d\ude80\ud83e\udd2f Experts in the lore of SQL optimization gather to mourn the sorrowful state of the Postgres planner, quietly forgetting that none of them remember to index their databases properly in the first place."
  },
  {
    "title": "An Empirical Evaluation of Columnar Storage Formats [pdf] (vldb.org)",
    "points": 47,
    "submitter": "eatonphil",
    "submit_time": "2024-05-15T18:29:36",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=40370613",
    "comments": [
      "Third, faster and cheaper storage devices mean that it is better to use faster decoding schemes to reduce computation costs than to pursue more aggressive compression to save I/O bandwidth. Formats should not apply general-purpose block compression by default because the bandwidth savings do not justify the decompression overhead.\n\nNot sure I agree with that. Have a situation right now where I am bottlenecked by IO and not compute.",
      "My point is near the opposite. Data formats should apply lightweight compression, such as lz4, by default because it could be beneficial even if the data is read from RAM.I have made a presentation about it: https://presentations.clickhouse.com/meetup53/optimizations/Actually, it depends on the ratio between memory speed, the number of memory channels, CPU speed, and the number of CPU cores.But there are cases when compression by default does not make sense. For example, it is pointless to apply lossless compression for embeddings.",
      "Last I checked you can't get much better than 1.5GB/s per core with LZ4 (from RAM), up to a maximum ratio < 3:1, and multicore decompression is not really possible unless you manually tweak the compression.The benchmarks above that are usually misleading, because they assume no dependence between blocks, which is nuts. In real scenarios, blocks need to be parsed, depend on their previous blocks, and you need to carry around that context.My RAM can deliver close to 20GB/s, and my SSD 7GB/s, and that is all commodity hardware.Meaning unless you have quite slow disks, you're better off without compression.",
      "> Last I checked you can't get much better than 1.5GB/s per core with LZ4you can partition your dataset and process each partition on separate core, which will produce some massive XX or even XXX GB/s?> up to a maximum ratio < 3:1this is obviously depends on your data pattern. If it is some low cardinality IDs, they can be compressed by ratio 100 easily.",
      "> you can partition your dataset and process each partition on separate core, which will produce some massive XX or even XXX GB/s?Yes, but as I mentioned:> multicore decompression is not really possible unless you manually tweak the compressionThat is, there is no stable implementation out there that does it. You will have to do that manually and painfully. In which case, you're opening the doors for exotic/niche compression/decompression, and there are better alternatives than LZ4 if you're in the niche market.> this is obviously depends on your data pattern. If it is some low cardinality IDs, they can be compressed by ratio 100 easily.Everything is possible in theory. Yet we have to agree on what is a reasonable expectation. A compression factor of around 3:1 is, from my experience, what you would get from a reasonable compression speed on reasonably distributed data.",
      "> Yes, but as I mentioned\n> multicore decompression is not really possible unless you manually tweak the compressionI don't understand your point. Decompression will be applied on separate partitions using separate cores the same way as compression..> Yet we have to agree on what is a reasonable expectation. A compression factor of around 3:1 is, from my experiencewell, my prod database is compressed by ratio 7 (many hundreds billions IDs).",
      "This is extremely common in genomics settings, and in the past I have spent far more time allocating disk iops, network bandwidth, and memory amounts for various pipeline stages than I have on CPUs in this space.  Muck up and launch 30x as many processes as your compute node has, and it's fairy fixable, but muck up the RAM allocation and disk IO and you may not be able to fix it in any reasonable time. And if you misallocate your network storage, that can bring the entire cluster to a halt, not just a few nodes.",
      "I think the idea is that you should design tools and pipelines to take advantage of current hardware. Individual nodes have more CPU cores, more RAM, and more and faster local storage than they used to. Instead of launching many small jobs that compete for shared resources, you should have large jobs that run the entire pipeline locally, using network and network storage only when it's unavoidable.",
      "That is exactly right, and optimizing for the current distribution of hardware is always the case; however most interesting problems still do not fit on a single node. For example, large LLMs that whose training data, or sometimes even model itself, do not fit on a single node. Lots of the same principles of allocation show up again.",
      "You mentioned genomics, and that's a field where problems have not grown much over time. You may have more of them, but individual problems are about the same size as before. Most problems have a natural size that depends on the size of the genome. Genomics tools never really embraced distributed computing, because there was no need for the added complexity."
    ],
    "link": "https://www.vldb.org/pvldb/vol17/p148-zeng.pdf",
    "first_paragraph": "",
    "summary": "The academics have unleashed yet another riveting PDF about columnar storage formats on VLDB, ensuring that the Carpocalypse of data storage debate continues unabated. Commenters, armed with their personal use-cases and anecdotal evidence, heroically miss the forest for the trees, eagerly citing their one-off situations where LZ4 gave them a 2.8% performance boost on their custom-built rigs. Amidst this joust of jargon, someone even manages to drop a *ClickHouse* presentation link, possibly mistaking VLDB for a tech meetup. As they artfully dance around the actual use of multi-core decompression, the rest of us wonder if it's just easier to buy more RAM or maybe a new laptop. \ud83c\udf7f"
  },
  {
    "title": "When to Split Patches (For PostgreSQL) (eisentraut.org)",
    "points": 19,
    "submitter": "todsacerdoti",
    "submit_time": "2024-05-14T07:31:50",
    "num_comments": 0,
    "comments_url": "",
    "comments": [],
    "link": "http://peter.eisentraut.org/blog/2024/05/14/when-to-split-patches-for-postgresql",
    "first_paragraph": "May 14, 2024",
    "summary": "On eisentraut.org, PostgreSQL enthusiasts tackle the monumental question of when to fragment their patches as if balancing the very fabric of digital civilization. The author earnestly dissects this riveting topic with the clinical excitement of watching paint dry, ensuring readers are well-equipped to ponder splitting patches at their next insomniac convention. Below, the obligatory chorus of commenters engage in a melee of pedantic one-upmanship, debating nuances invisible to the human eye and vital to absolutely no one. A thrilling escapade through the wilds of database management, destined to be bookmarked and forgotten. \ud83d\udcda\ud83d\udd0d\ud83d\udca4"
  },
  {
    "title": "Qualcomm's Oryon LLVM Patches (chipsandcheese.com)",
    "points": 15,
    "submitter": "ingve",
    "submit_time": "2024-05-15T21:02:30",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=40372315",
    "comments": [
      "sadly the size of the L1D is not said in this patch\n\nThis is because SchedMachineModel doesn't have an entry for cache sizes. However, if you have an actual CPU to run benchmarks on, it's pretty straightforward to empirically determine cache sizes with lscpu -C. This should list L1D, L1I, L2 and L3 cache sizes.",
      "1. Still ARMv8.2. \"*Nuvia Inc* Oryon processors\"",
      "Is Apple M4 considered ARMv8 or ARMv9?  Does it have MTE enabled?",
      "I don't think that's known yet. But M3 was ARMv8.6-A and Apple's been keeping up nicely.If Oryon is ARMv8.0-A then that's quite a disappointment. The LSE instructions are incredibly nice."
    ],
    "link": "https://chipsandcheese.com/2024/05/15/qualcomms-oryon-llvm-patches/",
    "first_paragraph": "",
    "summary": "In the latest riveting drama from the world of ones and zeroes, Qualcomm attempts to dazzle the eight people who still care with the introduction of Oryon LLVM patches. The real cliffhanger here: the <em>mystery of the missing L1D cache size</em>. Commenters are losing sleep, performing digital necromancy with `lscpu -C` to summon forth this arcane knowledge. Meanwhile, ARM architecture nuances stimulate heated debates akin to arguing over whether stale bread makes a better weapon or a crouton. It\u2019s high-octane excitement\u2014if your engine runs on mundane tech specs and unresolved CPU lore. \ud83c\udfad\ud83d\udcbb"
  }
]