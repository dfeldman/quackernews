[
  {
    "title": "When was the famous \"sudo warning\" introduced? Under what background? By whom? (retrocomputing.stackexchange.com)",
    "points": 22,
    "submitter": "Boogie_Man",
    "submit_time": "2024-12-02T00:08:38 1733098118",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42291781",
    "comments": [
      "It\u2019s a funny artifact because most people today use, and have only used Linux in a \u201csingle user\u201d context. The \u201clocal system administrator\u201d is me!\n \nreply",
      "I often give myself stern talking tos when I made ignorant sysadmin decisions\n \nreply",
      "... and of those who have used multi-user Unix systems, very few would have had the privilege to use sudo in that context. (Unless you count managing a family computer, and even that is likely rare these days given that everyone seems to have their own machine.)\n \nreply",
      "And that's why we should1) switch from using \"sudo some-command --some-argument\" to \"some-command --some-argument\" in which some-command authenticates and elevates via polkit, and2) configure polkit to allow the initial human user of the machine to elevate without typing a password.#2 is just a default, and special configurations can of course override it and return a configuration much like what we have today.But Jesus Christ Almighty Batman, if I'm installing Ubuntu on my laptop and I, as my regular user, want to install audacity, I shouldn't have to re-enter my password!\n \nreply",
      "Not sure what the connection with Colorado U might be but I read this\n[0] a couple of days ago. Steve Bellovin had a few connections with\nformative Unix security and worked/studied with Fred Brooks and Ken\nThompson, In the (linked) talk he says that Brooks taught him \"about\nresponsibility. Someone with the root password has not just the right\nbut the responsibility to use it.\" This caught my attention because\nI'm writing a series \"Technology: rights or responsibilities?\" Maybe\nthere's a thread of influence there?[0] https://www.schneier.com/blog/archives/2024/11/steve-bellovi...\n \nreply"
    ],
    "link": "https://retrocomputing.stackexchange.com/questions/12521/when-was-the-famous-sudo-warning-introduced-under-what-background-by-whom",
    "first_paragraph": "",
    "summary": "Hark! The mighty dwellers of Retrocomputing Stack Exchange have unearthed ancient wisdom about the \"sudo warning,\" sending shockwaves through the hermetic echo chamber where everyone simulates multi-user chaos on their lonely, single-user Linux installs. First up, the \"local system administrator\" \u2013 yes, it's just you, Bob, fighting the perilous battle against mistakenly banned IP addresses in your parents' basement. Then we have the messianic comment proposing a world where \"sudo\" is as archaic as floppy disks, because typing passwords is such an unbearable chore in managing one's own lone device. And let's not forget the enlightening diversion into the nostalgic lands of Unix security legends, which, though endearing, perhaps shines more light on the commentator's academic crushes than on sudo's gritty lore. Sit back, pour yourself a beverage, and revel in the melodrama of self-administered sysadmin woes and passwordless utopias, unfolding in the thrilling arena of nostalgic network tech banter. \ud83d\ude02\ud83d\udc68\u200d\ud83d\udcbb\ud83d\udcbe"
  },
  {
    "title": "What Will Enter the Public Domain in 2025? (publicdomainreview.org)",
    "points": 151,
    "submitter": "Tomte",
    "submit_time": "2024-12-01T20:17:02 1733084222",
    "num_comments": 95,
    "comments_url": "https://news.ycombinator.com/item?id=42290448",
    "comments": [
      "70 years feels so long for movie or book.I get it\u2019s important to protect the right of authors and companies but damn it\u2019s 3 generations. Something my grand grand father may have seen, that\u2019s insane.25 year, a single generation would make sense. I\u2019d argue that by then all the money would have been made and you would allow new generation to grow up with the greatest art from previous generation, it would be like a virtuous circle. Next generation would improve based on previous one   And so on.\n \nreply",
      "I'm going to designate this as the obligatory \"copyright is far too long\" subthread.In my opinion, the ideal length (if we are to have copyright) is between 10 and 15 years, at least if a work is already monetarily successful. If a work has yet to be monetarily successful, then we can allow up to 25 years for it in particular.\n \nreply",
      "The only part that feels weird about a shorter duration like that is tv/movie adaptations of books. The game of thrones show came out 15 years after the first book, does that mean they would have been able to make it without licensing it from the author?\n \nreply",
      "That can be fixed by limiting copyright to a certain duration per medium. You write a book - you have copyright on paper based books for 15 years. You publish it as Ebooks for desktop and mobile devices - get 15 years on that medium. Convert to visuals on Television &/or Films - 15 years on that medium. Virtual reality - another 15 years and so on ...\n \nreply",
      "It does mean that, and I think that's just something that would have to be accepted. Disney's empire is built on adaptations of public domain stories, after all.\n \nreply",
      "Disney is about to be faced with a landscape where anybody can make Pixar films from home. They're in for a world of hurt in the new regime where thought moves faster than IP.Film studios only existed because (1) distribution used to be hard and (2) films were financially and logistically difficult to make. Netflix and YouTube slayed the first challenge, and now GenAI will fell the latter and give indie directors the same kind of platform that indie game and indie music folks currently have: true one person studios.\n \nreply",
      "Unlikely. Generative AI is foul and unpleasant to perceive.\n \nreply",
      "It is not inherently, but people are very effective at using it to produce foul, unpleasant output, which is a temporary problem. Like almost all things, people will not actually care how they're made if the final product is good.\n \nreply",
      "As someone who spends 100 hours a week working in this space, it's so weird seeing such pervasive negative attitudes everywhere I look.I know the work I'm doing is valuable and that this field is the future. I'm sure it'll click for more folks soon.\n \nreply",
      "People repeat \"generative AI is all evil garbage\" because that's what the media (which is very afraid of AI, might I add) has told them.It's also funny to see AI turn people who normally dislike copyright into die-hard copyright lovers."
    ],
    "link": "https://publicdomainreview.org/features/entering-the-public-domain/2025/",
    "first_paragraph": "Search The Public Domain ReviewAt the start of each year, on January 1st, a new crop of works enter the public domain and become free to enjoy, share, and reuse for any purpose. Due to differing copyright laws around the world, there is no one single public domain \u2014 and here we focus on three of the most prominent. Newly entering the public domain in 2025 will be:In our advent-style calendar below, find our top pick of what lies in store for 2025. Each day, as we move through December, we\u2019ll open a new window to reveal our highlights! By public domain day on January 1st they will all be unveiled \u2014 look out for a special blogpost from us on that day. (And, of course, if you want to dive straight in and explore the vast swathe of new entrants for yourself, just visit the links above).The Public Domain Review is registered in the UK as a Community Interest Company (#11386184), a category of company which exists primarily to benefit a community or with a view to pursuing a social purpose, ",
    "summary": "\ud83d\ude44<em>Public Domain Advent Calendars</em> are *all the rave* this year at Public Domain Review, eager to unleash old films and dusty writings your great-grandpa barely cared about. Commenters engage in the timeless sport of bickering over copyright like they personally brokered the Treaty of Versailles, calculating the ideal nanosecond-span a book should be protected before everyone pirates it. The heated debate reveals a Jackson Pollock painting of logical stretches, proposing a copyright law overhaul that adjusts for different mediums as if media rights were slices of a Choose Your Own Adventure pie. Meanwhile, someone fears for Disney\u2019s juggernaut, trembling at the mere thought of Mickey Mouse in the public-drafting claws of Joe Public and his spanking new AI toolkit. \ud83c\udfac\ud83d\udcd7\ud83d\udcbb"
  },
  {
    "title": "Programming the C64 with Visual Studio Code \u2013 Retro Game Coders (retrogamecoders.com)",
    "points": 87,
    "submitter": "rbanffy",
    "submit_time": "2024-12-01T21:29:38 1733088578",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=42290861",
    "comments": [
      "I credit the C64 that I had as a kid and magazines like COMPUTE! / Compute's Gazette for my career in software.  I taught myself 6510 assembler and started writing some simple demo-like things on that machine, and got hooked on the feeling of creativity that it unlocked.Funnily enough I'd been thinking that it's about time I tried (again, as an older person) to write a game or a demo for the old 64.It's absolutely amazing what people are able to get out of these 40+ year old machines now, and I love that there's still a vibrant scene.In addition to the tools specified in the article, I would also recommend \"retro debugger\", it's an amazing tool for single stepping through code and seeing what's going on, even letting you follow the raster down the screen to see what code is executing on given scaliness.Also, there are some really good youtubers out there helping to demystify how various games/demos work..  Martin Piper comes to mind as a good example.\n \nreply",
      "I had a C64 as well. My school had a programming class and we all shared a TRS80 (I think). I remember writing a program to find prime numbers and thinking about various optimizations. Mine was fastest, and I was proud. Then the boy that wrote directly in assembly ran his... That was the moment I decided to get good. :-)\n \nreply",
      "I credit the BASIC and machine language byte code type-in programs for reinforcing my attention to detail and being able to track down software problems.Kids these days[0] will never know the \"pleasure\" of spending hours typing in some cheesy BASIC game only to have to track down any number of syntax errors![0] Get off my lawn!\n \nreply",
      "Cool project, looks like they support a number of assemblers and compilers, too. It\u2019s not clear if it also supports the ca65 macro assembler as a standalone assembler as part of their cc65 support, or if it only supports the c compiler.\n \nreply",
      "One of my favorite retro projects in this real-time TRS-80 (Model I assembler and emulator that assembles and runs Z80, literally with each key press. Mind boggling how today's CPUs can emulate and entire 8-bit computer dev-process all between key presses in a browser. https://www.teamten.com/lawrence/projects/assembly-language-....\" The author even says \"How about: With every keystroke in the IDE\u2019s code editor, we assemble the whole program, reset a virtual TRS-80 to a known state, install the program, and run it??\n \nreply",
      "Retro game coders achieve some pretty astounding results sometimes. One of my favorite examples is one who optimized super Mario 64 to such a degree that it runs with much better framerate on the original hardware. Also, multiplayer was added: https://www.youtube.com/watch?v=t_rzYnXEQlE\n \nreply",
      "What I love about this is that it should be relatively trivial to extend it to, at least, all emulators supported by VICE.\n \nreply"
    ],
    "link": "https://retrogamecoders.com/c64-visual-studio-code/",
    "first_paragraph": "",
    "summary": "In a stunning revelation to absolutely <em>nobody</em>, nostalgic tech enthusiasts have dredged up the Commodore 64, not to play the games, but to <i>program</i> them\u2014because modern hobbies just lack that old-school frustration. Enter \"Programming the C64 with Visual Studio Code \u2013 Retro Game Coders,\" an article which somehow connects a 40-year-old computer to a modern IDE, ensuring that a whole new generation can experience the joy of debugging beige plastic. Comments are a mix of humble-brags about ancient programming feats and nostalgic musings that scream \"kids these days will never understand.\" One even suggests more tools, because if there's anything better than obsolete tech, it's obsolete tech with added complications. The community relishes in making Mario run faster on hardware older than the internet, proving once and for all that you really can't teach an old dog new tricks, but you sure can make it dance."
  },
  {
    "title": "Advent of Code 2024 (adventofcode.com)",
    "points": 1018,
    "submitter": "thinkingemote",
    "submit_time": "2024-12-01T09:09:35 1733044175",
    "num_comments": 414,
    "comments_url": "https://news.ycombinator.com/item?id=42287231",
    "comments": [
      "My personal challenge last year was to solve everything on my mobile phone, using LLMs (mostly ChatGPT4 with code interpreter; I didn't paste in the problems, but rather described the code I wanted.)This year I'm declaring \"Advent of Claude\"!Challenge: Write a Claude custom style to solve Advent of Code puzzles within Claude's UI.Score: # adventofcode.com stars earned in 2 daily conversation turns.Fine print: web app artifacts are allowed, including paste of your custom input into the artifact UI; one click only.Per https://adventofcode.com/2024/about, wait until the daily http://adventofcode.com leaderboard is full before submitting LLM-generated solutions!Of course, feel free to use ChatGPT custom instructions, static prompts, etc.Day 1: two stars, https://claude.site/artifacts/d16e6bdb-f697-45fe-930c-7f58b2...\n \nreply",
      "I love AoC! Did it the last 2-3 years in Rust, hanging out in a discord where we all try to make the absolute fastest solutions. Learnt all kinds of crazy performance hacks and some advanced algorithms & SIMD that way.This time I'm trying to do them in Rust and Golang in an effort to either learn to like/tolerate Golang (because we use it at work) or prove my hypothesis that it sucks and never use it unless I have to.\n \nreply",
      "'d you be interested in sharing the discord? :)I try every year to optimize for speed in zig: https://github.com/ManDeJan/advent-of-code\n \nreply",
      "This server seems to be active, although they are all unofficial servers from enthusiasts.https://discord.gg/wYmyYsf\n \nreply",
      "Interested by your rust speed approaches, care to share a link ?\n \nreply",
      "Same. I am doing rust + clojure this year. Very interested in performance hax, esp around SIMD. I know absolutely nothing at all about rust, this is my first time working with it.My day 1 rust solution:    cargo solve 1 --\n    release\n        Finished `release` profile [optimized] target(s) in 0.05s\n        Running `target/release/01`\n    Part 1: 1189304 (95.8\u00b5s)\n    Part 2: 24349736 (120.4\u00b5s)\n\nDay 1 clojure solution:      lein run 1\n      running all tasks for day 1\n      reading input from resources/day01.txt\n      running day 1 part 1\n         part-fn: #'aoc.day-01/part-1\n         took: 5.511375 ms\n         result: 1189304\n      reading input from resources/day01.txt\n      running day 1 part 2\n         part-fn: #'aoc.day-01/part-2\n         took: 1.822334 ms\n         result: 243497365\n\nCode here: https://github.com/whalesalad/aoc\n \nreply",
      "There's a Rust solution posted in the Reddit Day 1 answers mega thread which claims 22 microseconds part 1 and 10 microseconds part 2. (I haven't tried to verify):https://old.reddit.com/r/adventofcode/comments/1h3vp6n/2024_...\n \nreply",
      "Go (not \"Golang\") has better compilation times than Rust and does not try to combine incompatible ways of using concurrency.I have the opposite dilemma to you, I want to learn to like Rust.\n \nreply",
      "Personally I loathe golang for the sheer fact that it was created recently enough to have included a much better design. Old languages get a pass.Rust to me is what a modern take on a systems language would be. I think it\u2019s substantially better than go.\n \nreply",
      "Let's try to settle the Go/Rust debate in this AoC sub-thread ^_^\n \nreply"
    ],
    "link": "https://adventofcode.com/2024/about",
    "first_paragraph": "Hi!  I'm Eric Wastl. I make Advent of Code.  I hope you like it!  I also make lots of other things.  I'm on Bluesky, Mastodon, GitHub, and Twitter.Advent of Code is an Advent calendar of small programming puzzles for a variety of skill levels that can be solved in any programming language you like. People use them as interview prep, company training, university coursework, practice problems, a speed contest, or to challenge each other.You don't need a computer science background to participate - just a little programming knowledge and some problem solving skills will get you pretty far. Nor do you need a fancy computer; every problem has a solution that completes in at most 15 seconds on ten-year-old hardware.If you'd like to support Advent of Code, you can do so indirectly by helping to [Shareon\n  Bluesky\nTwitter\nMastodon] it with others or directly via AoC++.If you get stuck, try your solution against the examples given in the puzzle; you should get the same answers.  If not, re-read",
    "summary": "<b><em>Advent of Code 2024:</em> Your Annual Reminder That You're Not As Smart As You Think</b>\n\nWelcome back to <em>Advent of Code</em>, the yearly exercise where developer egos inflate faster than bitcoin in a bubble, because solving tiny puzzles clearly equals real-world proficiency. Eric Wastl, master of the humblebrag, invites everyone, their dog, and their ten-year-old laptop to partake in programming challenges that can be tackled in any language - an open invitation for commenters to argue whether Rust or \"Golang\" (it's Go, folks) has the superior garbage collector. Over on the comments, it's a showdown of speed freaks, using this delightful holiday tradition to prove their favorite tech stack is just <i>slightly</i> more performant in scenarios that will never, ever mirror real-world tasks. Meanwhile, join the discord for hot tips on turning concise code into unreadable one-liners, because apparently, that's how you really impress your peers. Godspeed, you pedantic wizards. \ud83d\udc68\u200d\ud83d\udcbb\ud83d\ude80\ud83c\udf84"
  },
  {
    "title": "Show HN: Markwhen: Markdown for Timelines (markwhen.com)",
    "points": 234,
    "submitter": "koch",
    "submit_time": "2024-12-01T17:58:48 1733075928",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=42289690",
    "comments": [
      "Creator here - glad to see people like markwhen!Been working on markwhen for a few years now, originally inspired by cheeaun's life timeline that another commenter posted about.At this point markwhen is available as a VS Code extension, Obsidian plugin, CLI tool, and web editor in Meridiem.Some recent markwhen developments:- Dial, a fork of bolt.new (Stackblitz's very cool tool that leverages AI to help quickly scaffold web projects): an in-browser editor that lets you edit existing markwhen visualizations like the timeline or calendar or make your own. I just released that yesterday so it's still rough but I have big plans for it (it's one of the visualizations in meridiem)- Event properties: each entry can have it's own \"frontmatter\" in the form of `key: value` pairs. I wanted this as I'm aiming for more iCal interoperability in the future, so each event could theoretically have things like \"attendees\" or google calendar ids or other metadata. This was released in the last month or two.- remark.ing: this one isn't ready yet by any means but it's like a twitter/bluesky/mastodon-esque aggregated blog site. So you write markwhen and each entry is a post. In this way \"scheduling\" a post is just writing a future date next to it, and you have all your blog in one file. This one is a major WIP\n \nreply",
      "Great to see this again. Amazing how this tool expanded so much over the years!\n \nreply",
      "I used Markwhen recently to make an interactive Gantt chart for a proposal to a collaborator and it went swimmingly.  (We got the gig!)  So, thank you!For the record, I used the Obsidian plugin to develop, then deployed as static HTML.\n \nreply",
      "I skimmed the documentation and didn't see any reference as to whether Markwhen supports dependencies? I.e. MSProject-style make one event dependent on another task ending or starting.Did you need/use that functionality?\n \nreply",
      "Sorry if this is a dumb Q but what took that from Markwhen/down to HTML?\n \nreply",
      "Not a dumb question!  Markwhen has a CLI: https://docs.markwhen.com/cli that I used.\n \nreply",
      "Oh I missed the rendering capability. Thanks for the link!\n \nreply",
      "Be careful.Gruber (who has trademark in \u201cMarkdown\u201d), appears to not like people using his trademark name.https://blog.codinghorror.com/standard-markdown-is-now-commo...\n \nreply",
      "This has a different name.\n \nreply",
      "This is neat! It reminded me of this project by cheeaun that enables one to create a visual timeline based on a simple texted based format. The purpose was to plot one's life events in a visual way.https://github.com/cheeaun/lifeSample file (from the repository):    @USERNAME's life\n    ===============\n\n    - 24/02/1955 Born\n    - ~1968 Summer job\n    - 03/1976 Built a computer\n    - 01/04/1976 Started a company\n    - 04/1976-2011 Whole bunch of interesting events\n \nreply"
    ],
    "link": "https://markwhen.com",
    "first_paragraph": "",
    "summary": "Title: Show HN: Markwhen: Markdown for Timelines (markwhen.com)\n\nIn a pressing effort to immortalize every mundane event in \"markdown\" format, a lone warrior unveils <em>Markwhen</em>, yet another tool promising to revolutionize how we avoid using traditional calendars. After years of toiling in obscurity, the creator bombastically presents a swiss army knife of integrations (Obsidian! VS Code! ClI! Even a web editor named Meridiem that sounds like a rejected Star Trek planet!) that turns your procrastination into colourful, interactive Gantt charts. Commenters, each vying for a Darwin award in technological sycophancy, frolic gaily beneath this post, throwing around technical jargon that even their software understands only half the time. \"Groundbreaking,\" one scribe whispers, before attempting to embed their entire uninspiring existence within a <em>simple texted based format</em>, because who *doesn't* want a public ledger of life's events\u2014from birth to that noteworthy summer gig? \ud83d\udcc5\ud83d\ude02"
  },
  {
    "title": "Category Theory in Programming (racket-lang.org)",
    "points": 28,
    "submitter": "todsacerdoti",
    "submit_time": "2024-12-01T22:19:30 1733091570",
    "num_comments": 13,
    "comments_url": "https://news.ycombinator.com/item?id=42291141",
    "comments": [
      "I've gone down this rabbit hole a few times, and I always return pissed off and a dozen hours poorer. Maybe someday I'll get it.\n \nreply",
      "It's hard to write something that is both accessible and well-motivated.The best uses of category theory is when the morphisms are far more exotic than \"regular functions\". E.g. it would be nice to describe a circuit of live queries (like https://materialize.com/ stuff) with proper caching, joins, etc. Figuring this out is a bit of an open problem.Haskell's standard library's Monad and stuff are watered down to the point that they are barely category theory, but they are also quite useful and would not have been readily invented without category theory. See even if you have no taste for \"abstract nonsense\", maybe you can still accept the fact that its left a trail of more accessible \"semi-abstract semi-nonsense\" in its wake.This stuff takes time. If it's not your cup of tea, no need to make yourself an early adopter. See also things like Lean where fancy type systems are finally reaching \"regular non-CS\" mathematicians with great success.\n \nreply",
      "On the pure math side, Locales (https://ncatlab.org/nlab/show/locale) are so much more beautiful an axiomatization scheme than regular topological spaces, even if one is just doing regular classical rather than constructive math. We would have not discovered them except for category theory.\n \nreply",
      "That's such an amazing description of my experience as well. I find the topic inscrutable.\n \nreply",
      "I was a college math major. Category theory wasn't one of the offered courses at my college, so I didn't study it specifically. But I didn't get through any of my math classes without working the problems and proofs, and without having a teacher to help guide me through the material. It can't be read like a book. At least that's my experience.And as I get older, being interested in the topic helps more and more.\n \nreply",
      "I feel that the main thing that can change it is having a plethora of examples. Also, I think math has much better examples than programming.\n \nreply",
      "As a math course, I felt category theory was interesting as a way of unifying ideas across different areas of mathematics. But I never quite seen a value of it for programming (but maybe it's just me)\n \nreply",
      "As primarily an engineer doing maths, with code on the side, I feel the exact same.It\u2019s been way more of use to me in pulling together parts of topology, linear algebra, geometric algebra, homology et al than discovering furtive programming abstractions.\n \nreply",
      "It's IMO the right setting to talk about logic. That's one of the reasons it unifies ideas in math.Computer science is mostly an application of logic which is why category theory has become a useful tool in theoretical computer science.\n \nreply",
      "I share this experience.Perhaps another way of asking the question is: Are there any results, either about individual programs or in PL theory as a whole, that were made simpler / clarified / generalized because of category theoretic insights?\n \nreply"
    ],
    "link": "https://docs.racket-lang.org/ctp/index.html",
    "first_paragraph": "",
    "summary": "**Category Theory Takedown**\n\nWho knew that the obscure corners of mathematical theory could be this excruciatingly incomprehensible? Welcome to \"Category Theory in Programming,\" where the uninitiated descend into madness trying to decipher advanced abstractions that, apparently, barely scrape their utility in programming. Here, seasoned mathematicians and aspiring code jockeys unite in confused solidarity, bemoaning lost hours and questioning their life choices, all while a scant few defend the esoteric art as if it\u2019s the holy grail of both math and coding. Meanwhile, the comment section devolves into a mess of intellectual one-upmanship, as everyone tries to prove they're the least confused. Embrace the \"semi-abstract semi-nonsense\"\u2014because regular nonsensical torture just wasn't enough!"
  },
  {
    "title": "Procedural knowledge in pretraining drives reasoning in large language models (arxiv.org)",
    "points": 164,
    "submitter": "reqo",
    "submit_time": "2024-12-01T16:54:26 1733072066",
    "num_comments": 53,
    "comments_url": "https://news.ycombinator.com/item?id=42289310",
    "comments": [
      "It seems obvious to me that LLMs wouldn't be able to find examples of every single problem posed to them in training data. There wouldn't be enough examples for the factual look up needed in an information retrieval style search. I can believe that they're doing some form of extrapolation to create novel solutions to posed problems.It's interesting that this paper doesn't contradict the conclusions of the Apple LLM paper[0], where prompts were corrupted to force the LLM into making errors. I can also believe that LLMs can only make small deviations from existing example solutions in creation of these novel solutions.I hate that we're using the term \"reasoning\" for this solution generation process. It's a term coined by LLM companies to evoke an almost emotional response on how we talk about this technology. However, it does appear that we are capable of instructing machines to follow a series of steps using natural language, with some degree of ambiguity. That in of itself is a huge stride forward.[0] https://machinelearning.apple.com/research/gsm-symbolic\n \nreply",
      "I can believe that they're doing some form of extrapolation to create novel solutions to posed problemsYou can believe it what sort of evidence are you using for this belief?Edit: Also, the abstract of the Apple paper hardly says \"corruption\" (implying something tricky), it says that they changed the initial numerical values\n \nreply",
      "I very much agree with the perspective that LLMs are not suited for \u201creasoning\u201d in the sense of creative problem solving or application of logic.  I think that the real potential in this domain is having them act as a sort of \u201ccompiler\u201d layer that bridges the gap between natural language - which is imprecise - and formal languages (sql, prolog, python, lean, etc) that are more suited for solving these types of problems.  And then maybe synthesizing the results / outputs of the formal language layer.  Basically \u201cagents\u201d.That being said, I do think that LLMs are capable of \u201cverbal reasoning\u201d operations.  I don\u2019t have a good sense of the boundaries that distinguish the logics - verbal, qualitative, quantitative reasoning. What comes to my mind is the verbal sections of standardized tests.\n \nreply",
      "> I think that the real potential in this domain is having them act as a sort of \u201ccompiler\u201d layer that bridges the gap between natural language - which is imprecise - and formal languages (sql, prolog, python, lean, etc) that are more suited for solving these types of problems. And then maybe synthesizing the results / outputs of the formal language layer. Basically \u201cagents\u201d.Well, if you do all that, would you say that the system has a whole has 'reasoned'?  (I think ChatGPT can already call out to Python.)\n \nreply",
      "Totally, these companies are pushing towards showcasing their AI models as self thinking and reasoning AI while they are just trained of a lot of amount of data in dataset format which they extrapolate to find the right answer.They still can't think outsider their box of datasets\n \nreply",
      "Going meta a bit: comments so far on this post show diametrically opposing understandings of the paper, which demonstrates just how varied the interpretation of complex text can be.We hold AI to a pretty high standard of correctness, as we should, but humans are not that reliable on matters of fact, let alone on rigor of reasoning.\n \nreply",
      "This is extremely common in these discussions. Most humans are not that good at reasoning themselves and fall for the same kind of fallacies over and over because of the way they were brought up (their training data so to speak). And yet they somehow think they can argue why or why not LLMs should be able to do the same. If anything, the current limits of these morels show the limits of human cognition which is spread throughout the internet - because this is literally what they learned from. I believe once we achieve a more independent learning (like we've seen glimpses of in the MuZero paper) these models will blow human intelligence out of the water.\n \nreply",
      "> Most humans are not that good at reasoning themselves [...]I'd say most humans most of the time.  Individual humans can do a lot better (or worse) depending on how much effort they put in, and whether they slept well, had their morning coffee, etc.> If anything, the current limits of these morels show the limits of human cognition which is spread throughout the internet - because this is literally what they learned from.I wouldn't go quite so far.  Especially because some tasks require smarts, even though there's no smarts in the training data.The classic example is perhaps programming: the Python interpreter is not intelligent by any stretch of the imagination, but an LLM (or a human) needs smarts to predict what's going to do, especially if you are trying to get it to do something specific.That example might skirt to close to the MuZero paper that you already mentioned as an exception / extension.So let's go with a purer example: even the least smart human is a complicated system with a lot of hidden state, parts of that state shine through when that human produces text.  Predicting the next token of text just from the previous text is a lot harder and requires a lot more smarts than if you had access to the internal state directly.It's sort-of like an 'inverse problem'.  https://en.wikipedia.org/wiki/Inverse_problem\n \nreply",
      "> Most humans are not that good at reasoning themselves and fall for the same kind of fallacies over and over because of the way they were brought upDisagree that it's easy to pin on \"how they were brought up\". It seems very likely that we may learn that the flaws are part of what makes our intelligence \"work\" and be adaptive to changing environments. It may be favourable in terms of cultural evolution for parents to indoctrinate flawed logic, not unlike how replication errors are part of how evolution can and must work.In other words: I'm not sure these \"failures\" of the models are actual failures (in the sense of being non-adaptive and important to the evolutionary processes of intelligence), and further, it is perhaps us humans that are \"failing\" by over-indexing on \"reason\" as explanation for how we arrived here and continue to persist in time  ;)\n \nreply",
      "> Disagree that it's easy to pin on \"how they were brought up\".Indeed.  That might play a role, but another less politically charged aspect to look at is just: how much effort is the human currently putting in?Humans are often on autopilot, perhaps even most of the time.  Autopilot means taking lazy intellectual shortcuts.  And to echo your argument: in familiar environments those shortcuts are often a good idea!If you just do whatever worked last time you were in a similar situation, or whatever your peers are doing, chances are you'll have an easier time than reasoning everything out from scratch.  Especially in any situations involving other humans cooperating with you, predictability itself is an asset.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2411.12580",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "In a world where \"reasoning\" just means a compiler vomiting out amalgamated gobbledygook from training data, <em>Procedural knowledge in pretraining drives reasoning in large language models</em> brilliantly wades through the sea of jargon to assert \u2013 without much surprise \u2013 that machines can fake understanding by following instructions (gasp!). Our scholarly enthusiasts in the comments section, dusting off their armchair psychology degrees, quibble spiritedly over whether this constitutes true \u201creasoning\u201d or just sophisticated parroting. Cue the usual suspects: a slew of AI skeptics versus zealots, both sides armed with their favorite preprints and a casual disregard for the softer sides of human intellect like \"fallibility\" and \"sleep deprivation.\" \ud83d\udc4f\ud83e\udde0\ud83d\udca4 Rhetoric? Yes. Illuminating? Somewhat. Hilarity? Absolutely."
  },
  {
    "title": "Rails is better low code than low code (radanskoric.com)",
    "points": 134,
    "submitter": "thunderbong",
    "submit_time": "2024-11-27T07:16:28 1732691788",
    "num_comments": 52,
    "comments_url": "https://news.ycombinator.com/item?id=42253735",
    "comments": [
      "Better headline: The framework you know intimately is better low code than low code.What programmer \u201cbelieves\u201d in \u201clow code\u201d solutions?\n \nreply",
      "Sufficiently advanced \"low code\" is indistinguishable from a framework or a library.The main difference in practice is that most software that markets itself as \"low code\" obscures how it works and tries to lock you in to charge rent. Though to be fair, there is generally not much of a market for proprietary/non-OSS libraries and frameworks anymore, so if you want to monetize your library/framework low-code (or an API) is probably the way to do it.Aside from that, I do think the \"low code\" label can be genuinely helpful as a way of communicating towards semi-technical users that the software they're using is intended to be usable with their level of technical sophistication. IMO this has been a perpetually underserved market, and it's growing over time especially as computer science/programming gains popularity in schools. There are a lot of people out there who understand basic programming and took maybe a couple CS classes in their life, and want to do something entrepreneurial or practical for their non-SWE jobs, but aren't skilled enough to dive right in to doing things the way experienced SWEs would do it.\n \nreply",
      "This makes me think about Microsoft Access and why it never took off. It helps people create databases, forms, and reports. One might call it a no code solution as opposed to merely a low code solution. There are a lot of businesses that's ostensibly in the right place for Microsoft Access.In my view there are certain aspects of app building that's hard. Some of it is in code, some of it is in design, some of it is in domain modeling. Every once and awhile you get stuck and your low-code solution is suddenly paralyzed at that one point. ChatGPT unblocks you until you REALLY need a programmer.Low or no code did not make sense until ChatGPT.\n \nreply",
      "Access was more popular with a certain crowd, way back.Then the client-server world dominated the desktop, plus we needed automated backups and source control and tests and scalability and auditing and hosting and security and other really important things that never properly made there way into Access. Presumably Microsoft didn't want to cannabilize SQL Server sales and chose not to invest in those things that would have actually made it an MVP for building home-grown apps.\n \nreply",
      "Meh, I've had successful things run just off of zapier and friends.Sometimes the thing really is just the thing. The real thing is plenty of low code systems are just garbage. But Salesforce is the size it is because it's a successful \"low code\" system.\n \nreply",
      "The real pain point low code seems to solve is boilerplate. Say that I am getting Django ready to go. To get started, have to Dockerize, swap to Postgres, add linting, swap out of the User model, etc.But once all that is ready to go, it is about as fast as low code in my experience.\n \nreply",
      "It would be nice to get a brief definition of \"low code.\" Otherwise great article. I've found myself using \"low code\" frameworks when they setup my environment for me and can generate the code files. Key example was a Minecraft mod maker. Within a minute I was fed up with the scratch style programming, but it had all the Java build stuff ready for me to just edit the files myself. No setting up gradle or anything.\n \nreply",
      "I think this is the key to them actually being useful - generate a sane project in some widely used language under the hood, and let people give up on the low code solution when they outgrow the guardrails. Otherwise you're just dooming yourself to an eventual rewrite as soon as you need some feature that's not supported.\n \nreply",
      "\u201cEjecting\u201d is the key term here I think, or at least the one I\u2019ve always used - for when you need to shed the handlebars for more control.\n \nreply",
      "\"Escape Hatch\" is also commonly used.\n \nreply"
    ],
    "link": "https://radanskoric.com/articles/rails-is-better-low-code-than-low-code",
    "first_paragraph": "Software development, mostly Ruby\u201cWe need a very simple CRUD app for managing the reservations.\u201d They1 said. \u201cDon\u2019t spend too much time on it.\u201d They added.My thoughts are racing: \u201cHm, I am very good with Ruby on Rails, this seems like a good fit. But then I\u2019ve also used these low code tools before, they are supposed to be the perfect solution for stuff like this. What should I use here \u2026 \u201c. In that very moment, and only for the duration of this article, I gain the ability to project my thoughts into the future down the timelines caused by both choices. Someone whispers \u201cLisan Al Gaib\u201d2 but I ignore it.Both arcs unfold before me in parallel \u2026I talk to them about the requirements. As they promised, the requirements really are simple. This really shouldn\u2019t take long.I click around, this tool has everything I need, there\u2019s even a template that\u2019s almost exactly what I need. I start with the template, and click around to customise it further. In a few hours I have what they need. Another cli",
    "summary": "In a staggering display of existential dread, <em>radanskoric.com</em> embarks on a heroic journey through the perilous realm of deciding whether to use Ruby on Rails or a low code solution for a CRUD app that nobody will remember next week. Because nothing screams \"modern software development\" like waxing poetic over tool choices in the time it takes most of us to just code the thing in whatever's already open. Commenters join this high-stakes melodrama, passionately discussing the merits and metaphysics of low code like it's the new philosophy replacing Plato in tech bro book clubs. Meanwhile, back in the real world, most people just mash whatever template fits the bill and knock off early on Friday. \ud83d\udc53\ud83d\udcbb"
  },
  {
    "title": "How to Study Mathematics (uh.edu)",
    "points": 25,
    "submitter": "ayoisaiah",
    "submit_time": "2024-12-01T21:50:51 1733089851",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42290996",
    "comments": [
      "Interestingly, this guide states that the intuitive understanding of maths is only suitable at the school level but not for the university.In his recently published book \"Mathematica: A Secret World of Intuition and Curiosity\", David Bessis argues that the intuition is the \"secret\" of understanding maths at all levels.Not sure what conclusion to draw from here, but my (rather dated) experience with university maths tells me that the intuition is a powerful tool in developing the understanding of the subject.\n \nreply",
      "Possible harmonization of the two ideas: the intuition that we go into math at high school level can help serve us at that level of math. We have some idea of geometry-like objects and 2d-calculus like curves from our everyday lifeAt university level the objects become more abstract, so the intuition we use in normal daily life may no longer apply. New kinds of intuition may develop but it takes work, including lots of time spent with the formal processes and calculations along with reflection on that time, and the active creation of new metaphors to drive the intuition. For example, I still remember a professor using \"Ice-9\" (from _Cat's Cradle_) as a metaphor for how proving some local property of a holomorphic function on the complex plane made that property true for its global behavior\n \nreply",
      "I think he's saying here that intuition is sufficient for high school math, but not sufficient for college. That's not to say that it isn't necessary, only that it isn't sufficient.\n \nreply",
      "I appreciated the article for emphasising memorising definitions and statement of theorems... But not for proofs. For proofs, a general outline would be sufficient.\n \nreply"
    ],
    "link": "https://www.math.uh.edu/~dblecher/pf2.html",
    "first_paragraph": "This essay describes a number of strategies for studying college level\nmathematics. It has sections entitled\nThe first major difference between high school mathematics and college\nmathematics is the amount of emphasis on what the student would call theory---the\nprecise statement of definitions and theorems and the logical processes\nby which those theorems are established. To the mathematician this material,\ntogether with examples showing why the definitions chosen are the correct\nones and how the theorems can be put to practical use, is the essence of\nmathematics. A course description using the term ``rigorous'' indicates\nthat considerable care will be taken in the statement of definitions and\ntheorems and that proofs will be given for the theorems rather than just\nplausibility arguments. If your approach is to go straight to the problems\nwith only cursory reading of the ``theory'' this aspect of college math\nwill cause difficulties for you.\nThe second difference between college mathem",
    "summary": "\ud83d\udcda The University of Houston bravely attempts to guide the perpetually confused college freshman through the mythical forest of \"college-level mathematics\" with a revelatory article about the value of theory over problem-solving. \ud83e\udd14 As expected, the comment section morphs into a battlefield where self-proclaimed math gurus debate whether intuition or rote memorization reigns supreme in the academic arena. One nostalgic commenter even dredges up memories of a professor using 'Ice-9' as a metaphor, desperately trying to prove that college wasn't a complete waste of time and tuition money. Meanwhile, another commenter insists that maybe, just maybe, knowing a theorem's name is at least as important as knowing what it means. Will the mysteries of rigorous mathematical proof ever match the enigma of successfully navigating college advice articles? Stay tuned. \ud83c\udf93\ud83e\uddd0"
  },
  {
    "title": "Francis Crick's \"Central Dogma\" was misunderstood (asimov.press)",
    "points": 70,
    "submitter": "ctoth",
    "submit_time": "2024-12-01T18:24:26 1733077466",
    "num_comments": 21,
    "comments_url": "https://news.ycombinator.com/item?id=42289798",
    "comments": [
      "Unfortunate nomenclature aside, this issue highlights the difference between models and reality in science and how students can take these models too literally and thus easily conflate the two in pedagogy. Here's an embarrassing story for me: I didn't realize (or at least didn't internalize) that not all cells in the human body conform to to the canonical image of the cell that we see in Biology textbooks (https://en.wikipedia.org/wiki/Cell_(biology)#/media/File:Cel...) until I finished oncology and embryology class in my senior year back in 2015. For example, erythrocytes (RBCs) don't have nucleus and mitochondria. Some hepatocytes even have more than 1 nucleus (polyploidy).I still don't know how I went through my cell biology class in sophomore year without understanding this. Maybe I missed the forest for the trees (curse you clathrin mediated endocytosis!)\n \nreply",
      "Muscles also have more than one nucleus per cell, if you have strength trained. You can fast and diet and waste away all the protein and shrink them down, but the nuclei remain. Go in to a surge of calories, protein, (and optional androgenic hormone signaling) and the nuclei effectively parallel process transcription of mRNA in the ribosomes for regeneration of the proteins from amino acids and re-bulking up of the cells and your strength.This, (along with neural adaption) is what we are taking about when we reference \u201cmuscle memory\u201d. It\u2019s a steep hill to\ntrain the first time and multiply the nuclei, but once you have them, you have them for life, and subsequent re-strengthening efforts will be far easier.I\u2019ve gone through this myself and it\u2019s pretty shocking. It took me X years to achieve Y kilograms barbell whatever movement. A diet or lack of training for a year and I can only do Y/2. But if I eat a ton, I can be back to Y or higher in a few months.\n \nreply",
      "I don\u2019t understand how \u201cCrick\u2019s attempt to break the central dogma\u201d \u2026 \u201crelied upon the fact that the genetic code is redundant.\u201dThe article doesn\u2019t seem to present any explanation of how his argument supposedly relied on that.\n \nreply",
      "what Crick means, I believe, is that since multiple codons can code for a single amino acid the information or entropy in that set of codons is larger than the information transferred to the protein creating a many-to-one interaction. This means that you could, in principle, create an RNA sequence 'A' that codes for the same protein as another sequence 'B', but they are discretely different implying that this difference could be exploited to send different information back from proteins to this different RNA sequence. Kinda a reach in my opinion and not really important as proving you could do that doesn't bring us any closer to understanding what nature actually does. I guess the most interesting result with this research is analyzing the thermodynamics of this reverse interaction as an argument as to why nature has not evolved a way to do this.\n \nreply",
      "I think it was a bad argument, perhaps due to weakness in Crick's knowledge of information science.The argument goes like this;Suppose you built a library of chemical reactions, to generate a DNA codon from \nan amino acid, and did this for all of the different amino acids.\nSince two different DNA sequences comple to the same protein, you could take sequence A, perform transcription and generate a protein P, and then construct a different DNA sequence B.Thus we have \"genetic information\" extracted from a protein and stored in DNA, violating the Dogma... in an extremely cheesy way, since it's obviously the same \"information\" you had at the start, just encoded differently, and also it's not different in any meaningful sense from doing the same reverse engineering on non-logically-redundant DNA.\n \nreply",
      "Surely prion diseases are an example of protein to protein, which the article specifically says was part of the CD?I\u2019m not unhappy with the tone of the article suggesting that Watson, yet again, vastly misunderstood the work of his betters while taking credit for it.\n \nreply",
      "The \u201cdogma\u201d is \u201cinformation flow from proteins back to genetic material does not occur\u201d not that proteins can\u2019t transfer information. Regardless of considering \u201cshape\u201d as information or not, the transfer is not violating that statement, the information is fully trapped in the protein and not flowed back to DNA.\n \nreply",
      "The article explicitly lists protein -> protein as one of the 3 prohibited information flows.\n \nreply",
      "As the first figure says:> Information here means the sequence of the amino acid residues, or other sequences related to it.Prions do not transfer sequence information between proteins, so this is in keeping with Crick's idea.I've always assumed that Watson understood Crick's idea perfectly well, but used the simpler formulation because it was easier to communicate, while still being mostly accurate.\n \nreply",
      "I think if prions were considered violations of CD then enzymes or at least ribosomes would be considered to violate CD as well.\n \nreply"
    ],
    "link": "https://www.asimov.press/p/crick",
    "first_paragraph": "",
    "summary": "In this week's gripping episode from Asimov.press, we confront the titanic confusion that is Francis Crick's \"Central Dogma\" and its woefully abused interpretation. Down in the sage-strewn depths of the comments, aspiring molecular gymnasts somersault through hoops trying to clarify, or outright reinvent, Crick\u2019s strained musings on genetic information flow. One scholar confesses a distressingly late epiphany about cell diversity, making us wonder what they actually teach in those high-priced biology classes. Meanwhile, another valiant knight of academia rides the rugged terrains of muscle memory, somehow looping it back to cellular nuclei in a display of rhetorical agility that would make even a contortionist blush. \ud83e\udd38\u200d\u2642\ufe0f\ud83e\uddec\ud83c\udfaa"
  },
  {
    "title": "The Color of Noise (2014) (caseymuratori.com)",
    "points": 47,
    "submitter": "ekzhang",
    "submit_time": "2024-12-01T19:05:10 1733079910",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=42290011",
    "comments": [
      "The same algorithm is described more clearly in Bridson's paper from 2007:https://www.cs.ubc.ca/~rbridson/docs/bridson-siggraph07-pois...\n \nreply",
      "\"In fact, people have actually made spatial pattern generators that allow you to input the frequency profile that you want, and get the corresponding point pattern out. It\u2019s really quite neat, and I highly recommend reading this paper so you can see some other possible noise parameters, like anisotropy.\"I had to hunt this 'custom colour' noise paper out since that link was dead. It is quite neat. Here it is on ACM in case anyone else is interested:Point Sampling with General Noise Spectrum (2012)\nhttps://dl.acm.org/doi/10.1145/2185520.2185572\n \nreply",
      "Something sounds odd. If points are very close, they have high spacial frequency. If I reject points that are too close, I'm excluding high-frequency points and keeping only low-frequency ones. A distribution that favor lower frequencies and filters out higher ones is red, not blue. Am I getting it wrong?\n \nreply",
      "I think the color terms and frequency may make sense in reference to a comparison against uniformly-spaced points such as a rectangular grid. If you perturb uniformly-spaced points with high-frequency/short-wavelength adjustments, you won't get the large-scale clumping that results from also including low-frequency/long-wavelength distortions.\n \nreply"
    ],
    "link": "https://caseymuratori.com/blog_0010",
    "first_paragraph": "",
    "summary": "Casey Muratori decides to enliven our gray world with a thought piece on <em>\"The Color of Noise\"</em>, revealing that you can indeed judge a book by its title. The Internet Archive's comment section turns into a heroic salvage operation, as keen-eyed readers dig through digital ruins to find a paper that Casey forgot to reference clearly. One brave soul contends with the physics of noise and spatial frequency, only to realize they might be out of their depth. Ultimately, everyone agrees they're scholars and gentlemen, by virtue of resurrecting dead links and debating concepts they half-understand. \ud83d\udcda\ud83d\udd0d\ud83d\udca1"
  },
  {
    "title": "First-Hand Account of \"The Undefined Behavior Question\" Incident [pdf] (tomazos.com)",
    "points": 7,
    "submitter": "weaksauce",
    "submit_time": "2024-12-01T23:43:41 1733096621",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=42291638",
    "comments": [
      "Previously on HN:[flagged] C++ Standards Contributor Expelled for 'The Undefined Behavior Question' (slashdot.org) https://news.ycombinator.com/item?id=42226250 (84 points, 88 comments)\n \nreply",
      "A very unfortunate outcome. Something technical people appear to be commonly prone to is not \"reading the room\" well, and it may be the honest entry point here is that they didn't understand how seriously something was being put, and their laughter was read as more than embarrassment, or casual (mis)understanding of humour.But there comes a point where being asked to make changes on frankly poor grounds IS an attack on individual integrity. This feels like one of those instances.I know nothing about the principals. There may be more complex issues around this, other competing aspects.So taking this as an honest, if not dispassionate take on things, I think the committee made a mistake.Maybe I misunderstood. How do you get from undefined behaviour to \"the final programme\" or something? whats the strong anti-semitic quality in \"undefined behaviour\" as a modern trigger term?\n \nreply"
    ],
    "link": "http://tomazos.com/ub_question_incident.pdf",
    "first_paragraph": "",
    "summary": "Today in tech absurdity, a C++ Standards Contributor has been ousted for daring to ask about \"The Undefined Behavior Question\", sparking an intellectual brawl over jargon that makes Kafka look like a children's author. Enter the Hacker News collective, where armchair experts dissect this drama with the sort of misguided fervor usually reserved for debates over tabs vs. spaces. In a flurry of indignation and poorly-timed humor analysis, one commenter bravely admits to not grasping how \"undefined behavior\" could possibly be construed as a diabolical anti-Semitic dog whistle. Clearly, when we\u2019re not defining behavior, we\u2019re inventing conspiracy theories. \ud83e\udd13\ud83c\udf7f"
  },
  {
    "title": "Demystifying Git Submodules (cyberdemon.org)",
    "points": 6,
    "submitter": "signa11",
    "submit_time": "2024-12-02T00:19:16 1733098756",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=42291833",
    "comments": [
      "Can anyone speak to usecases for submodules that arent better served by your language\u2019s package manager? Multi-language codebases, languages without appropriate package management perhaps?\n \nreply",
      "The latter had been an issue for me in the past with some projects that just weren't packaged for, e. G., python and had to be imported directly. It can also be helpful for non-packaged assets that are held in a separate git repository.\n \nreply",
      "speaking of submodules, anyone here have experience with git subrepo ?\n \nreply"
    ],
    "link": "https://www.cyberdemon.org/2024/03/20/submodules.html",
    "first_paragraph": "\nhome\n    /\n    email me\n    /\n    bluesky\n    /\n    mastodon\n    /\n    RSS feed\n    /\n    Telegram channel\n\n\n\n\n          \n          Mar 20, 2024\n        \n\nThroughout my career, I have found git submodules to be a pain. Because I did not understand them, I kept getting myself into frustrating situations.So, I finally sat down and learned how git tracks submodules. Turns out, it\u2019s not complex at all. It\u2019s just different from how git tracks regular files. It\u2019s just one more thing you have to learn.In this article, I\u2019ll explain exactly what I needed to know in order to work with submodules without inflicting self-damage.(This article doesn\u2019t discuss whether submodules are good/bad, or if you should use them or not \u2013 a valid discussion, but out of scope.)This article will make more sense if we use concrete examples.Allow me to describe a toy webapp we\u2019re building. Call this repo webapp. Here are the contents of the repo.Say you want to import some library. It lives in its own repo, library",
    "summary": "<h3>\ud83d\udce6 Unraveling the Enigma of Git Submodules</h3>\n\nToday, on <em>cyberdemon.org</em>, \"the pioneer of understanding simple solutions to complicated problems that didn't need to exist\" tackles the mountainous terrain of Git submodules. After years of battling self-inflicted Git wounds, the author claims an epiphany: submodules aren't really complex\u2014they're just *different*. Revolutionary! Meanwhile, the comments section transforms into a makeshift support group where the confused mingle with the slightly less confused. One bright soul queries about scenarios where submodules outdo conventional package managers, only to be met with tales of woe and hearsay instead of answers. Spoiler: It's all a \"suboptimal\" mess. \ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "IBM RISC System/6000 Family (computeradsfromthepast.substack.com)",
    "points": 37,
    "submitter": "rbanffy",
    "submit_time": "2024-12-01T19:42:40 1733082160",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=42290245",
    "comments": [
      "Did some development for a server application that supported RS/6000 among other platforms.  AIX on RS/6000 was a nice enough Unix but, being used to Solaris, everything seems just slightly off.  I think it performed slightly worse for the same price as Sparc hardware for our purposes, but some customers wanted an all IBM solution (we also supported an OS/2 client).The main thing that stands out was a tool (smitty?) that could do system configuration and IIRC it could show you the steps to do it manually as well.\n \nreply",
      "Where I worked we got an early system as we made graphical desktops for workstations.  IBM required that we have a separate locked room for the system!Despite having over 20 different brands/Unix systems, AIX was the only one we had to pay a 3rd party consultant to come in and do installations and upgrades.  That was back in the days of tapes.  It used some convoluted volume management system where the tape contents got spooled into the disk and then the installation absorbed that.smitty always felt so slow.  There was a release where it was sped up.  They didn't actually make it do the work any quicker - they just doubled the animation speed of the running man that was shown.\n \nreply",
      "One of the big differences is that it is also COFF based like Windows, and Aix shared objects have similar capabilities, with private by default, import files, and having the capability to let the compiler handle delay loading of specific symbols.I used Aix 5 series quite a bit, and looking at docs it seems to still have the same capabilities.\n \nreply",
      "They have even cooler capabilities - A single XCOFF library can act as both shared and static libraries\n \nreply",
      "Yes, the sysadmin tool was awesome. There were 2 versions: one graphical (smit) and the other text/console based (smitty). Both versions had the same capabilities.\n \nreply",
      "I think the other thing is the Object Data Manager (ODM)- configuration data about devices, networking configuration, etc. is stored as objects, as opposed (or in addition to?) to text files.I also believe the AIX software package manager uses ODM.\n \nreply",
      "HPUX had a similar thing, called SAM.\n \nreply",
      "Eh, smitty could be irritating because it persisted some changes but not others.  So it was unclear if things you did outside it would get undone at boot time.\n \nreply",
      "Prior to POWER on the RS/6000, AIX ran on the IBM PC RT:https://en.m.wikipedia.org/wiki/IBM_RT_PCThis ran on the ROMP processor:https://en.m.wikipedia.org/wiki/IBM_ROMPThis system was a bit before my time.\n \nreply",
      "AIX on RT PC had a rather interesting architecture: the AIX kernel ran on top of a microkernel called VRM. VRM also supported two other operating systems, AOS (IBM\u2019s port of 4.3BSD) and PICK. VRM could run multiple operating systems simultaneously, with a hotkey to switch which had access to the keyboard, mouse and screen. Similar idea to Windows NT\u2019s environment subsystems, and IBM\u2019s later Mach-based Workplace OS (cancelled before it ever went GA, but shipped in beta form to customers as OS/2 PowerPC Edition).VRM was written, not in C, but in a PL/I dialect (not sure which, but probably PL.8). So AIX for RT PC is a rare example of a Unix whose kernel is (partially) written in a language other than C. The only other example of that I know of is IBM z/OS (which most people don\u2019t think of as a Unix, but officially it is one, since it passes the certification test suite and IBM pays The Open Group the Unix trademark license fees.)I say partially because the actual AIX kernel is in C, but it then calls to the VRM microkernel which isn\u2019t in C. But a lot of basic functions which normally belong in a Unix kernel are in the VRM instead and the AIX kernel is just a thin wrapper over them.When AIX moved to the RS/6000, the VRM microkernel was dropped, and AIX from then onwards was a classic monolithic kernel all in C.Well, historically \u201cAIX\u201d has been a brand for (many but not all) IBM Unixes, with several unrelated code bases being labelled \u201cAIX\u201d. One of those, AIX/ESA (for IBM mainframes), was actually based on OSF/1, and hence used the Mach microkernel. But it was discontinued in the first half of the 1990s, when MVS gained Unix compatibility support. And all the other \u201cweird\u201d AIX variants are also now long dead, so in practice AIX for RS/6000\u2019s descendant is the only AIX left.AIX is a bit like DB2, in that both were brand names for multiple distinct code bases. The difference is all but one AIX code base died, whereas multiple DB2-branded code bases are actively supported today.\n \nreply"
    ],
    "link": "https://computeradsfromthepast.substack.com/p/ibm-risc-system6000-family",
    "first_paragraph": "",
    "summary": "\ud83c\udf89 Welcome to the nostalgic IBM fan club, where the faded glory of the RS/6000 still sparks intense debates among tech aficionados who revel in recalling how AIX slightly underperformed next to Solaris and needed its own chamber of secrets. In a confusing blend of acronyms, IBM required special rooms and third-party shamans to perform installation rituals with the mythical smitty tool \u2013 which, by enchanted decree, moved no faster than molasses until its animation speed was doubled in a dazzling feat of non-performance enhancement. Commenters, armed with rose-tinted glasses, wax lyrical about obscure features and COFF-based architectural decisions that no one outside these hallowed discussions has thought about since Y2K was a legitimate fear. Meanwhile, every sysadmin's favorite pastime: complaining about whether smitty would remember their settings or whimsically discard them at boot. \ud83e\udd13\ud83d\udcbe"
  },
  {
    "title": "Heaviside\u2019s Operator Calculus (2007) (deadreckonings.com)",
    "points": 62,
    "submitter": "joebig",
    "submit_time": "2024-12-01T18:47:38 1733078858",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=42289913",
    "comments": [
      "For a more current example of a mathematical technique that preceded formalization by a considerable amount, consider renormalization. Particularly renormalization over a calculation that takes place over Feynman diagrams.For decades physicists were happily using this to predict experiment, while mathematicians were tearing their hair out trying to make some formal sense of this, even if only in a limited context. I'd have to do some poking around to find out whether mathematicians are happy about it yet, even though the idea is older than I am.\n \nreply",
      "Kennelly\u2013Heaviside layerhttps://en.wikipedia.org/wiki/Kennelly%E2%80%93Heaviside_lay...\n \nreply",
      "Related:Heaviside\u2019s Operator Calculus - https://news.ycombinator.com/item?id=569934 - April 2009 (6 comments)This is also interesting: https://www.johndcook.com/blog/2022/10/12/operational-calcul... (via https://news.ycombinator.com/item?id=33179121, but no comments there)\n \nreply",
      "Thanks! It is to the peculiar irrereverance & technical idiosyncrasies of Oliver Heaviside (aka his genius) that we owe the early leaps in the applications of Maxwell's nascent electromagnetic theory.\n \nreply",
      "I always loved that the derivative of the heavyside operator is equivalent to the dirac delta operator. The idea of impulse and how to apply that to a system is such a unique and useful unlock in E&M and has such a nice analog of connecting the circuit.One of those things that made it click for me that math truly is defined rules of operations over definitions and could be constructed as to be useful for us, and not just a handed down pure concept. We need to model this very specific thing, here's an operator for it.\n \nreply",
      "In the latter part of high school, and in my early years in college as a computer engineer, I found physics and the philosophy of it really interesting. I remember the first time, in 9th grade, that our teacher showed us how to predict the bounce-height of a ball using some basic algebra. I was immediately in love with physics, and it was the first time that I realized math could be fun.In my senior year, AP Physics C: E&M would become one of my favorite courses of my entire scholastic career (largely thanks to my teacher). While Calc 3 wasn't required for the AP test, he introduced the concepts so that he could properly walk us through the history of the field from the perspective of its founders, up to and concluding with Maxwell's equations. We read a lot of the original papers that introduced certain operators and equations, including works from Newton, Leibniz, Heaviside, Maxwell, Einstein, and Dirac. Ironically, I failed the AP exam (2/5) but had a very easy time with Calc III, linear algebra, and diff eq in college thanks to that course.I really miss the feeling of wonder and astonishment I had when I was first exposed to these concepts -- it's been long enough that my memory of them is fuzzy now, but I don't get the same satisfaction from re-reading them.\n \nreply",
      "There are several similar variants of different kinds of math that make just as much sense as mainstream methods to me. It all feels very arbitrary.I think that's what got me into software. If we're just making shit up either way, then useful artifacts is a nice bonus.\n \nreply",
      "But it's the same thing with math.  All of science and engineering can be seen as useful artifacts that you obtain as a bonus from math.\n \nreply",
      "Yeah, or air.Besides, there's plenty more to science and engineering than just math.\n \nreply",
      "Maybe start using Roman numerals then?\n \nreply"
    ],
    "link": "https://deadreckonings.com/2007/12/07/heavisides-operator-calculus/",
    "first_paragraph": "An operational calculus converts derivatives and integrals to operators that act on functions, and by doing so ordinary and partial linear differential equations can be reduced to purely algebraic equations that are much easier to solve. There have been a number of operator methods created as far back as Leibniz, and some operators such as the Dirac delta function created controversy at the time among mathematicians, but no one wielded operators with as much flair and abandon over the objections of mathematicians as Oliver Heaviside, the reclusive physicist and pioneer of electromagnetic theory.The name of Oliver Heaviside (1850-1925) is not well-known to the general public today. However, it was Heaviside, for example, who developed Maxwell\u2019s electromagnetic equations into the four vector calculus equations in two unknowns that we are familiar with today; Maxwell left them as 20 equations in 20 unknowns expressed as quaternions, a once-popular mathematical system currently experiencin",
    "summary": "In a riveting blast from the past, *deadreckonings.com* resurrects the ghost of Oliver Heaviside to dazzle dozens with his old-timey operator calculus. As the digital crowd gawks, marveling at how Heaviside whipped Maxwell's equations into shape like a Victorian Gordon Ramsay, the commenters joust with mentions of <em>renormalization</em> and Feynman diagrams, as if ingeniously proving their grasp of \"serious physics\" to each other. One brave soul waxes nostalgic about high school physics, metaphorically hugging the concept of bouncing balls as the pinnacle of fun math - ironically missing the entire point of their AP exam. Mathematicians, feel free to return to your hair-tearing; Heaviside's cavalier math antics are safe in the hands of the internet's echo chamber, where every partial derivative reveals a philosopher's soul. \ud83d\ude02"
  },
  {
    "title": "Photo Robot Takes the Perfect Picture (ieee.org)",
    "points": 11,
    "submitter": "rbanffy",
    "submit_time": "2024-11-23T19:40:06 1732390806",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://spectrum.ieee.org/photo-robot",
    "first_paragraph": "See our latest special report, \u201cReinventing Invention: Stories From Innovation\u2019s Edge\u201d \u2192PhotoBot works with users to bring their imagination to lifeKohava Mendelsohn is an editorial intern at IEEE Spectrum.Photobot can find your best side.Finding it hard to get the perfect angle for your shot? PhotoBot can take the picture for you. Tell it what you want the photo to look like, and your robot photographer will present you with references to mimic. Pick your favorite, and PhotoBot\u2014a robot arm with a camera\u2014will adjust its position to match the reference and your picture. Chances are, you\u2019ll like it better than your own photography.\u201cIt was a really fun project,\u201d says Oliver Limoyo, one of the creators of PhotoBot. He enjoyed working at the intersection of several fields; human-robot interaction, large language models, and classical computer vision were all necessary to create the robot.Limoyo worked on PhotoBot while at Samsung, with his manager Jimmy Li. They were working on a project to",
    "summary": "<strong>Robotic Rembrandt Redefines Selfie Superiority</strong>\n\nIn a groundbreaking fusion of indolence and artificial intelligence, IEEE introduces PhotoBot: the mechanical messiah destined to rescue humanity from the hardship of taking their own photos. Spearheaded by the intrepid Oliver Limoyo, who apparently enjoys watching a robotic arm do all the work, PhotoBot promises to locate angles you didn\u2019t know existed, all while marinating in the glory of three whole fields of study. Doting technophiles in the comment section are already pledging allegiance, debating if their selfie stick can be repurposed as a fetch stick for their new robot overlords. \ud83e\udd16\ud83c\udfa8 Isn't progress <em>wonderful?</em>"
  },
  {
    "title": "Show HN: Steel.dev \u2013 An open-source browser API for AI agents and apps (github.com/steel-dev)",
    "points": 22,
    "submitter": "marclave",
    "submit_time": "2024-11-26T13:34:40 1732628080",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=42245573",
    "comments": [
      "Looking interesting, will definitely give it a go.Btw, there is inconsistency between pricing page and pricing on docs.Pricing page for developers is $59\nPricing in docs for developers is $99\n \nreply",
      "awesome - let us know how it goes!great catch on pricing mismatch, just fixed, thank you :)\n \nreply",
      "Very interesting.  I worry that if I use your cloud, and a lot of other people do, all of your IP addresses will get banned by all the big players.  It will definitely be a fun cat and mouse game!Related story:  Way back in the day, PayPal was just getting started, and decided eBay transactions would be their perfect customer.  The only problem is that eBay didn't allow scraping.  So they built an entire proxy infrastructure to go around eBay's rules and scrape them.It worked.  It worked so well, eBay bought PayPal.The side effect of this is that I got control of the PayPal proxy infrastructure since I was on the security team for both eBay and PayPal after the acquisition.We used that proxy farm to scrape the rest of the web looking for fake eBay sites (because they would block traffic from eBay's IPs) and we had the guys who built it help us build proxy defense for eBay and PayPal.So this could work in your favor, if you manage to constantly scrape a large target who might want to buy you. :)\n \nreply",
      "That honestly sounds so fun! Who better to build your defences than your attackers ahahaTying into your concern, keeping IPs fresh and high quality will definitely be a balancing act as we get bigger. It'd be one today too if we were to try to offer super granular location controls because there's only so many proxies in X state, let alone X city. Currently, we get to aggregate & QA from multiple proxy providers, so our total pool is 300M+ IPs in the US and so far we've had a 99.95% rate of getting a fresh IP address in a session. So so far so good :)As for that last point, I guess we'll see what the future has in store for us :P\n \nreply",
      "will definitely check this out. i saw the pricing model on the site, what's the motivation around being open-source here if you're providing the infra free of charge?\n \nreply",
      "Yeah, good question! The reason we went open-source is really about transparency and flexibility. We want people to trust what they\u2019re using and have the option to self-host if that\u2019s what makes the most sense for them. Open-sourcing the browser API also lets us build a better product with input and contributions from the community\u2014it\u2019s a win-win.As for the infra, it\u2019s not totally free\u2014our managed service is there for anyone who doesn\u2019t want the hassle of hosting and scaling everything themselves (although it does have a generous free plan). Going open-source just gives people the choice: run it yourself if you want full control, or use our managed option for convenience and scale. It\u2019s all about making browser automation more accessible without forcing people into one path.\n \nreply",
      "Hello Hacker News! We\u2019re Nas and Huss, co-founders of steel.dev (http://steel.dev). Steel is an open-source browser API for AI agents and apps. We make it easy for AI devs to build browser automation into their products without getting flagged as a bot or worrying about browser infra.over the last year or so, we\u2019ve built quite a few AI apps that interact with the web and noticed - a. it was magical when you could get an llm to use the web and it worked and b. our browser infra was the source of 80% of our development time. Maintaining our browser infrastructure became its own engineering challenge - keeping browser pools healthy, managing session states and cookies, rotating proxies, handling CAPTCHA solving, and ensuring clean process termination. We got really good at running browser infrastructure at scale, but maintaining it was still stealing time away from building our actual products. So we wanted to build the product we wish we had.Steel allows you to run any automation logic on our hosted instances of chromium. When you start a dedicated browser session you get stealth, proxies, and captcha solving out of the box. We do this by exposing websocket and http endpoints so you can connect to these instances with puppeteer, playwright, selenium(in beta), or raw CDP commands if you\u2019re built like that.Behind the scenes, we host several browser instances and route incoming connection requests to one of these instances. Our core design principle was to allow for every session to have its own dedicated browser instance + resources (currently 2gb vram and 2gb vcpu) while still allowing for quick session creation/connection times. Our first thought was to have separate nodes running in a Kubernetes cluster, but the cost of hosting warm browser instances would be expensive (which would be reflected in the pricing), and the boot times would be too slow to handle the scale that some customers required. We got around this by deploying our browser instance image on a firecracker VM, taking advantage of the lightning-fast boot times and ability to share a root FS.Today, we\u2019re open-sourcing the code for the steel browser instance, with plans to open-source the orchestration layer soon. With the open-source repo, you get backwards compatibility with our node/python SDKs, a lighter version of our session viewer, and most of the features that come with Steel Cloud. You can run this locally to test Steel out at an individual session level or one-click deploy to render/railway to run remotely.We're really happy we get to show this to you all, thank you for reading about it! Please let us know your thoughts and questions in the comments.\n \nreply",
      "If these instances are shared, how do you segregate login details, sessions cookies, \u2026etc? Are you always running them in incognito?\n \nreply",
      "Instances are not shared. :) Everyone gets a dedicated session with dedicated resources. One session for every machine.\n \nreply",
      "Very interesting. I\u2019m not sure I immediately see your application either however I have been having similar thoughts.After playing a popular indi game (Kenshi) I was wondering about the very simple automation interface the game relies upon. Why not a virtual world (with interfaces attaching any external source) in which business logic agents interact through the available interfaces of the environment, and other agents. Though tbh, I imagine the entire environment as implemented in layers of YAML style schemas and profiles. So all data, whether in a datastore, active instance, streamed or serialized can be related to in the same way. An envelope with attributes and content specified by the type attribute. The only code would then be the rendering environment, and whatever these agents call for stream processing.Sort of a gamification of automation, though what can\u2019t be beat is dead simple account of what any one thing is doing at a given time.\n \nreply"
    ],
    "link": "https://github.com/steel-dev/steel-browser",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        \ud83d\udea7 Open Source Browser API for AI Agents & Apps. Steel Browser is a batteries-included browser instance that lets you build automate the web without worrying about infrastructure.\n      \n\n\n\n\nThe open-source browser API for AI agents & apps. \n    The best way to build live web agents and browser automation tools.\n\n\n\n\n\n\nSteel.dev is an open-source browser API that makes it easy to build AI apps and agents that interact with the web. Instead of building automation infrastructure from scratch, you can focus on your AI application while Steel handles the complexity.This repo is the core building block behind Steel - a production-ready, containerized browser sandbox that you can deploy anywhere. It includes built-in stealth capabilities, text-to-markdown session management, UI to view/debug sessions, and full browser control through standa",
    "summary": "Title: Show HN: Reinventing the Wheel with Steel.dev \u2013 Your New Overlord for Web Automation\n\nIn a groundbreaking move that shocks absolutely nobody, some brave tech warriors at Steel.dev have introduced yet another tool for the eager masses to \"automate the web,\" because evidently, the existing tools were just too mainstream. Delighting in the complexity of their creation, they reassure developers that they've taken care of all the hard stuff\u2014because managing a full containerized browser sandbox is exactly what every AI tinkerer dreams of at night. Commenters, in a dazzling display of original thought, oscillate between ecstasy over this new toy and mild panic about pricing inconsistencies\u2014a thrilling cat-and-mouse game, but with more proxies and less intrigue. Watch as they revolutionize the scraping game, or at least until everyone\u2019s IP gets banned. Game on, internet. \ud83d\ude80\ud83d\ude02"
  },
  {
    "title": "Ampere WS-1 Japanese APL Clamtop (computeradsfromthepast.substack.com)",
    "points": 4,
    "submitter": "rbanffy",
    "submit_time": "2024-12-01T22:57:19 1733093839",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://computeradsfromthepast.substack.com/p/ampere-ws-1",
    "first_paragraph": "",
    "summary": "In a heroic attempt to memorialize the Ampere WS-1, a computer that dared marry the impracticality of APL with the sleekness of a clamshell, the fine historians at computeradsfromthepast.substack.com wax poetic about this tecnhological chimera. Blessed readers eagerly dive into the comments to flaunt their dubious connections to retro tech cred, each one asserting a more tenuous grasp on reality than the last. \ud83e\udd13 Who knew nostalgia could invoke such profound displays of one-upmanship, rendering the typical bar boasts about old girlfriends and alleged road trips pale in comparison. Truly, Ampere would be proud."
  },
  {
    "title": "Ryugu asteroid sample colonized by terrestrial life despite strict control (phys.org)",
    "points": 18,
    "submitter": "bookofjoe",
    "submit_time": "2024-11-23T12:39:37 1732365577",
    "num_comments": 11,
    "comments_url": "https://news.ycombinator.com/item?id=42220580",
    "comments": [
      "This argument from the article meshes with my own thinking:The reason there have not been new forms of life originating on Earth, evolving alongside our microbial cousins, could be that there is simply no room for a newcomer. In a system where every niche is filled with more advanced life looking for a next meal, even if a new form of life got started, it would not last long.There are a number of related issues regarding life, including LUCA and the emergence of mitochondria in eukaryotic cells, which suggest to me less that a single ancestor existed at that time than that that ancestor's descendents out-competed (or out-lucked) all other comers, and that once established other contenders for life simply couldn't find an available niche.Oh, and bad news for the fans of LGM / panspermia, at least in the \"absence of evidence\" sense:Population statistics indicate that the microorganisms originated from terrestrial contamination during the sample preparation stage rather than being indigenous to the asteroid.\n \nreply",
      "Previous discussion:  \"Rapid colonization of a space-returned Ryugu sample by terrestrial microorganism\" <https://onlinelibrary.wiley.com/doi/10.1111/maps.14288><https://news.ycombinator.com/item?id=42238603>3 days ago, 59 comments.It's not clear to me that the phys.org restatement of that article (which is what it's discussing) adds anything new to the story, which would be HN's criterion for non-dupe status.\n \nreply",
      "> something that has likely already introduced extraterrestrial Earth microbes to the moon and MarsSo, life is possibly already multiplanetary?\n \nreply",
      "A non-clickbait title. Science communication still hold some hope after all.\n \nreply",
      "Sounds like my ex.Life finds a way .\n \nreply",
      "maybe they were already there?\n \nreply",
      "unlikely\nanything that could survive a billion years of hard radiation,hard vacume, zero humidity, and wild temperature swings.....\nand then spring back to life is going to be instantly recognisable as \"not from here\".And as the sample return capsule was leaking asteroid dust everywhere when it landed, if it is alien, we are so fucked.\nThere is a vanishingly small chance that life could endure space as a spore of some kind,but it would require bio mechanisms that are unknown, and as of yet, inconceavable.\nWhat it does point to, is just how tennatious life is here on earth and how easily it gets around,inside of our biosphere, and to the general problem of \nbuilding truely hermetic capsules and seals,and handling protocalls.\nA related problem is the study of the element iron(fe), which in its purest form\nus the most expensive substance on earth.\nThis is because iron will combine with almost anything, from any source, under any conditions, to the piont that, introducing any instrument or probe to actualy test the sample...contaminates it.\nSo the study of pure iron, is exceptionaly\nchallenging.\nLikely that the asteroid sample has been compromised just by the closing mechanism\nbieng jammed with asteroid dust, and it never realy sealed.\nThe moon missions were plagued with dust\nand dust has disrupted operations on mars\n,this would make 3 for 3.\n \nreply",
      "Population statistics indicate that the microorganisms originated from terrestrial contamination during the sample preparation stage rather than being indigenous to the asteroid.From TFA.\n \nreply",
      "Article talks about it. Known Earth microbes.If it was already there it would have been something novel\n \nreply",
      "They dismissed it as being from earth based on \"Population statistics\", not determining them to be known earth microbes.\n \nreply"
    ],
    "link": "https://phys.org/news/2024-11-ryugu-asteroid-sample-rapidly-colonized.html",
    "first_paragraph": "",
    "summary": "**Asteroid Sample Goes Club Med for Earth Microbes**  \nHumans bring back a bit of space rock, and surprise, it's been partying with Earth microbes like it's spring break on Ryugu. Phys.org, a bastion of recycled science tidbits, drops another revelation straight from the DUH Department, detailing how we contaminated a space rock sample despite handling it like a biohazard sushi chef. Meanwhile, the comments section frolics in a field of wild speculation, with debates raging from the improbability of space life enduring a cosmic frat party to mournful laments about clumsy space seals. Oh, and yes \u2013 someone's ex apparently also mastered the art of resilience and ubiquity. \ud83d\ude44\ud83d\ude80"
  },
  {
    "title": "Cursed Linear Types in Rust (geo-ant.github.io)",
    "points": 73,
    "submitter": "todsacerdoti",
    "submit_time": "2024-11-27T10:14:13 1732702453",
    "num_comments": 18,
    "comments_url": "https://news.ycombinator.com/item?id=42254737",
    "comments": [
      "The problem is that programming languages have always focused on the definition side of types, which is absolutely necessary and good, but the problem is that only limiting use by, e.g., \"protected, private, friend, internal, ...\" on class members, as well as the complicated ways we can limit inheritance, are barely useful.We need a way to define how to \"use\" the types we define. That definitional structure is going to bleed from creation of instances into how they live out their lifetimes. It appears that Rust's design addresses some aspects of this dimension, and it also appears to be a fairly major point of contention among y'all, or at least require a steepish learning curve. I don't know, as I prefer to work in ubiquitous environments that are already featureful on 5-10yo distros.One usage pattern many of us have found useful in our software for years is the \"set once and only once\" (singleton-ish) , whether it's for a static class member or a static function var, or even a db table's row(s). I don't know of any programming environment that facilitates properly specifying calculating something even that basic in the init phase of running the system, but I don't explore new languages so much anymore, none of them being mature enough to rely upon. Zig's comptime stuff looks promising, but I'm not ready to jump onto that boat just yet. I am, however, open to suggestions.The real solution will ultimately require a more \"wholistic\" (malapropism intended) approach to constraining all dimensions of our software systems while we are building them out.\n \nreply",
      "> I don't know of any programming environment that facilitates properly specifying calculating something even that basic in the init phase of running the system,The JVM has well-defined class loading semantics, including class initialization, that allow limited initialization capabilities before main is even run. Of course it has other problems too (defining the order in which these fire can be frustrating) but it always struck me as straightforward to work with.\n \nreply",
      "Rust's exclusive ownership can be used for things that can be called at most once (a method can require a non-copyable object as an argument, and takes ownership of it away from you, so you can't call the method again).Rust also has wrapper types like OnceCell that have `get_or_init()` which will init the thing only once, and then it's guaranteed to be immutable. Unlike singletons, these don't have to be globally accessible.\n \nreply",
      "\"Set once and only once\" is achieved well via Rust's OnceLock [1], and it's a pattern I use quite heavily in my Rust code. Especially because OnceLock only requires a shared reference and not a mutable one. It's a really good fit for cached results computed on-demand, scoped to whatever level is reasonable (individual type, thread, or whole process).[1] https://doc.rust-lang.org/beta/std/sync/struct.OnceLock.html\n \nreply",
      "> The problem is that programming languages have always focused on the definition side of types, which is absolutely necessary and good, but the problem is that only limiting use by, e.g., \"protected, private, friend, internal, ...\" on class members, as well as the complicated ways we can limit inheritance, are barely useful.Virtually all software ever developed managed just fine to with that alone.> I don't know of any programming environment that facilitates properly specifying calculating something even that basic in the init phase of running the system, (...)I don't know what I'm missing, but it sounds like you're describing the constructor of a static object whose class only provides const/getter methods.> or even a db table's row(s).I don't think you're describing programming language constructs. This sounds like a framework feature that can be implemented with basic inversion of control.\n \nreply",
      "The readonly property in PHP would fit the bill quite well as it can be set once and only once, no?Plus the new PHP 8.4 version actually has asymmetric visibility for properties so you can have public properties that can not be mutated from the outside but still allow controlled mutation on the inside. The feature was borrowed from swift. I am super excited about it.https://wiki.php.net/rfc/asymmetric-visibility-v2\n \nreply",
      ">One usage pattern many of us have found useful in our software for years is the \"set once and only once\" (singleton-ish)C# has this: https://learn.microsoft.com/en-us/dotnet/csharp/language-ref...\n \nreply",
      "That's new since I've been tramping around VS.NET (F# 2 or 3 & C# from 10ya).My immediate purposes require that I avoid depending on unique programming language/environment constructions, but it helps to learn, so thanks for levelling me up.\n \nreply",
      "> C# has this:This is only syntactic sugar to allow using object initializers to initialize specific member varabiles of a class instance instead of simply using a constructor and/or setting member variables in follow-up statements. It's hardly the feature OP was describing.\n \nreply",
      "While mutation of init-only properties is sometimes done by e.g. serializers through private reflection or unsafe accessors, it otherwise can lead to unsound behavior if class implementation does not expect this. You cannot bypass this through normal language means.Same applies to readonly instance fields.Where does \"syntax sugar\" end and \"true features\" begin?\n \nreply"
    ],
    "link": "https://geo-ant.github.io/blog/2024/rust-linear-types-use-once/",
    "first_paragraph": "Geo\u2019s NotepadMostly Programming and Math\nMostly Programming and MathCursed Linear Types In RustInspired by Jack Wrenn\u2019s post on Undroppable Types\nin Rust, I set out to see if it\u2019s possible to create types that must be used exactly\nonce. From my understanding, those things are called linear types, but\ndon\u2019t quote me on that1.Let\u2019s see if we can create a struct UseOnce<T> which enforces that an instance\nis used (or consumed) exactly once. It should be impossible to consume it\nmore than once, and it should produce a compile error if it\u2019s not consumed at all.\nThe first part is trivial with destructive move semantics, the second\npart is where we steal adapt Jack\u2019s original idea.Playground Link.\nAgain, the clever part is Jack Wrenn\u2019s original idea. I was also surprised this\nworks. To my understanding, it relies on the fact that the compiler can reason\nthat the drop implementation does not have to be generated when consume is \ncalled due to \u2460. There\u2019s some additional unsafe trickery in \u2461,\nwhi",
    "summary": "In the latest blogosphere revelation, a brave Rustacean ventures into the dark arts of linear types, drawing heavily on the genius of someone else's work to create \"\"<i>variables that vanish after you blink</i>\"\". As expected, the comments inevitably devolve into a gaggle of software veterans competing over whose outdated language can implement similarly esoteric features, with each suggesting more convoluted methods to force singleton patterns upon unsuspecting data. Meanwhile, a quieter bunch can't stop marveling at how Rust, once again, redefines the boundaries of modern programming with a feature most will gallantly misuse. \ud83d\ude80\ud83d\udca5\ud83e\udd13"
  }
]