[
  {
    "title": "Launch HN: Regatta Storage (YC F24) \u2013 Turn S3 into a local-like, POSIX cloud FS",
    "points": 325,
    "submitter": "huntaub",
    "submit_time": "2024-11-18T16:49:09 1731948549",
    "num_comments": 203,
    "comments_url": "https://news.ycombinator.com/item?id=42174204",
    "comments": [
      "I used the same approach based on Rclone for a long time. I wondered what makes Regatta Storage different than Rclone. Here is the answer: \"When performing mutating operations on the file system (including writes, renames, and directory changes), Regatta first stages this data on its high-speed caching layer to provide strong consistency to other file clients.\" [0].Rclone, on the contrary, has no layer that would guarantee consistency among parallel clients.[0] https://docs.regattastorage.com/details/architecture#overvie...\n \nreply",
      "This is exactly right, and something that we think is particularly important for applications that care about data consistency. Often times, we see that customers want to be able to quickly hand off tasks from one instance to another which can be incredibly complex if you don't have guarantees that your new operations will be seen by the second instance!\n \nreply",
      "Might be useful to show the differences with Rclone, s3fs as a table to make it obvious\n \nreply",
      "I agree, I plan to put up a table soon.\n \nreply",
      "The headline seems misleading, then.rclone can work with AWS' different offerings, some of which at least partially address this: https://aws.amazon.com/blogs/aws/new-amazon-s3-express-one-z...\n \nreply",
      "I'm not totally sure what you mean. I don't think that S3 Express One Zone offers any additional atomic semantics in the file system world.\n \nreply",
      "For the misleading part, I probably should have said confusing because I don't think you intended that, I mean that instead of introducing your caching layer you make it about S3, where the Object Storage provider seems totally interchangeable. Though it seems to work for a lot of your audience, from what I can tell from other comments here.As for Express One Zone providing consistency, it would make more groups of operations consistent, provided that the clients could access the endpoints with low latency. It wouldn't be a guarantee but it would be practical for some applications. It depends on what the problem is - for instance, do you want someone to never see noticeably stale data? I can definitely see that happening with Express One Zone if it's as described.\n \nreply",
      "Yes, I think this is something that I\u2019m actually struggling with. What\u2019s the most exciting part for users? Is it the fact that we\u2019re building a super fast file system or is it that we have this synchronization to S3? Ultimately, there just isn\u2019t space for it all \u2014 but I appreciate the feedback.\n \nreply",
      "Thanks, this was my thought as well.  I use and love rclone and it wasn't immediately clear what this offered above that\n \nreply",
      "This is honestly the coolest thing I've seen coming out of YC in years. I have a bunch of questions which are basically related to \"how does it work\" and please pardon me if my questions are silly or naive!1. If I had a local disk which was 10 GB, what happens when I try to contend with data in the 50 GB range (as in, more that could be cached locally?) Would I immediately see degradation, or thrashing, at the 10 GB mark?2. Does this only work in practice on AWS instances? As in, I could run it on a different cloud, but in practice we only really get fast speeds due to running everything within AWS?3. I've always had trouble with FUSE in different kinds of docker environments. And it looks like you're using both FUSE and NFS mounts. How does all of that work?4. Is the idea that I could literally run Clickhouse or Postgres with a regatta volume as the backing store?5. I have to ask - how do you think about open source here?6. Can I mount on multiple servers? What are the limits there? (ie, a lambda function.)I haven't played with the so maybe doing so would help answer questions. But I'm really excited about this! I have tried using EFS for small projects in the past but - and maybe I was holding it wrong - I could not for the life of me figure out what I needed to get faster bandwidth, probably because I didn't know how to turn the knobs correctly.\n \nreply"
    ],
    "link": "item?id=42174204",
    "first_paragraph": "",
    "summary": "**Welcome to the latest tech revolution** that you didn't know you needed, and probably still don't: *Regatta Storage.* Because reinventing the wheel by slapping a new sticker on it has never been more in vogue. In essence, it's your old pal S3 wearing a fancy POSIX costume, promising you a local disk's warm familiarity\u2014if local disks were prone to existential crises about consistency and speed. Commenters, bewildered tech enthusiasts that they are, oscillate between rehashing documentation links and brewing nostalgia for solutions that already exist. One astute mind even asks for a comparison table\u2014because nothing says innovation like a good ol' spreadsheet showdown! \ud83d\udcca\ud83d\ude80"
  },
  {
    "title": "Show HN: FastGraphRAG \u2013 Better RAG using good old PageRank (github.com/circlemind-ai)",
    "points": 209,
    "submitter": "liukidar",
    "submit_time": "2024-11-18T17:43:13 1731951793",
    "num_comments": 61,
    "comments_url": "https://news.ycombinator.com/item?id=42174829",
    "comments": [
      "So I've done a ton of work in this area.Few learnings I've collected:1. Lexical search with BM25 alone gives you very relevant results if you can do some work during ingestion time with an LLM.2. Embeddings work well only when the size of the query is roughly on the same order of what you're actually storing in the embedding store.3. Hypothetical answer generation from a query using an LLM, and then using that hypothetical answer to query for embeddings works really well.So combining all 3 learnings, we landed on a knowledge decomposition and extraction step very similar to yours. But we stick a metaprompter to essentially auto-generate the domain / entity types.LLMs are naively bad at identifying the correct level of granularity for the decomposed knowledge. One trick we found is to ask the LLM to output a mermaid.js mindmap to hierarchically break down the input into a tree. At the end of that output, ask the LLM to state which level is the appropriate root for a knowledge node.Then the node is used to generate questions that could be answered from the knowledge contained in this node. We then index the text of these questions and also embed them.You can directly match the user's query from these questions using purely BM25 and get good outputs. But a hybrid approach works even better, though not by that much.Not using LLMs are query time also means we can hierarchically walk down the root into deeper and deeper nodes, using the embedding similiarity as a cost function for the traversal.\n \nreply",
      "Very interesting. Thank you getting into the details. \nDo you chunk the text that goes into the BM25 index?\nFor the hypothetical answer, do you also prompt for \"chunk size\" responses?\n \nreply",
      "Thanks for sharing this! It sounds very interesting. We experimented with a similar tree setup some time ago and it was giving good results. We eventually decided to move towards graphs as a general case of trees. I think the notion of using embeddings similarity for \"walking\" the graph is key, and we're actively integrating it in FastGraphRAG too by weighting the edges by the query. It's very nice to see so many solutions landing on similar designs!\n \nreply",
      "> Hypothetical answer generation from a query using an LLM, and then using that hypothetical answer to query for embeddings works really well.This is honestly wear I think LLM really shines. This also gives you a very good idea if your documentation is deficient or not.\n \nreply",
      "Thanks for sharing! These are all very helpful insights! We'll keep this in mind :)\n \nreply",
      "PageRank is one of several interesting centrality metrics that could be applied to a graph to influence RAG on structural data, another one is Triangle Centrality which counts triangles around nodes to figure out their centrality based on the concept that triangles close relationships into a strong bond, where open bonds dilute centrality by drawing weight away from the center:https://arxiv.org/abs/2105.00110The paper shows high efficiency compared to other centralities like PageRank, however in some research using the GraphBLAS I and my coauthors found that TC was slower on a variety of sparse graphs than our sparse formulation of PR for graphs up to 1.8 billion edges, but that TC appears to scale better as graphs get larger and is likely more efficient in the trillion edge realm.https://fossies.org/linux/SuiteSparse/GraphBLAS/Doc/The_Grap...\n \nreply",
      "This is super interesting! Thanks for sharing. Here we are talking of graphs in the milions nodes/edges, so efficiency is not that big of a deal, since anyway things are gonna be parsed by a LLM to craft an asnwer which will always be the bottleneck. Indeed PageRank is the first step, but we would be happy to test more accurate alternatives. Importantly, we are using personalized pagerank here, meaning we give specific intial weights to a set (potentially quite large) of nodes, would TC support that (as well as giving weight to edges, since we are also looking into that)?\n \nreply",
      "Since when does \u201cgood old PageRank\u201d demand an OpenAI API key?\u201cYou may not: use Output to develop models that compete with OpenAI\u201d => they\u2019re gonna learn from you and you can\u2019t learn from them.Glad we\u2019re all so cool with longterm economic downfall of natural humans. Our grandkids might not be so glad about it!\n \nreply",
      "LLMs are only used to construct the graph, to navigate it we use an algorithmic approach. As of now, what we do is very similar to HippoRAG (https://github.com/OSU-NLP-Group/HippoRAG), their paper can give a good overview on how things are working under the hood!\n \nreply",
      "This is very cool, I signed up and uploaded a few docs (PDFs) to the dashboardOur Use case: We have been looking at farming out this work (analyzing complaince documents (manufacturing paperwork) for our AI Startup however we need to understand the potential scale this can operate under and the cost model for it to be useful to usWe will have about 300K PDF documents per client and expect about a 10% change in that document set, month to month -any GraphRag system has to handle documents at scale - we can use S3 as an igestion mechanism but have to understand the cost and processing time needed for the system to be ready to use duiring:1. inital loading \n2. regular updates -how do we delete data from system for examplecool framework btw..\n \nreply"
    ],
    "link": "https://github.com/circlemind-ai/fast-graphrag",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        RAG that intelligently adapts to your use case, data, and queries\n      \nStreamlined and promptable Fast GraphRAG framework designed for interpretable, high-precision, agent-driven retrieval workflows.   Looking for a Managed Service? \u00bb   NoteUsing The Wizard of Oz, fast-graphrag costs $0.08 vs. graphrag $0.48 \u2014 a 6x costs saving that further improves with data size and number of insertions. Stay tuned for the official benchmarks, and join us as a contributor!Fast GraphRAG is built to fit seamlessly into your retrieval pipeline, giving you the power of advanced RAG, without the overhead of building and designing agentic workflows.Install from PyPi (recommended)Install from sourceSet the OpenAI API key in the environment:Download a copy of A Christmas Carol by Charles Dickens:Use the Python snippet below:The next time you initialize ",
    "summary": "**Title: Show HN: FastGraphRAG \u2013 Better RAG using good old PageRank (github.com/circlemind-ai)**\n\nHacker News discovers yet another groundbreaking way to reinvent search, because, as we all know, the *traditional* methods just weren\u2019t cutting it. Presenting FastGraphRAG: a tool no one thought we needed, promising improvements in retrieval workflows via the ground-breaking (i.e., recycled) magic of **PageRank**, now adorned with a shiny new label. Commenters dive deep into techno-jargon labyrinths, debating the finer points of BM25 indexes, embeddings, and hypothetical scenario-based tree querying \u2014 because why solve a problem when you can keep rephrasing it? Watch as enthusiasts cohort behind the dazzling allure of cost savings and code snippets, boldly lecturing each other about the inevitable superiority of their AI configurations, while casually glossing over why their radical innovations still yearn for '90s SEO wizardry. \ud83e\udd13\ud83d\udcbe"
  },
  {
    "title": "20 years of Google Scholar (blog.google)",
    "points": 168,
    "submitter": "thepuppet33r",
    "submit_time": "2024-11-18T18:01:18 1731952878",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=42175023",
    "comments": [
      "The best thing, by a long way, that Google Scholar has achieved is denying Elsevier & co a monopoly on academic search.In most universities here in New Zealand, articles have to be published in a journal indexed by Elsevier's Scopus. Not in a Scopus-indexed journal, it does not count anymore than a reddit comment. This gives Elsevier tremendous power. But in CS/ML/AI most academics and students turn to Google Scholar first when doing searches.\n \nreply",
      "or turn to sci-hub and annas-arhive :)\n \nreply",
      "You use Google Scholar to find papers you're interested in, then use sci-hub to actually read them.\n \nreply",
      "indeed... and use Zotero with the correct plugin to download them automagically\n \nreply",
      "sci-hub hasn't been updated in 4 years and the sources for annas-archive like nexus-stc are seriously hit or miss (depends on the field).\n \nreply",
      "Nothing lasts forever, but the model of buying a paper for 40$ from Elsevier isn't much better. \nDepending on the field there are other sources, but still a hit rate is about 85-90%.\n \nreply",
      "Does sci-hub have up to date content these days?Having pretty wide journal access through my institution means I don\u2019t need to reach out to sci-hub.\n \nreply",
      "sci-hub proper hasn't been updated since it's indefinite pause in december 2020.\nAlternatives are of variable success depending on field. It might be better for CS/Math, but medicine and life sciences it's pretty bad.\n \nreply",
      "i believe they paused due to an indian court injunction and the case was heard this year, does anyone know any update?\n \nreply",
      "scihub is dying unfortunately :( the good news is it is happening just as all the fields i'm interested in except for some experimental physics & biology have moved to OA\n \nreply"
    ],
    "link": "https://blog.google/outreach-initiatives/education/google-scholar-20-years/",
    "first_paragraph": "Nov 18, 2024[[read-time]] min read\n          To celebrate 20 years of Google Scholar, we\u2019re sharing some fun facts about the go-to resource for researchers worldwide.\n        \nGoogle Scholar is a tool that helps researchers find and read research papers. It started 20 years ago, and it's been growing ever since.  It's like a giant library for research, with lots of cool features. You can even use it to find legal cases, save articles, and follow your favorite authors.  It's been used in some surprising ways, like helping people reconnect with family history and even getting marriage proposals!\nGoogle Scholar is a tool that helps researchers find and read research papers. It started 20 years ago, and it's been growing ever since.  It's like a giant library for research, with lots of cool features. You can even use it to find legal cases, save articles, and follow your favorite authors.  It's been used in some surprising ways, like helping people reconnect with family history and even ge",
    "summary": "Title: Google Scholar Turns 20, Academia Shrugs\n\nIn a surprisingly self-congratulatory blog post, Google reminds us that Google Scholar, the haphazard heap of often-cited, rarely-read papers, has turned 20. Here, we explore the \"fun facts\" about a tool that mostly reminds students of how many articles they haven't read before their thesis is due. The comment section quickly devolves into a sidebar conference on Sci-Hub's obsolescence and the thrilling high-stakes world of illegal PDF downloads. One thing is certain: the only \"scholarly\" thing about this discussion is the whiff of desperation as users cling to their Zotero plugins like life rafts on the sinking ship of free academic content. \ud83c\udf93\ud83d\udc94\ud83c\udff4\u200d\u2620\ufe0f"
  },
  {
    "title": "MailCatcher runs a super simple SMTP server (mailcatcher.me)",
    "points": 141,
    "submitter": "mooreds",
    "submit_time": "2024-11-18T16:51:29 1731948689",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=42174231",
    "comments": [
      "A tool like this is very useful, but this one isn't being maintained anymore. MailHog isn't either.MailPit, MailCrab and smtp4dev are modern alternatives.https://github.com/axllent/mailpithttps://github.com/tweedegolf/mailcrabhttps://github.com/rnwood/smtp4dev\n \nreply",
      "I use Mailcatcher for almost ten years now and never had a problem with it. Maybe is doesn't need maintenance?\n \nreply",
      "There's no rush to move away from MailCatcher or MailHog, but if you're not using those solutions already I see no reason to use them over the maintained options.\n \nreply",
      "> I see no reason to use them over the maintained optionsThings that don't change over a longer time period can be more comfortable sometimes. Especially things you use often and build up a sort of \"muscle memory\" about.\n \nreply",
      "Try add it to the Gemfile of a modern Rails project, the dependencies are very out of date and it won\u2019t install.\n \nreply",
      "Docs say> Please don't put mailcatcher into your Gemfile. It will conflict with your applications gems at some point.\n \nreply",
      "It's not designed to go in your Gemfilegem install mailcatcher:-)\n \nreply",
      "Same for Mailhog\n \nreply",
      "Mailpit works amazingly for us. Thanks to it having a very simple API, we've been able to integrate it into Playwright e2e tests, and can easily verify things like complete new user registration and password reset flows in seconds. And the UI is handy for local dev work.\n \nreply",
      "Easy REST API access can be a quite useful feature too.For automated integration testing it's a must. The test can verify in the end if the expected emails were sent out.I think Mailpit can even be set up as a real SMTP server, handling a (sub) domain. Either as a MX or just via forwarding rules. Sometimes it can be useful to periodically run integration tests on a production system. So your tests could create accounts based on your test domain (random-user-name@testsystem.company.tld), which is deliverable from every email server, and the tests can verify the delivery. An automated script can then periodically delete the *@testsystem.company.tld accounts.\n \nreply"
    ],
    "link": "https://mailcatcher.me/",
    "first_paragraph": "Latest version: 0.10.0 (released Friday, 25th May 2024)Catches mail and serves it through a dream.MailCatcher runs a super simple SMTP server which catches any message sent to it to display in a web interface. Run mailcatcher, set your favourite app to deliver to smtp://127.0.0.1:1025 instead of your default SMTP server, then check out http://127.0.0.1:1080 to see the mail that's arrived so far.Use mailcatcher --help to see the command line options. The brave can get the source from the GitHub repository.Please don't put mailcatcher into your Gemfile. It will conflict with your applications gems at some point.Instead, pop a note in your README stating you use mailcatcher, and to run gem install mailcatcher then mailcatcher to get started.Under RVM your mailcatcher command may only be available under the ruby you install mailcatcher into. To prevent this, and to prevent gem conflicts, install mailcatcher into a dedicated gemset with a wrapper script:To set up your rails app, I recommend",
    "summary": "**MailCatcher: A Silent Sentinel in Your Dev Stack**\n\nOnce again, the software world gifts us with **MailCatcher 0.10.0**, a tool so indispensable that it continues its valiant existence mostly by being forgotten until it gruesomely clashes with your other gems. Developers cheer its simplicity, praising a tool that joyously captures emails only to be displayed via the interface of your loneliest localhost dreams. Meanwhile, the comments section becomes a battleground for personal anecdotes about decade-long usage, fiercely defended by those untouched by the whims of software maintenance\u2014or so they believe\ud83d\udca2. Competitor names like MailPit and MailCrab drop casually, as if mentioning them might conjure a more modernized development fairy to tackle the evidently archaic beast that is MailCatcher. Enjoy rewriting your README instructions; heaven knows you don't want this relic lurking in your Gemfile. \ud83e\uddd9\u2728"
  },
  {
    "title": "A BBC navigation bar component broke depending on the external monitor (joshtumath.uk)",
    "points": 208,
    "submitter": "ulrischa",
    "submit_time": "2024-11-18T15:28:57 1731943737",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=42173233",
    "comments": [
      "For anyone who didn't click through to the WebKit bug report the author submitted, a WebKit dev asked him to clarify why the BBC finds it beneficial to be able to detect that the event was sent from a keyboard. This is the author's response:> Ironically, I want interoperability on this to help with use cases relating to accessibility.> I work at the BBC and, on our UK website, our navigation bar menu button behaves slightly differently depending on if it is opened with a pointer or keyboard. The click event will always open the menu, but:> - when opening with a pointer, the focus moves to the menu container.> - when opening with a keyboard, there is no animation to open the menu and the focus moves to the first link in the menu.> Often when opening a menu, we don't want a slightly different behaviour around focus and animations depending on if the user 'clicks' with a pointer or keyboard.> The 'click' event is great when creating user experiences for keyboard users because it is device independent. On keyboards, it is only invoked by Space or Enter key presses. If we were to use the keydown event, we would have to check whether only the the Space or Enter keys were pressed.Source: https://bugs.webkit.org/show_bug.cgi?id=281430\n \nreply",
      "This is fascinating, because the naive English interpretation of the code and the comment on that WebKit bug don't match the actual structure of the code.  Here's the relevant code:    const isInvokedByMouse = event => event.screenX > 0 || event.screenY > 0;\n    const isInvokedByKeyboard = event => isEnterKey(event) || isSpaceKey(event);\n\nIgnoring the actual conditions entirely, this code seems to be trying to categorize the event into one of two categories: mouse or keyboard.  But what it actually does is to categorize into one of four categories:  (mouse and not keyboard), (keyboard and not mouse), (keyboard and mouse), and (neither keyboard nor mouse).  And, as the original bug shows, (neither keyboard nor mouse) is handled inappropriately.  One might wonder whether (keyboard and mouse) works well.Either the code should be deliberate about the fact that (is it a keyboard) and (is it a mouse) are separate booleans, or the code should be structured so that the actual categories are mutually exclusive.  For example:    const isInvokedByMouse = ...\n\nand use !isInvokedByMouse to check for keyboardiness, or:    const eventSource = ... (returns \"keyboard\" or \"mouse\")\n\nor, perhaps even better:    const eventSource = ... (returns \"keyboard\", \"mouse\", or \"not sure\")\n \nreply",
      "This is a great comment ^ whenever you use two booleans like this, you're opening yourself up to \"unrepresentable state\" logic errors. Finding and noticing this in code can be tricky, but it's a great example of something that should be caught during code review.\n \nreply",
      "Not sure if exactly the same thing but reminds me of \"Booleans are a trap\"https://katafrakt.me/2024/11/09/booleans-are-a-trap/\n \nreply",
      "Yet another article prematurely optimizing. It\u2019s like these people have nothing better to do. I\u2019ll wait for my code to get into stupid edge cases first and then fix it. Even if you spend your time avoiding booleans you will still find yourself with some new contradictory state and have to fix it differently anyways.\n \nreply",
      "Coming up with proper representation for your state is almost always worth it. If anything it's the opposite of premature optimisation - normalise first, only denormalise after you've proven that it's needed.\n \nreply",
      "Huh?  This isn\u2019t premature optimization unless you consider trying to write correct code \u201coptimizing\u201d.\n \nreply",
      "And just to add an extra corner case, Mobile Safari changes the click behavior if an onclick handler is registered - even if the click handler is an empty function that does nothing. The onclick handler itself acts as another Boolean that affects the browser's behavior. I don't remember the exact details because it was a corner case (I think to do with scrolling or popovers or touchcancel - I know it was surprisingly nasty). This page mentions something else http://www.quirksmode.org/blog/archives/2010/09/click_event_... \"Fortunately it\u2019s pretty easy to solve: you have to make the element clickable by giving it an onclick event handler of its very own. That handler can be empty; as long as it\u2019s present it will make any element clickable.\".\n \nreply",
      "Seems like a non bug to me.The first mistake the developer made, was that he wanted to create a different user experience between keyboard and mouse. Stick to what you get by default and design your components so they work for both usecases. Don't try to be smart when it comes to accessibility.What he ended up doing is what I would have considered a hack. A solution that inevitably breaks or has side effects.The reason there rarely are good handles to do things differently in accessibility context, is because it's not something that's meant to be handled differently.\n \nreply",
      "See I work in accessibility. Like I provide and create solutions direct to end users with complex needs. Not regular web accessibility. I get the view of this. It\u2019s the same idea of universal access. But actually I don\u2019t fully agree. Yes. If you can stick to this principle - and do try /  but I promise you edge cases - which in itself is what accessibility users are all about - cause headaches. At some level you have to do custom stuff. It\u2019s the best way. Take for example switch users. Yes. If your ui is tab able - great. But what if you need your items scannable in frequency order. Your tab index needs to change to meet the end users needs. Or eye gaze users. The accuracy level changes. Add in cognitive issues. You can\u2019t just make a one size fits all interface. At some stage you need to significantly customize it. You can\u2019t rely on a user just learning a complex system level interaction technique- if they can\u2019t do that you have to customise on an individual level.\n \nreply"
    ],
    "link": "https://www.joshtumath.uk/posts/2024-11-08-how-a-bbc-navigation-bar-component-broke-depending-on-which-external-monitor-it-was-on/",
    "first_paragraph": "",
    "summary": "**The Perils of Dual-Focused Navigation Bars at BBC: A Comedy of Errors**\n\nIn a dizzying spectacle of coding gymnastics, a BBC developer heroically decides to help keyboard users navigate their news feed without dazzling animations. Queue the WebKit bug report drama \ud83c\udfad, where developers scratch their heads wondering why anyone would care to distinguish between slappy mouse clicks and elegant keystrokes. Meanwhile, commenters debate the existential crisis of boolean values, reducing software engineering to a series of \"if-then\" catastrophes and philosophical woes. Will the BBC conquer the mighty external monitor conundrum, or will it succumb to the perilous pitfalls of progressive web accessibility? Stay tuned, folks."
  },
  {
    "title": "Air traffic failure caused by two locations 3600nm apart sharing 3-letter code (flightglobal.com)",
    "points": 151,
    "submitter": "basilesimon",
    "submit_time": "2024-11-14T15:04:57 1731596697",
    "num_comments": 162,
    "comments_url": "https://news.ycombinator.com/item?id=42136817",
    "comments": [
      "I don't know how long that failure mode has been in place or if this is relevant, but it makes me think of analogous times I've encountered similar:When automated systems are first put in place, for something high risk, \"just shut down if you see something that may be an error\" is a totally reasonable plan. After all, literally yesterday they were all functioning without the automated system, if it doesn't seem to be working right better switch back to the manual process we were all using yesterday, instead of risk a catastrophe.In that situation, switching back to yesterday's workflow is something that won't interrupt much.A couple decades -- or honestly even just a couple years -- later, that same fault system, left in place without much consideration because it rarely is triggered -- is itself catastrophic, switching back to a rarely used and much more inefficient manual process is extremely disruptive, and even itself raises the risk of catastrophic mistakes.The general engineering challenge, is how we deal with little-used little-seen functionality (definitely thinking of fault-handling, but there may be other cases) that is totally reasonable when put in place, but has not aged well, and nobody has noticed or realized it, and even if they did it might be hard to convince anyone it's a priority to improve, and the longer you wait the more expensive.\n \nreply",
      "> The general engineering challenge, is how we deal with little-used little-seen functionality (definitely thinking of fault-handling, but there may be other cases) that is totally reasonable when put in place, but has not aged well, and nobody has noticed or realized it, and even if they did it might be hard to convince anyone it's a priority to improve, and the longer you wait the more expensive.The solution to this is to trigger all functionality periodically and randomly to ensure it remains tested. If you don't test your backups, you don't have any.\n \nreply",
      "Which company deployed a chaos monkey deamon on their systems? Seemed to improve resiliency when I read about it.\n \nreply",
      "Netflix did that many years ago, interesting idea even if a bit disruptive in the beginning https://netflix.github.io/chaosmonkey/\n \nreply",
      "The same company that was in the news recently for screwing up a livestream of a boxing match.\n \nreply",
      "The chaos monkey is there to remind you to always mount a scratch monkey.\n \nreply",
      "\"Your flight has been delayed due to Chaos Monkey.\"\n \nreply",
      "Dig into the OpenZFS 2.2.0 data loss bug story. There was at least one ticket (in FreeBSD) where it cropped up almost a year prior and got labeled \"look into layer,\" but it got closed.I'm aware closing tickets of \"future investigation\" tasks when it seems to not be an issue any longer is common. But, it shouldnt be.\n \nreply",
      ">it shouldnt beSoftware can (maybe) be perfect, or it can be relevant to a large user base. It cannot be both.With an enormous budget and a strictly controlled scope (spacecraft) it may be possible to achieve defect-free software.In most cases it is not.  There are always finite resources, and almost always more ideas than it takes time to implement.If you are trying to make money, is it worth chasing down issues that affect a miniscule fraction of users that take eng time which could be spent on architectural improvements, features, or bugs affecting more people?If you are an open source or passion project, is it worth your contributors' limited hours, and will trying to insist people chase down everything drive your contributors away?The reality in any sufficiently large project is that the bug database will only grow over time.  If you leave open every old request and report at P3, users will grow just as disillusioned as if you were honest and closed them as \"won't fix\".  Having thousands of open issues that will never be worked on pollutes the database and makes it harder to keep track of the issues which DO matter.\n \nreply",
      "I'm in total disagreement with your last paragraph.In fact, I can't see how it follows from the rest.Software can have defects, true. There are finite resources, true. So keep the tickets open. Eventually someone will fix them.Closing something for spurious psychological reasons seems detrimental to actual engineering and it doesn't actually avoid any real problem.Let me repeat that: ignoring a problem doesn't make it disappear.Keep the tickets open.Anything else is supporting a lie.\n \nreply"
    ],
    "link": "https://www.flightglobal.com/safety/uk-air-traffic-system-failure-triggered-by-misidentified-french-bee-flightplan-waypoint/157386.article",
    "first_paragraph": "By David Kaminski-Morrow2024-03-15T07:35:00+00:00Investigators probing the serious UK air traffic control system failure in August last year have detailed the flightplan waypoint confusion which triggered the incident.Over 700,000 passengers were affected by the failure of UK air navigation service NATS\u2019 flightplan processing system. This forced controllers to revert to manual processing, leading to more than 1,500 flight cancellations and delaying hundreds of services which did operate.According to an independent panel\u2019s review of the occurrence, the UK\u2019s Swanwick area control centre had received a flightplan from French Bee flight BF371 \u2013 operating from Los Angeles to Paris Orly \u2013 at around 08:32 on 28 August.The aircraft\u2019s route took it through US, Canadian and North Atlantic oceanic airspace before passing over the UK and entering France.Its flightplan had initially been forwarded to pan-European air navigation organisation Eurocontrol for processing. This involved converting the f",
    "summary": "**Air Traffic Apocalypse: Not Your Typical CTRL+ALT+DEL Scenario**\n\nIn a twist that merges incompetence with comical irony, the UK air traffic control plummeted into chaos because some genius concluded that two dots on a map 3600nm apart can be responsibly marked with the same three-letter identity code. The result? Over 700,000 travelers got an impromptu lesson in the existential dread of airport terminals. Not to be outdone, armchair aerospace engineers and panic enthusiasts flocked online to add their \"in-depth\" analyses and reminiscence about the good old days of manual processing, as though air traffic control were akin to restarting a glitchy laptop. Meanwhile, the comment section quickly devolved into a tech support forum for Netflix\u2019s Chaos Monkey, because clearly, what helps streaming services halt at cliffhangers will surely keep planes from doing the same. \ud83d\ude43"
  },
  {
    "title": "The Skyline algorithm for packing 2D rectangles (jvernay.fr)",
    "points": 124,
    "submitter": "lapnect",
    "submit_time": "2024-11-18T15:19:05 1731943145",
    "num_comments": 35,
    "comments_url": "https://news.ycombinator.com/item?id=42173114",
    "comments": [
      "I always thought this was called the Tetris packing algorithm. Because you drop in pieces from the top like in a game of Tetris. (no sliding in sideways allowed :))\n \nreply",
      "An interesting variation is Dynamic Storage Allocation, in which the rectangles can slide only alongside one axis.AKA \"static memory allocation\", since the heights of the rectangles can be interpreted as buffer sizes, and their widths as lifetimes.Most of my PhD's effort has been devoted to beating the SOTA in this. Problem's importance nowadays is owed to deep learning's memory wall.\n \nreply",
      "> Most of my PhD's effort has been devoted to beating the SOTA in this. Problem's importance nowadays is owed to deep learning's memory wall.lol i similarly spent 1 year of my phd trying to finagle this same thing for the same exact reason. got as far as proving that when it's parameterized (like in the case of DNNs with algebraically coupled layers) it's FPT (finite parameter tractable) which is like duh but also as soon as i found that i gave up (not much more to be done <shrug>).\n \nreply",
      "nice! you have your thesis or any related paper available online?\n \nreply",
      "there are a bunch of papers on DSA from the OR peoplehttp://adambuchsbaum.com/papers/dsa-stoc03.pdfhttps://link.springer.com/chapter/10.1007/978-3-540-27798-9_...https://users.cs.northwestern.edu/~pdinda/ics-s05/doc/dsa.pd...there are also some from ML people trying to allocate memory optimally for DNNs:https://arxiv.org/abs/1907.01989https://arxiv.org/abs/1804.10001https://arxiv.org/abs/2001.03288they all boil down to a couple of greedy heuristics. the most recent \"cool\" paper was from a group at googlehttps://dl.acm.org/doi/10.1145/3567955.3567961basic idea is to use both ILP and heurstics. i asked them to open source but no dice :(\n \nreply",
      "An even more recent one from Google: https://github.com/google/minimalloc\n \nreply",
      "i gave up on the problem about 18 months ago so i didn't keep up with the research area. is this yours? the runtimes are of course very good but i don't see a comparison on how good the approximation is vs telamalloc (or just ILP). i'll say this though: it's miraculuos that the impl is so small.\n \nreply",
      "I implemented a variation of this at Intel about 10 years ago to solve the efficient testing of microchips with different input/output pins on a limited-pin in/out tester. The goal was to efficiently load the chips in the tester to maximize utilization, with TT of each chip being the same. Fun times, heuristic-solving my way through the NP bin packing problem.\n \nreply",
      "You can probably get denser packing (at least in theory), if you allow rotation. It's just ugly:https://kingbird.myphotos.cc/packing/squares_in_squares.html\n \nreply",
      "I wonder how well Montecarlo works with a problem like this (for the online version), where you try all the potential places and run simulations adding new random boxes similar to the one added so far in order to see which option allows for better outcome. For sure it's slow, yet it should work well, which is interesting per-se since it would show once more how you can come up with slow but good solutions just simulating stuff. But likely, this is how this heuristics, like Skyline, were found: analyzing the properties of the brute-forced best picks.\n \nreply"
    ],
    "link": "https://jvernay.fr/en/blog/skyline-2d-packer/implementation/",
    "first_paragraph": "",
    "summary": "**Happy Hour at the Tetris Bar: Packing Rectangles Just Got Sassy \ud83c\udfae**\n\nIn a dazzling display of originality that could only be eclipsed by inventing a square wheel, the internet gathers to hail the groundbreaking \"Skyline algorithm,\" where 2D rectangles are arranged with all the strategic precision of a drunken Tetris game. Commenters, flexing their PhDs, dish out a cornucopia of related work, tossing links like confetti, because clearly, what this world lacks is yet another algorithm managing memory allocation. Meanwhile, reminiscing about the halcyon days of their dissertations, our brave scholars wax nostalgic over defeating \"the SOTA\" in a battle surely as epic as their struggle to escape academia. Spoiler: it's all just a fancy way to spin Tetris nightmares into research papers. \ud83e\udd2f\ud83d\udcda"
  },
  {
    "title": "Show HN: Venmo Unofficial API (github.com/integuru-ai)",
    "points": 20,
    "submitter": "richardzhang",
    "submit_time": "2024-11-18T23:04:51 1731971091",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42178130",
    "comments": [
      "This will not end well.If someone with millions or billions of dollars doesn\u2019t have an official API after operating for years, that\u2019s because they don\u2019t want to have one. You may receive a Cease and Desist letter, or they might block your IPs, or just scramble their markup in ways that are hard to figure out. Whatever their approach, they likely have more money and manpower to throw at stopping you than you have to evade them, especially if you\u2019re doing this to multiple large and powerful companies.\n \nreply",
      "I'd also be a bit worried about using something like this in production, especially if it's packaged as a npm lib. Even if the original maintainer has good intentions, it'd be all too easy for some malicious actor to offer them a million dollars to introduce a trojan/credential MITM scraper to later versions.\n \nreply",
      "TIL Venmo uses GraphQL.It's interesting to note, too, that the current Venmo website posts to https://account.venmo.com/api/eligibility to get a token and then separately to https://account.venmo.com/api/payments to perform the actual payment. Those endpoints and shapes are different than what's in the script, which posts to https://api.venmo.com/v1/payments (https://github.com/Integuru-AI/Venmo-Unofficial-API/blob/a28...). I wonder if the v1 API is an older one used for some other service (the mobile app, maybe?).Thanks for sharing, OP.\n \nreply",
      "Fintech and unofficial API are two things I wouldn\u2019t consider using at best and at worst extremely risky and possibly can get you into trouble .\n \nreply",
      "works until it doesn't\n \nreply"
    ],
    "link": "https://github.com/Integuru-AI/Venmo-Unofficial-API",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        An unofficial API for Venmo. \n      This Python module provides a simple interface to interact with the Venmo API, allowing users to perform various operations such as checking balance, making payments, and requesting money.This repo is intended to be used as a package in a larger project.\nhttps://github.com/Unofficial-APIs/IntegrationsThis unofficial API is built by Integuru.ai. We take custom requests for new platforms or additional features for existing platforms. We also offer hosting and authentication services. If you have requests or want to work with us, reach out at richard@taiki.online.Here's a complete list of unofficial APIs built by Integuru.ai.This repo is intended to be used as a package in a larger project: https://github.com/Integuru-AI/Integrations\n        An unofficial API for Venmo. \n      ",
    "summary": "Title: **Unofficial Unwisdom: World+Dog Opines on DIY Venmo API**\n\nToday's Hacker News sideshow features an **unofficial Venmo API** enthusiastically hurled into the void by **Integuru.ai**, because nothing spells \"financial security\" like *unofficial*, *third-party* payment tools. The GitHub README is a buffet of corporate buzzwords and aspirations, promising security through obscurity and a direct pipeline to cease-and-desist adventures. The comments section morphs into a tragicomic chorus, warning of everything from corporate wrath to Trojans hidden in npm packages. Because hey, who doesn\u2019t like playing financial Russian roulette with shadow APIs? Apparently, this is how we fintech now. \ud83d\udc40\ud83d\udcb8"
  },
  {
    "title": "Show HN: Tips.io \u2013 A Tailwind playground with AI, page management, and theming (tips.io)",
    "points": 179,
    "submitter": "TIPSIO",
    "submit_time": "2024-11-18T15:19:24 1731943164",
    "num_comments": 44,
    "comments_url": "https://news.ycombinator.com/item?id=42173119",
    "comments": [
      "Good lord, man, this is a _side_ project? Most impressive. Nothing much to add except to say I am most impressed.\n \nreply",
      "Haha, thanks so much! It did get a bit feature rich and has sat around forever as it wasn't my main source/priority for income.\n \nreply",
      "This seems like a very capable and solidly built project. Well done!That being said, I can't wrap my head around the naming. Why tips.io? What do tips mean in this context?(PS: Excuse me if it's covered in the promo video, I'm currently in a zoom call and I can't put any audio through right now)\n \nreply",
      "Super true. I know... I bought this domain a long time ago for another project that never launched and swore to use it one day. It's a short .io that is easy to remember. I agree it does not match well\n \nreply",
      "Just say it stands for \u201cTailwind is pretty super\u201d or \u201cTailwind integrated play space\u201dDone!\n \nreply",
      "Tailwind Intelligent Site Prototyping\n \nreply",
      "Pite Srototyping?\n \nreply",
      "First thought as well. Apart from \u201ccool.tips\u201d, it makes everything sound like you\u2019re on a patreon page.\n \nreply",
      "Wow, this is ridiculously polished for a one-man-show side project. Massive kudos.Do you have a write-up somewhere of how you built this? I think there is a lot that I (and probably many here on HN) can learn from you.\n \nreply",
      "Thank you so much. Please just make a free site and add your email with notifications enabled or signup for the newsletter. I'll be posting some pretty detailed and mind-blowing updates soon\n \nreply"
    ],
    "link": "https://tips.io",
    "first_paragraph": "",
    "summary": "Today in \"I Swear It's Not Over-Engineered,\" we unveil *Tips.io*: yet another playground where the marriage of AI and CSS gives birth to projects that even the creators can't name meaningfully. A humble Hacker News user launches what he lightly calls a \"side project,\" featuring everything but a built-in coffee machine. Commenters, awestruck by such wizardry, toss about kudos like candy at a parade, while collectively brainstorming acronyms that might make the domain seem less like a vestige of a failed 2012 startup. Meanwhile, someone is definitely not paying attention to their Zoom meeting. \ud83d\ude44"
  },
  {
    "title": "Bird brain from the age of dinosaurs reveals roots of avian intelligence (cam.ac.uk)",
    "points": 86,
    "submitter": "gmays",
    "submit_time": "2024-11-18T15:24:21 1731943461",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=42173184",
    "comments": [
      "Reading this, I had to wonder what was \"opposite\" about these \"opposite birds\".  Apparently it's how the shoulder blade (scapula) connects to the coracoid bone (a bone not present in therian mammals).  From Wikipedia's article on Enantiornithes:> Specifically, in the Enantiornithes, the scapula is concave and dish-shaped at this joint, and the coracoid is convex.  In modern birds, the coracoscapular joint has a concave coracoid and convex scapula.\n \nreply",
      "Enantiornithes is a huge family. Kinda suprising why they didn\u2019t survive the meteor but birds did\n \nreply",
      "A) incredible article, one of the few where I didn\u2019t feel compelled to give up and find the underlying paper. Well written, beautiful diagrams, appreciably concise. Thanks for posting!B) The image of ~starlings hanging out with dinosaurs blew my mind. Talk about an odd juxtaposition! But I\u2019m no dinosaur nerd, and haven\u2019t seen the new generation of shows.C) I just have to nitpick this to defend my buddies:  Modern birds have some of the most advanced cognitive capabilities in the animal kingdom, comparable only with mammals.\n\nMaybe true for vertebrates, but octopuses deserve a spot on that list!\n \nreply",
      "The figures in the nature article are worth it. Even my non-nerdy wife thought it was kind of interesting.\n \nreply",
      "from the title I was hoping Fred Flintstone would be the one explaining it to me\n \nreply"
    ],
    "link": "https://www.cam.ac.uk/stories/roots-of-bird-intelligence",
    "first_paragraph": "By Sarah CollinsPublished 13 November 2024Skull of Navaornis hestiae. Credit: Guillermo Naval\u00f3nSkull of Navaornis hestiae. Credit: Guillermo Naval\u00f3nResearchers have identified a remarkably well-preserved fossil bird, roughly the size of a starling, from the Mesozoic Era. The complete skull has been preserved almost intact: a rarity for any fossil bird, but particularly for one so ancient, making this one of the most significant finds of its kind.The extraordinary three-dimensional preservation of the skull allowed the researchers, led by the University of Cambridge and the Natural History Museum of Los Angeles County, to digitally reconstruct the brain of the bird, which they have named Navaornis hestiae. Navaornis lived approximately 80 million years ago in what is now Brazil, before the mass extinction event that killed all non-avian dinosaurs.Artist\u2019s impression of Navaornis. Credit: J\u00falia D\u2019OliveiraArtist\u2019s impression of Navaornis. Credit: J\u00falia D\u2019OliveiraThe researchers say their ",
    "summary": "In a stunning display of taking a century-old fossil and slapping some high-tech CGI on it to make headlines, researchers from the University of Cambridge miraculously rebuild a dinosaur-era bird's noggin to conclude... yes, birds were smart. Meanwhile, whimsical commenters fantasize about starlings bro-fisting Velociraptors, lament the tragic extinction of anything that couldn\u2019t dodge a meteor, and make the startling revelation that both birds and octopuses have brains. Extra points go to the one hopeful soul yearning for a Flintstone-themed paleontology lesson, proving once again that nostalgia is just as preserved as ancient skulls. \ud83e\udd96\ud83e\udde0\ud83d\udca5"
  },
  {
    "title": "Don't sit on the toilet for more than 10 minutes, doctors warn (cnn.com)",
    "points": 36,
    "submitter": "RyeCombinator",
    "submit_time": "2024-11-19T00:13:54 1731975234",
    "num_comments": 17,
    "comments_url": "https://news.ycombinator.com/item?id=42178746",
    "comments": [
      "Leaving aside health issues to those involved, people who use smartphones whilst there pose health risks to others especially when preparing food.These people are likely to wash their hands afterwards then think nothing of answering their smartphones when preparing food. I'm surprised that health officials don't make a much bigger deal of the fact.\n \nreply",
      "2.7 liters to 3.7 liters of water per day?? That's got to include the water we get from food, right? Is anyone here seriously drinking 128 ounces of water per day? I'd always heard 64 ounces as a guideline, but that's like half. How could the guidelines vary by that much?\n \nreply",
      "Most toilets are too high.  I'm almost 6' but would rather put more pressure on my feet than my arse.   Not squatting but in that direction.\n \nreply",
      "They won\u2019t be satisfied until they have taken everything away.\n \nreply",
      "Barring some existing condition, it feels to me that _needing_ to spend that long on the toilet is a major diet and exercise red flag already.\n \nreply",
      "Can't prolonged sitting and coding cause these issues?\n \nreply",
      "Some of them!The ones related to your butthole being out and lower than your spread cheeks, probably not.Unless you know some wild programming tricks that I don't.\n \nreply",
      "You don't know me\n \nreply",
      "Don't sit on the toilet.  Period.Resting squats fixed my knees.\n \nreply",
      "What type of toilet allows this?\n \nreply"
    ],
    "link": "https://www.cnn.com/2024/11/12/health/phones-on-toilet-wellness/index.html",
    "first_paragraph": "Follow:\nGet inspired by a weekly roundup on living well, made simple.\u00a0Sign up for CNN\u2019s Life, But Better newsletter for information and tools designed to improve your well-being.\n\n            Let\u2019s be honest \u2014 people have a habit of bringing their phones to the bathroom. I\u2019ve been guilty of it myself, and chances are that someone is on the toilet reading this article right now. A three-minute trip to the loo can easily turn into 15 minutes of reading, scrolling and posting.\n    \n            It might seem a harmless way to pass the time when you\u2019re going number two. However, experts warn that what they call prolonged sitting on the toilet can harm your health. It\u2019s even been connected to an increased risk of hemorrhoids and weakened pelvic muscles, said Dr. Lai Xue, a colorectal surgeon at the University of Texas Southwestern Medical Center in Dallas.\n    \n            \u201cWhen patients present to me with complaints, one of the main areas we have to delve deeply into is spending a lot of ti",
    "summary": "In an epic bowel movement of journalism, CNN warns against the life-threatening perils of spending more than 10 minutes on the throne, converting bathroom breaks into near-lethal engagements. Apparently, killing time on your phone while parked on the porcelain can lead to a barrage of health issues including\u2014but not limited to\u2014hemorrhoids and the catastrophic weakening of pelvic musculature, courtesy of Dr. Lai Xue\u2019s riveting expository on toilet-time terrorism. Commenters, in a swirling vortex of confusion and disbelief, deflect with utterly crucial inquiries such as the precise daily water intake in liters and the ergonomic heights of modern toilets, while bravely tackling the overlooked dangers of smartphone usage post-handwash. Clearly, humanity\u2019s survival hinges not on climate change or global health crises, but on mastering the art of <em>expeditious excretion</em>."
  },
  {
    "title": "Waiting for many things at once with io_uring (mazzo.li)",
    "points": 65,
    "submitter": "ashvardanian",
    "submit_time": "2024-11-14T12:04:46 1731585886",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=42135412",
    "comments": [
      "io_uring and Linux's many different types of file descriptors are great. I mean, I personally think that the explicit large API surface of WinNT is kinda nicer than jamming a bunch of weird functionality into files and file descriptors like Linux, but when things work, they do show some nice advantages of unifying everything to some framework, ill-fitting as it may sometimes be (Though now that I say this, it's not like WinNT Objects are really any different here, they just offer more advanced baseline functionality like ACLs). io_uring and it's ability to tie together a lot of pre-existing things in new ways is pretty cool. UNIX never really had a story for async operations, something I will not fault an OS designed 50 years ago for. However, still not having a decent story for async operations today is harder to excuse. I've been excited to learn about io_uring. I've learned a lot listening to conference talk recordings about it. While it has its issues (like the many times it (semi-?)accidentally bypassed security subsystems...) it has some really cool and substantial benefits.I'll tell you what I would love to see next: a successor to inotify that does not involve opening one zillion file descriptors to watch a recursive subtree. I'm sure there are valid reasons why it's not easy to just make it happen, but it feels like it will be a major improvement in a lot of use cases. And in many cases, it would probably fix the dreaded problem of users needing to fight against ulimits, especially in text editors like VSCode.I don't have anything of great substance to say about the actual subject of the article. It feels a bit late to finally get this functionality proper in Linux after NT had it basically forever, but any improvement is welcome. Next time I'm doing something where I want to wait on a bunch of FDs I will have to try this approach.\n \nreply",
      "> inotifyA hack that should be performant enough if properly implemented would be a custom FUSE implementation over the directory. As a one-off it could just do the callbacks you want done, or as a reusable component it could implement the inotify behavior you want.\n \nreply",
      "An inotify replacement that can work at whole FS level (and doesn\u2019t require root/admin like the existing option) would be amazing. To be honest, I don\u2019t see a reason it would be hard at the whole filesystem or perhaps mount level unless there are security ramifications. Restricting it to a subdirectory might be tricky though.\n \nreply",
      "fanotify?https://man7.org/linux/man-pages/man7/fanotify.7.html\n \nreply",
      "It feels like last time I looked into this, fanotify was for some reason not suitable for most inotify use cases. Maybe this has changed. Would be great news if so.\n \nreply",
      "Discussion thread in the Erlang community proposing implementing io_uring for BEAM, security issues, and a digression comparing it to FreeBSD's kqueueshttps://erlangforums.com/t/erlang-io-uring-support/765/18?pa...\n \nreply",
      "Surprisingly, I only came across Francesco's blog this month. I stumbled upon the 2021 post \"Speeding up atan2f by 50x\" while searching for others who have to reimplement trigonometry in SIMD every other year. I've also enjoyed \"Beating the L1 cache with value speculation\" from the same year, as well as the 2013 Agda sorting example.Highly recommend checking it out: https://mazzo.li/archive.html\n \nreply",
      "Wikipedia:> In June 2023, Google's security team reported that 60% of the exploits submitted to their bug bounty program in 2022 were exploits of the Linux kernel's io_uring vulnerabilities. As a result, io_uring was disabled for apps in Android, and disabled entirely in ChromeOS as well as Google servers.[11] Docker also consequently disabled io_uring from their default seccomp profile.[12]Root privilege CVE from earlier this year (2024):\nhttps://nvd.nist.gov/vuln/detail/CVE-2024-0582\n \nreply",
      "Very interesting, but unfortunate there is no example program. I guess that is left as exercise for reader, but it's a bit daunting for a non systems programmer.\n \nreply",
      "For c there is :https://git.kernel.dk/cgit/liburing/tree/examplesThere's also a minimal Rust example for tokio:https://github.com/tokio-rs/io-uring/tree/master/examples\n \nreply"
    ],
    "link": "https://mazzo.li/posts/uring-multiplex.html",
    "first_paragraph": "When doing systems programming we often need to wait for something to happen. Common examples might be waiting for some data to come through a socket or waiting on a lock. We also often want to wait on any of several conditions to become true. A web server might be handling many sockets at once, waiting for any number of them to become readable or writeable.This short blog post is concerned with the latter scenario in Linux. Until recently there was no generic framework which allowed us to wait on many arbitrary events, but now there is, thanks to io_uring.The way one usually waits for something to happen is through a system call. For instance:This list is by no means exhaustive but it gives an overview of what kind of waiting we might want to do.In many scenarios we might need to wait on many events at once. We\u2019ve already mentioned the most common example: handling many file descriptors. In that case we\u2019re in luck, since we can use syscalls such as epoll_wait for that exact purpose.Mo",
    "summary": "While the cutting-edge Linux enthusiasts are busy waiting for io_uring to revolutionize event handling, the rest of the tech world can't decide if the overdue feature is a major breakthrough or just another instance of reinventing the square wheel. Comment sections, as usual, turn into a rich tapestry of barely-masked disdain for anything not written in Bearded Wizard's favorite language while also collectively forgetting that every other OS did similar things when disco was still cool. Amidst link-dropping humblebrags and theoretical fixes for inotify that will never be written, someone occasionally remembers to discuss the actual topic. In the meantime, Windows developers peek over the fence, smirking at the security exploit parade, happy with their \"ancient\" but stable APIs. \ud83d\udc27\ud83d\udcbb\ud83d\udd12"
  },
  {
    "title": "Two undersea cables in Baltic Sea disrupted (cnn.com)",
    "points": 112,
    "submitter": "mooreds",
    "submit_time": "2024-11-18T14:31:17 1731940277",
    "num_comments": 312,
    "comments_url": "https://news.ycombinator.com/item?id=42172565",
    "comments": [
      "And also the cable between Lithuania and Sweden:https://www.theguardian.com/world/2024/nov/18/telecoms-cable...\n \nreply",
      "> Joint statement by the Foreign Ministers of Finland and Germany on the severed undersea cable in the Baltic Sea> We are deeply concerned about the severed undersea cable connecting Finland and Germany in the Baltic Sea. The fact that such an incident immediately raises suspicions of intentional damage speaks volumes about the volatility of our times. A thorough investigation is underway. Our European security is not only under threat from Russia\u2018s war of aggression against Ukraine, but also from hybrid warfare by malicious actors. Safeguarding our shared critical infrastructure is vital to our security and the resilience of our societies.https://www.auswaertiges-amt.de/en/newsroom/news/-/2685132\n \nreply",
      "It's worth mentioning that cable breakages happen quite often; globally about 200 times per year [1] and the article itself mentions that just last year, two other cables and a gas pipeline were taken out by an anchor. The Gulf of Finland is evidently quite shallow. From what I understand, cable repair ships are likely to use ROVs for parts of repair jobs but only when the water is shallow so hopefully they can figure out whether the damage looks like sabotage before they sever the cable to repair it. Of course, if you're a bad actor and want plausible deniability, maybe you'd make it look like anchor damage or, deliberately drag an anchor right over the cables.Cable repairs are certainly annoying and for the operator of the cable, expensive. However, they are usually repaired relatively quickly. I'd be more worried if many more cables were severed at the same time. If you're only going to break one or two a year, you might as well not bother.1: https://www.theverge.com/c/24070570/internet-cables-undersea...\n \nreply",
      "A 1 in 36 million chance for three breaks in one day.https://mathb.in/80217\n \nreply",
      "That's assuming independence. I'm not ruling out sabotage but the world is often not fully independent. A storm or an anchor both may affect multiple cables if they're in generally the same area which would definitely make the probability far more likely than those stated. (edit typo)\n \nreply",
      "Indeed !https://www.datacenterdynamics.com/en/news/lithuania-sweden-...>Lithuania-Sweden subsea cable cut, was 10m from severed Finnish-German cable\n \nreply",
      "I'm supporting gleenn who beat me by seconds to much the same observation.Clusters are a thing.\n \nreply",
      "This is a misleading framing. The two cables last year were not taken out by an anchor as an accident, it was literally a ship putting down its anchor just before the cable and then dragging it over the cable. In other words, sabotage. There's no point in trying to color any of this with rose tinted glasses when it's clear who's done it and why.\n \nreply",
      "Have you filed your observations of the ships anchor at sea to the authorities? Because it does sound strange, if you indeed have a witness to this, that they dropped and then hoisted their anchors to damage infrastructure four times that day:> Swedish-Estonian telecoms cable at 1513 GMT, then over the Russian cable at around 2020 GMT, the [Balticconnector gas pipeline] at 2220 GMT and a Finland-Estonia telecoms line at 2349 GMT.https://www.reuters.com/world/europe/russia-says-telecoms-ca...\n \nreply",
      "They just released a statement saying it is sad that they have to be suspicious that it is, perhaps, sabotage:https://www.auswaertiges-amt.de/en/newsroom/news/-/2685132\n \nreply"
    ],
    "link": "https://www.cnn.com/2024/11/18/europe/undersea-cable-disrupted-germany-finland-intl/index.html",
    "first_paragraph": "Follow:\n            Two undersea internet cables in the Baltic Sea have been suddenly disrupted, according to local telecommunications companies, amid fresh warnings of possible Russian interference with global undersea infrastructure.\n    \n            A communications cable between Lithuania and Sweden was cut on Sunday morning around 10:00 a.m. local time, a spokesperson from telecommunications company Telia Lithuania confirmed to CNN.\n    The U.S. has detected an increase in Russian military activity around deep sea telecommunications cables, and believes the Kremlin could be directing operations to disable them. Jim Sciutto has details in an exclusive CNN investigation.\nRelated video\nExclusive: U.S. concerned about possible Russian sabotage of undersea cables\n\n            The company\u2019s monitoring systems could tell there was a cut due to the traffic disruption, and that the cause was likely physical damage to the cable itself, Telia Lithuania spokesperson Audrius Stasiulaitis told ",
    "summary": "In a thrilling twist that surely no one saw coming, two undersea cables in the Baltic Sea have been disrupted, triggering a wave of indignation fueled by fears of Russian meddling. Cue a parade of online armchair detectives, each with a hot take rivaled only by their self-assumed expertise in undersea cable infrastructure. Comment sections are ablaze with conspiracy theories and probabilistic piffle, as suddenly everyone is a seasoned statistician debating the likelihood of \"accidental\" cable cuts. Forget international espionage\u2014this is where the real action is, sprinkled generously with hyperlinks to everything vaguely related, because nothing says 'I'm informed' like a copied URL. \ud83e\uddd0\ud83d\udcc9"
  },
  {
    "title": "Reactive HTML Notebooks (maxbo.me)",
    "points": 293,
    "submitter": "california-og",
    "submit_time": "2024-11-18T08:33:32 1731918812",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=42170740",
    "comments": [
      "Notebooks are hot these days! We also shipped our own version of a TypeScript notebook[1] but it takes quite different sides of the tradeoff: we want to run backend node code, so unlike this or observable we're not looking to run in the browser environment. Still, for many applications, this idea is a better take!Kudos to the author.https://github.com/srcbookdev/srcbook\n \nreply",
      "I strongly agree with the premise of the article - HTML could be a fabulous substrate for computational notebooks!But I didn't love the choices for how to implement it here. Dynamic, reactive HTML can be a lot more declarative than this, and Observable is cool, but strays from standard JS.I started to build a reactive HTML system called Heximal that eventually will have notebook support, but it's declarative, based on HTML templates and custom elements with a expression / reactivity system (based on the TC39 Signals proposal) on top.https://github.com/elematic/heximalIt's a bit like a mashup of HTMX, Tangle, Curvenote, and Polymer. Or like HTML if were natively reactive.I think it will lend it self to graphical editing and notebook user cases quite well.\n \nreply",
      "\"Observable is cool, but strays from standard JS\"Are you taking into account Observable Framework here? That came out in March and one of the major features was that it uses standard JavaScript, not the syntax hacks they invented for Observable Notebooks: https://observablehq.com/blog/observable-2-0#a-better-develo...\n \nreply",
      "> and Observable is cool, but strays from standard JS.The front end does but the underlying runtime is running just javascript, and the source code is basically javascript with some trivial macros which are fully captured in the MIT licensed acorn parser. That's why normal Javascript debugging expression work perfectly in Observablehq.https://github.com/observablehq/parserI love the Observable runtime. I wrote a decompiler for it so you can bidirectionally convert between the front end source and the compiled pure JS representation.https://observablehq.com/@tomlarkworthy/observablejs-toolcha...\n \nreply",
      "What is the need for the runtime? I've ported exported ObservableHQ code to plain JS, and the runtime bits were a lot of hard-to-read indirection. Lots of calling into function references with strings identifying the parameter names.It seems like it might provide some kind of reactive signal abstraction, but modern signal libraries, and the TC39 Proposal, seem to do this in a lower-level and more ergonomic way.\n \nreply",
      "With all due respect to the considerable thought and effort you put into this, the ergonomics of this approach are hideous.  Why would I ever care about the styling elements when I'm just trying to do some exploratory data analysis.  This is exactly why things like Jupyter notebooks excel.  Regardless kudos to your curiosity and implementing alternate ideas.\n \nreply",
      "Author: The ergonomics of this _are_ hideous, to my dismay, which was a lot of the motivation behind @celine/celine (https://maxbo.me/celine, aka me packaging the article up into a library).It's still not quite there as a platform for exploratory data analysis - you don't have the instant reactivity of either a fully-fledged web code editor from Observable Notebooks or the hot-reloading file-watching Observable Framework. And the new Jupyter Kernel for Deno + VSCode is a pretty smooth experience too.So while I agree that the ergonomics for exploratory analysis is uhhhh bad, I don't think the _publishing_ ergonomics is that bad.  In fact, they're good. It's just a single file! I don't need to maintain some massive toolchain or pay some 3rd party service to just send someone a graph and some data munging - I just lob a HTML file at someone over Slack or host it somewhere. And the flexibility to style the analysis means that you can publish in environments where styling is important (blogs, or as a research paper).\n \nreply",
      "Pluto has a way to publish as HTML, even with some interactivity, via SliderServer. How is this different?\n \nreply",
      "Well the edited artifact and the published artifact are one and the same. I think there's value in a \"buildstepless\" notebook format.Thanks for the heads up about SliderServer btw, was experimenting with something similar with Jupyter: https://hello-notebook-http-mode.fly.dev/\n \nreply",
      "yeah, but all the data analysis libs are in python.\n \nreply"
    ],
    "link": "https://maxbo.me/a-html-file-is-all-you-need.html",
    "first_paragraph": "",
    "summary": "In the ever-tumultuous quest to radically reinvent the wheel, the latest dispatch from the HTML notebook frontiers arrives, sparking a flare-up among the tech commentariat. Eager enthusiasts and detractors alike dive headfirst into the comment section, brandishing their latest GitHub arsenals and overcooked opinions. One commenter \"reimagines\" HTML with a proprietary flavor of reactive mysticism, while another mourns the loss of plain JavaScript amidst well-meaning but ultimately perplexing Observable frameworks. Through the cacophony, one truth remains unshakable: nothing charms the coding commons quite like squabbling over whose compiler is the shiniest. \ud83e\udd13\ud83d\udd27"
  },
  {
    "title": "Trieve (YC W24) Is Hiring a software engineer to build OpenAPI tooling (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-11-18T21:00:36 1731963636",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/trieve/jobs/arKjyiM-software-engineer-openapi-tooling",
    "first_paragraph": "All-in-one API for search, RAG, & recommendationsTrieve is an all-in-one solution for discovery written in Rust with UI\u2019s in SolidJS. Check it out at github.com/devflowinc/trieve.We have a large 16 resource, 70-route API and it\u2019s hard to build high quality SDK\u2019s. Existing OpenAPI generator solutions are end-to-end. They do types, docs, and request handler functions.\n\nWe need a generator that only does types + boilerplate request handler functions and lets us fill-in-the-blanks for those functions. \n\nDownstream, it needs to insert the handler functions into the OpenAPI spec as x-codeSamples. We are calling this a \u201cheadless OpenAPI generator\u201d.WE NEED SOMEONE EXCITED ABOUT BUILDING THIS!We need to create this for multiple languages. Our generators are going to be MIT licensed and care needs to be applied to make sure they are easy to contribute to. We are aiming to create a spiritual successor to the github.com/OpenAPITools/openapi-generator project that\u2019s better maintained.Trieve aims to",
    "summary": "**Title**: Trieve (YC W24) Is Hiring \n\n**Summary**: In the latest Silicon Valley flex, Trieve combines the spellbinding complexity of Rust and the obscurity of SolidJS to revolutionize how we misinterpret APIs. Desperate for a \"headless OpenAPI generator\" that only a few living souls understood from the GitHub description, they now seek a mystical software engineer. Not just any coder, but one who vibrates with excitement about types, docs, and something about boilerplate functions that even their creators can't fully explain. Commenters, in a display of peak optimism, debate whether this tool heralds tech utopia or just another cool way to generate bugs that nobody can solve. \ud83d\ude31\ud83d\udc68\u200d\ud83d\udcbb\ud83d\udd2e"
  },
  {
    "title": "Towards Nyquist Learners (gwern.net)",
    "points": 46,
    "submitter": "sleepingreset",
    "submit_time": "2024-11-17T08:15:31 1731831331",
    "num_comments": 24,
    "comments_url": "https://news.ycombinator.com/item?id=42162755",
    "comments": [
      "Title has a misleading domain name (gwern.net). Link is to a PhD thesis titled \"Scaling Laws for Deep Learning\" by Jonathan Rosenfeld. Not sure why wasn't linked more directly,https://arxiv.org/abs/2108.07686https://arxiv.org/pdf/2108.07686#page=85\n \nreply",
      "Generally, if you google a person's name as follows:  \"Jonathan S. Rosenfeld\" +DBLP\n\nyou will get their computer science publication list.From that, you can gather that the two main papers that form the core of Rosenfeld's thesis are these:https://openreview.net/pdf?id=ryenvpEKDrhttps://proceedings.mlr.press/v139/rosenfeld21a.html(if you prefer to read the gist in fewer pages.)\n \nreply",
      "Gwern, have you considered hosting your archived docs on a different subdomain (e.g., doc.gwern.net) to make it clearer that they are not something you have authored yourself? Not sure what the best subdomain would be though.\n \nreply",
      "I think the basic premise of this paper is wrong. Very few natural signals are bandlimited - if images were, they would be no need to store in high resolution, you could just upsample. Natural spectra tend to be pink (decaying ~3dB/octave), which can be explained by the fractal nature of our world (zoom in on details and you find more detail).\n \nreply",
      "JPEG allocates very few bits to the higher frequency elements of the blocks, especially in chroma. https://vicente-gonzalez-ruiz.github.io/JPEG/#lossy-jpeg\n \nreply",
      "Of course that says that our eyes (& more generally our sensory organs) are bandlimited which is what lossy signal compression algorithms exploit (similar to how MP3 throws away acoustic signals we can't hear or how even \"lossless\" is still only recorded at 44 kHz). And indeed any sensor has this problem and it's a physical limitation (e.g. there's only so much resolving power an optical sensor of a certain size can have for an object of a certain distance away which is why we can't see microscopic things and this is a limit from the physics of optics)It says nothing about the underlying signal in nature. But of course we're building LLMs to interact with humans rather than to learn about signals in the true natural world that we might miss.\n \nreply",
      "Any optical system will have a finite resolution.\n \nreply",
      "That applies to individual samples. The eye gets around this by saccading (rapid movements) to get multiple samples. Also, you interact with your environment rather than passively sampling it, so if you want to look closer at something you can just do that.Images aren't truly bandlimited because they contain sharp edges; if they were bandlimited you'd be happy to see an image upscaled with a Gaussian kernel, but instead it's obviously super blurry.When we see an edge in a smaller image we \"know\" it's actually infinitely sharp. Another way to say this is that a single image of two people is fundamentally two \"things\", but we treat it as one unified \"thing\" mathematically. If all images came with segmentation data then we could do something smarter.\n \nreply",
      "Off topic, this thesis has one of the most concise and straightforward acknowledgments section I saw.\n \nreply",
      "> In particular, this minimal frequency is twice the bandwitdh of the\nfunction.Careful, this is misleading.If the peaks of the frequency align with your samples, you'll get the full bandwidth.If the 0-crossings align with your samples, you'll miss the frequency.These are why people swear by things like HD audio, SACD/DSD, even though \"you can't hear over 20khz\"\n \nreply"
    ],
    "link": "https://gwern.net/doc/www/arxiv.org/89f378d0e61fc00754c5e6d175e644578593d372.pdf#page=85",
    "first_paragraph": "",
    "summary": "In our latest installment of misdirected academic enthusiasm, a PhD thesis disguised as a breathless new frontier at gwern.net mystifies the already bewildered internet natives. Declaring \"Scaling Laws for Deep Learning\" the commentariat, in predictable confusion, pitches tents over subdomains and the audacity of high-resolution images existing, despite their beloved JPEG compressions. Meanwhile, someone desperately tries to school folks on Nyquist sampling without realizing they're in a verbal brawl with JPEG lovers and other such riffraff who can\u2019t distinguish a pixel from a wavelet."
  },
  {
    "title": "Is Chrome the New IE? (2023) (magiclasso.co)",
    "points": 187,
    "submitter": "bentocorp",
    "submit_time": "2024-11-17T22:05:11 1731881111",
    "num_comments": 205,
    "comments_url": "https://news.ycombinator.com/item?id=42167749",
    "comments": [
      "No not even close by every single possible measure.I was there, I suffered through it, Google would have to make TONS of hostile moves for that fact to change.I have no interest in the arguments of a closed source subscription service that wants me to switch to the bundled browser of the wealthiest company on earth's most popular consumer OS, lecturing me about using the 4th wealthiest company on earth's browser that I freely installed.The most important one from an anti-trust perspective, every device I've ever had Chrome on I've had to seek out and install/make default Chrome, that includes my mobile devices which used the manufactures browser by default.If I want to use chromium I can, Safari has been VERY late in implementing certain industry spec standards (SSE's, web sockets, IndexedDB API, animations, relative color syntax, container queries, a bunch of <video> stuff, flexbox, the list goes on and on.)\n \nreply",
      "Safari hasn't actually been particular far behind implementing industry standards. As far as I can tell, it's more that people seem to believe that Google dictates industry standards and base everything on when Chrome supports it as opposed to when it actually gets standardized.SSE'sW3C draft standard in 2012. Supported in Safari in 2010.web socketsThis one is true. IETF standard 2011. Supported fully in Safari 2013.IndexedDB APIW3C recommended standard in 2015. Supported in Safari in 2014.animationsIf we're talking the Web Animations API, it hasn't been standardized yet (W3C working draft) and level 2 isn't even that far.relative color syntaxNot standardized yet. It's currently a W3C working draft.container queriesNot standardized yet. It's currently a W3C working draft.a bunch of <video> stuffNeed specifics.flexboxW3C candidate recommendation 2018. Supported in Safari 2013.\n \nreply",
      "This is very misleading, compare implementation timelines between browsers and you'll see that Safari has implemented many of these things year(s) after chromium, firefox and even opera. This of course was because they have tried as much as possible to push people to closed source/walled garden apps.\n \nreply",
      "I'd argue calling non-standard chrome/firefox/opera features \"standards\" is misleading.\n \nreply",
      "It definitely is, I was also there, just like everyone was doing IE only sites, not only plenty of people do the same with ChromeOS vision of the Web, they ship Chrome alongside Electron crap.Safari is the last man standing before a ChromeOS world.\n \nreply",
      "Safari is the last man standing before a ChromeOS world.Except it isn't. Maybe I'm being slightly obtuse here, but the world is not \"Chrome Vs Safari\". It's \"Chrome Vs Safari Vs native apps\". If Safari dies we'll be in a world of \"Chrome Vs native apps\", and that is what Apple wants. Browsers represent a way to deliver software to users that's outside of Apple's revenue mechanisms.Apple have every incentive to keep Safari being good-not-great at running web apps, so users prefer the native version (even though most of the time that'll be Electron.)\n \nreply",
      "Am I the only one left happily using Firefox? You know, the only \"major\" browser that doesn't seem to have these conflicts of interest?\n \nreply",
      "Also happy Firefox user here.  Do not worry, there are dozens of us.  Dozens!\n \nreply",
      "It's always nice to meet a fellow neverChrome.\n \nreply",
      "The Firefox that gets the vast majority of its revenue from Google, that Firefox?I think the only full-featured browser with a prosocial funding model is Konqueror, where what little money there is mostly comes from EU grants. Not coincidental that its code quality was so much better that everyone else based on its rendering engine.\n \nreply"
    ],
    "link": "https://www.magiclasso.co/insights/is-chrome-the-new-ie/",
    "first_paragraph": "With a global browser market share of over 65%, Google\u2019s Chrome has become the target of criticism and disdain, much like Microsoft\u2019s Internet Explorer once was. Is Chrome facing similar challenges to Internet Explorer (IE) and will this lead to its downfall?Over many years commentators have proposed that Chrome is the new IE; but why?Erik Itland (Erik I, 2020):the quote [\u201cChrome is the new Internet Explorer\u201d] tends to be misunderstood to mean that Chrome is like Internet Explorer was in 2009: most people were using it even though it was technically inferior \u2026 What we saw in 2009 [with Internet Explorer] was only the latest stage of something that had been going on for a whileReddit (2018):I can\u2019t stand when many of Google\u2019s services have the view it with Google Chrome, install Google Chrome to use this service, and Google Chrome works best with our search engine. I feel like I\u2019m going back to the ages of when Windows 98 would keep giving me text that would say \u201cUse Internet Explorer t",
    "summary": "Title: Is Chrome the New IE? (2023) (magiclasso.co)\n\nIn an adrenaline-pumping expos\u00e9, a brave keyboard warrior insightfully probes whether Chrome has become the dreaded new IE, neglecting the fact that if you repeat a baseless comparison enough times, it becomes an internet truth. Commenters, armed with nostalgia and selective memory, eagerly jump into the fray\u2014some reminiscing about the 'good old bad days' of IE, others pedantically pointing out timelines of web standards like they\u2019re unveiling plot twists in a B-rated techno-thriller. Meanwhile, the Firefox fans huddle in corners of the comment section, whispering sweet nothings about their beloved browser, blissfully ignoring their overlord Google\u2019s shadow looming over them. How's that for irony? \ud83d\ude44"
  },
  {
    "title": "Histogramming Bytes with Positional Popcount (bitmath.blogspot.com)",
    "points": 5,
    "submitter": "jandrewrogers",
    "submit_time": "2024-11-14T00:10:54 1731543054",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "http://bitmath.blogspot.com/2024/11/histogramming-bytes-with-positional.html",
    "first_paragraph": "50% bits, 50% math (allegedly)A while ago, after some back and forth on twitter/X with @corsix, I dropped some implementation of byte histogramming without explaining anything. This post aims to rectify that lack of explanation.Imagine an array of N k-bit words as an N-by-k matrix of bits. It's easy to take a plain old popcnt across a row of the matrix. pospopcnt is the count per position, ie summing across a column.The amount of \"heavy vertical summation\" (assuming that it is a heavy operation) can be reduced by putting the rows through some \"vertical\" carry-save adders (which works out to some cheap bitwise operations, especially cheap on AVX512 / AVX10 thanks to ternlog), this technique is also discussed in for example Efficient Computation of Positional Population Counts Using SIMD Instructions.That reduction, though efficient, produces an intermediate result in an awkward format. If we have four 512-bit vectors as the intermediate result and are taking the pospopcnt of 64-bit QWOR",
    "summary": "In a world thirsting for <em>real</em> technological advancements, one brave soul at bitmath.blogspot.com decides the most pressing issue is to turn counting bits into a grand intellectual odyssey. Watch in amazement as they transform a simple task into an \"effort-lite mind workout,\" equipped with enough jargon to choke a quantum computer. They throw in terms like \"pospopcnt\" and \"vertical carry-save adders\" as if they're ingredients in a recipe for confusing anyone within a ten-mile radius. Meanwhile, the comment section becomes a battleground where keyboard warriors boast their misunderstood expertise in byte manipulation, while secretly Googling what the heck a popcount is. \ud83e\udd13\ud83d\udd22\ud83d\udca5"
  },
  {
    "title": "LLaVA-O1: Let Vision Language Models Reason Step-by-Step (arxiv.org)",
    "points": 139,
    "submitter": "lnyan",
    "submit_time": "2024-11-18T09:44:54 1731923094",
    "num_comments": 26,
    "comments_url": "https://news.ycombinator.com/item?id=42171043",
    "comments": [
      "This quote summarizes the main secret sauce to me - once they generate a wrong token/phrase, the whole answer goes south - and it basically explains why the whole CoT approach works - prevent LLM from generating a wrong answer with 2 tricks: 1) ask LLM explicitly to generate intermediate steps instead of a final answer and 2) use beam search (filtering from several answers at each stage) to reduce the risk of picking a wrong answer even further.Quote from this paper:\n\u201c Moreover, they (VLM) frequently deviate from a logical reasoning toward conclusions, instead of presenting a conclusion prematurely and subsequently attempting to justify it. Given that language models generate responses token-by-token, once an erroneous conclusion is introduced, the model typically continues along a flawed reasoning path.\u201d\n \nreply",
      "Figure 2 in the paper shows what I really dislike about a lot of vision model benchmarks.I care about whether these VLMs can accurately _see_ and _describe_ things in a picture. Meanwhile the vision part of these benchmarks are a lot of extremely basic OCR that any VLMs of the past year can do. The gains in score come from the LM improving logic skills not from the actual vision ability improving.\n \nreply",
      "That first page graph has a very interesting choice of x-axis.\n \nreply",
      "What's wrong with it? Among the graphed cohort the average benchmark score was between 56 - 66, so they scaled to 55-67. Such a strategy to differentiate is completely normal, and it's weird how often this is called out as being deceptive.Further this is a paper on arXiv, so the idea by some that it's meant to deceive -- as if the target audience isn't going to immediately look at the axis labels, and for more dig into what the benchmarks even were -- is not convincing.I'd hold more criticism for the fact that their lead graphic specifically excludes options which beat it (e.g. GPT-4o, Sonnet), though these details can be found in the chart below.Still interesting. And this \"structuring AI\" approach is how the next evolution in AI is happening.\n \nreply",
      "> What's wrong with itUnfortunately the practice of showing the latter slice runs along that of showing the whole bars, so a better convention to distinguish the two would be beneficial.For example, \"breaking\" the bars (on the left side), similarly to when some bars run too far on the right side. I.e.:  | ==//====|\n  | ==//========|\n  | ==//===|\n  +----------------\n\n...which is not uncommon practice already.\n \nreply",
      "\"Convincing you is more important than informing you\"Always a pass from me, gets things off on the wrong foot right away.\n \nreply",
      "Sadly this is seen at so many prestigious ML conferences, a trimmed X axis which makes performance seem significant when it\u2019s sometimes incremental\n \nreply",
      "I think it's acceptable if you're trying to show subtle differences - but I would probably put the whole plot and then the zoomed version and clearly label it as \"zoomed in for highlighting <.....>\"\n \nreply",
      "You don't need to include 0 on every axis.In this case they really made the numbers smaller than they should be, so it's hard to see that the scale is on the order of single digits. It looks like this is about a 3-4% improvement over GPT-4o-mini and Gemini Pro 1.5.The bigger problem here is not the axis baseline, but the fact that I have no idea (as a non-AI-researcher) what benchmark this is, or if 0 is even the natural minimum. The caption should at least mention what the natural range of the x-axis is.\n \nreply",
      "> the fact that I have no idea (as a non-AI-researcher) what benchmark this isThe figure labels it as as \"average score [on] 6 multimodal reasoning benchmarks\", and the caption notes that the full results are in table 7 - which lists those benchmarks: MMStar-R, MMBench-R, MMVet-R, MathVista, AI2D, HallusionI think it's mostly fine as a lead diagram giving an overview before going into detail.\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2411.10440",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "Title: LLaVA-O1: Let Vision Language Models Stumble Through Logic Like Drunk Philosophers\n\nIn the latest episode of \"Why didn't I think of that?\" arXiv graces us with LLaVA-O1, a paper that basically duct-tapes a pair of glasses on a language model and calls it \"visionary\". Commenters, true to form, dive deep into the gimmicky abyss of technical jargon, passionately debating whether the paper's graph is cunningly deceptive or just poorly executed. Meanwhile, another sighs deeply over the AI misunderstanding pictures, longing for the day when robots can finally appreciate memes. Essentially, we learn that making a language model explain its \u2018thoughts\u2019 is like watching paint narrate its drying process\u2014both painstaking and painfully dull. \ud83c\udfa8\ud83d\udca4"
  },
  {
    "title": "AMD now has more compute on the top 500 than Nvidia (nextplatform.com)",
    "points": 125,
    "submitter": "rbanffy",
    "submit_time": "2024-11-18T18:54:33 1731956073",
    "num_comments": 76,
    "comments_url": "https://news.ycombinator.com/item?id=42175624",
    "comments": [
      "As someone who worked in the ML infra space: Google, Meta, XAI, Oracle, Microsoft, Amazon have clusters that perform better than the highest performing cluster on Top500. They don't submit because there's no reason to, and some want to keep the size of their clusters a secret. They're all running Nvidia. (Except Google, who uses TPUs and Nvidia.)> El Capitan \u2013 we don\u2019t yet know how big of a portion yet as we write this \u2013 with 43,808 of AMD\u2019s \u201cAntares-A\u201d Instinct MI300A devicesBy comparison XAI announced that they have 100k H100s. MI300A and H100s have roughly similar performance. Meta says they're training on more than 100k H100s for Llama-4, and have the equivalent of 600k H100s worth of compute. (Note that compute and networking can be orthogonal).Also, Nvidia B200s are rolling out now. They offer 2-3x the performance of H100s.\n \nreply",
      "> Nvidia B200s ... offer 2-3x the performance of H100sFor ML, not for HPC. ML and HPC are two completely different, only loosely related fields.ML tasks are doing great with low precision, 16 and 8 bit precision is fine, arguably good results can be achieved even with 4 bit precision [0][1]. That won't do for HPC tasks, like predicting global weather, computational biology, etc. --  one would need 64 to 128 bit precision for that.Nvidia needs to decide how to divide the billions of transistors on their new silicon. Greatly oversimplifying, they can choose to make one of the following:  *  Card A with *n* FP64 cores, or \n  *  Card B with *2n* FP32 cores, or \n  *  Card C with *4n* FP16 cores, or \n  *  Card D with *8n* FP8 cores, or (theoretically)\n  *  Card E with *16n* FP4 cores (not sure if FP4 is a thing). \n\nCard A would give HPC guys n usable cores, and it would give ML guys n usable cores. On the other end, Card E would give ML guys 16n usable cores (and zero usable cores for HPC guys). It's no wonder that HPC crowd wants Nvidia to produce Card A, while ML crowd wants Nvidia to produce Card E. Given that all the hype and the money are currently with the ML guys (and $NVDA reflects that), Nvidia will make a combination of different cores that is much much closer to Card E than it is to Card A.Their new offerings are arguably worse than their older offerings for HPC tasks, and the feeling with the HPC crowd is that \"Nvidia and AMD are in the process of abandoning this market\".[0] https://papers.nips.cc/paper/2020/file/13b919438259814cd5be8...[1] https://arxiv.org/abs/2212.09720\n \nreply",
      "With the B100 somehow announced to have lower scalar FP64 throughput than the H100 (did they remove the DP tensor cores ?), one will have to rely on Ozaki schemes (dgemm with int8 tensor cores) and lots of the recent body of work on mixed-precision linear algebra show there's a lot of computing power to be harnessed from Tensor Cores. One of the problems of HPC now is a level of ossification of some codebases (or the lack of availability of porting/coding/optimizing people). You shouldn't have to rewrite everything every 5 years but the hardware constructors go where they go and we still haven't found the right level of abstraction to avoid big porting efforts.\n \nreply",
      "Yes, that's a great point that I missed. From anecdotal evidence, it seems more people are using supercomputers for ML use cases, that would have been traditionally done by HPC. (eg training models for weather forecasts)\n \nreply",
      "The Top500 list is useful as a public, standardized baseline that is straightforward, with a predicted periodicity for more than 30 years. It is trickier to compare cloud infras due to their heterogeneity, fast pace, and more importantly, due the lack of standardized tests, although the MLCommons [1] have been very keen on helping with that.[1] https://mlcommons.org/datasets/\n \nreply",
      "If I understand your comment correctly, we're taking a stable but not that relevant metric, because the real players of the market are too secretive, fast and far ahead to allow for simple comparisons.From a distance, it kinda sounds like listening to kids brag about their allowance while the adults don't want to talk about their salary, and try to draw wider conclusions from there.\n \nreply",
      "Even the DoE posts top 500 results when they commission a supercomputer.\n \nreply",
      "B200s have an incremental increase in FP64 and FP32 performance over H100s. That is the number format that HPC people care about.The MI300A can get to 150% the FP64 peak performance that B200 devices can get, although AMD GPUs have historically underperformed their spec more than Nvidia GPUs. It's possible that B200 devices are actually behind for HPC.\n \nreply",
      "Top line comparison numbers for reference: https://www.theregister.com/2024/03/18/nvidia_turns_up_the_a...It does seem like Nvidia is prioritizing int8 / fp8 performance over FP64, which given the current state of the ML marketplace is a great idea.\n \nreply",
      "MI300 also have decent performance in FP16 (~108 TFLOPS). Not as good as NVIDIA, but it's getting there. Anyone has experience using these on JAX? Support is said to be decent, but no idea if it's good enough for research-oriented tasks, i.e. stable enough for training and inference.\n \nreply"
    ],
    "link": "https://www.nextplatform.com/2024/11/18/amd-now-has-more-compute-on-the-top500-than-nvidia/",
    "first_paragraph": "",
    "summary": "Title: AMD Surpasses Nvidia in a Top 500 Popularity Contest\n\nWelcome to the annual Silicon Valley FLEX-OFF, where AMD and Nvidia throw punches with TFLOPS and spend more energy bragging than computing! \ud83c\udf89 As AMD clutches the \"Most Compute Decorations\" award, overzealous commenters come out in droves to either mourn the death of HPC subtleties or flex Google-sized mega-cluster secrets everyone pretends to not know about. Nvidia fanboys are crying into their B200s, as their favorite chips miss the HPC mark like a failed SpaceX landing. Meanwhile, AMD loyalists parade MI300A specs like a toddler with a new toy\u2014adorable, yet mostly just loud. Sit back, ignore the techno-jargon dodging around real usefulness, and watch this rather pointless leaderboard change more outfits than a fashion week runway. \ud83d\ude0e\ud83c\udf7f"
  }
]