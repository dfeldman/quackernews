[
  {
    "title": "Mpv \u2013 A free, open-source, and cross-platform media player (mpv.io)",
    "points": 354,
    "submitter": "Bluestein",
    "submit_time": "2024-08-17T18:56:36",
    "num_comments": 157,
    "comments_url": "https://news.ycombinator.com/item?id=41277014",
    "comments": [
      "I use it to inspect video frames by frames, particularly being able to go back one frame. VLC doesn't support it, this thread about the feature is hilarious https://forum.videolan.org/viewtopic.php?t=120627\n \nreply",
      "Wow those answers are indeed funny. I agree that as an OSS dev/maintainer, it's easy to fall on the vice of over-generalization and crusade for the perfect solution, and it feels that's exactly what happened there.> this feature is algorithmically impossible> You're just looking at one specific video, not the general problem.> is not generally possible.As a fellow multimedia dev, man, who cares? Sometimes we forget that software ought to be useful, not hypothetical ideals of truth. Just implement the feature for those codecs that support it and which probably are in the 98% percentile of what users actually use, regardless of the damned \"general case\".Or accept and announce shamelessly that you don't have either the knowledge or the development resources to tackle such a complex feature. But excuses about not being possible for absolutely every possible codec in a completely generic way is just denying that the world is just a chaotic and dirty place where things are not ideal nor perfect. Just give your users a real-world solution (or rejection).\n \nreply",
      "VLC is pretty bad and it shows the more you use it.It doesn't let the audio clip/exceed safe volumes and always applies some sort of a limiter, even if it's disabled everywhere in the settings. Try using an EQ on a bass-heavy track and see that it's limited.Try jumping back multiple times in a song at its beginning, and it will play in lower pitch.\n \nreply",
      "> VLC is pretty bad and it shows the more you use it.Compared to what? Such a statement is meaningless without comparison.\n \nreply",
      "> Like said earlier codec frame access is very problematicWow. Talk about not understanding the user.\n \nreply",
      "I have ran into this issue as well. I believe my solution was to use pot player but i am not sure if its open source\n \nreply",
      "i wonder why he's such an ass about it, and totally adamant that it's impossible when multiple players already do this fast. ego?\n \nreply",
      "I think technically he\u2019s correct (I haven\u2019t worked on media decoding code, but I understand how common video encoding formats work). If you have a long video with only a single key frame at the beginning then to step back you would need to, starting from the beginning of the video, decode every frame up to the previous frame you wanted to jump to in order to apply frame deltas, also assuming you have some sort of frame counter to determine when you\u2019ve reached the target frame. In the worst case this does require a lot of compute, but this is an edge case if you primarily care about common video formats with normal encoding settings. I assume seeking backwards is also painfully slow on videos encoded in this manner, so why stepping back 1 frame is out of the question when compared to seeking backwards, I don\u2019t fully understand, it must have something to do with precise frame counts being unavailable on some hardware decoders for some formats (and there being no good workaround) so you _may_ not actually go back 1 frame.I don\u2019t see any reason it couldn\u2019t be supported for a set of formats with reasonable encoding/decoding settings, and provide some error message for other formats if a user attempts to step back, e.g. reverse frame stepping unavailable for current video due to format/encoding/decoding settings.\n \nreply",
      "> video with only a single key frame at the beginningI've literally never seen a video like that in my life, but I'd still expect it to work. Just decode everything starting from frame 1. My desktop can decode H265 at 1,666 fps. I can wait.https://docs.nvidia.com/video-technologies/video-codec-sdk/1...\n \nreply",
      "> If you have a long video with only a single key frame at the beginning then...you can't support the scrub bar efficiently either, so no one encodes video that way.Typically to go to a frame you find the last IDR frame before it (and in reasonable encodings those are frequent enough) and decode forward until you get to the frame of interest. Doing that every time the user presses the single frame back button really doesn't seem that bad, and neither does holding onto some extra reference images for at least like 1080p frames. (8k video and such starts getting more expensive but maybe even then start doing all some references after the first press of the frame back button in this GOP or some such.)It's of course work to do, and I'm not super motivated to send them that patch, but it's possible.> I don\u2019t see any reason it couldn\u2019t be supported for a set of formats with reasonable encoding/decoding settings, and provide some error message for other formats if a user attempts to step back, e.g. reverse frame stepping unavailable for current video due to format/encoding/decoding settings.Yeah, this. That's likely more or less what they already do with the scrub bar.\n \nreply"
    ],
    "link": "https://mpv.io/",
    "first_paragraph": "",
    "summary": "**Mpv \u2013 An Open-Source Media Player For Pedants**\n\nWelcome to another episode of \"Open-Source Software: Where Practicality Meets Pedantry.\" Today\u2019s feature: **Mpv**, the media player that invites you to inspect videos frame by painstaking frame, because clearly, everyone loves to watch paint dry in ultra slow-mo. In the thrilling world of OSS commentary, users clash flamboyantly over features like stepping back a frame\u2014a herculean task apparently akin to rewriting the laws of physics, according to the laureates debating on VLC forums. \ud83c\udf93\ud83d\udc94 Meanwhile, the VLC defense squad serves up a hot plate of excuses as to why basic functionality is as elusive as a coherent plot in a soap opera. Remember, it's not a bug; it's a philosophical impasse on how to implement reality! \ud83e\udd21"
  },
  {
    "title": "Magic Wormhole: get things from one computer to another, safely (github.com/magic-wormhole)",
    "points": 363,
    "submitter": "tosh",
    "submit_time": "2024-08-17T16:59:47",
    "num_comments": 148,
    "comments_url": "https://news.ycombinator.com/item?id=41275920",
    "comments": [
      "I've used this for years when passing large files between systems in weird network environments, it's almost always flawless.For some more exotic testing, I was able to run my own magic wormhole relay[1], which let me tweak some things for faster/more reliable huge file copies. I still hate how often Google Drive will fall over when you throw a 10s-of-GB file at it.[1] https://www.jeffgeerling.com/blog/2023/my-own-magic-wormhole...\n \nreply",
      "> For some more exotic testing, I was able to run my own magic wormhole relay[1], which let me tweak some things for faster/more reliable huge file copies.The lack of improvement in these tools is pretty devastating. There was a flurry of activity around PAKEs like 6 years ago now, but we're still missing:* reliable hole punching so you don't need a slow relay server* multiple simultaneous TCP streams (or a carefully designed UDP protocol) to get large amounts of data through long fat pipes quicklyLast time I tried using a Wormhole to transmit a large amount of data, I was limited to 20 MB/sec thanks to the bandwidth-delay product. I ended up using plain old http, with aria2c and multiple streams I maxed out a 1 Gbps line.IMO there's no reason why PAKE tools shouldn't have completely displaced over-complicated stuff like Globus (proprietary) for long distance transfer of huge data, but here we are stuck in the past.\n \nreply",
      "I overall agree, but \"reliable holpunching\" is an oxymoron. Hole punching is by definition an exploit of undefined behavior, and I don't see the specs getting updated to support it. UPnP IGD was supposed to be that, but well...\n \nreply",
      "Well, with v6 you're down from NAT-hole-punching to Firewall-hole-punching, which in principle should be as simple as arranging the IP:Port pairs of both ends via the setup channel, and then sending a \"SYN\" packet in both directions at once.Then, trying to use e.g. TCP Prague (or, I guess, it's congestion control with UDP-native QUIC) as a scalable congestion controller, to take care of the throughout restrictions caused by high bandwidth delay product.\n \nreply",
      "20MB/sec is 160Mbps, so wormhole wasn't that far off the 1Gbps. Sure not maxing out but within a factor of 6.\n \nreply",
      "> you need a machine that can handle whatever link speeds you needI would have expected the relay server only being used for initial handshake to punch through NAT, after which the transfer is P2P. Only in the case of some network restrictions the data really flows through the relay. How could they afford running the free relay otherwise?\n \nreply",
      "There are two servers. The \"mailbox server\" helps with handshakes and metadata transfers, and is super-low bandwidth, a few hundred bytes per connection. The \"transit relay helper\" is the one that handles the bulk data transfer iff the two sides were unable to establish a direct connection.I've been meaning to find the time to add NAT-hole-punching for years, but haven't managed it yet. We'd use the mailbox server messages to help the two sides learn about the IP addresses to use. That would increase the percentage of transfers that avoid the relay, but the last I read, something like 20% of peer-pairs would still need the relay, because their NATs are too restrictive.The relay usage hasn't been expensive enough to worry about, but if it gets more popular, that might change.\n \nreply",
      "The folks on the wormhole-rs fork (who appear to share your Github organization? [1]) already have NAT punching working 95+% of the time in my testing, so maybe what they're doing could be ported over to the Python implementation.[1] https://github.com/magic-wormhole\n \nreply",
      "You cant make a p2p connection over a NAT without exposing a port on the public side of the NAT.\n \nreply",
      "Go check out STUN and ICE.The best article I've found about NAT traversal is this article from Tailscale: https://tailscale.com/blog/how-nat-traversal-works\n \nreply"
    ],
    "link": "https://github.com/magic-wormhole/magic-wormhole",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        get things from one computer to another, safely\n      \n\n\n\n\n\nGet things from one computer to another, safely.This package provides a library and a command-line tool named wormhole,\nwhich makes it possible to get arbitrary-sized files and directories\n(or short pieces of text) from one computer to another. The two endpoints are\nidentified by using identical \"wormhole codes\": in general, the sending\nmachine generates and displays the code, which must then be typed into the\nreceiving machine.The codes are short and human-pronounceable, using a phonetically-distinct\nwordlist. The receiving side offers tab-completion on the codewords, so\nusually only a few characters must be typed. Wormhole codes are single-use\nand do not need to be memorized.For complete documentation, please see https://magic-wormhole.readthedocs.io\nor the docs/ subdirec",
    "summary": "**Magic Wormhole: A Renaissance Fair of File Transfer**\n\nIn the latest installment of \"magic technology terms make anything sound new,\" the Magic Wormhole promises to transfer your preciously oversized PDFs and mysteriously unnamed media files from one decrepit laptop to another without using actual *wizardry*. Users, enamored by the novelty of not using a flash drive, gush over how it's the dark magic solution to Google Drive\u2019s dreaded large-file handling failures. Meanwhile, tech enthusiasts descend into heated debates about theoretical max speeds, network nostalgia, and why their cleverly customized relay setups should be the next Silicon Valley breakthrough. Yet through all the mystical code words and network acrobatics, one wonders if a simpler time of USB sticks and \"just email it to me\" might magically reappear. \ud83d\ude31\u2728"
  },
  {
    "title": "Blockbuster Video VHS insert template (github.com/rfinnie)",
    "points": 154,
    "submitter": "zdw",
    "submit_time": "2024-08-17T18:09:49",
    "num_comments": 16,
    "comments_url": "https://news.ycombinator.com/item?id=41276605",
    "comments": [
      "This is beautiful. What warm memories this evokes. Now I can adjust the dimensions and make some dust covers for my books in the Blockbuster video style!\n \nreply",
      "Blockbuster does still exist and the trademarks are owned by Dish Network. The sole remaining franchisee is in Bend, Oregon.\n \nreply",
      "> owned by Dish NetworkThat explains www.blockbuster.com linking Sling TV.\n \nreply",
      "I feel like it should instead link to a video in the sling.com domain with almost patronizing instructions on how to convert their login to Sling and promote it as \"Your very own Blockbuster at home\".\n \nreply",
      "I think that I saw a news story, that it was closed, now.Doesn't mean the trademarks aren't valid, though. Here, there be [legal] dragonnes...\n \nreply",
      "they seem to still be getting in new releases, at least as of the end of July:https://bendblockbuster.com/new-releases/the Alaska blockbuster did close though.\n \nreply",
      "I suspect that what I saw, was about the Alaska store, then.\n \nreply",
      "Looks like it's still open, at least based on Google Maps.https://maps.app.goo.gl/cE9MCA7TZAdwYSkB6\n \nreply",
      "https://t.ly/Sk3MBAppears the \"OPEN\" light was on in July of 2021 from street view.\n \nreply",
      "The most recent review on Google, which includes interior photos of the open store, is just 4 days old.\n \nreply"
    ],
    "link": "https://github.com/rfinnie/blockbuster",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Blockbuster Video VHS insert template\n      This is an editable Inkscape template for a Blockbuster Video VHS clamshell case insert.  Please note that this doesn't strive to be 100% accurate to any specific Blockbuster design, as they made incremental design changes to their clamshells over the years.  Instead, it's meant to be a good enough period piece of late 80s through 90s.The fields you'd want to change are all in Liberation Sans and Liberation Sans Narrow fonts, but if you don't have them, similar sans-serif fonts should work fine.The back blurb area is Nimbus Mono PS font, but again, any typewriter-like monospace font should be fine. I've also seen \"The BLOCKBUSTER difference\" blurb in a sans-serif font.  Later on in the 90s, this space tended to be used for actual movie information, in a sans-serif font.The \"BLOCKBUSTER VID",
    "summary": "In a world where nostalgia outweighs practicality, a heroic GitHub user gifts the internet with a <em>Blockbuster Video VHS insert template</em>. Because nothing screams \"cutting-edge DIY\" like meticulously recreating extinct movie rental packaging. Commenters, drowning in wistful yearnings for the good old days of late fees, flock to reminisce and fabricate book covers styled like 90\u2019s VHS cases. Meanwhile, discussions about the legal nuances of the still-operating Blockbuster in Oregon clash spectacularly with musings on web domain redirection\u2014because obviously, this is <i>vital</i> information for mastering your next arts and crafts session."
  },
  {
    "title": "Postmortem of my 9 year journey at Google (tinystruggles.com)",
    "points": 14,
    "submitter": "delive",
    "submit_time": "2024-08-17T23:30:00",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41278907",
    "comments": [
      "Thanks for writing this! I work on AppEngine / Serverless as a SWE. Nice to see that you worked on it as an SRE and I can totally relate to the cognitive complexity of the systems! :)\n \nreply",
      "You forgot the action items to be moved into the next sprint.\n \nreply",
      "that\u2019s just standard at all Fortune 500s. I called it \u201ccooking the charts\u201d so clueless executives don\u2019t idiotically/blindly cut budgetsscrum master: \u201cpoints per sprint is going down guys!1!1 (after moving stories to next sprint). Velocity is gucci\u201d\n \nreply",
      "Working at the modern Google seems so overrated. Wonder what it was like in early 2000s when Larry and Sergei were not C-level douches\n \nreply",
      "A bit salty are we?\n \nreply",
      "I don\u2019t recommend the article unless you enjoy reading exactly what you think this is going to be.\u201cI made lots of money at Google, it got boring, now I\u2019m gonna do something else but I don\u2019t even know what.\u201dPretty much all there is here. I\u2019m just not sure what the audience is supposed to get out of it.I would also say that almost all of the negatives brought up about working at Google are like\u2026bring out the tiniest violin. Like, boo hoo, my boring job is lucrative and has good work-life balance. But it\u2019s so horrible how it\u2019s US-focused and I\u2019ve already leveled up to the maximum pay scale for my role.\n \nreply",
      "Light on details and mostly seems to revolve around money.\n \nreply",
      "> Please don't post shallow dismissals, especially of other people's work. A good critical comment teaches us something.\n \nreply",
      "It\u2019s a shallow postmortem.\n \nreply"
    ],
    "link": "https://tinystruggles.com/posts/google_postmortem/",
    "first_paragraph": "Always hacking something \ud83e\uddd1\u200d\ud83d\udd2c.I started writing this retrospective during my last week at Google, I have already wrapped up everything, had my goodbyes. In the spirit of SRE (as an ex-SRE), I thought it would be fun to write a little retrospective in the form of a postmortem.I joined Google young and relatively inexperienced and had spent about 9 years there.I started my journey in software at 19 (first internship) and then continued working part and full time while continuing my degree in Applied Physics. I got disillusioned with working in physics during the course of my degree, software turned out to be a more promising career path.At some point I got head hunted by a Google sourcer. That resulted in an internship in London when I was 22, later I joined full time in Dublin. I worked in several teams around three products: Bigtable, Persistent Disk and GCE VMs (virtual machines). I include a detailed timeline at the bottom of this post.After I got a taste of Google during my London in",
    "summary": "**Title:** A Decade of Debugging: A Googler\u2019s Overly Dramatic Exit\n\n**Body:** Watch in awe as another disillusioned tech bro recounts *nine* soul-searching years at Google's sanctified halls, starting as an intern and peaking at\u2014well, continuing to be an intern, but in spirit. \ud83e\uddd1\u200d\ud83d\udd2c Our hero, ever eager to dissect **the cognitive complexity** of switching from playing with physics to pushing Git commits, managed to document this heart-wrenching experience right before running out of free Google snacks. \n\nComment section is a delightful dumpster fire where some empathize, some trivialize, and others simply monetize their snark. The real entertainment, however, is witnessing the subtle art of back-patting thinly veiled as commiseration on shared Google-induced traumas. Stand back folks, the burnout is contagious. \ud83c\udfbb\ud83d\udd25"
  },
  {
    "title": "FlightAware Leaks Customer Data (Name, Email Addresses and Passwords) (loyaltylobby.com)",
    "points": 58,
    "submitter": "croemer",
    "submit_time": "2024-08-17T19:54:12",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=41277429",
    "comments": [
      "Until there are significant financial damages associated with each of these breaches, companies just won't invest enough to secure the information. These sorts of breaches should be existential to the company- they should never happen. And yet because the penalties are almost nothing, companies just are not incentivized to secure the data appropriately.If a breach meant the firing of the CEO and the CTO and the board, then you'd know that companies would spend a lot more on security and privacy.\n \nreply",
      "I\u2019m completely sympathetic to that view. However, until such data breaches regularly lead to severe outcomes for subjects whose data was leaked, and those outcomes can be causally linked to the breaches in an indisputable manner, I\u2019m afraid that we won\u2019t see any such penalties.\n \nreply",
      "Very good point. In addition, my email address etc has been breached so many times that each extra leak has almost no marginal impact.\n \nreply",
      "Is almost like publicly accessible information like email, phone, address, and, honestly, ss# at this point shouldn't be used for anything serious that doesn't require some sort of authentication beyond itself.Weird.\n \nreply",
      "It would mean that if all current & prospective customers abandon them for it.\n \nreply",
      "I can confirm the veracity of the email. I got it myself. Note that they say they leaked passwords. They didn't mention whether they were hashed or not, and if so whether with salt or not. I couldn't find a blog post either. The notification email took more than 3 weeks, not impressed.\n \nreply",
      "As someone who used to work there, the passwords were definitely stored salted and hashed in the database.The email mostly makes it sound like what\u2019s in the user account table, though last 4 of credit card I didn\u2019t think was in it. And mentioning passwords, not salted/hashed passwords, makes me think it was more.I\u2019m wondering if this is an Apache or Apache Rivet issue that possibly intercepted everything you sent to the server, which could then be your actual password if you logged in during the timeframe or even credit card if you bought something.Also Rivet was full of footguns. IIRC, variables would exist for the life of the Apache child, so you had to clear them out or the next request had access to them, so if someone deleted or didn\u2019t run the huge \u201cdelete all variables we probably set\u201d proc, and someone was able to get an \u201cinfo var\u201d output, they\u2019d see everything set in the previous request or further back if nothing overrode it. Like user info, which was just stored in a big global \u201cuser\u201d array\n \nreply",
      "Thanks that the most informative thing about this incident that exists at this point. Nothing on the website at all. It's terrible.The blog mentions recently moving away from TCL. Could it have been related to that?Do you have an idea why the emails arrive as a drip, spread over days?\n \nreply",
      "The website still looks like the TCL monster it has always been, so I doubt it. But I have no intimate knowledge of the inner workings there soon after the Raytheon buyout.\n \nreply",
      "Oh it is owned by Raytheon a huge listed company. That makes it more surprising that the communication is so bad. I'd expect a public company with market cap of $150b to have its act together regarding crisis communication.\n \nreply"
    ],
    "link": "https://loyaltylobby.com/2024/08/16/flightaware-leaks-customer-data-name-email-addresses-passwords/",
    "first_paragraph": "",
    "summary": "<h1>Another Day, Another Data Debacle: FlightAware's Oopsie-Daisy</h1>\n<p>In a world where personal data security is just a myth told to scare young hackers into using VPNs, <em>FlightAware</em> decides to join the league of Ordinary Reckless Companies by casually leaking customers' names, emails, and possibly unseasoned passwords. The outraged yet perpetually resigned citizens of comment-land express their shock with the fervor of a damp cracker, pondering existential questions like \"to hash or not to hash?\" and sharing nostalgic tales from the good old days of former employment revelations. Meanwhile, somewhere in the vast, disconnected universe of corporate accountability, a wayward ex-TCL script laughs quietly into its sleeve. \ud83e\udd21\ud83d\udcc9\ud83d\udd12</p>"
  },
  {
    "title": "Alien \u2013 CUDA-powered artificial life simulation program (github.com/chrxh)",
    "points": 118,
    "submitter": "apitman",
    "submit_time": "2024-08-17T16:41:21",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/chrxh/alien",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        ALIEN is a CUDA-powered artificial life simulation program.\n      \nArtificial LIfe ENvironment (ALIEN) is an artificial life simulation tool based on a specialized 2D particle engine in CUDA for soft bodies and fluids. Each simulated body consists of a network of particles that can be upgraded with higher-level functions, ranging from pure information processing capabilities to physical equipment (such as sensors, muscles, weapons, constructors, etc.) whose executions are orchestrated by neural networks. The bodies can be thought of as agents or digital organisms operating in a common environment. Their blueprints can be stored in genomes and passed on to offspring.\n\nThe simulation code is written entirely in CUDA and optimized for large-scale real-time simulations with millions of particles.\nThe development is driven by the desire ",
    "summary": "In the latest ode to overengineering, hobbyist code-slingers have birthed <em>\"ALIEN\"</em>\u2014a CUDA-powered techno-fantasy promising the thrilling simulation of digital slop. It seems every virtual gunk particle, delicately crafted with \"high-level functions,\" aims to feed the insatiable nerd-craving for watching dot clusters outwit each other in 2D pixel soup. Commenters, neck-deep in the thrill, swap notes on optimizing their pretend life forms, as though these pixel beasts might leap from their monitors and land a Silicon Valley gig. Apparently, the future of artificial life is less about new frontiers in science and more about crafting elaborate screensavers that devour your GPU."
  },
  {
    "title": "Build your own SQLite with Rust, Part 1 (sylver.dev)",
    "points": 10,
    "submitter": "upmind",
    "submit_time": "2024-08-17T23:13:53",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://blog.sylver.dev/build-your-own-sqlite-part-1-listing-tables",
    "first_paragraph": "13 min readAs developers, we use databases all the time. But how do they work? In this series, we'll try to answer that question by building our own SQLite-compatible database from scratch.Source code examples will be provided in Rust, but you are encouraged to follow along using your language of choice, as we won't be relying on many language-specific features or libraries.As an introduction, we'll implement the simplest version of the tables command, which lists the names of all the tables in a database. While this looks simple, we'll see that it requires us to make our first deep dive into the SQLite file format.The complete source code is available on Github.To keep things as simple as possible, let's build a minimalistic test database:This creates a database with two tables, table1 and table2, each with a single column, id. We can verify this by running the tables command in the SQLite shell:Let's start by creating a new Rust project. We'll use the cargo add to add our only depend",
    "summary": "**Build Your Own SQLite, Because Real Jobs Are Overrated**\n\nThe great internet brain trust has decided it's time to reinvent the wheel again, but this time with <i>Rust</i>\u2014because apparently, reading documentation is too mainstream. In the latest blog-turned-epic, a brave keyboard warrior guides us through \"building our own SQLite-compatible database from scratch,\" starting with something as exhilarating as listing table names. \ud83d\udee0\ufe0f Commenters, awe-struck by the revolutionary concept of following along in any language (but seriously, just use Rust), collectively reminisce about their unfinished projects from 2018. One eager beaver even proposes integrating blockchain for that extra sprinkle of relevance. Stay tuned for Part 2, where we might just declare this a new blockchain and start selling tokens."
  },
  {
    "title": "Increasing Retention Without Increasing Study Time [pdf] (ed.gov)",
    "points": 247,
    "submitter": "JustinSkycak",
    "submit_time": "2024-08-17T13:53:11",
    "num_comments": 94,
    "comments_url": "https://news.ycombinator.com/item?id=41274602",
    "comments": [
      "[Mods: it might be helpful to tag this paper as written in 2007]It seems to me this paper is bringing to light the idea of spaced repetition for learning and this has become quite popular in the productivity/learning culture of today (e.g. Ali Abdaal).> Alternatively, mathematics textbooks could easily adopt a format that engenders spacing.I tutor middle school students in mathematics and this is definitely being implemented in their textbooks! At the end of each chapter, there a is normal chapter review practice test followed by a \"Cumulative Practice\" which reviews topics from the previous chapters in the book. These are especially beneficial to my students as, like the paper highlights, it promotes long-term memory of those topics.> For example, although computer-based instruction typically provides extensive retrieval practice and rapid feedback, it offers a currently unexploited opportunity to schedule study sessions in ways that optimize long-term retention.There is an immensely popular software called Anki which implements exactly this \"spaced repetition\"-type protocol.\n \nreply",
      "This is not a critique of the (nice) study, but what seems to me the overall context that should be kept in mind, especially once we are talking about \"optimizing\".Optimizing \"study, retention phase, test\" for greatest knowledge retention at a delayed test time, is very different from optimizing for greatest value of knowledge learned.To optimize learning value, learn things that are immediately useful, you can immediately incorporate into learning something else, and ideally both.The sooner and more you use something, the greater its value AND the greater your retention will be.If you have to learn something valuable but with no short term use (how to handle a rare brain surgery complication), find a way to use it. Create an ongoing useful project that will revisit that knowledge during the \"retention interval\" (e.g. a concise summary of rare situations you need to handle, for you and others, that you can revisit and improve with additional and updated knowledge).So optimize \"topic choice\", \"topic progression\", and \"study, (optionally) test, use, use, use\", for total value of learning.\"Use\" is motivation, test, study review, and value realization put together.\n \nreply",
      "I think a lot of topics are much less sequential the further you go. As an adult I spend most of my time repeating the fundamentals of my field, and learning a topic deeply as needed. For children it probably makes sense to cram the multiplication tables.\n \nreply",
      "I would think that the opportunities to immediately \"use\" multiplication, instead of just practice it for tests, or some future numerate citizenship, would be omnipresent.If you don't use something after you learned it, you miss out on:1. Learning how it is actually applied2. Discovering how the knowledge is useful for you personally, in ways you may not expect if you don't actually experience using it3. Deeper understanding and mastery of the knowledge4. Much much much better retentionIt is worth creating some immediate use for new knowledge, even the smallest possible useful or creative project, for better retention alone.> learning a topic deeply as neededThat is the ultimate use-driven learning model.As for non-sequential, I agree. The more we manage our own learning, the more it is a directed graph (i.e. prerequisites translate to many follow up paths), and eventually just graphs (many ways to order topics, and alternate combinations of prerequisites for each topic, in any complex area).\n \nreply",
      "I sure hope my brain surgeon isn\u2019t trying to create an opportunity to practice dealing with rare complications!\n \nreply",
      "The brain you lose in the name of practice, may save millions!\n \nreply",
      "That\u2019s why I made Manabi Reader for language learning (currently only Japanese): https://news.ycombinator.com/item?id=41275227It encourages finding content from native sources, learning new words in context from materials that interest you rather than textbook content, and helps you build your own personal corpus of sentences as you encounter them in the wild. Will soon expand to more media types than just web and epub, to include YouTube/comics/HDMI input/game emulators etcIf you find flashcards from others, you also get easy tools for discovering more sentences from source materials that might interest you or from your own corpus. More coming for this and other word/kanji-level tracking analytics. Offline-first and privacy friendly.\n \nreply",
      "\"Teach others what are you trying to teach yourself better\" is the best long-term retention strategy in my personal experience.\n \nreply",
      "I push the new guys to fix project documentation once they\u2019ve figured out a tricky bit. It helps solidify their knowledge, and helps us double check that they understood, and it\u2019s something they can contribute when they still haven\u2019t become part of the bus number on anything yet.That all sounds reasonable and smart, but the real reason I do it is the Curse of Knowledge. People in a system can\u2019t see it from the outside. They make assumptions, use opaque or even misleading jargon, and employ circular logic. The new guy doesn\u2019t know the lingo, or the circular logic. Their explanation will make more sense to the next hire than anything I can say. And having it written down this way can also give me new perspective on the system. Maybe it doesn\u2019t have to work this way.\n \nreply",
      "Adding a new smart person to the team is one of those golden moments for a team, IMO. You get a tiny window of watching them struggle until the tribal team knowledge seeps into them by osmosis. During that time they don't yet know who to ask the questions to directly so they will post to team slack channels or the lead directly.One must capitalize on this brief period because smart programmers are flexible and adaptable. Very quickly they will acclimatize themselves to the mess that surrounds them and they will become as blind to the deficiencies as the rest of the team.\n \nreply"
    ],
    "link": "https://files.eric.ed.gov/fulltext/ED505647.pdf",
    "first_paragraph": "",
    "summary": "**Increasing Retention Without Increasing Study Time: A Satirical Take**\n\nIn a shocking turn of events, a dusty 2007 paper resurfaces, presenting the radical notion of **_spaced repetition_**, a concept so avant-garde it could revolutionize the entire five followers of the productivity blogosphere. Commenters\u2014championing the raw, untempered enthusiasm only the Internet can breed\u2014praise this rediscovered ancient wisdom likening it to the mathematics renaissance in middle school textbooks. Somewhere, a software called Anki nods in solemn validation, mourning the spotlight it thought it monopolized. Meanwhile, keyboard warriors disseminate this epiphany across forums, heralding a new era of maximized retention rates sans increased study time, all while blissfully ignoring their actual homework due tomorrow."
  },
  {
    "title": "Surgeons Cut a Giant Tumor Out of My Head. Is There a Better Way? (bloomberg.com)",
    "points": 24,
    "submitter": "melling",
    "submit_time": "2024-08-14T16:39:27",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.bloomberg.com/news/features/2024-08-14/a-neurosurgeon-who-operated-on-biden-wants-to-treat-disease-with-sound",
    "first_paragraph": "To continue, please click the box below to let us know you're not a robot.Please make sure your browser supports JavaScript and cookies and that you are not\n            blocking them from loading.\n            For more information you can review our Terms of\n                Service and Cookie Policy.For inquiries related to this message please contact\n            our support team and provide the reference ID below.",
    "summary": "In a spectacle rivaled only by the entertainment value of its notoriously decried paywall, <i>Bloomberg</i> delves deep into the world of neurosurgical advancements by starting off with a captious CAPTCHA conundrum to ensure readers are neither bots nor bereft of modern tech. The article masquerades as a hopeful inquiry into alternative tumor removal techniques, but swiftly devolves into a conventional melody singing praises to high-tech scalpels and marvels of medical investments. Commentators, displaying their usual prowess, sidestep the scientific discourse to quarrel over healthcare economics, squabble on insurance loopholes, and share unsolicited anecdotes about that one time they visited a hospital. As expected, the blissful nexus of cluelessness comprises both the article's optimism and the comment section\u2019s misdirection. \ud83e\udd16\ud83d\udc89\ud83d\udc7d"
  },
  {
    "title": "Building an EEG with a Children's Toy (geofflord.substack.com)",
    "points": 7,
    "submitter": "gl44snip",
    "submit_time": "2024-08-14T22:24:42",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://geofflord.substack.com/p/brainwaves-on-a-budget",
    "first_paragraph": "",
    "summary": "At Geoff's Nifty Tech Eden, our intrepid blogger reinvents the wheel - or in this case, an EEG - by gutting a children\u2019s toy only a lunatic would entrust with a toddler\u2019s neurological development. Confident that adding random wires to plastic will unleash the boundless mysteries of the human mind, Geoff enlightens his six readers on the marvels of pseudo-neuroscience. In the comments, an army of deranged Redditors, each convinced they're one soldering job away from a Nobel Prize in Medicine, exchange safety tips they Googled five minutes ago and argue the ethics of neurohacking their hamsters. \ud83e\udde0\ud83d\udd27"
  },
  {
    "title": "The Blue Collar Jobs of Philip Glass (honest-broker.com)",
    "points": 90,
    "submitter": "samclemens",
    "submit_time": "2024-08-13T21:42:07",
    "num_comments": 47,
    "comments_url": "https://news.ycombinator.com/item?id=41240152",
    "comments": [
      "Could a new composer replicate this? is too broad a question. Rather,- could they support themselves with a blue collar job?- could they support a family with said job, as the primary breadwinner?- could they do it in X, where X is the geographic location of the cultural center of their art form?Glass was lucky enough to hit all three. Today's young artists might be lucky to hit 1, and maybe the internet helps with 3 a bit, but if you work in an art form that really, really needs in person connection (eg theater, or you need to be where the gallerists and dealers are), I guess it doesnt solve the problem. 2 is very difficult.The perfect trifecta is when you have a dense urban center that happens to be the center of your art form, but still has enough of a rough edge to it that you can live and work cheaply. New York, 1970s, classical music. \nBerlin, visual arts, 2000s. Or you can create the scene yourself given enough mass and energy (see: Atlanta, 1990s, rap, or NYC, 1980s, hip hop).The next Glass is therefore more likely working in an urban coffee shop than a suburban landscaping crew, living with roomies or parents rather than alone, has no children or spouse to support, and exists on the fringes of a city like London, NY, or Berlin.\n \nreply",
      "> Just imagine reading Siddhartha or The Glass Bead Game at a steel mill\u2014but that\u2019s Philip Glass for you.I don't know what this means. I worked a lot of \"blue collar jobs\" in my time, and I'd often see people reading \"serious books\" on break. It's not uncommon. I read a lot of Hemingway because of recommendations I got from a dude I used to work with cleaning concrete truck drums with jackhammers.\n \nreply",
      "> \u201cI could manage quite well working as few as twenty to twenty-five hours a week\u2014in other words, three full days or five half days. Even after I returned from Paris or India in the late 1960s and well into the 1970s, I could take care of my family by working no more than three or four days a week.\u201dWould today's youth, even if equally gifted and ambitious, have the same opportunity?  I think now there is such a great imbalance in cost of living and pay rates, it may no longer be possible to follow a similar path and get similar results.\n \nreply",
      "Sure, you could move out to rural nowhere, where housing costs next to nothing. Find some part time job, and live your life.I'm from a place like that, and a bunch of my old classmates from HS have lived like that their entire adult lives working part time. They work 2-3-4 days a week.Of course, you'll be sacrificing lots of materialistic things, but that's a given.\n \nreply",
      "If Philip Glass had had to live in \"rural nowhere\" in order to afford to make music, we would have never heard of Philip Glass. vatys isn't asking if you can make any living this way, because of course you can. The specific conditions that allowed Philip Glass to work part time jobs and still live in the same city as people like Steve Reich and institutions like The Kitchen don't exist anymore.\n \nreply",
      "> If Philip Glass had had to live in \"rural nowhere\" in order to afford to make music, we would have never heard of Philip Glass.Congratulations-- you're officially wrong on the internet!Meet Harry Partch:https://en.wikipedia.org/wiki/Harry_PartchHe roamed as a hobo for years during the Great Depression. The text for Barstow is graffiti that he saw scrawled on highway railings during that time. It's featured in most music history books that cover the 20th century. (Also, many of the instruments he invented are visual works of art, in addition to being musically beautiful.) And if you liked the recent HN article on just intonation, well... let's just say you're gonna love Harry Partch!There's also Conlon Nancarrow, who got pissed at the U.S. harrassing him when he got back from fighting the fascists in the Spanish Civil War. He moved to Mexico City and hand-punched player piano rolls in seclusion, for decades.Other composers and musicians made pilgrimages to his studio, just to hear what it sounds like when, say, a 12-voice canon has each voice moving at different tempo.[1]Nancarrow received the MacArthur fellowship back in 1982. At one point there was a piano duo who taught themselves to play a selection of his pieces as a four-hands duet for one piano.People who care about music will find interesting musicians, no matter where they live. This goes back at least to J.S. Bach, who reportedly walking hundreds of miles to listen to Buxtehude improvise at the organ.1: and what are the proportions for the voices of that canon? You guessed it-- they're the ratios from a just intonation chromatic scale, which Nancarrow probably got from a book by Henry Cowell (New Musical Resources, IIRC).Edit: typos\n \nreply",
      "Congratulations, you missed the point of the comment while also being unnecessarily condescending! Another internet point!The commenter remarked \"we would never have heard of Philip Glass.\" Who among the laity would have heard of Philip Glass and the people you listed? I expect that Venn diagram is really two circles.",
      "> If Philip Glass had had to live in \"rural nowhere\" in order to afford to make music, we would have never heard of Philip Glass.I'm not so sure. A lot of art comes out of \"affordable areas\" \u2014 sometimes small college-town ghettos like Athens, Georgia, for example. Why couldn't we get a Philip Glass from Manhattan, Kansas?\n \nreply",
      "100% - but people don\u2019t want to make those sacrifices. They want to live and do what they\u2019ve always done. It has and likely always will be possible to pick up stumps and move somewhere very cheap and get on with a personal creative endeavour - not many have the courage though.\n \nreply",
      "You can\u2019t participate in performance arts remotely.Not to say anything about networking, which is critical for most arts.\n \nreply"
    ],
    "link": "https://www.honest-broker.com/p/the-blue-collar-jobs-of-philip-glass",
    "first_paragraph": "",
    "summary": "**The Blue Collar Jobs of Philip Glass: Internet Scholars Unravel the Mysteries of Making Ends Meet**\n\nAs honest-broker.com churns through yet another groundbreaking think piece, this time analyzing if today's artists can live on blue-collar wages like the fabled Philip Glass, comment sections light up with insights ranging from borderline naive to nostalgically delusional. One genius points out the daring feat of reading existential literature while drilling through concrete\u2014because apparently, intellectual multitasking is now a class trait. Meanwhile, a hopeful commenter reminisces about a mystical era where twenty hours of work a week could feed a family, clearly having missed the last few decades of economic newsletters. Let's not forget the armchair experts who suggest budding artists just move to \"rural nowhere,\" because inspiration and cultural movements thrive best next to cornfields and cow pastures. Forget New York or Berlin; your next avant-garde artist is probably zoning out in a field, wondering if their internet connection is stable enough to google \"Who is Philip Glass?\""
  },
  {
    "title": "Ask HN: What do you monitor on your servers?",
    "points": 147,
    "submitter": "gorkemcetin",
    "submit_time": "2024-08-13T22:13:25",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=41240379",
    "comments": [
      "> What are the key assets you monitor beyond the basics like CPU, RAM, and disk usage?* Network is another basic that should be there* Average disk service time* Memory is tricky (even MemAvailable can miss important anonymous memory pageouts with a mistuned vm.swappiness), so also monitor swap page out rates* TCP retransmits as a warning sign of network/hardware issues* UDP & TCP connection counts by state (for TCP: established, time_wait, etc.) broken down by incoming and outgoing* Per-CPU utilization* Rates of operating system warnings and errors in the kernel log* Application average/max response time* Application throughput (both total and broken down by the error rate, e.g. HTTP response code >= 400)* Application thread pool utilization* Rates of application warnings and errors in the application log* Application up/down with heartbeat* Per-application & per-thread CPU utilization* Periodic on-CPU sampling for a bit of time and then flame graph that* DNS lookup response times/errors> Do you also keep tabs on network performance, processes, services, or other metrics?Per-process and over time, yes, which are useful for post-mortem analysis\n \nreply",
      "When it comes to \"what\" to monitor, many usual suspects already posted in this thread, so in an attempt not to repeat what's there already, I will mention just the following (will somewhat assume Linux/systemd):- systemd unit failures - I install a global OnFailure hook that applies for all the units, to trigger an alert via a mechanism of choice for a given system,- restarts of key services - you typically don't want to miss those, but if they are silent, then you quite likely will,- netfilter reconfigurations - nftables cli has useful `monitor` subcommand for this,- unexpected ingress or egress connection attempts,- connections from unknown/unexpected networks (if can't just outright block them for any reason).\n \nreply",
      "My two cents: monitoring RAM usage is completely useless, as whatever number you consider an \u201cused/free RAM\u201d is meaningless (and the ideal state is that all of the RAM is somehow \u201cused\u201d anyway). You should monitor for page faults and cache misses in block device reads.\n \nreply",
      "Depends. \"free\" reports the area used for disk buffers and programs, hence \"available\" and \"free\" numbers.On my servers I want some available RAM which means \"used - buffers\", because this means I configured my servers correctly and nothing is running away, or nothing is using more than it should.On the other hand, you want \"free\" almost zero on a warmed up server (except some cases which hints that heaps of memory has been recently freed) since the rest is always utilized as disk cache.Similarly having some data on swap state doesn't harm as long as it's spilled there because some process has ran away and used more memory than it should be.So, RAM usage metrics carry a ton of nuance and can mean totally different things depending on how you use that particular server.\n \nreply",
      "One of the older arguments I get to keep having over and over is No, You May Not Put Another Service on These Servers. We are using those disk caches thank you very much.I do not enjoy showing up to yet another discussion of why our response times just went up \u201cfor no reason\u201d. Learn your latency tables people.\n \nreply",
      "Yeah, people tend to think server utilization as black and white.Look, we're using just 50% of that RAM. Look, there're two cores that are almost idle.No & No. Rest of the RAM is your secret for instant responses, and that spare CPU resource is for me to do system management without you notice or to front the odd torrent of requests we have semi regularly (e.g.: /. hug of death. Remember?).\n \nreply",
      "I need to find a really good intro to queuing theory to send people to. A full queue is a slow queue. You actually want to aim for about 65% utilization.\n \nreply",
      "Also, there was a formula for determining the optimal cache size. I forget the name all the time. IIRC, in the end, caching most popular 10 items was enough to respond to 95% of your queries without hitting the disk.\n \nreply",
      "Correct identification but wrong prescription.Cache misses don't have anything to do with memory pressure, they're related to caching effectiveness.Production systems shouldn't have any page faults because they shouldn't be using swap.The traditional way Linux memory pressure was measured using a very small swap file and check for any usage of it. Modern Linux has the PSI subsystem.Also, monitoring for OOM events also means a system needs more RAM or a workload needs to be tuned or spread out.\n \nreply",
      "Perhaps I can hijack this post to ask some advice on how to monitor servers.I don't do this professionally. I have a small homelab that is mostly one router running opnsense, one fileserver running TrueNAS, and one container host running Proxmox.Proxmox does have about 10-15 containers though, almost all Debian, and I feel like I should be doing more to keep an eye on both them and the physical servers themselves. Any suggestions?\n \nreply"
    ],
    "link": "item?id=41240379",
    "first_paragraph": "",
    "summary": "**Hacker News Becomes Sysadmin Training Ground**\n\nToday in Hacker News land, the \"architects\" of tomorrow cluster desperately around a digital watering hole seeking answers to age-old mysteries: What do you monitor on your servers when you\u2019ve gone above and beyond CPU, RAM, and disk usage? \ud83e\udd13 One bold user throws down a veritable grocery list of everything from TCP retransmits to DNS lookup response times, inadvertently revealing more about their anxiety levels than their system's operational health. Commenters chime in, crafting more layers of complexity with terms that make them sound clever\u2014perhaps convinced the right cocktail of metrics will ward off the impending doom of... using their server effectively? Meanwhile, the noob in the corner bravely asks for advice on monitoring his home-lab like it\u2019s a SpaceX mission control center\u2014because, why not? \ud83d\ude80 No overkill here, folks, just everyday tech enthusiasts preparing for their own digital apocalypse."
  },
  {
    "title": "pg_duckdb: Splicing Duck and Elephant DNA (motherduck.com)",
    "points": 59,
    "submitter": "jonbaer",
    "submit_time": "2024-08-17T16:40:13",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41275751",
    "comments": [
      "Very much agreed with this general idea, and believe a lot of this was inspired by the team we hired at Crunchy Data to build it as they were socializing it for a while. Looking forward to pg_duckdb advancing in time for now it still seems pretty early and has some maturing to do. As others have said, it needs to be a bit more stable and production grade. But the opportunity is very much there.We recently submitted our (Crunchy Bridge for Analytics-at most broad level based on same idea) benchmark for clickbench by clickhouse (https://benchmark.clickhouse.com/) which puts us at #6 overall amongst managed service providers and gives a real viable option for Postgres as an analytics database (at least per clickbench).\n \nreply",
      "Looking forward to this getting supported on Neon!\n \nreply",
      "Very soon, it needs to get a bit more stable.\n \nreply",
      "Would be helpful to list the features. This link has the details:https://github.com/duckdb/pg_duckdbSounds like it would be useful for Postgres users to interact with Parquet and CSV data within a single SQL query and in a performant way (due to DuckDB's vectorization).\n \nreply",
      "Postgres IS missing an analytics engine. benchmark.clickhouse.com puts it at the bottom of the list and ~1000x slower than @duckdb and @ClickHouseDB.Here are the scenarios and how to address them1. Query Parquet and Iceberg from Postgres. When  Parquet files are stored in S3 Postgres should be able to  run analytical queries on them.2. Postgres should allow creation of columnstore tables inside Postgres storage subsystem. Analytical queries on top of these table should be FAST. Top 10 on Clickbench fast. This allows to run analytics without S3 and have super low latencies for analytics.3. Postgres should allow creation of secondary columnstore indexes to speed up analytical queries in mixed workloads. This is super useful for Oracle migrations since Oracle had this feature for a while.So How do we get there? 10 years ago it would be a MASSIVE project, but today we have @duckdb - super fast analytical engine with an open license. The work is still not trivial, but it is much much simpler.First you need to integrate an analytical query processor into Postgres and today @duckdblabs announced github.com/duckdb/pg_duck\u2026. Yay and congrats!This plugin runs duckdb alongside with Postgres and integrated Postgres syntax with the @duckdb query processor (QP)With that it now can trivially query external files from S3. This addresses scenario 1.With that it now can trivially query external files from S3. This addresses scenario 1.Building columnar table requires either implementing columnar storage from scratch or integrating duckdb storage into the Postgres subsystem. You can of course let duckdb create duckdb files on local disk, but then all the Postgres machinery: replication, backup, recovery won't workDuckdb tables have to mapped into 8kb Postgres pages pushed through the Postgres WAL for replication, recovery and transactionality. This will give us scenario 2Scenario 3 is even more work. You need secondary index maintenance and it will require hybrid query execution. We will need to modify Postgres executor so that it can mix and match regular Postgres query operators and \"vectorized\" query operators from duckdb. Or built vectorized operators into PostgresScenarios 2 and 3 will take some time, but I'm excited for this roadmap: this will unlock a huge world for millions of  Postgres users and simplify the lives of many developers dealing with moving data between transactional and analytical systems.\n \nreply",
      "re: columnar - https://github.com/hydradatabase/hydra/tree/main/columnara much updated fork of citus' columnar using Postgres tableam.re: duckdb handling storage - https://github.com/hydradatabase/pg_quack/tree/branch-0.0.1an earlier implementation\n \nreply"
    ],
    "link": "https://motherduck.com/blog/pg_duckdb-postgresql-extension-for-duckdb-motherduck/",
    "first_paragraph": "*NEW* Community Workshops come to Small Data SF: Build bigger with small data and AI on 9/23 + 9/24I'm in2024/08/15Subscribe to MotherDuck BlogYou can have your analytics and transact them tooWe're excited to announce pg_duckdb, an open-source Postgres extension that embeds DuckDB's analytics engine into Postgres for fast analytical queries in your favorite transactional database.Postgres is generating a lot of excitement, having been named 2023 DBMS of the Year by DB-Engines and recognized as the most popular database in the 2024 Stack Overflow Developer Survey twice in a row. It is popular for good reasons; it is a robust way to be able to create, update, and store data about your application.Postgres is great at a lot of things, but if you try to use it for analytics, you hit a wall pretty quickly. That is, it is great at creating, finding and locating individual rows, but if you want to understand what is going on in a data set, it can be painfully slow. For example, you might want",
    "summary": "**Title: Database Dr Frankensteins in Flannel Suspenders**\n\nMotherduck announces its latest mad science experiment, pg_duckdb, promising to *magically* allow Postgres users to perform analytics at the speed of thought\u2014or at least not glacially slow. While Postgres wins popularity contests, it flounders in crunching big data. Enter DuckDB, stapled awkwardly onto beloved ElephantSQL, because **everyone** needs a query to run before the heat death of the universe, right? Meanwhile, the commentariat revels in technobabble and future dreams, barely disguising their hope that this patchwork creature won't trip over its own feet in production. \ud83e\udd86 meets \ud83d\udc18, what could go wrong?"
  },
  {
    "title": "Continue (YC S23) Is Hiring a Software Engineer in San Francisco (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-08-17T21:00:44",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/continue/jobs/smcxRnM-software-engineer",
    "first_paragraph": "The leading open-source AI code assistantContinue is seeking an outstanding software engineer to help us build state-of-the-art autocomplete and codebase retrieval, who thinks rigorously and pays attention to the smallest details. In this role, you will work on fundamental, but highly open-ended problems where deliberate measurement, rapid experimentation, and empathy for users push forward the product.About youPlease keep in mind that we are describing the background that we imagine would best fit the role. If you don\u2019t meet all the requirements, but you are confident that you are up for the task, we absolutely want to get to know you!What you will doWe\u2019re a startup, so you\u2019ll have to be ready to do whatever is required to accomplish our mission. However, you can definitely expect to:We believe there is an opportunity to create a future where developers are amplified, not automated. This is why we are building the leading open-source AI code assistant and layering an enterprise produc",
    "summary": "Hacker News discovers yet another aspiring tech godsend in \"Continue,\" a startup that surely invented the concept of an \"open-source AI code assistant.\" Candidates dreaming of being the next demiurge in San Francisco's pantheon can apply to solve \"highly open-ended problems,\" essentially doing what every other software engineer in the Bay area claims on their resume. Commenters oscillate between hailing this as the harbinger of coding utopia and nitpicking the job description like their lives depend on decoding a dry rub recipe. Truly, a must-miss opportunity if you ever wanted to witness another generic iteration of AI hype coupled with thinly-veiled job desperation. \ud83d\ude02\ud83d\udc68\u200d\ud83d\udcbb"
  },
  {
    "title": "DifuzCam: Replacing Camera Lens with a Mask and a Diffusion Model (arxiv.org)",
    "points": 69,
    "submitter": "dataminer",
    "submit_time": "2024-08-17T16:50:10",
    "num_comments": 27,
    "comments_url": "https://news.ycombinator.com/item?id=41275832",
    "comments": [
      "This would be impressive if the examples weren't taken from the same dataset (Laion-5B) that was used to train the Stable Diffusion model it's using.\n \nreply",
      "For those interested in various approaches to lens-free imaging, Laura Waller at Berkeley has been pursuing this area for some time.https://waller-lab.github.io/DiffuserCam/\nhttps://waller-lab.github.io/DiffuserCam/tutorial.html includes instructions and code to build your own\nhttps://ieeexplore.ieee.org/abstract/document/8747341\nhttps://ieeexplore.ieee.org/document/7492880\n \nreply",
      "This is quite amazing that using a diffuser rather than a lens, then using a diffusion model can reconstruct an image so well.The downside of this is that is heavily relies on the model to construct the image. Much like those colorisation models applied to old monochrome photos, the results will probably always look a little off based on the training data. I could imagine taking a photo of some weird art installation and the camera getting confused.You can see examples of this when the model invented fabric texture on the fabric examples and converted solar panels to walls.\n \nreply",
      "The model basically guesses and reinvents what these diffuse pixels might be. It's more like a painter producing a picture from memory.It inevitably means that the \"camera\" visually parses the scene and then synthesizes its picture. The intermediate step is a great moment to semantically edit the scene. Recolor and retexture things. Remove some elements of the scene, or even add some. Use different rendering styles.Imagine pointing such a \"camera\" at person standing next to a wall, and getting a picture of the person with their skin lacking any wrinkles, clothes looking more lustrous as if it were silk, not cotton, and the graffiti removed from the wall behind.Or making a \"photo\" that turns a group of kids into a group of cartoon superheroes, while retaining their recognizable faces and postures.(ICBM course, photo evidence made with digital cameras should long have been inadmissible in courts, but this would hasten the transition.)\n \nreply",
      "Kinda reminds me of this a bit: https://arstechnica.com/gadgets/2020/11/nvidia-used-neural-n...\n \nreply",
      "This is not a 'camera' per se. It's more like a human vision system that samples light and hallucinates an appropriate image based on context. The image is constructed from the data more than it is reconstructed. And like human vision, it can be correct more often than not to be useful.\n \nreply",
      "Thanks for the summary. I was looking for this.\n \nreply",
      "I don't understand the use of a textual description. In which scenario do you not have enough space for a lens and yet have a textual description of the scene?\n \nreply",
      "Does a camera without a lens make any physics sense? I cannot see how the scene geometry could be recoverable. Rays of light travelling from the scene arrive in all directions.Intuitively, imagine moving your eye at every point along some square inch. Each position of the eye is a different image. Now all those images overlap on the sensor.If you look at the images in the paper, everything except their most macro geometry and colour pallet is clearly generated -- since it changes depending on the prompt.So at a guess, the lensless sensor gets this massive overlap of all possible photos at that location and so is able, at least, to capture minimal macro geometry and colour. This isn't going to be a useful amount of information for almost any application.\n \nreply",
      "I wonder how it \"reacts\" to optical illusions? The ones we're familiar with are optimized for probing the limits of the human visual system, but there might be some overlap\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2408.07541",
    "first_paragraph": "Coming this September, the Forum is free, virtual, and open to all. Sign Up and Learn more.Grab your spot at the free arXiv Accessibility ForumHelp | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "In the latest episode of \"Why Bother?\", arXiv enthusiasts unveil the DifuzCam\u2014an innovative dumpster fire that replaces the pesky, physics-abiding camera lens with magic dust and wishful thinking, known as a diffusion model. Over in the comments, amateur photo-theorists are entranced, celebrating the camera's ability to invent fabric textures and re-envision similar photons as wonderfully misleading scenes. One brainy commenter, however, just can\u2019t wrap their head around this lens-less wonder, because apparently, their PhD in comment-section physics didn't cover \"guesswork imaging.\" Meanwhile, another genius ponders the profound implications this might have on seeing cartoon superheroes in the wild. Welcome to the future, where our images are as real as the Easter Bunny, and just as reliable. \ud83d\udcf8\u2728"
  },
  {
    "title": "What the heck are reverse mapped types? (andreasimonecosta.dev)",
    "points": 35,
    "submitter": "PaulHoule",
    "submit_time": "2024-08-14T15:59:19",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://andreasimonecosta.dev/posts/what-the-heck-are-reverse-mapped-types/",
    "first_paragraph": "Reverse mapped types are a powerful yet little-known feature of TypeScript that allow us to \u201crun mapped types backward\u201d. They are mainly a mechanism for inferring a function\u2019s type parameters from values; however, the same inference steps can be performed at the type level using the infer keyword.The purpose of this article is to serve as a comprehensive guide to reverse mapped types, explaining what they are and how they can be used to set interesting constraints on the values of a type and to provide useful context sensitive information. Various references to the compiler source code will be made in order to provide a deeper understanding of the topic.\u00a0Let\u2019s take a simple generic function like the following:We are not surprised that TypeScript can infer the type T of the array elements from the argument passed to the function:But what if we had a mapped type in place of ReadonlyArray<T>? Would TypeScript still be able to infer the type T? This is what we are referring to when we talk",
    "summary": "Title: A Hitchhiker's Guide to Obscure TypeScript Sorcery\n\nIn an earth-shattering blog post, *andreasimonecosta.dev* dives deep into the arcane world of TypeScript's reverse mapped types, a feature so esoteric that even the compiler needed a map and a flashlight to understand it. The author gallantly attempts to enlighten us mere mortals on how to \"run mapped types backward\", utilizing the powerful <em>infer</em> keyword, a concept sure to thrill the three people who thought \"Yes, but how do we reverse this?\" Commenters, knee-deep in intellectual hubris, battle over the nuance differences between obscure compiler flags, evidently struggling to infer their own dinner plans, let alone complex type parameters. \ud83e\uddd9\u200d\u2642\ufe0f\ud83d\udca5\u2728"
  },
  {
    "title": "The Key to Bizarro's Symbols (bizarro.com)",
    "points": 20,
    "submitter": "litoE",
    "submit_time": "2024-08-15T05:05:49",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.bizarro.com/secret-symbols",
    "first_paragraph": "Ever notice a seemingly random piece of pie on the floor in one of my cartoons? Or an eyeball peeking out from behind something, a stick of lit dynamite under a chair, or a small one-eyed alien floating near the ceiling? These are the Bizarro Secret Symbols.Now that you\u2019ve found the page explaining them, you can discover what they mean and how they can improve your life. The tiny number above my signature in each cartoon will tell you how many symbols I\u2019ve placed in that cartoon.But keep all of this to yourself, or they won\u2019t be secret anymore.\u00a0We\u2019ve got Secret Symbol patches and stickers in the Bizarro shop\u2014check \u2018em out!The Pipe of Ambiguity\u00a0honors surrealist artist Ren\u00e9 Magritte (1898-1967), a figure of inspiration at Bizarro Studios. His 1929 painting,\u00a0The Treachery of Images, embodies\u00a0Bizarro's comic aesthetic. It shows a pipe floating above the words\u00a0Ceci n'est pas une pipe, French for\u00a0This is not a pipe.Magritte was fascinated by the interplay of words and images, and in 1913 he",
    "summary": "In what appears to be an unprecedented breakthrough in cartoon analysis, a lone genius has bravely cataloged the \"secret symbols\" lurking in the backgrounds of comic strips. \ud83e\udd67\ud83d\udc41\ufe0f\ud83e\udde8 Yes, *now* we can finally sleep at night knowing exactly what a piece of pie or a dynamite stick hidden beneath a chair *truly* symbolizes, and how these intricate details are poised to change our lives. For those not satisfied with merely absorbing life-altering art, there's branded merchandise to solidify their allegiance to comic strip iconography. Meanwhile, the comments section boils over with debates on whether Magritte would roll in his grave or monetize his pipe."
  },
  {
    "title": "BMX: A Freshly Baked Take on BM25 (mixedbread.ai)",
    "points": 59,
    "submitter": "breadislove",
    "submit_time": "2024-08-14T22:12:26",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41251061",
    "comments": [
      "> Entropy-weighted similarity: We adjust the similarity scores between query tokens and related documents based on the entropy of each token.Sounds a lot like BM25 weighted word embeddings (e.g. fastText).\n \nreply",
      "Very cool! Glad to see continued research in this direction. I\u2019ve really enjoyed reading the Mixedbread blog. If you\u2019re interested in retrieval topics, they\u2019re doing some cool stuff.\n \nreply",
      "baguetter library for the win!\n \nreply",
      "Gemischtes Brot!\n \nreply"
    ],
    "link": "https://www.mixedbread.ai/blog/intro-bmx",
    "first_paragraph": "Sean LeeML BakerAamir ShakirFounding BakerJulius LippFounding BakerRui HuangML BakerWe are proud to announce that researchers from Mixedbread and the Hong Kong Polytechnic University have developed a new lexical search algorithm, BMX, that outperforms the current standard BM25 across the board and is easy to use via Mixedbread's open-source Baguetter library.Read on to learn about BMX, how it works, our benchmarks, and how to use it in practice. If you want to jump right in, you can look into the paper and the library right here:TLDR: \nOur new BMX search algorithm iterates on the long-standing industry standard\nand can be accessed via our fully open-source Baguetter library.Most text search engines are powered by lexical (keyword) search algorithms, the most prominent of which is BM25. It powers search engines for web search, e-commerce, legal search engines, and many more applications. A key strength of BM25 is that it performs really well in an out of distribution setting, meaning th",
    "summary": "Title: BMX: A Freshly Baked Take on BM25 (mixedbread.ai)\n\nResearchers at Mixedbread and Hong Kong Polytechnic are thrilled to proclaim their revolutionary discovery: BMX, a search algorithm that boldly claims to dethrone BM25. In what is essentially a rehash of every tech innovation announcement ever, the team assures us that BMX, accessible through the oh-so-aptly named Baguetter library, is \ud83d\udd25<em>the next big thing</em>\ud83d\udca5. Meanwhile, the comment section transforms into a cozy echo chamber where every echo sounds suspiciously like \"innovation!\" and \"breakthrough!\" but reads more like a baker's dozen of the same old half-baked ideas. \"Baguetter library for the win!\" chirps an enthusiast, inadvertently summarizing the depth of critique we've come to expect from such scholarly exchanges."
  },
  {
    "title": "Low level of Magnesium linked to disease-causing DNA damage (newatlas.com)",
    "points": 32,
    "submitter": "clumsysmurf",
    "submit_time": "2024-08-17T22:45:19",
    "num_comments": 4,
    "comments_url": "https://news.ycombinator.com/item?id=41278636",
    "comments": [
      "There are so many different types of magnesium supplements out there, which type of magnesium should people consider?\n \nreply",
      "It also causes excess stress and anxiety and mental health issues.There are magnesium gummies.\n \nreply",
      "And great dietary magnesium sources like spinach and black beans.\n \nreply",
      "Gonna stick with my gummies, thanks.\n \nreply"
    ],
    "link": "https://newatlas.com/health-wellbeing/nutrient-dna-damage/",
    "first_paragraph": "",
    "summary": "In a desperate bid to escape the clutches of Big Pharma, armchair nutritionists on newatlas.com have latched onto a groundbreaking discovery that not having enough magnesium might \u2013 just might \u2013 scribble a bit of graffiti on your precious DNA. Cue the predictable onslaught of supplement savants wrestling each other in the comment section to peddle their favored form of this magic element, from gummies that could double as car tires to the ancient grains of the Andes. Meanwhile, someone suggests eating an actual spinach leaf, but is quickly shouted down by gummy enthusiasts, because if you can't chew your nutrients in a fun, fruity form, are you even trying to avoid global pharmaceutical conspiracies? \ud83e\udd37\ud83d\udc40"
  },
  {
    "title": "The Vala Language (2017) (bassi.io)",
    "points": 39,
    "submitter": "goranmoomin",
    "submit_time": "2024-08-17T17:16:23",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=41276063",
    "comments": [
      "Vala is an interesting language - it looks like C#, but is a compiled, reference counted, GObject based language (which is Gnome's/GTK's  object model library), that simplifies/streamlines writing app code, compared to writing it in C.Which brings on an interesting historical tangent - back in the day, Windows applications were written against the COM object model, either in pure C, or C++ with MFC. The parallels are obvious - Microsoft needed a language that was easier to use and better suited for the needs for applications programmers.The obvious thing would've been to build something like Vala - a COM based reference counted compiled language - but they decided to build .NET/C# a garbage-collected JITted language with an entirely alien library and execution model.  And while it became somewhat a success in the world of generic software dev, it never fulfilled this niche, with none of the core MS products ever integrating it, and most of the internal teams treating it with animosity.I wonder why Microsoft decided to go down this route.\n \nreply",
      "Did you forget Visual Basic? That was a COM-consuming, easier to use language.\n \nreply",
      "I low-key wish they would open source the compiler so the open source community could make an attempt at rebuilding the IDE. I have heard they cannot open source all of it, wish they would open what they can.\n \nreply",
      "And a massive success in games development, particularly on mobile. (Unity!)\n \nreply",
      "Perhaps it was an era when everyone was trying to compete with (or embrace) Java?\n \nreply",
      ".NET and C# appeared to counter the proliferation of Java after J++ has been sidelined in court: https://en.wikipedia.org/wiki/Visual_J%2B%2B#Sun's_litigatio...\n \nreply",
      "Yep, there was also J# an intermediary language so you could run old J++ code on .NET iirc which obviously became defunct sooner rather than later since C# became defacto.I have yet to meet a C# dev who actually touched J++ or J# to be honest.\n \nreply",
      "Speaking of, today you can use https://gircore.github.io for rich GObject (and GTK4) interop. It is relatively new hence not widely known but is already used by e.g. https://www.pinta-project.com. It's a proper and actively maintained successor to GtkSharp.\n \nreply",
      "That gives me a messed up idea of using Vala to implement a main GUI and then embedding mono to let users expand your app using this.\n \nreply",
      "Vala was super neat as it came out of the ElementaryOS line of work. Their holistic focus towards usability and approachability of a Linux distro was inspiring enough that I even supported them with my meager student developer income. Since then, I've moved to Mac and haven't dabbled much in desktop Linux machines. As it grew older and some things got broken up (like serenity and ladybird), the novelty-energy wore off and I looked away for years.As a language, I hoped Vala would pave the way for a beautiful high performance application tool. Instead, we got Electron.\n \nreply"
    ],
    "link": "https://www.bassi.io/articles/2017/02/13/on-vala/",
    "first_paragraph": "It seems I raised a bit of a stink on Twitter last\u00a0week:PSA: if you want to write a new @gnome application, don't use Vala; if you're already using it, consider porting to a non-dead\u00a0language.Of course, and with reason, I\u2019ve been called out on this by various people.\nLuckily, it was on Twitter, so we haven\u2019t seen articles on Slashdot and\nPhoronix and LWN with headlines like \u201cGNOME developer says Vala is dead and\nwill be removed from all servers for all eternity and you all suck\u201d. At\nleast, I\u2019ve only seen a bunch of comments on Reddit about this, but nobody\ncares about that particular cesspool of\u00a0humanity.Sadly, 140 characters do not leave any room for nuance, so maybe I should\nprobably clarify what I wrote on a venue with no character\u00a0limit.First of all, I\u2019d like to apologise to people that felt I was attacking them\nor their technical choices: it was not my intention, but see above, re:\ncharacter count. I may have only about 1000 followers on Twitter, but it\nseems that the network effe",
    "summary": "In a thrilling twist of computing history, an enthusiast nibbles at the edges of programming languages and astutely observes that Vala is, like, C#'s estranged cousin who still thinks DVDs are cutting-edge. Meanwhile, an illustrious crowd of internet commentators dives into semantic gymnastics to discuss everything from Microsoft's language strategy to why just open-sourcing everything won't solve our deepest tech woes. Of course, no tech debate is complete without invoking the ghost of past Microsoft efforts - J++ and J#, anyone? - tucked between cries for open-source Vala and the magical disappearing act of user interest when Electron showed up. \ud83d\udc7b\ud83c\udfad"
  }
]