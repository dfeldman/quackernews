[
  {
    "title": "QEMU: Define policy forbidding use of AI code generators (github.com/qemu)",
    "points": 160,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-25T23:26:55 1750894015",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=44382752",
    "comments": [
      "Open source and libre/free software are particularly vulnerable to a future where AI-generated code is ruled to be either infringing or public domain.In the former case, disentangling AI-edits from human edits could tie a project up in legal proceedings for years and projects don't have any funding to fight a copyright suit.  Specifically, code that is AI-generated and subsequently modified or incorporated in the rest of the code would raise the question of whether subsequent human edits were non-fair-use derivative works.In the latter case the license restrictions no longer apply to portions of the codebase raising similar issues from derived code; a project that is only 98% OSS/FS licensed suddenly has much less leverage in takedowns to companies abusing the license terms; having to prove that infringers are definitely using the human-generated and licensed code.Proprietary software is only mildly harmed in either case; it would require speculative copyright owners to disassemble their binaries and try to make the case that AI-generated code infringed without being able to see the codebase itself.  And plenty of proprietary software has public domain code in it already.reply",
      "I understand what experienced developers don't want random AI contributions from no-knowledge \"developers\" contributing to a project. In any situation, if a human is review AI code line by line that would tie up humans for years, even ignoring anything legally.#1 There will be no verifiable way to prove something was AI generated beyond early models.#2 Software projects that somehow are 100% human developed will not be competitive with AI assisted or written projects. The only room for debate on that is an apocalypse level scenario where humans fail to continue producing semiconductors or electricity.#3 If a project successfully excludes AI contributions (not clear how other than controlling contributions to a tight group of anti-AI fanatics), it's just going to be cloned, and the clones will leave it in the dust. If the license permits forking then it could be forked too, but cloning and purging any potential legal issues might be preferred.There still is a path for open source projects. It will be different. There's going to be much, much more software in the future and it's not going to be all junk (although 99% might.)reply",
      "I am of two minds of it having now seen both good coders augmented by AI and bad coders further diminished by it ( I would even argue its worse than stack overflow, because back then they would at least would have had to adjust code a little bit ).I am personally somewhere in the middle, just good enough to know I am really bad at this so I make sure that I don't contribute to anything that is actually important ( like QEMU ).But how many people recognize their own strengths and weaknesses? That is part of the problem and now we are proposing that even that modicum of self-regulation ( as flawed as it is ) be removed.FWIW, I hear you. I also don't have an answer. Just thinking out loud.reply",
      "> #2 Software projects that somehow are 100% human developed will not be competitive with AI assisted or written projectsStill waiting to see evidence of AI-driven projects eating the lunch of \"traditional\" projects.reply",
      "It's happening slowly all around. It's not obvious because people producing high quality stuff have no incentive at all to mark their changes as AI-generated. But there are also local tools generated faster than you could adjust existing tools to do what you want. I'm running 3 things now just for myself that I generated from scratch instead of trying to send feature requests to existing apps I can buy.It's only going to get more pervasive from now on.reply",
      "that's like driving big personal vehicles and having a bunch of children and eating a bunch of meat and do nothing about because marine and terrestrial ecosystems weren't fully destroyed by global warmingreply",
      "> #2 Software projects that somehow are 100% human developed will not be competitive with AI assisted or written projects\"competitive\", meaning: \"most features/lines of code emitted\" might matter to a PHB or Microsoftbut has never mattered to open sourcereply",
      "> If a project successfully excludes AI contributions (not clear how other than controlling contributions to a tight group of anti-AI fanatics), it's just going to be cloned, and the clones will leave it in the dust.Yeah I don\u2019t think so. But if it does then who cares? AI can just make a better QEMU at that point I guess.They aren\u2019t hurting anyone with this stance (except the AI hype lords), which I\u2019m pretty sure isn\u2019t actually an anti-AI stance, but a pragmatic response to AI slop in its current state.reply",
      "Quoting them:> The policy we set now must be for today, and be open to revision. It's best to start strict and safe, then relax.So, no need for the drama.reply",
      "If AI can generate software so easily and which performs the expected functions, why do we even need to know that it did so? Isn't the future really just asking an AI for a result and getting that result? The AI would be writing all sorts of bespoke code to do the thing we ask, and then discard it immediately after. That is what seems more likely, and not 'so much software we have to figure out rights to'.reply"
    ],
    "link": "https://github.com/qemu/qemu/commit/3d40db0efc22520fa6c399cf73960dced423b048",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n           There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.",
    "summary": "In an audacious move that dares to challenge the gospel of GitHub, QEMU proposes to ban AI-generated vomit from sullying the virginal codebase that open-source holy warriors have died defending. Commenters oscillate between chest-thumping defenses of human ingenuity and doom-laden prophecies about AI overlords coding us into obsolescence. Meanwhile, a contingent of pragmatists wonders whether staving off the AI apocalypse in software development is like trying to stop a flood with a sieve, while at least one disillusioned soul confesses they can barely code a Hello World without summoning an AI genie. What unfolds is a slapstick farce of existential dread and technological hand-wringing, QEMU style \u2013 proving once again that the only thing tech enthusiasts love more than new technology is fearing and complaining about new technology."
  },
  {
    "title": "A new pyramid-like shape always lands the same side up (quantamagazine.org)",
    "points": 247,
    "submitter": "robinhouston",
    "submit_time": "2025-06-25T20:01:07 1750881667",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=44381297",
    "comments": [
      "Worst D-4 ever! But more seriously, I wonder how closely you could get to an non-uniform mass polyhedra which had 'knife edge' type balance. Which is to say;1) Construct a polyhedra with uneven weight distribution which is stable on exactly two faces.2) Make one of those faces much more stable than the other, so if it is on the limited stability face and disturbed, it will switch to the high stability face.A structure like that would be useful as a tamper detector.reply",
      "You jest, but I knew a DND player with a dice addicting that loved  showing off his D-1 Mobius strip dice - https://www.awesomedice.com/products/awd101?variant=45578687...For some reason he did not like my suggestion that he get a #1 billard ball.reply",
      "That's like saying a donut only has one side.The linked die seems similar to this: https://cults3d.com/en/3d-model/game/d1-one-sided-die which seems adjacent to a M\u00f6bius strip but kinda isn't because the loop is not made of a two sided flat strip. https://wikipedia.org/wiki/M%C3%B6bius_stripMight be an Umbilic torus: https://wikipedia.org/wiki/Umbilic_torusThe word side is unclear.reply",
      "Love it - any sphere will do.A ping pong ball would be great - the DM/GM could throw it at a player for effect without braining them!(billiard)reply",
      "> the DM/GM could throw it at a player for effect without braining them!If you're prepared to run over to wherever it ended up after that, sure.I learned to juggle with ping pong balls. Their extreme lightness isn't an advantage. One of the most common problems you have when learning to juggle is that two balls will collide. When that happens with ping pong balls, they'll fly right across the room.reply",
      "Or any mobius stripreply",
      "I think a spherical D1 is far more interesting than a M\u00f6bius strip in this case.Dn: after the Platonic solids, Dn generally has triangular facets and as n increases, the shape of the die tends towards a sphere made up of smaller and smaller triangular faces.  A D20 is an icosahedron.  I'm sure I remember a D30 and a D100.However, in the limit, as the faces tend to zero in area, you end up with a D1.  Now do you get a D infinity just before a D1, when the limit is nearly but not quite reached or just a multi faceted thing with a lot of countable faces?reply",
      "> Love it - any sphere will do.That's basically what the link shows. A M\u00f6bius strip is interesting in that it is a two-dimensional surface with one side. But the product is three-dimensional, and has rounded edges. By that standard, any other die is also a d1. The surface of an ordinary d6 has two sides - but all six faces that you read from are on the same one of them.reply",
      "I've always seen a D1 as a bingo ball...reply",
      "The keyword is \"mono-monostatic\", and the G\u00f6mb\u00f6c is an example of a non-polyhedra one: https://en.wikipedia.org/wiki/G%C3%B6mb%C3%B6cHere's a 21 sided mono-monostatic polyhedra: https://arxiv.org/pdf/2103.13727v2reply"
    ],
    "link": "https://www.quantamagazine.org/a-new-pyramid-like-shape-always-lands-the-same-side-up-20250625/",
    "first_paragraph": "\nAn editorially independent publication supported by the Simons Foundation.\n\nGet the latest news delivered to your inbox.\nCreate a reading list by clicking the Read Later icon next to the articles you wish to save.Type search term(s) and press enterPopular\n                                    SearchesJune 25, 2025This shape can only rest on one of its four sides.G\u00e1bor DomokosContributing WriterJune 25, 2025In 360 BCE, Plato envisioned the cosmos as an arrangement of five geometric shapes: flat-sided solids called polyhedra. These immediately became important objects of mathematical study. So it might be surprising that, millennia later, mysteries still surround even the simplest shape in Plato\u2019s polyhedral universe: the tetrahedron, which has just four triangular faces.One major open problem, for instance, asks how densely you can pack \u201cregular\u201d tetrahedra, which have identical faces. Another asks which kinds of tetrahedra can be sliced into pieces that can then be reassembled to form a",
    "summary": "In a brave but futile attempt to resurrect Platonic excitement, Quantamagazine decides to push scholarly boundaries by probing the mind-bending mysteries of... a tetrahedron. Because, in 2023, nothing screams \"cutting-edge science\" like questioning the stability of geometric shapes that have been around since the dawn of Euclidean disco. Commenters, eager to escape the ennui of tetrahedral discourse, dive into a Dungeons & Dragons dice debate, because why face real-world problems when you can quibble over imaginary game accessories? Meanwhile, someone suggests using a ping pong ball as a dice alternative, brilliantly ignoring that unlike blog comments, ping pong balls actually have to adhere to the laws of physics."
  },
  {
    "title": "The Hollow Men of Hims (alexkesin.com)",
    "points": 92,
    "submitter": "quadrin",
    "submit_time": "2025-06-25T22:59:23 1750892363",
    "num_comments": 55,
    "comments_url": "https://news.ycombinator.com/item?id=44382582",
    "comments": [
      ">The real tragedy is not that Hims exists, but that it works so perfectly. Every day, thousands of people choose their compounded weight-loss drugs over FDA-approved alternatives, their combination ED pills over established single-ingredient treatments, their algorithmic consultations over actual medical care. They make these choices not because the products are better, but because the entire experience has been optimized to feel more like shopping and less like confronting the mortality and vulnerability that define the human condition.Strongly disagree with almost everything in this article, but specifically this. The reason people make these choices is not because of slick marketing working against them, it's because the existing process to get medical treatment is paternalistic, hard to navigate and often expensive.If you want safe and really high quality medical care you should absolutely have a personal physician you have a personal relationship with, who understands your lifestyle, your risk factors for side effects, and your medical needs deeply. How many Americans have that? Maybe a few dozen? The market has responded to just how terrible the existing system is.reply",
      "> If you want safe and really high quality medical care you should absolutely have a personal physician you have a personal relationship with, who understands your lifestyle, your risk factors for side effects, and your medical needs deeply. How many Americans have that? Maybe a few dozen?A bit of a tangent: I have this here in the US, through a model called Direct Primary Care. I pay $50/mo for a single provider, unlimited visits / communication, and highly discounted labs. She makes house calls on occasion. This doctor is working solely in my interest, and has little concern of insurance, except to help me navigate that system should I need a specialist, prior authorization, etc.I do worry that it's sustainable, but I think there must by a way to scale up this practice of the general practitioner working in the interest of the patient.My previous doctor was part of a large health system, who also happens to be directly associated with the large regional insurance provider whom my employer supplied to me without another choice.  Every 8 minute visit centered around insurance and billing, with my health seeming to be a distant second. It seemed every visit had to end in some kind of prescription or referral, arrived at quickly and without much discussion. It quickly became clear they were not working in my interest, and I sought other options, eventually landing on the Direct Primary Care model.  Now I have full 1 hour visits, and someone who seeks to understand what is happening for me completely, not through the lens of a payer.reply",
      "It\u2019s possible for both you and the article to be right.The system sucks, but Hims are also terrible, and medical care should not be like Amazon prime.reply",
      "> and medical care should not be like Amazon prime.Speak for yourself; that is exactly what I want.  And anyone else who wants a similar experience should be able to purchase it.reply",
      "I don't think it will take more than 5 teenage overdose deaths to get most Americans to disagree.reply",
      "It would only take that many for lobbyists to misrepresent the size of the problem and convince the public that this was a huge issue.  Then they would enact regulations to widen the moat of legacy health care companies, under the guise of \"protecting the children\".",
      "That quiet literally is the paradigm for many countries. Get off a cruise ship in Latin America and walk to the nearest port side pharmacy and get almost anything you want.This article is pure FUD pearl clutchingreply",
      "One's a storefront. The other is a foundation of society.These are not the same magnitude of sin, particularly since one's shortcomings are large reason for the existence of the other.reply",
      "I helped setup vaccination sites during the pandemic. One thing that struck me was how the providers who were working and the patients were all like \u201cthis is so much easier than normal medical care\u201d.Hims is like that.reply",
      "But access to medical treatment, particularly drugs, should be \"paternalistic\", so I strongly agree.reply"
    ],
    "link": "https://www.alexkesin.com/p/the-hollow-men-of-hims",
    "first_paragraph": "",
    "summary": "**The Hollow Misconceptions of Telehealth Enthusiasts**\nIn a bold new manifesto on alexkesin.com titled \"The Hollow Men of Hims,\" the horrors of convenience in healthcare are laid bare, sending shockwaves through the commenter intelligentsia. Confrontation with mortality is, evidently, out of vogue; replaced by the sinister allure of shopping for prescriptions as if they were the latest iPhone model. Commenters ignite battles over this dystopia, swinging wildly between cries for a return to the golden age of personal physicians (as rare as unicorns, it seems) and advocating for healthcare to mimic the speed of Amazon Prime deliveries. Somewhere in the background, the sound of our forefathers spinning in their graves provides a fitting soundtrack to the spectacle. \ud83e\udd37\u200d\u2642\ufe0f\ud83d\udc8a\ud83d\udce6"
  },
  {
    "title": "-2000 Lines of code (folklore.org)",
    "points": 184,
    "submitter": "xeonmc",
    "submit_time": "2025-06-25T19:53:13 1750881193",
    "num_comments": 65,
    "comments_url": "https://news.ycombinator.com/item?id=44381252",
    "comments": [
      "One of my best commits was removing about 60K lines of code, a whole \"server\" (it was early 2000's) with that had to hold all of its state in memory and replacing them with about 5k of logic that was lightweight enough to piggyback into another service and had no in-memory state at all. That was pure a algorithmic win - figuring out that a specific guided subgraph isomorphism where the target was a tree (directed, non cyclic graph with a single root) was possible by a single walk through the origin (general) directed bi-graph while emitting vertices and edges to the output graph (tree) and maintaining only a small in-process peek-able stack of steps taken from the root that can affect the current generation step (not necessarily just parent path).I still remember the behemoth of a commit that was \"-60,000 (or similar) lines of code\". Best commit I ever pushed.Those were fun times. Hadn't done anything algorithmically impressive since.reply",
      "Sounds interesting. Have you written about it in more detail somewhere?reply",
      "In college I worked for a company whose goal was to prove that their management techniques could get a bunch of freshman to write quality code.They couldn't. I would go find the code that caused a bug, fix it and discover that the bug was still there. Because previous students had, rather than add a parameter to a function, would make a copy and slightly modify it.I deleted about 3/4 of their code base (thousands of lines of Turbo Pascal) that fall.Bonus: the customer was the Department of Energy, and the program managed nuclear material inventory. Sleep tight.reply",
      "> make a copy and slightly modify itIn addition to not breaking existing code, also has added benefit of boosting personal contribution metrics in eyes of management. Oh and it's really easy to revert things - all I have to do is find the latest copy and delete it. It'll work great, promise.reply",
      "I mean\u2026when you have a pile of spaghetti, there is only so much you can do.reply",
      "Ask for more staff, reorganize the team into a set of multiple teams, and hire more middle management! Win win for the manager.reply",
      "I once had to deal with some contractors that habitually did this, when confronted on how this could lead to confusion they said \"that's what Ctrl+F is for.\"reply",
      "I work with someone who has a habit of code duplication like this. Typically it\u2019s an effort to turn around something quickly for someone who is demanding and loud. Refactoring the shared function to support the end edge case would take more time and testing, so he doesn\u2019t do it. This is a symptom of the core problem.reply",
      "Was this in Blacksburg by any chance?reply",
      "Related. Others?Negative 2000 Lines of Code (1982) - https://news.ycombinator.com/item?id=33483165 - Nov 2022 (167 comments)-2000 Lines of Code - https://news.ycombinator.com/item?id=26387179 - March 2021 (256 comments)-2000 Lines of Code - https://news.ycombinator.com/item?id=10734815 - Dec 2015 (131 comments)-2000 lines of code - https://news.ycombinator.com/item?id=7516671 - April 2014 (139 comments)-2000 Lines Of Code - https://news.ycombinator.com/item?id=4040082 - May 2012 (34 comments)-2000 lines of code - https://news.ycombinator.com/item?id=1545452 - July 2010 (50 comments)-2000 Lines Of Code - https://news.ycombinator.com/item?id=1114223 - Feb 2010 (39 comments)-2000 Lines Of Code (metrics == bad) (1982) - https://news.ycombinator.com/item?id=1069066 - Jan 2010 (2 comments)Note for anyone wondering: reposts are ok after a year or so (https://news.ycombinator.com/newsfaq.html).In addition to it being fun to revisit perennials sometimes (though not too often), this is also a way for newer cohorts to encounter the classics for the first time\u2014an important function of this site!reply"
    ],
    "link": "https://www.folklore.org/Negative_2000_Lines_Of_Code.html",
    "first_paragraph": "In early 1982, the Lisa software team was trying to buckle down for the big push to ship the software within the next six months.  Some of the managers decided that it would be a good idea to track the progress of each individual engineer in terms of the amount of code that they wrote from week to week.  They devised a form that each engineer was required to submit every Friday, which included a field for the number of lines of code that were written that week.",
    "summary": "**Silicon Valley reinvents subtraction: measuring engineer value by lines of code removed**\n\nIn a captivating leap back to 1982, the Lisa software team crafts a forward-thinking (read: hilariously misguided) metric: lines of code written per week. Because obviously, software quality scales directly with code volume, like stuffing a turkey to ensure its taste. \ud83e\udd83 Commenters trip over themselves to reminisce about that one time they deleted more code than they wrote, pitching these stories as the height of their coding careers. It's like a coder's version of fishing tales, but instead of \"the one that got away,\" it's \"the thousands of lines I saved.\" Heroes don't wear capes; they press delete. \ud83e\uddb8\u200d\u2642\ufe0f\ud83d\udcbb"
  },
  {
    "title": "Gemini CLI (blog.google)",
    "points": 934,
    "submitter": "sync",
    "submit_time": "2025-06-25T13:10:46 1750857046",
    "num_comments": 528,
    "comments_url": "https://news.ycombinator.com/item?id=44376919",
    "comments": [
      "I gave it a shot just now with a fairly simple refactor.  +19 lines, -9 lines, across two files. Totally ballsed it up. Defined one of the two variables it was meant to, referred to the non-implemented one. I told it \"hey you forgot the second variable\" and then it went and added it in twice. Added comments (after prompting it to) which were half-baked, ambiguous when read in context.Never had anything like this with claude code.I've used Gemini 2.5 Pro quite a lot and like most people I find it's very intelligent. I've bent over backwards to use Gemini 2.5 Pro in another piece of work because it's so good. I can only assume it's the gemini CLI itself that's using the model poorly. Keen to try again in a month or two and see if this poor first impression is just a teething issue.I told it that it did a pretty poor job and asked it why it thinks that is, told it that I know it's pretty smart. It gave me a wall of text and I asked for the short summary> My tools operate on raw text, not the code's structure, making my edits brittle and prone to error if the text patterns aren't perfect. I lack a persistent, holistic view of the code like an IDE provides, so I can lose track of changes during multi-step tasks. This led me to make simple mistakes like forgetting a calculation and duplicating code.reply",
      "I love how fragmented Google's Gemini offerings are. I'm a Pro subscriber, but I now learn I should be a \"Gemini Code Assist Standard or Enterprise\" user to get additional usage. I didn't even know that existed! As a run of the mill Google user I get a generous usage tier but paying them specifically for \"Gemini\" doesn't get me anything when it comes to \"Gemini CLI\". Delightful!reply",
      "Google suffers from Microsoft's issues: it has products for almost everything, but its confusing product messaging dilutes all the good things it does.I like Gemini 2.5 Pro, too, and recently, I tried different AI products (including the Gemini Pro plan) because I wanted a good AI chat assistant for everyday use. But I also wanted to reduce my spending and have fewer subscriptions.The Gemini Pro subscription is included with Google One, which is very convenient if you use Google Drive. But I already have an iCloud subscription tightly integrated with iOS, so switching to Drive and losing access to other iCloud functionality (like passwords) wasn\u2019t in my plans.Then there is the Gemini chat UI, which is light years behind the OpenAI ChatGPT client for macOS.NotebookLM is good at summarizing documents, but the experience isn\u2019t integrated with the Gemini chat, so it\u2019s like constantly switching between Google products without a good integrated experience.The result is that I end up paying a subscription to Raycast AI because the chat app is very well integrated with other Raycast functions, and I can try out models. I don\u2019t get the latest model immediately, but it has an integrated experience with my workflow.My point in this long description is that by being spread across many products, Google is losing on the UX side compared to OpenAI (for general tasks) or Anthropic (for coding). In just a few months, Google tried to catch up with v0 (Google Stitch), GH Copilot/Cursor (with that half-baked VSCode plugin), and now Claude Code. But all the attempts look like side-projects that will be killed soon.reply",
      "I subscribed to Google One through the Google Photos iOS app because I wanted photos I took on my iPhone to be backed up to Google. When I switched to Android and went into Google One to increase my storage capacity in my Google account, I found that it was literally impossible, because the subscription was tied to my iCloud account. I even got on a line with Google Support about it and they told me yeah it's not even possible on their side to disconnect my Google One subscription from Apple. I had to wait for the iCloud subscription to Google One to end, and then I was able to go into Google One and increase my storage capacity.reply",
      "The root problem here lies with Apple. It's so frustrating how they take a 30% cut for the privilege of being unable to actually have a relationship with your customers. Want to do a partial refund (or a refund at all)? Want to give one month free to an existing subscriber? Tough luck. Your users are Apple's customers, not yours.reply",
      "I implemented Google One integration in an iOS app. This comment chain is accurate. Users want to pay with Apple (like other app subscriptions) but then your \u201caccount\u201d is inside their payments world. Which is super confusing since users (rightly) think they are dealing with their Google account.reply",
      "Same as a shopping centre, clothing retailer, or any other non-bazaar marketplace with its own brand and transaction processing.Apple is selling you a huge lucrative market.Customers buy Apple\u2019s curated marketplace.Apple takes a cut for being in the middle and enabling all of this.Believe me, I would never pay for most of the apps that I did pay for via Apple if it wasn\u2019t via their marketplace and their consumer protections.There is no counterfactual scenario where you and millions(!) of other ISVs get 100% of the same money without Apple.What\u2019s difficult to understand about these business relationships?reply",
      "Lmao no.Okay, all the app developers pull out of iOS because they're not actually useful, in fact they should be paying Apple!How many people do you think would still buy iPhones if there are 0 apps on the app store? Lmaooo, it's almost like it's a co-operative relationship and Apple don't deserve a huge cut because it's the apps that sell their phones.reply",
      "> Apple takes a cut for being in the middle and enabling all of this.Enabling this like Ticketmaster enables selling tickets.In ticketmaster's case I believe they give kickbacks and lucrative exclusive contracts with large venues, to squeeze smaller ones, maybe making whole tours use it but only kicking back to the biggest or select venues on the tour I think.Apple sometimes does special deals and special rules with important providers, among many other tactics behind their moat. All single signons must also offer apple single sign-on, for instance, and they have even disabled access to customer accounts using their single sign-on for unrelated business disputes, though they walked it back in the big public example I'm aware of, the threat is there if you go against them in any way.reply",
      "Amazon or Wallmart are much better analogies.Ticketmaster is in no way comparable, because they gouge customers and provide no protections.Someone in the music industry explained that both bands and venues like Ticketmaster because then Ticketmaster is the \"bad guy\" and the band can just shrug their shoulders and pretend to be the victim while profiting enormously from Ticketmaster's evil practices.reply"
    ],
    "link": "https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/",
    "first_paragraph": "Jun 25, 2025\n          Free and open source, Gemini CLI brings Gemini directly into developers\u2019 terminals \u2014 with unmatched access for individuals.\n         Upgrade your terminal experience with Gemini CLI today.For developers, the command line interface (CLI) isn't just a tool; it's home. The terminal\u2019s efficiency, ubiquity and portability make it the go-to utility for getting work done. And as developers' reliance on the terminal endures, so does the demand for integrated AI assistance.That\u2019s why we\u2019re introducing Gemini CLI, an open-source AI agent that brings the power of Gemini directly into your terminal. It provides lightweight access to Gemini, giving you the most direct path from your prompt to our model. While it excels at coding, we built Gemini CLI to do so much more. It\u2019s a versatile, local utility you can use for a wide range of tasks, from content generation and problem solving to deep research and task management.We\u2019ve also integrated Gemini CLI with Google\u2019s AI coding a",
    "summary": "Huzzah for Google, savior of disorganized techies, and their latest half-baked contribution to the developer's toolkit: the Gemini CLI. In a move that combines the guesswork of coding via Microsoft Clippy with the reliability of a chocolate teapot, this CLI ensures that you'll be debugging more than just your subconscious fear of commitment. Meanwhile, the ever-enthusiastic techno-masochists in the comment section celebrate by sharing tales of how gloriously Gemini mangled their codebase, all while meticulously circling around the real problem: their inexplicable loyalty to Google's fragmented AI ecosystem. At this rate, they'll have Marie Kondo-ed their entire workflow by 2026, retaining only items that spark joy or utter despair. \ud83d\ude31\ud83d\udc68\u200d\ud83d\udcbb\ud83d\udd25"
  },
  {
    "title": "A new PNG spec (programmax.net)",
    "points": 468,
    "submitter": "bluedel",
    "submit_time": "2025-06-24T13:01:17 1750770077",
    "num_comments": 456,
    "comments_url": "https://news.ycombinator.com/item?id=44365754",
    "comments": [
      "Author here. Hello everyone!\nFeel free to ask me anything.\nI'll go ahead and dispel some doubts I already see here:- It isn't really a \"new format\". It's an update to the existing format.\n- It is very backwards compatible.\n-- Old programs will load new PNGs to the best of their capability. A user will still know \"that is a picture of a red apple\".There also seems to be some confusion about how PNGs work internally. Short and sweet:\n- There are chunks of data.\n-- Chunks have a name, which says what data it contains. A program can skip a chunk it doesn't recognize.\n- There is only one image stream.reply",
      "Do you have any examples on hand of PNGs that use the new features of the spec? It would be cool to see a little demo page with animated or HDR images, especially to download to test if our programs support them yet.reply",
      "Sure!Chris Lilley--one of the original PNG co-authors--has a post with an example HDR image: https://svgees.us/blog/cICP.html\nIt is about half way down, with the birthday cake.\nGenerally, us tech nerds have phones that are capable of displaying it well. So perhaps view the page on your phone.What you should look for is the cake, the pink tips in her hair, and the background being more vivid. For me, the pink in the cake was the big give-away.There is also the Web Platform Tests (WPT) which we use to validate browser support: https://wpt.fyi/results/png/cicp-chunk.html?label=master&lab...Although, that image is just a boring teal. See it live in your browser here: https://wpt.live/png/cicp-chunk.htmlFor an example of APNG, you can use Wikipedia's images: https://en.wikipedia.org/wiki/APNGBut you have a bigger point: I should have live demonstrations of those things to help people understand.reply",
      "Thank you for the examples. I tried the one with a pink cake. Turns out that on my machine only web browsers are capable of displaying the image properly. All  viewers (IrfanView, XnView, Nomacs, Windows Photos) and editors (Paint .NET, GIMP) that I've tried only showed the \"washed out\" picture.reply",
      "Yeah. We were able to get buy-in from some big players. We cannot contact every group, though. My hope is since big players have bought in, others will hear the message and update their programs.Sooooo file some bugs :DAlso, be kind to them. This literally launched yesterday.reply",
      "Thanks, I appreciate all of these links. :)reply",
      "So, I'm a big fan of metaformats with generalized tooling support. Think of e.g. Office Open XML or ePub \u2014 you don't need \"an OOXML parser\" / \"an ePub parser\" to parse these; they're both just zipped XML, so you just need a zipfile library and libxml.For the lifetime of PNG so far, a PNG file has almost, but just barely not, been a valid Interchange File Format (IFF) file.IFF is a great (simple to understand, simple to implement support for, easy to generate, easy to decode, memory-efficient, IO-efficient, relatively compact, highly compressible) metaformat, that more people should be aware of.However, up to this point, the usage of IFF has consisted of:\u2022 some old proprietary game-data and image formats from the 1980s that no modern person has heard of\u2022 some popular-yet-proprietary AV formats [AIFF, RIFF] that nobody would write a decoder for by hand anyway (because they would need a DSP library to handle the resulting sample-stream data anyway, and that library may as well offer container-format support too)\u2022 The object files of an open but uncommon language runtime (Erlang .beam files), where that runtime exposes only high-level domain-specific parsing tooling (`beam_lib`) rather than IFF-general decoding tooling\u2022 An \"open-source but corporate-steered\" image format that people are wary of allowing to gain ecosystem traction (WebP \u2014 which is more-specifically a document in a RIFF container)\u2022 And PNG... but non-conformantly, such that any generic IFF decoder that could decode the other things above, would choke on a PNG file.IMHO, this is a major reason that there is no such thing as \"generalized IFF tooling\" today, despite the IFF metaformat having all the attributes required to make it the \"JSON of the binary world\". (Don't tell me about CBOR; ain't nobody hand-rolling a CBOR encoder out of template strings.)If you can't guess by now, my wishlist item for PNGv3, is for PNG files to somehow become valid/conformant IFF files \u2014 such that the popularity of PNG could then serve as the bootstrap for a real IFF tooling ecosystem, and encourage awareness/use of IFF in new greenfield format-definition use-cases.---Now, I've written PNG parsers, and generic IFF parsers too. I've even tried this exact unification trick before (I wanted an Erlang library that could parse both .beam files and PNG files. $10 if you can guess the use-case for that!)Because of this, I know that \"making PNG valid per IFF\" isn't really possible by modifying the PNG format, while ensuring that the resulting format is decodable by existing PNG decoders. If you want all the old [esp. hardware] PNG parsers to be compatible with PNGv3s, then y'all can't exactly do anything in PNGv3 like \"move the 4-byte CRC inside the chunk as measured by the 4-byte chunk length\" or \"make the CRCs into their own chunks that reference the preceding record\".But I'm not proposing that. I'm actually proposing the opposite.Much of what PNGv2 did in contravention of the IFF spec, is honestly a pretty good idea in general. It's all stuff that could be \"upstreamed\" \u2014 from the PNG level, to the IFF level.I propose: formalizing \"the variant of IFF used in PNG\" as its own separate metaformat specification \u2014 breaking this metaformat out from the PNG spec itself into its own standards document.This would then be the \"Interchange File Format specification, version 2.0\" (not that there was ever a formal IFFv1 spec; we all just kind of looked at what EA/Commodore had done, and copied it in our own code since it was so braindead-easy to implement.)This IFF 2.0 spec would formalize, at least, a version or \"profile\" of IFF for which PNGv2 images are conformant files. It would have chunk CRCs; chunk attribute bits encoded for purposes of decoders + editors via meaningful chunk-name letter-casing; and an allowance for some number of garbage bytes before the first valid chunk begins (for PNG's leading file signature that is not itself a valid IFF chunk.)This could be as far as the IFF 2.0 spec goes \u2014 leaving IFFv1 files non-decodable in IFFv2 parsers. But that'd be a shame.I would suggest going further \u2014 formalizing a second IFFv2 \"profile\" against which IFFv1 documents (like AIFF or RIFF files) are conformant; and then specifying that \"generic\" IFFv2-conformant decoders (i.e. a hypothetical \"libiff\", not a format-specific libpng) MUST implement support for decoding both the IFFv1-conforming and the PNGv2-conforming profiles of IFF.It could then be up to the IFF-decoding-tooling user (CLI command user, library caller) to determine which IFFv2 \"profile\" to apply to a given document... or the IFFv2 spec could also specify some heuristic algorithm for input-document \"profile\" detection. (I think it'd be pretty easy; find a single chunk, and if what follows its chunk-length is a CRC that validates that chunk, then you have the PNGv2-like profile. Whereas if it's not that, but is instead four bytes of chunk-name-valid character ranges, then you've got the IFFv1-like profile. [And if it's neither, then you've got a file with a corrupted first chunk.])---And, if you want to go really far, you could then specify a third entirely-novel \"profile\", for use in greenfield IFF applications:\u2022 A few bytes of space aren't so precious; we can hash things much faster these days, with hardware-accelerated hashing instructions; and those instructions are also for hashes that do much better than CRC to ensure integriaty. So either replace the inline CRCs with CRC chunks, or with nested FORM-like container records (WCRC [len] [CRC4] [interior chunk]). Or just skip per-chunk CRCs and formalize a fHsh chunk for document-level integrity, embedding the output of an arbitrary hash algorithm specified by its registered https://github.com/multiformats/multihash \"hash function code\".\u2022 Re-widen the chunk-name-valid character set to those valid in IFFv1 documents, to ensure those can be losslessly re-encoded into this profile. To allow chunks with non-letter characters to have a valid attribute decoding, specify a document-level per-chunk-name \"attributes of all chunks of this type\" chunk, that can either be included into a given concrete format's header-chunk specification, or allowed at various points in the chunk stream per a concrete format's encoding rules (where it would then be expected to apply to any successor + successor-descendant chunks within its containing chunk's \"scope.\") Note that the goal here is to keep the attribute bits in some way or another \u2014 they're very useful IMHO, and I would expect an IFF decoder lib to always be emitting these boolean chunk-attribute fields as part of each decoded chunk.\u2022 Formalize the magic signature at the beginning into a valid chunk, that somehow encodes 1. that this is an IFF 2.0 \"greenfield profile\" document (bytes 0-3); 2. what the concrete format in use is (bytes 4-7). (You could just copy/generalize what RIFF does here [where a RIFF chunk has the semantics of a LIST chunk but with a leading 4-byte chunk-name type], such that the whole document is enclosed by a root chunk \u2014 though this is painful in that you need to buffer the entire document if you're going to calculate the root-chunk length.)I'm just spitballing; the concrete details of such a greenfield profile don't matter here, just the design goal \u2014 having a profile into which both IFFv1 and PNGv2 documents could be losslessly transcoded. Ideally with as minimal change to the \"wider and weirder/more brittle ecosystem\" side [in this case that's IFFv1] as possible. (Compare/contrast: how HTML5 documents are a profile of HTML that supersedes both HTML4 and XHTML1.1 \u2014 supporting both unclosed tags and XML-namespaced element names \u2014 allowing HTML4 documents to parse \"as\" HTML5 without rewrites, and XHTML1.1 documents to be transcoded to HTML5 by just stripping some root-level xmlns declarations and changing the doctype.)reply",
      "Strangely, I was familiar with AIFF and RIFF files but never made the connection that they're both IFF. I hadn't known about IFF before your post. Thank you :)W3C requires that we do not break old, conformant specs. Meaning if the next PNG spec would invalidate prior specs, they won't approve it. By extension, an old, conformant program will not suddenly become non-conformant.I could see a group of people formalizing IFFv2, and adapting PNG to it. But that would effectively be PNGIFF, not PNG. It would be a new spec. Because we cannot break the old one.That might be fine. But it comes with a new set of problems, like adoption.Soooo I like the idea but it would probably be a separate thing. FWIW, it would actually be nice to make a formal IFF spec. If there was no governing body that owns it, we can find an org and gather interest.I doubt W3C would be the right org for it. ISO subgroup??reply",
      "They pretty much say the same thing halfway through. Don't change PNG but adapt IFF to work with PNG's flavour of IFF.reply",
      "Right. Sorry, that was supposed to be a \"yes, and...\" to provide some additional context.reply"
    ],
    "link": "https://www.programmax.net/articles/png-is-back/",
    "first_paragraph": "\nA new PNG spec was just released!\n\t\t\t\t\t\tEveryone, go update your 2003 forum avatars.\n\t\t\t\t\tJokes aside, this is exciting news. PNG is back to its former glory after its progress stalled for over two decades. Did you know the U.S. Library of Congress, Library and Archives Canada, and the National Archives of Australia recommend PNG? It is important that we keep PNG current and competitive. After 20 years of stagnation, PNG is back with renewed vigor!Figure 1. Adapted from Wikipedia's CIE xy 1931 Rec. 2020 and Rec. 709 images under the Creative Commons Attribution-Share Alike 3.0 Unported (CC BY-SA 3.0) license.\n\t\t\t\t\t\t\t\tFigure 1 shows the colors our eyes can see.\n\t\t\t\t\t\t\t\tThe smaller, inner triangle represents the color space of most images.\n\t\t\t\t\t\t\t\tThe larger, outer triangle represents the colors that are typical with a High Dynamic Range (HDR) image.\n\t\t\t\t\t\t\tThis new HDR support uses only 4 bytes (plus the usual PNG chunk overhead).Chris Lilley\u2014one of PNG's original co\u2010authors and curren",
    "summary": "**The Resurrection of PNG: Now with More Bytes!**\n\nIn an epic display of late-stage digital necromancy, the tech wizards over at Programmax have unleashed the latest PNG specification, finally giving that much-needed facelift to your tragically dated forum avatar from the early 2000s. The new spec promises a breathtaking advancement: it can handle <em>colors</em> and <em>stuff</em>, thanks to the inclusion of High Dynamic Range \u2013 because your smartphone definitely needed more ways to consume its own battery life. Comment sections are now flooded with digital art connoisseurs and amateur coder philosophers debating fiercely if a red apple in a PNG really looks like a red apple, while the original PNG co-author beams like a proud parent at a kindergarten graduation. Feel free to marvel at the complex color triangles and 4 extra bytes that herald this groundbreaking update, because everyone knows it\u2019s not a real tech improvement if it doesn\u2019t require at least one more byte."
  },
  {
    "title": "What Problems to Solve (1966) (cat-v.org)",
    "points": 301,
    "submitter": "jxmorris12",
    "submit_time": "2025-06-25T17:08:44 1750871324",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=44379606",
    "comments": [
      "This was a beautiful letter to read, with a simple piece of wisdom about life, spelled out for the student.I am grateful that this was submitted to Hacker News, and that I was able to read it.reply",
      "Man while Feynman was a genius, I think it's underappreciated just how articulate and philosophical he was.  I've always loved reading his work because he just knew how to say things the right way.This letter really allows that side of him to shine through.reply",
      "He could would shrink the complex into something that could fit in even my head.I like this one:This particle is a perfect ball bearing that can move at a single speed in one of six directions.from \"Feynman the Explainer\" in:https://longnow.org/essays/richard-feynman-connection-machin...also:\"Don't say `reflected acoustic wave.' Say [echo].\" Or, \"Forget all that `local minima' stuff. Just say there's a bubble caught in the crystal and you have to shake it out.\" Nothing made him angrier than making something simple sound complicated.reply",
      "That is the main reason why he is appreciated imoreply",
      "> You will get the pleasure of success, and of helping your fellow man, even if it is only to answer a question in the mind of a colleague less able than you.> innumerable problems that you would call humble, but which I enjoyed and felt very good about because I sometimes could partially succeed.> You met me at the peak of my career when I seemed to you to be concerned with problems close to the gods.As problem solvers, we need encouragement to face the difficulties that lie in exploring problems.  We need to believe that it can be solved but more so that WE/I can solve it. We need to raise our egos to healthy amounts (not sure what is the precise definition of healthy) so we don't back down or give up.  And Mr. Feynman alludes to this with \"the pleasure of success\", \"helping your fellow man\", \"answer a question in the mind of a colleauge\", \"I enjoyed ... because I sometimes could partially succeed\", and \"problems close to the gods\".I am exploring (and absolutely denouncing) this egotism for it leads to frustration, disconnection, illusion, entitlement, and shielding. I feel that (good) school/university/work environments raise ego levels (with \"good job!\") and aloof you from _........ (which is a utopian place with a healthy encouragement to do more work and work harder to a point where it does not overwhelm you).The identify of this _........ place keeps occuring to me and flees from me as quickly as it occurs to me. If there is anyone who works without ego, please let me know.reply",
      "Original sin mate. We must suffer an appreciation of the divine while being simultaneously unable to fulfill it. Accept you humanity and be kind to yourself about it.reply",
      "> \"...Do not remain nameless to yourself \u2013 it is too sad a way to be. now (sic) your place in the world and evaluate yourself fairly, not in terms of your na\u00efve ideals of your own youth, nor in terms of what you erroneously imagine your teacher\u2019s ideals are...\"Wise wordsreply",
      "This echoes what I have thought about my career. What to work on.I've been blessed to have a good paying career in software engineering, but I've never really felt passionate about the products I work on. At the end of the day, my job is a paycheck. I do feel joy solving problems for others, improve society, be able to answer colleagues questions when they \"come to my office\". My family is happy that I can provide and that I am a role model for them.I sometimes think I should work on things that make me happier. Sometimes I think that my career path is a mistake, I should work on problems \"closer to god\", make more meaningful contributions, build the next Kubernetes/ChatGPT/Google/<insert revolutionary product>, advance AI, climate change. I end giving up, I'm not that ambitious or driven.I'm important to my family and colleagues. That may be good enough.reply",
      "> That may be good enough.I would argue it is.I have had discussions with peers recently around doing the big flash-y <insert revolutionary product>. An interesting analogy surfaced. The nuts in the studs of the infrastructure of the many structurally sound homes in existence are just as important (meaningful) as the doors, windows, and more flash-y features. They may be _more_ important in some cases. They all make up the home.It made me realize it might not be all about maximizing ambitious pursuits. Maybe it is more about experiencing the joy of solving the next problem and the fulfillment that comes from simply being needed pretty regularly.reply",
      "The vast majority of human existence from million years ago to now is toil. I don't spend anytime feeling bad about being well compensated at an air conditioned office working on CRUD.reply"
    ],
    "link": "http://genius.cat-v.org/richard-feynman/writtings/letters/problems",
    "first_paragraph": "A former student, who was also once a student of Tomonaga\u2019s, wrote to extend\nhis congratulations. Feynman responded, asking Mr. Mano what he was now doing.\nThe response: \u201cstudying the Coherence theory with some applications to the\npropagation of electromagnetic waves through turbulent atmosphere\u2026 a humble and\ndown-to-earth type of problem.\u201d",
    "summary": "In a touching display of nostalgic overreach, Hacker News commenters wax lyrical over a letter involving Richard Feynman \ud83e\udde0, effectively turning a simple academic exchange into the second coming of philosophical enlightenment. One commenter gets misty-eyed about Feynman's knack for \"simplifying the complex,\" possibly mistaking brevity for profundity. Meanwhile, others dive headfirst into the emotion pond, pontificating on careers \"close to the gods\" and the existential crises that accompany coding for cash rather than passion. The collective insistence that every mundane professional hiccup might be a cosmic trial truly captures the peak of tech-bubble solipsism. Truly, some souls on Hacker News seek not just to code, but to ascend directly into coder nirvana. \ud83d\ude4f\ud83d\udcbb"
  },
  {
    "title": "OpenAI charges by the minute, so speed up your audio (mand.is)",
    "points": 437,
    "submitter": "georgemandis",
    "submit_time": "2025-06-25T13:17:25 1750857445",
    "num_comments": 132,
    "comments_url": "https://news.ycombinator.com/item?id=44376989",
    "comments": [
      "With transcribing a talk by Andrej, you already picked the most challenging case possible, speed-wise. His natural talking speed is already >=1.5x that of a normal human. One of the people you absolutely have to set your YouTube speed back down to 1x when listening to follow what's going on.In the idea of making more of an OpenAI minute, don't send it any silence.E.g.    ffmpeg -i video-audio.m4a \\\n      -af \"silenceremove=start_periods=1:start_duration=0:start_threshold=-50dB:\\\n                         stop_periods=-1:stop_duration=0.02:stop_threshold=-50dB,\\\n                         apad=pad_dur=0.02\" \\\n      -c:a aac -b:a 128k output_minpause.m4a -y\n\nwill cut the talk down from 39m31s to 31m34s, by replacing any silence (with a -50dB threshold) longer than 20ms by a 20ms pause. And to keep with the spirit of your post, I measured only that the input file got shorter, I didn't look at all at the quality of the transcription by feeding it the shorter version.reply",
      "Andrej's talk seemed normal to listen at 2x but I've also listened to everything at 2x for a long time.Unfortunately a byproduct of listening to everything at 2x is I've had a number of folks say they have to watch my videos at 0.75x but even when I play back my own videos it feels painfully slow unless it's 2x.For reference I've always found John Carmack's pacing perfect / natural and watchable at 2x too.A recent video of mine is https://www.youtube.com/watch?v=pL-qft1ykek. It was posted on HN by someone else the other day so I'm not trying to do any self promotion here, it's just an example of a recent video I put up and am generally curious if anyone finds that too fast or it's normal. It's a regular unscripted video where I have a rough idea of what I want to cover and then turn on the mic, start recording and let it pan out organically. If I had to guess I'd say the last ~250-300 videos were recorded this way.reply",
      "To me you talk at what I would consider \"1.2x\" of podcast speed (which to me is a decent average measure of spoken word speed - I usually do 1.5x on all podcasts). You're definitely still in the normal distribution for tech YouTubers, in my experience - in fact it feels like a lot of tech YouTube talks like they've had a bit too much adderall, but you don't come off that way. Naturally people may choose to slow down tutorials, because the person giving the tutorial can never truly understand what someone learning would or wouldn't understand. So overall I think your speed is totally fine! Also, very timely video, I was interested in the exact topic, so I'm happy I found this.reply",
      "> \"[I]n fact it feels like a lot of tech YouTube talks like they've had a bit too much adderall, [...]\"Funnily enough, if you actually have ADHD, then stimulants like adderall or even nicotine, will calm you down.> Naturally people may choose to slow down tutorials, [...]For me it also depends on what mood I'm in and whether I'm doing anything else at the same time.  If I'm fully concentrating on a video, 2x is often fine.  If I'm doing some physical task at the same time, I need it slower than that.If I'm doing a mental task at the same, I can forget about getting anything out of the video.  At least, if the mental task involves any words.  So eg I could probably still follow along a technical discussion at roughly 1x speed while playing Tetris, but not while coding.reply",
      "> Andrej's talk seemed normal to listen at 2x but I've also listened to everything at 2x for a long time.We get used to higher speeds when we consume a lot of content that way. Have you heard the systems used by experienced blind people? I cannot even understand the words in them, but months of training would probably fix that.reply",
      "Yeah, you sound around 1.25-1.5x than the average videos I watchreply",
      ">  I didn't look at all at the quality of the transcription by feeding it the shorter version.guys how hard is it to toss both versions into like diffchecker or something haha youre just comparing textreply",
      "Why use diffchecker when there\u2019s a perfectly good LLM you could ask right there? lolreply",
      "Assuming sarcasm but if not, because deterministic vs. nondeterministic output?reply",
      "because a lot of LLMs will just eat tokens to call a diffchecker.really it becomes a question of whether or not the friction of invoking the command or the cost of tokens is greater.as I get older and more rsi'd the tokens seem cheaper.reply"
    ],
    "link": "https://george.mand.is/2025/06/openai-charges-by-the-minute-so-make-the-minutes-shorter/",
    "first_paragraph": "",
    "summary": "**OpenAI's New Minute-Milking Feature: The Time-Saver That Isn't**\n\nIn the latest exercise of capitalist ingenuity, OpenAI finds yet another way to drain your wallet by charging you for each ***precious*** minute you dare feed silence into its transcribing service. One intrepid hacker proposes a *dazzling* solution: why not just surgically remove the silences with ffmpeg, compressing a 39-minute talk down to 31 minutes of non-stop, breathless tech babble? Commenters enthusiastically miss the point, bragging about their superhuman ability to consume sped-up content, likely leading to a future where human conversation sounds eerily like a chipmunk reciting Shakespeare. Meanwhile, debates on the quality of transcription remain a distant second concern, because who cares about accuracy when you're shaving off minutes in a race against your billing cycle? \ud83d\ude80\ud83d\udcb8\ud83e\udd16"
  },
  {
    "title": "The Offline Club (theoffline-club.com)",
    "points": 79,
    "submitter": "esher",
    "submit_time": "2025-06-25T19:42:57 1750880577",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=44381168",
    "comments": [
      "This is [Meetup](https://www.meetup.com). Meetup has obviously not aged well, but this is mostly due to changes in ownership and leadership. It\u2019s original mission of \u201ca Meetup Everywhere about Most Everything\u201d is pretty much exactly what The Offline Club seems to be seeking.I think they\u2019ll find a lot of the same challenges:    1. Finding space to have events\n    2. Ensuring that people who said \u201cI\u2019m going\u201d actually end up going. \n    3. Bootstrapping groups such that when I stumble upon The Offline Club, I can signup for something relevant to me, happening a short time from now. \n    4. Keeping organizers willing to continue hosting events\n    5. Keeping away organizers who see it as lead gen for their sales job\n\nBasically, good luck!Edit: On second look, this is different than Meetup in that it\u2019s not centered around a specific topic \u2026 except for being \u201coffline\u201d together, which obviously could create other opportunities for hobbies, etc.reply",
      "There's a meetup dynamic which has previously been explained to me, it goes something like this: someone starts a meetup where a mix of cool people and weirdos show up, the events continue until the ratio gets really bad which causes the cool people to splinter off into their own private group. I wonder if this product is able to escape that pattern.reply",
      "We run meetups for systems programmers [0] and have mostly addressed these challenges.> 1. Finding space to have eventsTalk to a coffee shop owner. Promise them your group will (reliably) order drinks or snacks. In exchange, every month we get an area \"cordoned off\" just for us.> 2. Ensuring that people who said \u201cI\u2019m going\u201d actually end up going.Aside from sending a general newsletter, I personally ping and catch up with individuals. This is a lot of work. It pays off when they evangelize your event on your behalf.> 3. Bootstrapping groups such that when I stumble upon The Offline Club, I can signup for something relevant to me, happening a short time from now.See #2> 4. Keeping organizers willing to continue hosting eventsThat's tougher. However, if the event is specialized/niche/unique enough, the organizers will be conferred high social status by the community.> 5. Keeping away organizers who see it as lead gen for their sales jobMmm, could we define sales job? On the business front, the meetups are used to promote our (indie) conferences. The meetup groups don't mind when I ask them to buy a ticket. They can just say no and we're not pushy about it.[0] https://handmadecities.com/meetupsreply",
      "Yes. The group I used to run also addressed a lot of these challenges. However, this isn\u2019t so easy for everyone who runs meetups.Part of the promise of WeWork buying Meetup, for instance, was \u201coh look! We have access to tons of real estate to house Meetups in.\u201d A large amount of organizer support was providing ideas for places to have events.I worked at Meetup for a couple of years. There were often Meetup groups that started up in the guise of $GENERIC get together, that ended up actually being literal lead gen for a pyramid scheme. This wasn\u2019t likely a tech meetup thing, but perhaps a knitting circle, or whatever.reply",
      "> started up in the guise of $GENERIC get together, that ended up actually being literal lead gen for a pyramid schemeAh yes yes. That's horrifying.reply",
      "Yeah, agree that none of the issues are problems without solutions.The issue is vulture capitalism and misalignment of incentives for platform vs host vs participants. I've been a part of groups that solved these and grew to 8000-member communities. It's simply that meetup wasn't actually interested to solve the challenges because they needed to extract wealth and pass extraction down the chain (no incentivise to protect underlying communities as a commons)reply",
      "It definitely has things in common with meetup.com.  But it looks meaningfully distinct to me because the appear to specifically have some kind of strong preference against connected devices.  Honestly, I've been wishing for things in this vein recently because of the feeling that our world is growing too superficial with our faces buried in phones and being fed by addictive algorithms.That being said, I think you're right about some of the challenges that an effort like this will encounter.reply",
      "One thing I\u2019ve noticed with Meetup is that a lot of events went virtual during Covid, then never went back. When I go there it seems like so many things near me are simply Zoom meetings, which I have no interest in.I understand needing that during that period, but it seems like if they want to get back to the real purpose of the site, they need to do away with that option.reply",
      "These places already exist, they're your local game stores! Show up, play games with other people. If you like competition most host official tournaments for various TCG's and table top war-games. These tournaments usually forbid devices while playing, since they can be used to gain unfair advantage, so you are forced to be offline by default. (Plus it's considered rude to be on your phone during a match.)reply",
      "I'm not a huge table top gamer or anything and have checked out my local shop to play with others. It was definitely fun, but I'll admit that it wasn't for me and I stopped going. If you are into board games, you definitely should check them out though. I just found it required more interest in board games than I was willing to partake in when I really just wanted a third space to hang out.On the flip side, I was into swing dancing for a few years and I found that was a great place to socialize. Of course, like the board games thing, it's not for everyone. That said, I did find it was a bit easier to just show up there and just socialize. Once you're a regular, you don't necessarily have to be constantly dancing or anything. (I did when I got started, though.) You can just hang out and have a good conversation with another regular, and often there's a bar where you can chill with a drink. It's definitely intimidating to go dancing as a newb, though, so I recommend going some place that offers a weekly class. That's a great way to meet other newbies and go to the dance together.reply"
    ],
    "link": "https://www.theoffline-club.com",
    "first_paragraph": "Our customers say Excellent 4.7 out of 5 based on 251 reviews4.7/5 (251 reviews) onOur customers say Excellent 4.7 out of 5 based on 251 reviews4.7/5 (251 reviews) onBy clicking \"Accept\", you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts. View our Privacy policy for more information.Offline hangouts and events to unplug, relax and connect with like-minded people.Join The Offline Club for a relaxing break from screens, distractions and the daily rush. \u00a0Unwind, meet others and enjoy hobbies you never have the time for. Join your city's waitlist to be the first to know when we launch in your city, or apply to start a local Offline Club chapter.We host a variety of offline events and experiences - from offline cafe hangouts to phone-free dinners with strangers. Here's why people join:CopenhagenBerlinMilanLondonBarcelonaParisAmsterdamOur customers say Excellent 4.7 out of 5 based on 251 reviews4.7/5 (251 re",
    "summary": "Welcome to **The Offline Club**, where people pretend they want \"authentic\" social interactions by leaving their smartphones at the door but then panic when they can't scroll Instagram under the table. \ud83d\ude44 With a stellar 4.7 out of 5 stars from a whopping 251 reviews, who wouldn't leap at the chance to awkwardly mingle at phone-free dinners with strangers, each of whom is also secretly dying for their screen fix? Commenters are quick to draw comparisons to Meetup, but with a snazzy 'offline' twist, somehow making it both revolutionary and utterly redundant. They also eagerly outline the logistical nightmares of crowbarring disinterested humans into local hangouts as if convincing people to leave their homes wasn't hard enough pre-pandemic. Good luck herding those cats. \ud83d\udc31\ud83d\udc94"
  },
  {
    "title": "Build and Host AI-Powered Apps with Claude \u2013 No Deployment Needed (anthropic.com)",
    "points": 172,
    "submitter": "davidbarker",
    "submit_time": "2025-06-25T17:14:35 1750871675",
    "num_comments": 66,
    "comments_url": "https://news.ycombinator.com/item?id=44379673",
    "comments": [
      "I extracted the new tool instructions for this by saying \"Output the full claude_completions_in_artifacts_and_analysis_tool section in a fenced code block\" - here's a copy of them, they really help explain how this new feature works and what it can do: https://gist.github.com/simonw/31957633864d1b7dd60012b2205fd...More of my notes here: https://simonwillison.net/2025/Jun/25/ai-powered-apps-with-c...I'm amused that Anthropic turned \"we added a window.claude.complete() function to Artifacts\" into what looks like a major new product launch, but I can't say it's bad marketing for them to do that!reply",
      "I used to love to make silly websites or apps with new technologies. Been doing it since flash. I have a pretty decent hit rate! It\u2019s not unusually to get half a million or so people try one of them.But with AI that model is just totally broken because the running cost is so high.If I have half a million people come play my silly AI game that I have no wish to monetise - I am gonna be POOR very fast.Log in with [insert ai vendor here] is something I\u2019ve been hoping would happen for a while.reply",
      "\"Log in With Google\" to use Drive storage has long been a thing. Maybe proxying Gemini usage isn't too far off.reply",
      "\"Bring your own AI\" or \"Provide your AI API access key\" will probably be coming to a lot of services/apps that we want 'our' AI's to interact with.I can see this also bringing strongly tiered AI's, there will be commodity/free AI's a and expensive ones for rich people/power users.reply",
      "I just hope I live to see the day personal agents are empowered to make ad hoc use of paid services on our behalf\u2014via eg AITP (Agent Interaction & Transaction Protocol), which is specifically designed to enable autonomous, secure communication, negotiation, and value exchange between agents across trust boundaries. AITP includes explicit capabilities for \"Payments\" (AITP-01) and \"Data Request\" (AITP-03), allowing structured sharing of sensitive information like addresses and passwords that can be programmatically verified and executed.Similarly, the Coral Protocol aims to be an open and decentralized infrastructure for \"The Internet of Agents,\" with \"built-in economic transactions\" at its core. This means agents can be compensated for their contributions via on-chain micropaymentsOh, damn, no, that sounds like an expense-tracking nightmare. Budgeting becomes the principal executive input.reply",
      "Snowflake does revenue sharing, it\u2019s possible that AI providers can start doing that too.reply",
      "Agreed, it's an interesting model. I wonder what the approval ui looks like for the app end-user? Is it super clear to them that they're financially responsible for their usage?reply",
      "Yeah I wonder how that actually works - because I would guess people are logging  in with their consumer login not an api login, so they\u2019re not really even in the mindset of limits and cost per token.reply",
      "Precisely. You click on a claude link, and suddenly it's, \"You are now financially responsible for your actions from here on...\" I'm sure they've spent a lot of time thinking through the ui/ux of this.reply",
      "Users of Claude-hosted apps can't thereby incur financial liability, because it counts against the usage limit of their consumer Claude plan, which either is free or has a fixed monthly subscription price. The worst that can happen is that they run out of quota and can't use Claude anymore until it resets, which happens every day on free plans and every five hours on paid ones. In no case is usage attributed to an API key with metered pricing.reply"
    ],
    "link": "https://www.anthropic.com/news/claude-powered-artifacts",
    "first_paragraph": "",
    "summary": "**The Future is Now and It's Expensive**: Anthropic decides that appending \".complete()\" to a line of JavaScript warrants the pomp and circumstance of a royal wedding. Internet hobbyists across the globe mourn the death of spontaneous innovation for casual fun, as the skyrocketing costs of running AI-driven applications ensure that only the most affluent nerds can dominate this new playground. Commenters, in a fit of technobabble and fear, digress into visions of a dystopia where their personal AI spends their money on whims without them, because *emerging autonomous protocols* sound like a blend between a tax audit and an episode of Black Mirror. Somewhere, a startup founder is already pitching this as a Series A funding round. \ud83e\udd16\ud83d\udcb8"
  },
  {
    "title": "Libxml2's \"no security embargoes\" policy (lwn.net)",
    "points": 123,
    "submitter": "jwilk",
    "submit_time": "2025-06-25T19:36:55 1750880215",
    "num_comments": 80,
    "comments_url": "https://news.ycombinator.com/item?id=44381093",
    "comments": [
      "A lot of these \"security bugs\" are not really \"security bugs\" in the first place. Denial of service is not resulting in people's bank accounts being emptied or nude selfies being spread all over the internet.Things like \"panics on certain content\" like [1] or [2] are \"security bugs\" now. By that standard anything that fixes a potential panic is a \"security bug\". I've probably fixed hundreds if not thousands of \"security bugs\" in my career by that standard.Barely qualifies as a \"security bug\" yet it's rated as \"6.2 Moderate\" and \"7.5 HIGH\". To say nothing of gazillion \"high severity\" \"regular expression DoS\" nonsense and whatnot.And the worst part is all of this makes it so much harder to find actual high-severity issues. It's not harmless spam.[1]: https://github.com/gomarkdown/markdown/security/advisories/G...[2]: https://rustsec.org/advisories/RUSTSEC-2024-0373.htmlreply",
      "> Denial of service is not resulting in ...DoS results in whatever the system happens to do. It may well result in bad things happening, for example stopping AV from scanning new files, breaking rate limiting systems to allow faster scanning, hogging all resources on a shared system for yourself, etc. It's rarely a security issue in isolation, but libraries are never used in isolation.reply",
      "An AV system stopping because of a bug in a library is bad, but that's not because the library has a security bug. It's a security problem because the system itself does security. It would be wild if any bug that leads to a crash or a memory leak was a \"security\" bug because the library might have been used by someone somewhere in a context that has security implications.A bug in a library that does rate limiting arguably is a security issue because the library itself promises to protect against abuse. But if I make a library for running Lua in redis that ends up getting used by a rate limiting package, and my tool crashes when the input contains emoji, that's not a security issue in my library if the rate limiting library allows emails with punycode emoji in them.\"Hogging all of the resources on a shared system\" isn't a security bug, it's just a bug. Maybe an expensive one, but hogging the CPU or filling up a disk doesn't mean the system is insecure, just unavailable.The argument that downtime or runaway resource use due is considered a security issue but only if the problem is in someone else's code is some Big Brained CTO way of passing the buck onto open source software. If it was true, Postgres autovacuuming due to unpleasant default configuration would be up there with Heartbleed.Maybe we need a better way of alerting downstream users of packages when important bugs are fixed. But jamming these into CVEs and giving them severities above 5 is just alert noise and makes it confusing to understand what issues an organization should actually care about and fix. How do I know that the quadratic time regexp in a string formatting library used in my logging code is even going to matter? Is it more important than a bug in the URL parsing code of my linter? It's impossible to say because that responsibility was passed all the way downstream to the end user. Every single person needs to make decisions about what to upgrade and when, which is an outrageous status quo.reply",
      "> An AV system stopping because of a bug in a library is bad, but that's not because the library has a security bug.(And other examples)\nThat's a fallacy of looking for the root cause. The library had an issue, the system had an issue and together they resulted in a problem for you. Some issues will be more likely to result in security problems than others, so we classify them as such. We'll always deal with probabilities here, not clear lines. Otherwise we'll just end up playing a blame game \"sure, this had a memory overflow, but it's package fault for not enabling protections that would downgrade it to a crash\", \"no it's deployments fault for not limiting that exploit to just this users data partition\", \"no it's OS fault for not implementing detailed security policies for every process\", ...reply",
      "DoSing autonomous vehicle brake controls...reply",
      "I hope my brakes aren't parsing xmlreply",
      "Everything is a \"security bug\" in the right (wrong?) context, I suppose.reply",
      "Well, that's sort of the problem.It's true that once upon a time, libxml was a critical path for a lot of applications.  Those days are over.  Protocols like SOAP are almost dead and there's not really a whole lot of new networking applications using XML in any sort of manor.The context where these issues could be security bugs is an ever-vanishing usecase.Now, find a similar bug in zlib or zstd and we could talk about it being an actual security bug.reply",
      "SOAP is used far more than most people realize. I deal extensively in \"cutting edge\" industries that rely heavily on SOAP or SOAP based protocols. Supply chain systems and manufacturing.reply",
      "> A lot of these \"security bugs\" are not really \"security bugs\" in the first place. Denial of service is not resulting in people's bank accounts being emptied or nude selfies being spread all over the internet.That is not true at all. Availability is also critical. If nobody can use bank accounts, bank has no purpose.reply"
    ],
    "link": "https://lwn.net/SubscriberLink/1025971/73f269ad3695186d/",
    "first_paragraph": "\n\n\nWelcome to LWN.net\n\nThe following subscription-only content has been made available to you \nby an LWN subscriber.  Thousands of subscribers depend on LWN for the \nbest news from the Linux and free software communities.  If you enjoy this \narticle, please consider subscribing to LWN.  Thank you\nfor visiting LWN.net!\n\n\n\n\n\n\n           By Joe BrockmeierJune 25, 2025\n           \nLibxml2, an\nXML parser and toolkit, is an almost perfect example of the successes\nand failures of the open-source movement. In the 25 years since its\nfirst release, it has been widely adopted by open-source projects, for\nuse in commercial software, and for government use. It also\nillustrates that while many organizations love using open-source software,\nfar fewer have yet to see value in helping to sustain it. That has led\nlibxml2's current maintainer to reject security embargoes and sparked\na discussion about maintenance terms for free and open-source\nprojects.\nA short libxml2 history\nThe original libxml,\nalso k",
    "summary": "**Libxml2's Security Showdown: \"No Embargoes\"? More Like \"No Support\"**\n\nIn a world where only a few brave souls dare to tread, libxml2 stands tall as *the poster child* of every open-source textbook success and blunder. As it turns out, liberating a piece of code into the wild with a mantel of \"Free for All\" might just magnetize an array of unenthused (and unpaid) participants debating the philosophical nuances of what actually constitutes a \ud83d\udea8\"security bug.\"\ud83d\udea8 As Joe Brockmeier takes us down the memory lane of libxml2, he forgets to remind us that while XML may no longer be the belle of the tech ball, the gaslighting about its security implications continues in comments more tangled than XML itself. The commenters, engaging in high-stakes mental gymnastics, argue tirelessly if a DoS in a library could ever herald the apocalypse, or if it's merely just another bug to swat in the endless summer of open-source festivities. \ud83c\udf89"
  },
  {
    "title": "Better Auth, by a self-taught Ethiopian dev, raises $5M from Peak XV, YC (techcrunch.com)",
    "points": 61,
    "submitter": "bundie",
    "submit_time": "2025-06-25T18:07:02 1750874822",
    "num_comments": 30,
    "comments_url": "https://news.ycombinator.com/item?id=44380185",
    "comments": [
      "So pumped for Bereket. Better Auth is awesome.I am also interested on how they plan to monetise it. I love the library and the success story but hope that the weight of this VC money doesn\u2019t impact its awesomenessreply",
      "I think they\u2019re rolling out their own managed auth service, may have already done so actually.reply",
      "Pretty sure auth is not something I want a self-taught dev (or even most CS-graduate devs) writing.Oauth2, JWT's, hashes, timestamps, validations, and such, are all totally simple until they're not. The black hats have way more experience and way more time invested in this space than most any normal dev.reply",
      "I learnt to program (in a very basic way) before doing the whole paper qualification thing. Am I self taught? Is that some kind of signifying badge one loses once one gets a 'proper' education? I also know many people _with_ the paper qualification I wouldn't necessarily trustRhetorical questions of course as we all know it's a clickbait title, but perhaps it would be nice for this label to stop being thrown around like it has any real consistent meaning or significance?reply",
      "It's funny, we've watched for two decades as the click-driven dynamics of the internet have degraded the meanings of words. At first, I was outraged on a daily basis. Then, as we all did, I learned, against my will, to forgive. \"Can't blame them for chasing clicks! Who among us wouldn't cheapen a word if it meant a view?\"But - and this is the funny part - I feel like my teen-angsty self has been vindicated. I'm so burnt out on exaggeration, not a single news site has gotten regular clicks from me in over a decade, nor do I comment or read comments. I listen to a little history dork YouTube before bed, or for tutorials. I'm free.reply",
      "> The black hats have way more experience and way more time invested in this space than most any normal dev.Surely the black hats you refer to are themselves self-taught? They didn't find a school that would teach them about crime, right? In that case it seems like self-taught can be good enough.reply",
      "Black hats have to be right once, white hats have to be right every time.They can spray and pray, you have to write proofs.reply",
      "Auth is really not difficult to write. It's don't roll your own crypto, not don't roll your own auth. People need to stop spreading this fud.reply",
      "What? No!There are plethora of mistakes one can make in implementing AuthN/AuthZ, and many of them almost immediately will lead to either the direct leak of PII or can form the start of a chain of exploits.Storing password hashes in an inappropriate manner -> BOOM, all your user's passwords are reversible and can be used on other websitesNot validating a nonce correctly -> BOOM, your user's auth tokens can be re-used/hijackedNot validating a session timestamps correctly -> BOOM, your outdated tokens can be used to gain the users PIIreply",
      "None of those things are difficult to do correctly.reply"
    ],
    "link": "https://techcrunch.com/2025/06/25/this-self-taught-ethiopian-dev-built-an-authentication-tool-and-got-into-yc/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nIt\u2019s rare to see a solo founder building a widely adopted developer infrastructure tool. Even more so if the founder happens to be from Africa. Bereket Engida, a self-taught programmer from Ethiopia, is quietly building what some developers say is the best authentication tool they\u2019ve ever used.Engida\u2019s startup, Better Auth, offers an open source framework that ",
    "summary": "In a heartwarming tale of underdog triumph, an Ethiopian solo founder miraculously convinces Silicon Valley to throw $5 million at a project dubbed \"Better Auth\", presumably because <em>Mediocre Auth</em> was taken. Bereket Engida, having learned programming through sheer will and not the hallowed halls of Stanford, somehow creates what is deemed by the tech-elite as the authentication Messiah Tool. Commenters oscillate between blind admiration and prophetic doom-speak about the perils of self-taught developers touching anything more complex than a \"Hello World\" in Python. Meanwhile, the collective tech wisdom obsesses over monetization strategies and privacy pitfalls, because, of course, no startup romance is complete without selling out before proving your model. \ud83d\ude44"
  },
  {
    "title": "Getting ready to issue IP address certificates (letsencrypt.org)",
    "points": 211,
    "submitter": "Bogdanp",
    "submit_time": "2025-06-25T16:21:45 1750868505",
    "num_comments": 120,
    "comments_url": "https://news.ycombinator.com/item?id=44379034",
    "comments": [
      "So all the addressing bodies (e.g., ISPs and cloud providers) are on board then right?  They sometimes cycle through IP's with great velocity.  Faster than 6 days at least.Lots of sport here, unless perhaps they cool off IPs before reallocating, or perhaps query and revoke any certs before reusing the IP?If the addressing bodies are not on board then it's a user responsibility to validate the host header and reject unwanted IP address based connections until any legacy certs are gone / or revoke any legacy certs.  Or just wait to use your shiny new IP?I wonder how many IP certs you could get for how much money with the different cloud providers.reply",
      ">So all the addressing bodies (e.g., ISPs and cloud providers) are on board then right? They sometimes cycle through IP's with great velocity. Faster than 6 days at least.>Lots of sport here, unless perhaps they cool off IPs before reallocating, or perhaps query and revoke any certs before reusing the IP?I don't see how this is any different than custom/vanity domains you can get from cloud providers. For instance on azure you can assign a DNS name to your VMs in the form of myapp.westus.cloudapp.azure.com, and CAs will happily issue certificates for it[1]. There's no cooloff for those domains either, so theoretically someone else can snatch the domain from you if your VM gets decommissioned.[1] https://crt.sh/?identity=westus.cloudapp.azure.com&exclude=e...reply",
      "There is in fact weird cool off times for these cloud resources. I\u2019m less familiar with AWS, but I know in azure once you delete/release one of these subdomains, it remains tied to your organization/tenant for 60 or 90 days.You can reclaim it during that time, but any other tenant/organization will get an error that the name is in use. You can ping support to help you there if you show them you own both organizations. I was doing a migration of some resources across organizations and ran into that issuereply",
      "AWS sets CAA records for their domains and thus you can\u2019t issue certs for themreply",
      "> So all the addressing bodies (e.g., ISPs and cloud providers) are on board then right?My guess is that it's going to be approached the other way around. After all, it's not the ISPs' job to issue IP addresses in conformance with TLS; it's a TLS provider's job to \"validate identity\" \u2014 i.e. to issue TLS certificates in conformance with how the rest of the ecosystem attaches identity to varyingly-ephemeral resources (IPs, FQDNs, etc.)The post doesn't say how they're going to approach this one way or the other, but my intuition is that LetsEncrypt is going to have/maintain some gradually-growing whitelist for long-lived IP-issuer ASNs \u2014 and then will only issue certs for IPs that exist under those ASNs; and invalidate IP certs if their IP is ever sold to another ASN not on the list. This list would likely be a public database akin to Mozilla's Public Suffix List, that LetsEncrypt would expect to share (and possibly co-maintain) with other ACME issuers that want to do IP issuance.reply",
      "Iirc AWS Application Load Balancers (HTTP/L7) will cycle through IPs as fast as every 30 minutes (based on me tracking them ~5 years ago). I think they set a 10 minute ttl on their DNS recordsreply",
      "Even less - 60 second TTL.reply",
      "You can renew your HTTPS certificate for 90 days the day before your domain expires. Your CA can't see if the credit card attached to your auto renewal has hit its limit or not.I don't think the people using IP certificates will be the same people that abandon their IP address after a week. The most useful thing I can think of is either some very weird legacy software, or Encrypted Client Hello/Encrypted SNI support without needing a shared IP like with Cloudflare. The former won't drop IPs on a whim, the latter wouldn't succeed in setting up a connection to the real domain.reply",
      "> I wonder how many IP certs you could get for how much money with the different cloud providers.I wonder if they'll offer wildcard certs at some point.reply",
      "> So all the addressing bodies (e.g., ISPs and cloud providers) are on board then right? They sometimes cycle through IP's with great velocity. Faster than 6 days at least.... or put multiple customers on the same IP address at the same time. But presumably they wouldn't be willing to complete the dance necessary to actually get certs for addresses they were using that way.Just in case, though, it's probably a good idea to audit basically all software everywhere to make sure that it will not use IP-based SANs to validate connections, even if somebody does, say, embed a raw IP address in a URL.This stuff is just insanity.reply"
    ],
    "link": "https://community.letsencrypt.org/t/getting-ready-to-issue-ip-address-certificates/238777",
    "first_paragraph": "We're almost ready to issue certificates for IP address SANs from Let's Encrypt's production environment. They'll only be available under the shortlived profile (which has a 6-day validity period), and that profile will remain allowlist-only for a while.Please note: We have more work to do before we're ready to launch this feature for the public. We don't yet have a timeline, and aren't ready to accept allowlist requests.Here's a sample staging certificate, and a site using it:Please speak up if you see anything interesting, weird, or wrong!I think I already found a bug in Firefox's display of IP address SANs; BZ #1973855.https://[2602:ff3a:1:abad:c0f:fee:abad:cafe]/Lol, also a bug in Discourse I guess? The URL is blue, but it's just a <a>...</a> without the href= part et cetera.. So an anchor, yes, but hyperlink? No..Anyway, gotta get myself on that shortlived profile somehow How can I make my browser NOT follow the 302 redirect? Whilst being funny, the whole Starbucks thing, I cannot",
    "summary": "**Techno-Masochism Meets IP Addresses: A Tragicomedy**\n\nThe wizards at Let's Encrypt have decided that the internet doesn't already resemble a dumpster fire enough, so they are prepping to issue certificates for IP address SANs with a laughable six-day validity. Truly, a lifetime in internet years! The dynamic readership\u2014bless their ever-confused hearts\u2014dives into a whirlwind, debating whether ISPs would join this frenzied dance, swapping IPs faster than a gambler tossing dice. Mysteriously absent, of course, is any real solution discussed amidst their frantic speculations on IP lifecycle management... because why solve practical issues when one can rant about potential ones? \ud83c\udf0d\ud83d\udd25\ud83d\udcbb"
  },
  {
    "title": "Earths largest camera:3B pixel images (nytimes.com)",
    "points": 10,
    "submitter": "wglb",
    "submit_time": "2025-06-22T14:53:59 1750604039",
    "num_comments": 3,
    "comments_url": "https://news.ycombinator.com/item?id=44347399",
    "comments": [
      "https://archive.ph/2025.06.19-151703/https://www.nytimes.com...reply",
      "Archive.ph hasn't been working for at least a week or so: it never loads.reply",
      "Never mind, it's just me. Maybe my ISP blocks the site.reply"
    ],
    "link": "https://www.nytimes.com/interactive/2025/06/19/science/rubin-observatory-camera.html",
    "first_paragraph": " By Jonathan Corum and Kenneth Chang Photographs by Marcos ZegersAt the heart of the new Vera C. Rubin Observatory in Chile is the world\u2019s largest digital camera. About the size of a small car, it will create an unparalleled map of the night sky.The observatory\u2019s first public images of the sky are expected to be released on June 23. Here\u2019s how its camera works.Sensor3 inchesRAFTDeadsegmentNoisysegmentDamaged sensorRelative sizeof the moonThe camera is visible in a clean room before installation.Its focal plane is just over two feet wide: a grid of sensors sealed in a vacuum and supercooled to minus 148 Fahrenheit to prevent grainy or speckled images.Each sensor is about 1.6 inches wide and holds over 16 million pixels.The sensors are grouped into 21 rafts of nine sensors each. Each raft contains over 144 million pixels.Corner rafts detect guide stars and help the telescope correct for optical distortions and temperature changes.In a camera with 3.2 billion pixels, there will be defects",
    "summary": "Title: Hobbyists Gape at Billion-Pixel Behemoth\n\nAt last, the wizards of pixelcraft have enthroned their new king, a camera the size of a Prius, because when it comes to digital photography, size matters more than utility. The Vera C. Rubin Observatory flexes its photographic muscles by promising pictures of space pebbles in unprecedented, yet mostly unnecessary, detail. Meanwhile, the comment section morphs into a tragicomic display of technical ignorance, where someone inevitably struggles to link to the archived discussion, probably blaming cosmic rays for their internet issues. Witness the birth of a new era where amateur astronomers and internet moaners unite under the banner of \"bigger is better,\" heedless of whether it's pixels or problems."
  },
  {
    "title": "Writing a basic Linux device driver when you know nothing about Linux drivers (crescentro.se)",
    "points": 159,
    "submitter": "sbt567",
    "submit_time": "2025-06-22T10:29:42 1750588182",
    "num_comments": 14,
    "comments_url": "https://news.ycombinator.com/item?id=44345681",
    "comments": [
      "> I also thought I\u2019d message the vendor and ask them if they could share any specifications or docs regarding their protocol. To my surprise, Nanoleaf tech support responded to me within 4 hours, with a full description of the protocol that\u2019s used both by the Desk Dock as well as their RGB strips.How cool is that? Too many vendors still think that they have valuable intellectual property in such relative trivialities. And that handing out the specs freely helps their competitors more than themselves.reply",
      "Yeah that part of the article put a big smile on my face.I did the same thing back in college, when I was in a lab. We wanted to do some research on Wi-Fi signals, and I happened to own a bunch of Wi-Fi adaptors produced by SomeSmallTech Co. Ltd., which featured relatively new Atheros chips and didn't have Linux drivers at the time.So I sent an email to the company's public email address, asking for some datasheets, \"for science\". To my disappointment, presumably a PR person replied that they \"don't have a company policy to collaborate with academic research\". (But they did send a quick reply, kudos to that.)Funnily enough, years later I ended up working for said company. Naturally, when I first logged into the company network, I searched for the datasheets I asked for. There were \"classified\" watermarks all over the PDFs :)reply",
      "I almost had that experience with one of the popular PC liquid cooling hardware vendors around 10 years ago.I emailed them saying I'd be interested in developing drivers for their hardware for Linux as I was a happy customer and was immediately put in touch with one of the managers and their engineering team.Made quite a bit of progress before the whole thing was shut down because one of their component vendors threatened them saying it'd be a breach of their contract with them.Apparently that vendor sold a \"datacenter\" (non consumer) version of that hardware for which they charged a hefty license fee for the management software (which was Linux compatible).Jokes on them, someone reverse engineered the whole thing with a USB analyzer years later and published it XD. (not me)reply",
      "I had the same reaction. Nano leaf is extremely cool for that.reply",
      "This has to put them in the top 0.01% of companies that make consumer electronics.I can think of only a few companies that bother to publish any details... And most of them are focused on industrial customers where it isn't unreasonable to need certain protocol details for integration or even just compliance with certain regulatory systems.Maybe things are changing?I have noticed that some of the LED light controllers you see on AliExpress are leaning in to open firmware standards. 5 years ago, you bought the controller and had to flash your own firmware. Now, there's an option at checkout to select an open source firmware. Some even have a USB port built in for flashing!reply",
      "It\u2019s a userspace USB HID driver in rust, which is honestly more interesting/applicable to me than a kernel driver, which is what I thought it meant from the title.reply",
      "I enjoyed this post, but I'm eager to hear what the next step would be for a real \"production\" userspace driver. Are these typically just daemons that are configured to run at start up? And then some configuration GUI communicates with it over a socket or something?reply",
      "I really enjoyed the way this post was written, i.e. it includes the code, how it was run, the false paths, etc. You almost get to live through the author's journey and how he figured out just enough to get something working.reply",
      "> Let\u2019s run it again to make sure it was not a fluke!I understood that referencereply",
      "I wish this was done in C so I didn't have to learn Rust.  But maybe it is time to learn Rust.reply"
    ],
    "link": "https://crescentro.se/posts/writing-drivers/",
    "first_paragraph": "",
    "summary": "**Another Hero Battles the Linux Driver Dungeon**\n\nAn intrepid coder with the audacity of an over-caffeinated raccoon decides to take on the Herculean task of writing a Linux device driver without knowing a thing about it. \ud83e\udd2f Commenters quickly transform into an unpaid support group, sharing war stories about pleading with uncooperative vendors to unlock the sacred \"specs\" of their products. Meanwhile, one commenter briefly turns into a cheerleader for a company that actually responded in a helpful manner, setting the bar shockingly low for customer service expectations. This turns into a geek version of storytime, where every coder blooms into an amateur lawyer or reverse-engineer overnight, because asking for permission is clearly too mainstream. \ud83e\udd37\u200d\u2642\ufe0f"
  },
  {
    "title": "LM Studio is now an MCP Host (lmstudio.ai)",
    "points": 146,
    "submitter": "yags",
    "submit_time": "2025-06-25T17:27:35 1750872455",
    "num_comments": 59,
    "comments_url": "https://news.ycombinator.com/item?id=44379792",
    "comments": [
      "Just ordered a $12k mac studio w/ 512GB of integrated RAM.Can't wait for it to arrive and crank up LM Studio. It's literally the first install. I'm going to download it with safari.LM Studio is newish, and it's not a perfect interface yet, but it's fantastic at what it does which is bring local LLMs to the masses w/o them having to know much.There is another project that people should be aware of:\nhttps://github.com/exo-explore/exoExo is this radically cool tool that automatically clusters all hosts on your network running Exo and uses their combined GPUs for increased throughput.Like HPC environments, you are going to need ultra fast interconnects, but it's just IP based.reply",
      "I love LM studio but I\u2019d never waste 12k like that. The memory bandwidth is too low trust me.Get the RTX Pro 6000 for 8.5k with double the bandwidth. It will be way betterreply",
      "(Replying to both siblings questioning this)If the primary use case is input heavy, which is true of agentic tools, there\u2019s a world where partial GPU offload with many channels of DDR5 system RAM leads to an overall better experience.  A good GPU will process input many times faster, and with good RAM you might end up with decent output speed still.  Seems like that would come in close to $12k?And there would be no competition for models that do fit entirely inside that VRAM, for example Qwen3 32B.reply",
      "Why would they pay 2/3 of the price for something with 1/5 of ram?The whole point of spending that much money for them is to run massive models, like the full R1, which the Pro 6000 cantreply",
      "You can't run deepseek-v3/r1 on the RTX Pro 6000, not to mention the upcomming 1 million context qwen models, or the current qwen3-235b.reply",
      "I'm using it on MacBook Air M1 / 8 GB RAM with Qwen3-4B to generate summaries and tags for my vibe-coded Bloomberg Terminal-style RSS reader :-) It works fine (the laptop gets hot and slow, but fine).Probably should just use llama.cpp server/ollama and not waste a gig of memory on Electron, but I like GUIs.reply",
      "8 GB of RAM with local LLMs in general is iffy: a 8-bit quantized Qwen3-4B is 4.2GB on disk and likely more in memory. 16 GB is usually the minimum to be able to run decent models without compromising on heavy quantization.reply",
      "But 8GB of Apple RAM is 16GB of normal RAM.https://www.pcgamer.com/apple-vp-says-8gb-ram-on-a-macbook-p...reply",
      "I'd love to host my own LLMs but I keep getting held back from the quality and affordability of Cloud LLMs. Why go local unless there's private data involved?reply",
      "Offline is another use case.reply"
    ],
    "link": "https://lmstudio.ai/blog/lmstudio-v0.3.17",
    "first_paragraph": "",
    "summary": "**LM Studio: The Costly Frontier of Local Machine Learning Wizards**\n\nIn an exciting turn of events, LM Studio is now hosted on MCP, sparking joyous applause from all three individuals who think spending the GDP of a small island nation on a computer to run beta software is a reasonable idea. One soon-to-be-broke enthusiast can\u2019t wait to use their new Mac Studio with more RAM than a sheep farm to run local LLMs, because-- let\u2019s face it-- nothing screams \"cutting-edge tech\" like downloading new software with Safari. Meanwhile, in the comment cesspool, a wild debate erupts over whether it\u2019s wiser to throw $12k at a setup better suited for simulating black holes or just sticking to an 8GB MacBook Air that moonlights as a space heater. \ud83d\ude80\ud83d\udd25 As tech elitism clashes with everyday logic, the usual suspects conclude that, indeed, throwing excessive money at hardware to run oversized models inefficiently in a living room setup is the pinnacle of technological innovation. \ud83e\udd11"
  },
  {
    "title": "Ambient Garden (ambient.garden)",
    "points": 25,
    "submitter": "fipar",
    "submit_time": "2025-06-23T17:38:08 1750700288",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=44358148",
    "comments": [
      "Reminds me of Tres Lunas and Maestro, created by British musician Mike Oldfield (more famous for \"Tubular Bells\"), in 2002 and 2004, making them very ahead of their time.https://en.wikipedia.org/wiki/Tres_Lunas#Computer_gamehttps://en.wikipedia.org/wiki/Maestro_(video_game)reply",
      "Beautiful. I really like how the static sky looks.reply"
    ],
    "link": "https://ambient.garden",
    "first_paragraph": "Navigation:",
    "summary": "Today in the rarefied world of <em>nostalgic tech</em> aficionados, a groundbreaking blog post unveils Ambient Garden (ambient.garden), inviting the horde of three and a half Mike Oldfield superfans to reminisce about software nobody else remembers. Comments quickly devolve into an arms race of obscurity, with one user proudly parading their affection for low-res celestial JPEGs as if graphic design has stood still since 2002. Someone mentions Maestro like it's the second coming of Crysis, proving once more that nostalgia is indeed a drug best served pixelated. Who knew that ancient CD-ROM bonus features could evoke such passion in 2023? \ud83c\udfb5\ud83d\udcbe"
  },
  {
    "title": "Iroh: A library to establish direct connection between peers (github.com/n0-computer)",
    "points": 141,
    "submitter": "gasull",
    "submit_time": "2025-06-25T16:34:51 1750869291",
    "num_comments": 38,
    "comments_url": "https://news.ycombinator.com/item?id=44379173",
    "comments": [
      "I work on connet [1] and from what I've seen iroh seem pretty cool. A few random thoughts I had while watching the presentations/reading the docs:* the relays serve both for discovery and relay. In connet these are separate responsibilities, e.g. it has control server for discovery and a relay server for relaying connections.* it seems that the connections to the relays in iroh are TCP (at least what was said in one of the videos), while connet uses QUIC in all cases. This probably makes iroh more resilient, but multiplexing on top of TCP might suffer from head of line blocking.* it is pretty cool that iroh can seamlessly upgrade from relay to direct connection, connet doesn't do that on a connection level. It will however use direct in the subsequent virtual connections.* using ALPNs for protocol selection is pretty cool, connet only offers \"virtual connections\" protocol, where one of the peers is \"server\" and the other is a \"client\".* since there is a separate discovery server (with auth), in connet the endpoints are named separately with logical names, they don't represent peers necessarily. Because of this, you can have multiple peers with \"server\" role and \"client\" roles.Anyhow, thanks for posting this, iroh looks great and I will draw some inspiration from it for sure.[1] https://github.com/connet-dev/connetreply",
      "I attended a workshop for iroh a while ago and really enjoyed it, and from what I can tell on the Discord server the folks developing it are gearing for a 1.0 release soon-ish.There's also Dumb Pipe and SendMe which are demos (I believe) built on iroh to showcase some of its uses, and at the workshop we were shown a video of a startup using iroh for video game streaming (something similar to the old OnLive).From what I understood (in spite of my lack of networking knowledge) and if I remember correctly clients have to be on the same relay (I think there's one for Europe and one for North America) and they use the Bittorent DHT Mainline (I had to google the iroh blog post about it because I forgot the exact name) for discovery. There was some stuff about BGP too, but it went over my head sadly.I hope somebody more knowledgeable chimes in because iroh is really exciting, I feel like I could throw together a p2p application and it wouldn't be a daunting task due to it.reply",
      "(disclosure: I work on iroh): you're selling yourself short! All of this is accurate, except for maybe the BGP stuff :)Dumb Pipe & Sendme me are indeed demos, we do provide a set of default, public relays to use for free. The relay code is also open source, and if you want to pay us we can run a network for you.We try to provide a few different options for discovery, the one we think has the most general utility is a custom DNS server, but both local mDNS and Bittorrent Mainline are also pluggable options.reply",
      "Pardon me for jumping in the discussion, but I didn't know where else to ask this. Does Iroh support streaming, instead of moving blobs? I want to write a little p2p tool to forward one port from one machine to another.\nAlso, forwarding UDP packets doesn't require the congestion control of QUIC. Does Iroh allow disabling it for a certain \"message\" or stream?reply",
      "Yes. Iroh itself provides direct QUIC connections. iroh-blobs is a protocol on top of iroh that provides content-addressed data transfer of BLAKE3 hashed data.What you describe sounds like https://www.dumbpipe.dev/ , a tool/demo built on top of iroh to provide a bidirectional pipe across devices, somewhat like netcat.Dumbpipe also has a mode where it listens on a port using TCP.It sounds like you want to basically build dumbpipe for UDP. You can of course use a QUIC stream, but QUIC has an extension, which we support, to send datagrams: https://docs.rs/iroh/latest/iroh/endpoint/struct.Connection....This basically allows you to opt out of QUIC streams, but you still do get TLS encryption.reply",
      "It looks like they have examples with unreliable channels: https://github.com/n0-computer/iroh/tree/main/iroh/examplesYou'll prob have to check the max packet size that you want to forward because quic adds a bit of overhead.reply",
      "Some years ago, \"iroh\" was supposed to a replacement for ipfs.  However since then, they (very smartly, in my opinion) dropped those ambitions and are just focused on being a high-quality library for anyone writing a P2P app (like ipfs).I often see projects attempting to be a universe tool to solve every possible problem, and I think the iroh folks were smart to scale back and narrow their focusreply",
      "appreciate the feedback, it was a hard decision to make, but has felt more right everyday since we made itreply",
      "Iroh is very cool and their YouTube explainers are pretty great: https://youtube.com/@n0computerI just need good FFI now, which is on the roadmap!reply",
      "Can't wait to be able to use it in Go or Python :)reply"
    ],
    "link": "https://github.com/n0-computer/iroh",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        peer-2-peer that just works\n       There was an error while loading. Please reload this page.\nThere was an error while loading. Please reload this page.\n\n\n\n\n\n\nIroh gives you an API for dialing by public key.\nYou say \u201cconnect to that phone\u201d, iroh will find & maintain the fastest connection for you, regardless of where it is.The fastest route is a direct connection, so if necessary, iroh tries to hole-punch.\nShould this fail, it can fall back to an open ecosystem of public relay servers.\nTo ensure these connections are as fast as possible, we continuously measure iroh.Iroh uses Quinn to establish QUIC connections between nodes.\nThis way you get authenticated encryption, concurrent streams with stream priorities, a datagram transport and avoid head-of-line-blocking out of the box.Use pre-existing protocols built on iroh instead of writ",
    "summary": "Title: <em>Iroh: Reinventing the Wheel with More Steps</em>\n\nThe open-source refuge, GitHub, is thrilled to host yet another peer-to-peer connectivity solution: <em>Iroh</em>. In an adorable attempt to reinvent digital wheel-to-wheel communication, Iroh vows to <strong>connect devices</strong> as long as you\u2019re ready to scale the ever-growing mountain of \u2018simple\u2019 documentation and inevitable loading errors. Meanwhile, the comment section transforms into a humble-brag derby as developers from parallel projects toss around buzzwords like \u201cQUIC\u201d, \u201cTCP\u201d, and \u201cstream priorities\u201d, all while subtly dismissing each other's approaches under the guise of mutual admiration. \ud83d\ude80 Who knew establishing a basic connection could sound so convoluted? Get ready, set, <i>reload!</i>"
  },
  {
    "title": "America\u2019s incarceration rate is in decline (theatlantic.com)",
    "points": 82,
    "submitter": "paulpauper",
    "submit_time": "2025-06-25T17:14:29 1750871669",
    "num_comments": 170,
    "comments_url": "https://news.ycombinator.com/item?id=44379670",
    "comments": [
      "https://archive.is/pe7eH",
      ">Rapidly declining numbers of youth are committing crimes, getting arrested, and being incarcerated. This matters because young offenders are the raw material that feeds the prison system: As one generation ages out, another takes its place on the same horrid journey.Another factor which will soon impact this, if it isn't already, is the rapidly changing nature of youth. Fertility rates have been dropping since 2009 or so. Average age of parents is increasing. Teen pregnancy on a long and rapid decline.All of these working together means that each year the act of having a child is much more deliberate and the parents likely having more resources. Which in turn should mean fewer youth delinquency, which as the article notes is how most in prison started out.reply",
      "It's lead.Lead concentration in America \"rapidly increased in the 1950s and then declined in the 1980s\" [1]. There is a non-linear discontinuity among kids born in the mid 80s, with linear improvements through to those born in the late 2000s [2].Arrest rates for violent crimes are highest from 15 to 29 years old (particularly 17 to 23-year olds) [3]. They're particularly low for adults after 50 years old.We're around 40 years from the last of the high-lead children. 17 years ago is the late 2000s.[1] https://www.sciencedirect.com/science/article/abs/pii/S10406...[2] https://ehp.niehs.nih.gov/doi/10.1289/EHP7932[3] https://kagi.com/assistant/d2c6fdd5-73dd-4952-ae40-1f36aef1e...reply",
      "It is insane to just confidently assert that the only factor in the decrease in crime is Lead. Treating an insanely nuanced issue as an absolute doesn't make your argument more compelling, it is actually kind of baffling.reply",
      "But it's so satisfying to one's ego that a single cause is the issue. All complexity of societal changes in the last 50 years can be outmanuevered. Simplification is sexy.reply",
      "How much research have you done into this?Rejecting a single variable explanation stated with confidence is a good heuristic, but sometimes a single variable really does have an overwhelming impact on the outcome.This is the best article I've seen on the subject: https://www.motherjones.com/kevin-drum/2018/02/an-updated-le...Lead is a powerful neurotoxin. We know that it impairs brain development in children, particularly in areas related to aggression, impulse control, and executive function. We know that it decreases IQ. This is an awful combination of effects. When these children reach their late teens and 20's, they're more prone to committing violent crimes.MRI scans have shown that adults who were exposed to lead as children have physical differences in their brains, particularly a loss of gray matter in areas responsible for self-control.Long term studies have followed groups of children into adulthood showing children with higher levels of lead exposure were significantly more likely to be arrested for violent crimes later in life.This is the most convincing point to me: Dozens of studies have shown a strong correlation between a population's exposure to lead and its violent crime rate about 20 years later. This holds true across neighborhoods, cities, states, countries. The rise and fall of leaded gasoline usage in the 20th century is followed, with a roughly 20 year lag, by an almost identical rise and fall in violent crime rates. In other words, across different cultures, regulatory environments, genetic populations, time periods, etc etc, when an area regulated away the use of leaded gasoline, 20 years later there was a remarkable fall in violent crime rates for that area which matches the increase which appeared when leaded gasoline was introduced.Here are some more sources I could find:https://anthonychigney.github.io/home/assets/images/LeadCrim...https://pmc.ncbi.nlm.nih.gov/articles/PMC10393136/https://cdn.serc.carleton.edu/files/integrate/teaching_mater...https://www.biologicaldiversity.org/campaigns/get_the_lead_o...",
      "There was a crime decline in many rich countries from the 1990s as well.https://en.wikipedia.org/wiki/Crime_drop#Decline_since_the_e...Maybe they were doing similar things with lead or something else is a big factor. Perhaps the rise of ever more cheap entertainment for young males who are most likely to commit crime. That's a global thing.reply",
      "Why bother stopping at crime rates with that confidence?The 1st recorded cases of fatty liver disease and T2D in children were in the 1980\u2019s are have continued growing since - lead must have been protecting  children\u2019s health.Testosterone has been on a sharp decline during this same time period - lead must promote healthy testosterone production.Debt of all kinds, from the national debt, to household debt, to student loans debt has increased exponentially and consistently with lead removal - lead must promote financial literacy.reply",
      "What exactly are you claiming?Your points say old people have more lead, but then you say young people are more violent. That doesn't square with the articles point that incarceration rates are falling.reply",
      "I think lead is nasty stuff, but if it was the single cause of high crime, surely we'd see a similar effect in other domains, like a rebound effect on IQs (another thing lead was blamed for)?Instead the Flynn Effect seems to have been strongest during the era of high lead, and it's tailing-off now.reply"
    ],
    "link": "https://www.theatlantic.com/ideas/archive/2025/06/prisoner-populations-are-plummeting/683310/",
    "first_paragraph": "",
    "summary": "In a staggering display of journalistic innovation, <i>The Atlantic</i> publishes an article with the breathtaking conclusion that fewer kids being bad equals fewer adults in prison. \ud83d\udd75\ufe0f\u200d\u2642\ufe0f Enthralling! Meanwhile, the comment section transforms into a pseudo-scientific battle royale, as armchair criminologists argue feverishly whether lead poisoning is the real villain or a sensationalized scapegoat. One brave soul suggests that maybe, just maybe, complex social issues can't be boiled down to a single environmental factor, but is quickly drowned out by the lead-is-everything brigade. Who knew environmental toxicology could be so <em>divisive</em>?"
  },
  {
    "title": "Web Embeddable Common Lisp (turtleware.eu)",
    "points": 99,
    "submitter": "todsacerdoti",
    "submit_time": "2025-06-25T15:33:20 1750865600",
    "num_comments": 33,
    "comments_url": "https://news.ycombinator.com/item?id=44378486",
    "comments": [
      "This is just a test page. Today I've shared an information about an accepted grant proposal:https://functional.cafe/@jackdaniel/114742776265318353The work will go towards improving browser integration and porting to WASI.reply",
      "Will this include some form of SLIME support?reply",
      "I didn't plan explicitly for SLIME, but loading swank shouldn't be much of a problem. The missing piece would be a bridge between a websocket and tcp I think.reply",
      "I recently shared a project I did using Hoot, by Spritely institute.\nIt's Guile Scheme compiling to WASM, and it works really well!\nSee https://spritely.institute/hoot/Latest on my project, in case you want to try it out:\nhttps://deosjr.github.io/dynamicland/whiskers.htmlreply",
      "Related:The Gambit Scheme REPL that comes with a tutorial, supports threads in the browser and has a JS FFI: https://try.gambitscheme.orgGambit in emacs in the browser: https://feeley.github.io/gambit-in-emacs-in-the-browser/reply",
      "This is another comment of the author about maxima in the browser from four months ago: I'm experimenting with WASI and the GC extension for WASM, but that's months from today if we speak about complete port (given my time capacity at the moment). Don't know if the gc extension is used in this example.reply",
      "Btw, eval (ed \"wecl.lisp\") to see some interesting function definitions, like canvas or webgl access drafts.reply",
      "I was going to ask if maxima (a symbolic computation system) can be implemented in the browser, but it was answered four months ago [1][1] https://news.ycombinator.com/item?id=42853528reply",
      "Do you use maxima? Would you say it's still worth using and learning?reply",
      "It depends of what you need, but for example for calculus is a nice program. There is also sympy and Wolfram Mathematica.  For symbolic computation I think that Mathematica is the strongest then maxima and then sympy, but sympy is based on python and I think it will get stronger.  If you need numerical computation then there is octave or matlab or julia.reply"
    ],
    "link": "https://turtleware.eu/static/paste/wecl-test-gl/main.html",
    "first_paragraph": "",
    "summary": "**Lispers Rejoice and Reinvent the Wheel\u2026 Again!**\n\nEver felt the inexplicable urge to run Common Lisp in your browser? You're in luck\u2014three whole people on the internet share your fervor, as evidenced by the breathtaking discussion under a blog post that no one else read. The blog post celebrates a grant that will shove Common Lisp, presumably kicking and screaming, into browser compatibility and WASI (whatever that is), igniting fiery debates about essential features like SLIME support. Meanwhile, commenters engage in an awe-inspiringly niche pissing contest about which obscure Scheme variant compiles into WASM most effectively. \ud83c\udf89 Brace yourself for impassioned discussions that manage to simultaneously fly over everyone else\u2019s heads and tunnel deep into the ground, featuring links to projects you'll never look at twice but definitely should bookmark to seem smart."
  }
]