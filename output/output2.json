[
  {
    "title": "Show HN: Clippy \u2013 90s UI for local LLMs (felixrieseberg.github.io)",
    "points": 654,
    "submitter": "felixrieseberg",
    "submit_time": "2025-05-06T15:02:22 1746543742",
    "num_comments": 174,
    "comments_url": "https://news.ycombinator.com/item?id=43905942",
    "comments": [
      "Great idea! I've been humorously referring to chat agents as next gen Clippy because of their chipper, talky default personas which I find insufferably annoying.I'm kind of shocked Microsoft didn't already do this as an alt version of their CoPilot UI. Really a huge miss on their part because I hate the overbearingly intrusive way they keep forcing it into their OS, apps and my fucking laptop keyboard. If they at least acknowledged their behavior and owned it (with a sly wink), I'd hate it a little less. I might even be up for a \"Clippy is my CoPilot\" sticker on my laptop (calling back to the old 80s \"Jesus is my Copilot\" bumper stickers).\n \nreply",
      "> I'm kind of shocked Microsoft didn't already do this as an alt version of their CoPilot UI.Seriously!  This makes me think nobody at Microsoft with the authority to approve something like that has a sense of humor and/or good business sense.  The nostalgia would be enormous.  Hell I'm a linux person now and I'd install Clippy if it supported Fedora\n \nreply",
      "Clippy was a laughing stock and target of derisive comedy for years. It has such bad brand recognition that nobody should be surprised that they aren\u2019t using it.\n \nreply",
      "I think that is what makes it a humor goldmine.Clippy was useless.But attaching a Clippy to a language model? Still nominally useless, but mindfully so!It would be self-deprecating (un-deprecated???) humor for Microsoft, which would take the edge off of the often pushy and tone-deaf corporate look they continually and crassly paint themselves into by default.And actually potentially useful as a branding touchstone: a visual and interface link across otherwise seemingly disparate model interfaces. Clearly delineating and bridging MS AI tools from all the other mixes of tools we are accumulating.They could lean into the \u201cclip\u201d in Clippy with a side app for saving and organizing clippings and logs of notable interactions with any MS model, akin to a notes app. With features for compressing convos into compact topic cheat sheets (with retained sources & convos), lists and other helpful info gathering and leveraging tasks.An ongoing accumulated compressed common core of context for both (hu)man and machine, er \u2026 Clippy.\n \nreply",
      "I sneak Clippy into most of the presentations I give.\n \nreply",
      "But pushy and tone-deaf is what they are. Unless they change their whole corporate structure for this, it\u2019d be equally tone-deaf for someone from their marketing department to pretend that Microsoft is hip and self-aware now. Better to be honest.\n \nreply",
      "I remember Clippy, but I don't remember why it was annoying. I am thinking that Robert Brooke's 3 laws of robotics applies here. (He had written one for AI but I think his thoughts on robotics are more relavent to AI agents).\n \nreply",
      "Do it on April 1. With a \u201cwe told you so\u201d tagline. Have a few 2025/6 templates, like writing a presidential executive order because there are so many of them.\n \nreply",
      "Microsoft Bob would be even funnier, you know it.\n \nreply",
      "It was, but as someone without a dog in the hunt, I loved that ol' Clipmeister.Especially the old 'suicide note' joke image... guess would be called a meme today.\n \nreply"
    ],
    "link": "https://felixrieseberg.github.io/clippy/",
    "first_paragraph": "\n            You might be wondering: What\u2019s the point of all this? Why? Clippy is\n            basically art. Don\u2019t get me wrong, I don\u2019t think that this little\n            app is all that magnificent or something that belongs in a museum, I\n            mean \u201cart\u201d in the sense that I\u2019ve made it like other people do\n            watercolors or pottery - I made it because building it was fun for\n            me. If you get as little as a small chuckle out of it, I\u2019m happy to\n            hear it.\n          \n            You can find me on\n            felixrieseberg.com,\n            Blue.sky, or\n            GitHub.\n          \n            You can find the source code for this app on\n            GitHub.\n          \n            I am so grateful to Microsoft - not only for everything they've done\n            for Electron, but also for giving us one of the most iconic\n            characters and designs of computing history.\n          \n            Clippy lets you run a variety of large language model",
    "summary": "Title: Hacker News Discovers Irony, Finally\n\nSummary: A developer throws back to the 90's by resurrecting Clippy and slapping it onto modern language models, because who doesn't want a dose of nostalgia with their cutting-edge, AI-driven nuisances? In an art project disguised as development, users on Hacker News engage in their favorite pastime: bemoaning Microsoft's lack of nostalgia-exploiting marketing while simultaneously griping about its present intrusiveness. Commenters oscillate between pining for a Clippy-driven AI future and reminiscing about when software annoyances had a friendly paperclip face. Amid groans and half-baked AI ethics, someone suggests Microsoft should mark this fusion of past irritation and future frustration with a \u201c<em>We told you so</em>\u201d tagline, confirming that no one really knows what they want, least of all HN commenters."
  },
  {
    "title": "FTC bans hidden fees for live events and short-term rentals, effective May 12 (techcrunch.com)",
    "points": 154,
    "submitter": "toomuchtodo",
    "submit_time": "2025-05-06T23:41:13 1746574873",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=43910794",
    "comments": [
      "> Live-event tickets include those for concerts, sporting events, music, theater, and other live performances that audiences watch as they occur, but not pre-recorded audio or visual performances.What could be the reason pre-recorded audio/visual performances are excluded? Shouldn't this just be for everything? Why would some types of tickets be required to be truthful about fees and not allowed to lie about the total price in ads, while others are allowed?\n \nreply",
      "I would suggest that a pre-recorded performance is inherently more fungible.If movie theater A is going to charge you a $20 fee when you get to the theater, you can just go to theater B later and see the same show.If you're trying to see the single Chicago date of Tropical Fu Dogs' Coconut Cream tour, well, you either pay the fee or you don't ever see that show.\n \nreply",
      "This works until Theater A signs an exclusivity deal with the publisher of that pre-recorded performance, such that going to Theater B to see the same show is not possible because Theater B is not legally permitted to show it.Granted, such exclusivity deals are rare (because generally publishers want to maximize their revenue rather than artificially constrain it), but not unheard of (for example: films made specifically for IMAX's fancy setups).\n \nreply",
      "Ok, so why is the government endorsing hidden fees for fungible products? There is no situation where this benefits anyone but shareholders.\n \nreply",
      "The best faith thing I can think of here is that these rules are requiring that live events do much more than \"just\" display the full price. For example in ads they _must_ show the all inclusive price.An example of a thing that movie theaters can do that live event shows can no longer do is say \"a ticket costs $14\", while in reality there's a booking fee to cover card payments if you pay online, but you can walk into a movie theater and pay just $14.I think the FTC is saying that while there might be good faith reasons to have booking fees, the industry clearly is using this stuff in bad faith so the industry no longer has this sort of good faith \"out\" to simplify messaging on pricing.\n \nreply",
      "Movie theaters/studios lobbied harder\n \nreply",
      "\"The Commission notes that the harms of bait-and-switch pricing and the misrepresentation of fees and charges are particularly pronounced in industries such as these, in which most transactions occur online. Consumers trying to comparison shop across multiple websites, or even on the same website, when deciding what tickets to purchase or where to travel are unable to do so effectively because some businesses hide the true total price and instead force consumers to go to different sites and click through multiple webpages for each offer to learn the true total price\"\n \nreply",
      "\"The Commission notes that the harms of bait-and-switch pricing and the\nmisrepresentation of fees and charges are particularly pronounced in industries such as these, in which most transactions occur online. Consumers trying to comparison shop across multiple websites, or even on the same website, when deciding what tickets to purchase or where to travel are unable to do so effectively because some businesses hide the true total price and instead force consumers to go to different sites and click through multiple webpages for each offer to learn the true total price\"Where as these are not typical concerns for movie producers and movie theaters which are already operated as a legal cartel.\n \nreply",
      "Movies are fungible and available in masses. Say a big hit name like AC/DC, they're showing up in some entire country once in a decade. The competition to get a ticket for this specific show is insane.A movie however? There's competition both in time (a showing is booked out? fine, I'll just go a week later), in venues (at least in most cities there's at least two) and there's an effective ceiling on price, particularly as consumers are already struggling financially.\n \nreply",
      "Discussion (658 points, 5 months ago, 663 comments) https://news.ycombinator.com/item?id=42445037\n \nreply"
    ],
    "link": "https://techcrunch.com/2025/05/05/ftc-bans-hidden-fees-for-live-events-and-short-term-rentals-effective-may-12/",
    "first_paragraph": "\n\n\t\tLatest\t\n\n\n\t\tAI\t\n\n\n\t\tAmazon\t\n\n\n\t\tApps\t\n\n\n\t\tBiotech & Health\t\n\n\n\t\tClimate\t\n\n\n\t\tCloud Computing\t\n\n\n\t\tCommerce\t\n\n\n\t\tCrypto\t\n\n\n\t\tEnterprise\t\n\n\n\t\tEVs\t\n\n\n\t\tFintech\t\n\n\n\t\tFundraising\t\n\n\n\t\tGadgets\t\n\n\n\t\tGaming\t\n\n\n\t\tGoogle\t\n\n\n\t\tGovernment & Policy\t\n\n\n\t\tHardware\t\n\n\n\t\tInstagram\t\n\n\n\t\tLayoffs\t\n\n\n\t\tMedia & Entertainment\t\n\n\n\t\tMeta\t\n\n\n\t\tMicrosoft\t\n\n\n\t\tPrivacy\t\n\n\n\t\tRobotics\t\n\n\n\t\tSecurity\t\n\n\n\t\tSocial\t\n\n\n\t\tSpace\t\n\n\n\t\tStartups\t\n\n\n\t\tTikTok\t\n\n\n\t\tTransportation\t\n\n\n\t\tVenture\t\n\n\n\t\tEvents\t\n\n\n\t\tStartup Battlefield\t\n\n\n\t\tStrictlyVC\t\n\n\n\t\tNewsletters\t\n\n\n\t\tPodcasts\t\n\n\n\t\tVideos\t\n\n\n\t\tPartner Content\t\n\n\n\t\tTechCrunch Brand Studio\t\n\n\n\t\tCrunchboard\t\n\n\n\t\tContact Us\t\nThe U.S. Federal Trade Commission (FTC) on Monday released new documentation detailing its new \u201cRule on Unfair or Deceptive Fees.\u201d\u00a0The rule, set to take effect on May 12, prohibits hidden fees for live events, hotels, and short-term rentals. It also bans practices such as \u201cbait-and-switch pricing\u201d and any actions that conceal or misrepresent total prices and fe",
    "summary": "The U.S. Federal Trade Commission heroically decides to protect the hapless consumer from the big bad world of <em>surprise expenses</em> at live events and short-term stays, enforcing the kind of clarity only previously seen in IKEA assembly instructions. Commenters erupt in bewildered fury, shocked to discover that businesses might prefer murky pricing to keep their yachts afloat. They dive deep into the underbrush of exceptions and exclusivity deals, with the spleen and vigor of legal scholars, minus the degree. One brave soul, confused by capitalism, asks why pre-recorded performances are given a pass on honesty, forgetting that in Hollywood, fiction is always stranger \u2013 and more profitable \u2013 than truth. \ud83c\udfad\ud83e\udd11"
  },
  {
    "title": "Launch HN: Exa (YC S21) \u2013 The web as a database",
    "points": 248,
    "submitter": "willbryk",
    "submit_time": "2025-05-06T16:18:42 1746548322",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=43906841",
    "comments": [
      "I searched for 'data providers that start with the letter R that sell job postings data', and it's been 15 minutes and it barely verified the first row.But if it filtered it first to \"start with the letter R\", it would only have to look at perhaps 5% of the results it's trying to verify!So it's doing needless verification of results that will be thrown out by another filter that should've been applied first!\n \nreply",
      "We were down for a bit! Ran your search, got 8 matches after analyzing 100 results. Took 40 seconds for the first match, and another 80 seconds for the other matches.We use an agentic search planner that adapts its search strategy as matches are found, but it could be smarter with substrings.https://websets.exa.ai/cmad36arq009fl30i4dvkc7wn\n \nreply",
      "This is super cool! It took a while, but did a great job of evaluating the results, and the airtable-like results UI feels great.Congrats on your launch. With the natural way this lends itself to comparison shopping this is an amazing tool for people trying to find \"the best X for me\" whether that's a TV, a school, etc. So much content that you find on Google when trying to answer that type of query, is designed to trick, bamboozle, and to hide the facts that you might use to answer this question (but most of all to get you to click affiliate links).\n \nreply",
      "I found the hallucination detector demo: https://demo.exa.ai/hallucination-detectorThe search engine was impressive enough but I think this implementation was a nice cherry on top.\n \nreply",
      "> The second is that LLMs provide the last-mile intelligence needed to verify every result. Each result and piece of data is backed with supporting references that we used to validate that the result is actually a match for your search criteria.Evals on this would be great to benchmark the gap between using websets versus a generic web search tool. Otherwise to a developer, it's just marketing.\n \nreply",
      "https://exa.ai/blog/websets-evals\n \nreply",
      "I was so excited for this, but sadly it doesn't work at all, not even UI feedback for the error:(The UI showed literally no change. So I checked and the console shows:```\nTry: 14  Not Found 681-7df1b139fa2dc9f0.js:14:3379\nTry: 15  Not Found 681-7df1b139fa2dc9f0.js:14:3379\nTry: 16  Not Found 681-7df1b139fa2dc9f0.js:14:3379\nTry: 17  Not Found 681-7df1b139fa2dc9f0.js:14:3379\nTry: 18  Not Found 681-7df1b139fa2dc9f0.js:14:3379\nTry: 19  Not Found 681-7df1b139fa2dc9f0.js:14:3379\nTry: 20  Not Found 681-7df1b139fa2dc9f0.js:14:3379\n Gave up after 10 seconds. 681-7df1b139fa2dc9f0.js:14:3379\nfilteredSuggestions \nArray(3) [ {\u2026}, {\u2026}, {\u2026} ]\n681-7df1b139fa2dc9f0.js:14:3379\n```Also your table doesn't fit in the viewport so I can't see the results.Firefox Ubuntu.\n \nreply",
      "When OpenAI was rumored to acquire Windsurf last week I went to their site and switched languages. When I tried to switch back it got into a weird state and didn't display the original language. Not sure what to think of that other than vibe coding may need a little more oversight. (Who is working on AI QA? Winning pickaxe and shovel business right there.)\n \nreply",
      "I also thought the UX had silently died on me, but over the course of a few hours, results slowly rolled in. And they were pretty good, for what it's worth! It's clear they have far more demand than supply, at least than can be reasonably offered for free.\n \nreply",
      "We were down for a bit, back up now! Lmk how the search quality turns out for you\n \nreply"
    ],
    "link": "item?id=43906841",
    "first_paragraph": "",
    "summary": "**Title: Launch HN: Exa (YC S21) \u2013 The web as a database**\n\nIn an earth-shattering demonstration of over-promising under-delivery, Exa attempts to revolutionize failure by dressing it up as innovation. Users become unwitting beta testers, celebrating the rare occasion the system successfully fishes out a datum from the vast digital ocean, typically after enough time to watch a sitcom episode. Commentators, bedazzled by jargon like \"agentic search planner,\" bravely defend the snail-paced search results, mistaking complexity for sophistication. Meanwhile, the hallucination detector remains the only feature not hallucinating itself into usefulness. Embrace the future, where your search results come slower than postal mail \ud83d\udc0c\ud83d\udcec!"
  },
  {
    "title": "Why Bloat Is Still Software's Biggest Vulnerability (2024) (ieee.org)",
    "points": 23,
    "submitter": "kristianp",
    "submit_time": "2025-05-06T23:33:54 1746574434",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43910745",
    "comments": [
      "I am beginning to think that the terrible situation with dependency management in traditional C and C++ is a good thing.Now, with systems like npm, maven or cargo, all you need to do to get a package is to add a line in a configuration file, and it fetches all the dependencies you need automatically from a central repository. Very convenient, however, you can quickly find yourself with 100+ packages from who knows where and 100s of MB of code.In C, traditionally, every library you include requires some consideration. There is no auto-download, and the library the user has may be a different version from the one you worked with, and you have to accommodate it, and so does the library publisher. Or you may have to ship is with your own code. Anyways, it is so messy that the simplest solution is often not to use a library at all and write the thing yourself, or even better, realize that you don't need the feature you would have used that library for.Bad reason, and reinventing the wheel comes with its own set of problems, but at least, the resulting code is of a manageable size.\n \nreply",
      "Yet if you deliver a system without a modern bloated framework or a massive cloud stack and you are \"old fashioned\" and \"out of touch\" - been there done that, got the tee-shirt.\n \nreply",
      "In my last job, just to run the software on my local machine, I had to launch 6 different microservices running in a containerized, Linux virtualized environment on Windows and had to launch them in a particular order and had to keep each one in a separate console for debugging purposes. It took about 20 minutes to launch the software to be able to test it locally. The launch couldn't be automated easily because each service was using a mix of containers and plain Node.js servers with different versions and it was Windows so I would probably have to write some unfamiliar code for Windows to automate opening all the necessary git bash tabs...The services usually persisted except for automatic updates so I only had to restart all the services a few times per week so it didn't make sense to invest time to automate.\n \nreply",
      "I had a discussion with team members and we agreed that we will make our next systems fully deployable with one script or installer. It requires a little more thought and discipline but will result in much cleaner architecture and will also document itself this way.\n \nreply",
      "I like that they are containerized microservices, but you have to launch them in a particular order. Hahaha. What a nightmare. Congrats on it being a former job. Move on to better things? Well, unemployment would be preferable.\n \nreply",
      "No argument from me, I also believe bloat is a very large problem.A get of my lawn section :)I remember when GUIs started becoming a thing, I dreaded the move from Text to GUIs due to complexity.  I also remember most programs I wrote when I started on minis were 64k code and 64k text.  They were rather powerful even by today's standards, they did one thing and people had to learn which one to use to perform a task.Now we have all in one where in some cases you need to page through endless menus or buttons to find an obscure function.  In some cases you just give up looking and move on.  Progress I guess.\n \nreply"
    ],
    "link": "https://spectrum.ieee.org/lean-software-development",
    "first_paragraph": "The May issue of IEEE Spectrum is here!A 2024 plea for lean software This post is dedicated to the memory of Niklaus Wirth, a computing pioneer who passed away 1 January 2024. In 1995 he wrote an influential article called \u201cA Plea for Lean Software,\u201d published in Computer, the magazine for members of the IEEE Computer Society, which I read early in my career as an entrepreneur and software developer. In what follows, I try to make the same case nearly 30 years later, updated for today\u2019s computing horrors. A version of this post was originally published on my personal blog, Berthub.eu.Some years ago I did a talk at a local university on cybersecurity, titled \u201cCyber and Information Security: Have We All Gone Mad?\u201d It is still worth reading today since we have gone quite mad collectively.The way we build and ship software these days is mostly ridiculous, leading to apps using millions of lines of code to open a garage door, and other simple programs importing 1,600 external code libraries",
    "summary": "Title: Why Bloat Is Still Software's Biggest Vulnerability (2024) (ieee.org)\n\nWelcome back to the annual ghost dance of software developers, as IEEE revives the cry for skinny jeans in a code-obese world. <em>Niklaus Wirth is rolling in his grave</em> as his plea for lean software turns into a gripping nostalgia piece for the modem era. The article rehashes ideas older than the commenters' unwashed coffee mugs, lamenting that today's software requires an astrophysical degree and a summoning circle to deploy \"Hello World.\" Meanwhile, commenters trip over their robes in an orgy of self-congratulation, reminiscing about the golden era of floppy disks and declaring modern frameworks as the source of all evil. Oh, <i>and</i> someone finally fixed their printer using blockchain. Bravo! \ud83d\ude44"
  },
  {
    "title": "OpenAI reaches agreement to buy Windsurf for $3B (bloomberg.com)",
    "points": 368,
    "submitter": "swyx",
    "submit_time": "2025-05-06T00:57:48 1746493068",
    "num_comments": 359,
    "comments_url": "https://news.ycombinator.com/item?id=43900877",
    "comments": [
      "https://archive.md/l6n9H",
      "Windsurf and Cursor feel like temporary stopgaps, products of a narrow window in time before the landscape shifts again.Microsoft has clearly taken notice. They're already starting to lock down the upstream VSCode codebase, as seen with recent changes to the C/C++ extension [0]. It's not hard to imagine that future features like TypeScript 7.0 might be limited or even withheld from forks entirely. At the same time, Microsoft will likely replicate Windsurf and Cursor's features within a year. And deliver them with far greater stability and polish.Both Windsurf and Cursor are riddled with bugs that don't exist upstream, _especially_ in their AI assistant features beyond the VSCode core. Context management which is supposed to be the core featured added is itself incredibly poorly implemented [1].Ultimately, the future isn't about a smarter editor, it's about a smarter teammate. Tools like GitHub Copilot or future agents will handle entire engineering tickets: generating PRs with tests, taking feedback, and iterating like a real collaborator.[0] https://www.theregister.com/2025/04/24/microsoft_vs_code_sub...[1] https://www.reddit.com/r/cursor/comments/1kbt790/rules_in_49...\n \nreply",
      "The thing is: we should not need standalone editors just to use AI coding agents. They could be just plugins, but Microsoft does not want to bend the plugin API enough for that. Windsurf has a \"plugin edition\" for JetBrains IDEs that works really, really well[0] (they also have a VSCode plugin[1] but it's lacking in comparison).However, given that JetBrains also have their own AI offering[2], I'm not sure how long that will last too...[0] https://plugins.jetbrains.com/plugin/20540-windsurf-plugin-f...[1] https://marketplace.visualstudio.com/items?itemName=Codeium....[2] https://www.jetbrains.com/ai/\n \nreply",
      "I suspect JetBrains will never limit this. I've yet to recall anything in the past where they have done this even when they have a similar offering.In fact, their own AI extension appears to be pluggable in and of itself. I think they see the value in being easy to adapt different AI solutions to rather than trying to only provide their own.\n \nreply",
      "JetBrain's main business model depends on buying the editor, and if users still see the overall editor better, any AI plugin support will likely just increase the sales.\n \nreply",
      "100% i like some thinks cursor gives me, but i\u2019m to invested in how to use the navigation inside pycharm, i don\u2019t wanna give up that\n \nreply",
      "There are already a bunch of open source, free, and popular \"AI coding agent\" extensions for VS Code:1) Cline (1.4mil downloads)2) Roo Code (a fork of Cline, 450k downloads)Still a drop in the bucket compared to Cursor in terms of # of users, but they're growing pretty fast.Disclaimer: I maintain Kilo Code, which competes with 1) and 2) so I'm pretty familiar with this space/the growth patterns.\n \nreply",
      "I am constantly surprised how seldom aider is mentioned in threads like this. I understand that it's not directly integrated into the editor, but the \"editor + parallel CLI tool chain\" paradigm feels so natural to me because we drop to terminal for so many other parts of building software. If you haven't tried it (particularly the architect/editor modality), it's worth a couple of hours of experimenting.\n \nreply",
      "Aider doesn\u2019t provide any interface that\u2019s integrated into the editor tool, as you point out.  That might be true for other similar side-by-side tools that I am not aware of.But, if you tell aider to watch your files, you can drop a specially formatted comment into your file, and aider will see that and use it as a prompt.So the integration is sort of \u201cimplicit\u201d.  Which sounds kinda like the cheap way to go, in comparison to the current brand name tools that have their own chat boxes, dropdowns with mode selectors (ask, edit, agent), and so on.But look further into the future and an ambient interface is probably where we end up. Something where the Ai agent is just watching what you do, maybe even watching your eyes and seeing what you\u2019re attending to, and then harmonizing its attention to what you are attending to.But I dunno, i\u2019m just guessing\n \nreply",
      "Continue.dev as well\n \nreply"
    ],
    "link": "https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion",
    "first_paragraph": "",
    "summary": "**OpenAI Buys Windsurf for $3 Billion: Who Asked for This?**\n\nIn a startling burst of spendthrift enthusiasm, OpenAI decides to throw a cool $3 billion at Windsurf, a platform so bug-ridden it makes a '90s website look polished. Commenters, in a stunning display of missing the point, chatter excitedly about the bleak future of integrated AI tools while Microsoft polishes its predator claws to potentially steal, polish, and commercialize anything left breathing. Meanwhile, tech enthusiasts scribble furiously in forums, convincing themselves that a buggy optional plugin in an obscure IDE will revolutionize coding as they ignore the approaching sound of Microsoft\u2019s industry-gobbling footsteps. OpenAI, please show us the genius who penned this deal\u2014do they get a free set of steak knives? \ud83e\udd14\ud83d\udd2a\ud83d\udcb8"
  },
  {
    "title": "Show HN: Whippy Term - GUI terminal for embedded development (Linux and Windows) (whippyterm.com)",
    "points": 33,
    "submitter": "SurvivorTed",
    "submit_time": "2025-05-06T23:02:46 1746572566",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=43910565",
    "comments": [
      "New GUI based serial terminal for embedded development (Linux and Windows)Hi all, just wanted to let everyone know of my new open-source serial (and TCP/IP) terminal program aimed at embedded developers.I wasn't happy with what was available on Linux, so I decided to write my own.  The goals were to have a modern GUI (tab based, pull out panels, etc) and I wanted support for binary protocols.It has support for ANSI escape sequences, XModem (up/down), binary blocks, hex dumps, bridging 2 open connections, and more.Source link:\nhttps://github.com/TheBeef/WhippyTermThis is the first release (version 1.0), and I am hoping people will have a look (and hopefully like it).\n \nreply",
      "This seems really nice, thank you! Does it support auto-reconnect, like `tio` does?\n \nreply",
      "It doesn't currently support it, but I was planning in the next release (a number of people have asked for it so it's a definitely going to add).\n \nreply",
      "Why no macOS support?\n \nreply",
      "I don't have a mac.  I did have a guy who was going to add support for it but he backed out.The main GUI is built in QT so making a mac port shouldn't be too hard (the serial port detection would be the hardest part I think).\n \nreply",
      "Oh damn, this could easily replace my own bespoke serial monitors.Can a plugin filter the list of available ports? For instance serial over Bluetooth creates two virtual ports for initiating and accepting connections (on windows at least). My bespoke monitor filters these and only shows the outbound ports to the user. It also pulls in the Bluetooth device name from WinRT, etc.\n \nreply",
      "There isn't any filtering of serial ports (it lists everything it finds).However it does support bookmarks.  With bookmarks you open the serial port you want to use then select to bookmark it.  WhippyTerm will take the current config you are using and save it for later.  You can then just pick the bookmark later to reopen that connection.  Bookmarks also store some of the settings (like is it a binary or text connection, terminal size, colors, etc).It's a lot like bookmarks in a web browser.\n \nreply"
    ],
    "link": "https://whippyterm.com",
    "first_paragraph": "\n        Whippy Term is a modern terminal program.  It has been designed to have\n        a modern UI and to run on modern OS's like Windows and Linux.  It also\n        has a number of unique features such as bookmarks, built in hex dumps,\n        extending through plugins, and native support for binary protocols.\n    \n        It is primarily aimed at embedded developers that need to talk to their\n        devices. It supports serial communication such as serial ports\n        (RS232, RS485, RS422, TTL UART), TCP/IP, UDP. It also supports\n        protocols such as I2C and SPI though plugins.\n    \n        Terminal emulations such as ANSI and VT100 can be added (ANSI is builtin\n        and always available) through plugins.\n    \n        WhippyTerm also support working with binary protocols, in both\n        serial streams like RS232, and message block protocols like UDP.\n        Support for sending blocks of binary or AscII data is included to\n        better work with embedded devices and bi",
    "summary": "Ah, <em>Whippy Term</em>: the groundbreaking revolution in terminal technology, if your definition of \"groundbreaking\" involves reinventing wheels with added glitter. Its developer, filled with the original sin of not liking existing options, spawns another GUI terminal that blesses Linux and Windows users with the magical power to communicate with devices the old-fashioned way but with \"modern UI\" fluff. The announcement sets off a classic Hacker News parade: cheerleaders praising obsolete needs, tech hipsters lamenting the absence of macOS support, and of course, a tragic tale of collaboration gone awry. All while the developer contemplates the Herculean task of adding an auto-reconnect feature, previously spotted in realms of more arcane software like <em>tio</em>. \ud83d\ude31\ud83c\udf89"
  },
  {
    "title": "Sutton and Barto Book Implementation (github.com/ivanbelenky)",
    "points": 27,
    "submitter": "ivanbelenky",
    "submit_time": "2025-05-06T22:43:06 1746571386",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43910423",
    "comments": [
      "I want to add a second comment:Professors White & White (a husband and wife team) have a very good set of courses on RL on Coursera:https://www.coursera.org/specializations/reinforcement-learn...\n \nreply",
      "Lovely!\n \nreply",
      "The authors were professor and grad student at UMass Amherst, and are the current winners of the Turing Award.https://www.cics.umass.edu/https://www.nsf.gov/news/ai-pioneers-andrew-barto-richard-su...\n \nreply",
      "Let me know if anyone fills out the true online Sarsa section with a working example in a robot\n \nreply",
      "Very nice, thanks for doing this.I have experimented a lot with the \"official\" Common Lisp and Python examples for the Sutton/Barto RL book, and I will enjoy your implementations also!For reference, original examples in Lisp and Python: http://incompleteideas.net/book/code/code2nd.htmlA bunch of implementations with all kinds of use cases (e.g., using OpenAI RL Gym, etc.):Here are some resources with code examples and implementations related to the Sutton and Barto \"Reinforcement Learning: An Introduction\" book:Code for Sutton & Barto Book: Reinforcement Learning: An Introduction: The official website for the book provides links to various software and re-implementations in different languages, including Python, Julia, and Lisp. This is a great starting point to find code directly associated with the book's examples and exercises.Link: http://incompleteideas.net/book/code/code2nd.html\njovsa/rl-examples-sutton-and-barto-book on GitHub: This repository offers Python implementations of examples from the book, organized by chapter. It includes code for figures and examples from various chapters, covering topics like Gridworld, Blackjack, and the Mountain Car task.Link: https://github.com/jovsa/rl-examples-sutton-and-barto-book\nkamenbliznashki/sutton_barto on GitHub: This repository provides Python implementations of RL algorithms for the examples and figures in the Sutton and Barto book. It covers a wide range of topics from multi-armed bandits to policy gradient methods.Link: https://github.com/kamenbliznashki/sutton_barto\nboldyshev/sutton on GitHub: This repository contains Python implementations of example experiments (figures) and programming exercises from the second edition of the book. Chapters are added as the author studies the book, making it a potentially growing resource.Link: https://github.com/boldyshev/sutton\nAntonioSerrano/Implementation-of-RL-algorithms-from-Sutton-and-Barto-2018 on GitHub: This repository offers implementations in Python using OpenAI Gym and Tensorflow, covering exercises and solutions to complement the book and David Silver's RL course. It includes various algorithms like Dynamic Programming, Monte Carlo, Temporal Difference, and Policy Gradient methods.Link: https://github.com/AntonioSerrano/Implementation-of-RL-algor...\n \nreply",
      "my code is not as good as anything above most probably. Ive done this exploring while studying. No linter no typechecker, grug engineer mentality. But thanks nevertheless for the comment :)\n \nreply",
      "Damn this is a lot of work. Bookmarked.\n \nreply",
      "It has not been stress tested, or optimized, tread lightly and thanks a lot for appreciating the work.\n \nreply",
      "123\n \nreply"
    ],
    "link": "https://github.com/ivanbelenky/RL",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        R.L. methods and techniques. \n      \nThis repository contains code that implements algorithms and models from Sutton's book on reinforcement learning. The book, titled \"Reinforcement Learning: An Introduction,\" is a classic text on the subject and provides a comprehensive introduction to the field.The code in this repository is organized into several modules, each of which covers differents topics.All model free solvers will work just by defining states actions and a trasition function. Transitions are defined as a function that takes a state and an action and returns a tuple of the next state and the reward. The transition function also returns a boolean indicating whether the episode has terminated.Single State Infinite Variance Example 5.5Monte Carlo Tree Search maze solving plotWhile the code in this package provides a basic imp",
    "summary": "**Today in Amateur Hour: Reinforcement Learning Style**\n\nAnother soul braves the wilds of GitHub, convinced that everyone is waiting for yet another groundbreaking implementation of Sutton and Barto\u2019s dusty tome on reinforcement learning. This repository\u2014marvelously stuffed with pretty basic code and a lot of hope\u2014claims to organize algorithms into \u201cseveral modules\u201d and promises that all you need is states and a transition function to solve the universe\u2019s mysteries. Meanwhile, in the comments, a gaggle of enthusiast programmers throw links around like confetti at a geek parade, competing for who has bookmarked more half-read tutorials and untested repos. Someone praises the work while admitting their code quality might just qualify for caveman-level achievements. \ud83e\udd14\ud83d\udcbb\ud83c\udf89"
  },
  {
    "title": "Creativity Came to Pass \u2013 AI Dystopia (vale.rocks)",
    "points": 6,
    "submitter": "clacker-o-matic",
    "submit_time": "2025-05-07T00:28:46 1746577726",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://vale.rocks/posts/creativity-came-to-pass",
    "first_paragraph": "FictionThis is written of a future. Not the future, but a future \u2013 one of many possibilities.When AI as we know it now first came to be, it was primitive. Examples from the first few years of what we know now as the AI age are rudimentary. Text generation didn\u2019t take long to get to a decent enough stage for basic tasks, but visual creation, music composition, and other complex forms of creative expression took a bit longer.I\u2019ve seen some of the images generated during those early stages. They were hallmarked by distorted appearances, fuzzy definitions, and deformed limbs \u2013 hands seemed like they were especially difficult. Ethereal, low-resolution abstract was its only visual style. Early music often felt generic or repetitive, and initial attempts at complex writing or design lacked nuance as well. Most people wrote them off.Only a few years after that, though, things had progressed a lot. Image generation still struggled with lots of details, but overall quality had improved a lot. Mu",
    "summary": "**Hindsight Is 20/20 In The Low-Res World Of AI**  \nAnother day, another tech prophet waxes nostalgic about the quaint, pixelated dawn of AI\u2014this time on *vale.rocks*, because apparently, subtlety is dead and so are domain name standards. The author treats us to a riveting parade of *AI's greatest misses*, depicting early AI art like someone trying to finger-paint during an earthquake. Commenters, in a stunning display of missing the point, argue whether this glorified slideshow means AI will enslave us or just continue ruining modern art. Meanwhile, everyone ignores the real horror: that early AI music might make a comeback as a retro trend. \ud83e\udd16\ud83c\udfa8\ud83c\udfb6"
  },
  {
    "title": "ACE-Step: A step towards music generation foundation model (github.com/ace-step)",
    "points": 39,
    "submitter": "wertyk",
    "submit_time": "2025-05-06T20:38:00 1746563880",
    "num_comments": 19,
    "comments_url": "https://news.ycombinator.com/item?id=43909398",
    "comments": [
      ">  RapMachineFine-tuned on pure rap data to create an AI system specialized in rap generation\n Expected capabilities include AI rap battles and narrative expression through rap\n Rap has exceptional storytelling and expressive capabilities, offering extraordinary application potentialUsing a certain other music generator I got it to accidentally say ***. It said it with a Latino American accent too.In fact for whatever reason this tool couldn\u2019t use a typical AAVE voice. Just Sage Francis / Atmosphere like dictionary raps and a few Latino American ones.A big limitation of AI sloop is it tries to not offend anyone.Art that can\u2019t even try to offend is barely art.\n \nreply",
      "> Art that can\u2019t even try to offend is barely art.Good art is secondary to avoiding some journalist writing a hit piece about how they used your company\u2019s AI generator to depict Hitler saying the gamer word.\n \nreply",
      "To be fair, nothing AI generated is inherently \u201cgood art\u201d if you take artistic intent into account.\n \nreply",
      "AI generated articles about ai generated music which is generated from reading ai generated articles.Shall it continue in an unholy loop until the end of time ?\n \nreply",
      "Instrumental music doesn\u2019t count as art? Come on.\n \nreply",
      "Instrumental music can absolutely offend!It can challenge the old standards, it can push genres into new places.AI music can\u2019t. It\u2019s too safe.\n \nreply",
      "Maybe some specific implementations are \u201ctoo safe,\u201d but I don\u2019t believe that there\u2019s any combination of notes that an AI couldn\u2019t generate, in theory.\n \nreply",
      "> Instrumental music can absolutely offend!The Rite of Spring (Stravinsky) has entered the chat!And if that's not offensive enough, Music in Similar Motion by Glass, or Metal Machine Music by Pat Metheny or any of Glenn Branca's \"guitar symphonies\" will likely do the job for most people.\n \nreply",
      "Lou Reed - Metal Machine MusicPat Metheny - Zero Tolerance for Silence\n \nreply",
      "As a musician, the things I want most from generative AI is:1. Being able to have the AI fill in a track in the song, but use the whole song as input to figure out what to generate. Ideally for drums this would be a combination of individual drum hits, effects and midi so I'm able to tweak it after generation. If it used the Ableton effects and drum rack then that would be perfect.2. Take my singing and make it both sound great and like any combination of great singers (e.g. give me a bit of Taylor Swift combined with Cat Power)I've had a play with the style transfer between singers (bullet point 2 above) but when I last tried it, it was garbage in / garbage out, and my singing is garbage.What I don't want: To just generate a whole song. Adobe does this style of assistive AI well in the photo editing space but no one seems to have brought it to audio yet.\n \nreply"
    ],
    "link": "https://github.com/ace-step/ACE-Step",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        ACE-Step: A Step Towards Music Generation Foundation Model\n      \nProject |\n    Checkpoints |\n    Space Demo |\n     Discord\n\n\n\n\nWe introduce ACE-Step, a novel open-source foundation model for music generation that overcomes key limitations of existing approaches and achieves state-of-the-art performance through a holistic architectural design. Current methods face inherent trade-offs between generation speed, musical coherence, and controllability. For instance, LLM-based models (e.g., Yue, SongGen) excel at lyric alignment but suffer from slow inference and structural artifacts. Diffusion models (e.g., DiffRhythm), on the other hand, enable faster synthesis but often lack long-range structural coherence.ACE-Step bridges this gap by integrating diffusion-based generation with Sana\u2019s Deep Compression AutoEncoder (DCAE) and a lightwei",
    "summary": "<h1>ACE-Step: Another AI Toy Pretending to Rock Your World</h1>\n<p><i>We</i> introduce ACE-Step, <em>the revolutionary</em> AI destined to produce next-gen musical *masterpieces* by basically mixing a bit of this and a bit of that from existing tech. Watch in awe as it effortlessly blends incoherent beeps and boops, making early adopters rave about how it kinda, sorta, almost aligns a rap beat without offending your grandma. \ud83c\udfa4\ud83d\udc75</p>\n<p>Meanwhile, armchair critics in the comments section embark on a <em>philosophical</em> quest; debating if an AI that can\u2019t offend can still be considered *artistic*, or why your dog\u2019s whining sounds more *melodic* than computer-generated symphonies. They ponder deep existential crises like the never-ending loop of AI creating AI creating chaos. Next up: AI-generated critiques of AI-generated music reviews, now with fewer human emotions!</p>\n<p>Watch the circular firing squad of debates continue to miss the point that whether it's instrumental or AI-generated whispers, nobody is getting the next Grammys nod here. But hey, at least we\u2019re not offending anyone, right?</p>"
  },
  {
    "title": "India launches attack on 9 sites in Pakistan and Pakistani Jammu and Kashmir (reuters.com)",
    "points": 66,
    "submitter": "alephnerd",
    "submit_time": "2025-05-06T20:41:47 1746564107",
    "num_comments": 20,
    "comments_url": "https://news.ycombinator.com/item?id=43909430",
    "comments": [
      "The bad news: there is some real potential for escalation due to the suspension of Indus Waters Treatyhttps://economictimes.indiatimes.com/news/india/indias-water...Wasn't there something in the intro of \"Mad Max fury road\" about water wars?\n \nreply",
      "Related articles (from threads we've merged hither):https://www.nytimes.com/2025/05/06/world/asia/india-pakistan... (https://archive.ph/Bph7S)https://www.cnn.com/2025/05/06/asia/india-pakistan-kashmir-c...https://www.bloomberg.com/news/live-blog/2025-05-06/india-st... (https://archive.ph/eypzA)https://www.bbc.com/news/live/cwyneele13qt\n \nreply",
      "From the article:> Pakistan said India hit three sites with missiles, and a military spokesman told Reuters his country shot down five Indian aircraft, a claim not confirmed by India.That\u2019s a huge loss of aircraft! Are there any corroborating reports or more details about the aircraft/shootdown?\n \nreply",
      "Hasn't been confirmed. The Indian government says all pilots are accounted for.It's the fog of war, and OSINT/couch generalling in the manner that people did with Israel or Ukraine won't work with India and Pakistan.India has been leveraging the DPDP and national security laws really heavily to remove leaks on social media. All major social media platforms have a representative the Indian government coordinates with on information takedowns.Major reason Musk backed off on his stance about X takedowns with India unlike with Brazil.\n \nreply",
      "I hope this cools down into a propagation war after the initial bombing and shooting. Nationalism served, and every one gets what they want, well, except the dead ones...\n \nreply",
      "So far, military action is confined to Kashmir. If it gets out of that disputed area, this becomes a major war.\n \nreply",
      "Not just Kashmir.> The Indian government said its forces had struck nine sites in Pakistan and on Pakistan\u2019s side of the disputed Kashmir region. Pakistani military officials said that five places had been hit, in Punjab Province and its part of Kashmir.https://www.nytimes.com/2025/05/06/world/asia/india-pakistan...\n \nreply",
      "India calls this a targeted strike so for now at least they don't seem to be messaging a strong stance on expanding the strike.\n \nreply",
      "I don\u2019t think Pakistan will escalate. They are at a severe disadvantage unless china gives them the green light.\n \nreply",
      "Can you explain?  What does China's green light give to Pakistan?  Are they not able to attack full strength without China's permission?  And if so, why?\n \nreply"
    ],
    "link": "https://www.reuters.com/world/india/india-launches-attack-9-sites-pakistan-pakistan-occupied-jammu-kashmir-2025-05-06/",
    "first_paragraph": "",
    "summary": "**Global Waterfight Newsflash**: India decides the best approach to diplomacy is hurling explosives across borders, targeting nine scenic locations in Pakistan and its stylishly contentious area of Kashmir. Keyboard generals and couch strategists gleefully dust off their geopolitical playbooks, speculating with glee over the potential escalation that could turn their live blogs into the next big wartime soap opera. Meanwhile, the comment section lights up with amateur hour at the UN, as everyone debates whether Pakistan will wait for a \"green light\" from China, or if they\u2019ll just dive into the chaos solo. What a time to be alive\u2014unless you're at one of those nine sites, of course. \ud83e\udd37\u200d\u2642\ufe0f\ud83d\ude80 \u2694\ufe0f"
  },
  {
    "title": "Brush (Bo(u)rn(e) RUsty SHell) a POSIX and Bash-Compatible Shell in Rust (github.com/reubeno)",
    "points": 92,
    "submitter": "voxadam",
    "submit_time": "2025-05-06T18:47:49 1746557269",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=43908368",
    "comments": [
      "Hm I wasn't aware of this project, it seems to have started in 2022Other than OSH, it seems to be the only shell that aims for POSIX/bash compatibility, out of dozens of alternative shells: https://github.com/oils-for-unix/oils/wiki/Alternative-Shell...As far as I know, OSH is the most POSIX- and bash-compatible shell:Nine Reasons to Use OSH - https://oils.pub/osh.htmlIf I have time, I will run this through our spec tests: https://oils.pub/release/0.29.0/test/spec.wwz/osh-py/index.h...---About this part: There are a number of other POSIX-ish shells implemented in a non-C/C++ implementation languageOSH is implemented in an unusual style -- we wrote an \"executable spec\" in typed Python, and then the spec is translated to C++.That speeds it up anywhere from 2x-50x, so it's faster than bash on many workloadse.g. a \"fibonacci\" is faster than bash, as a test of the interpreter.  And it makes 5% fewer syscalls than bash or dash on Python's configure (although somehow this doesn't translate into wall time, which I want to figure out)It's also memory safe, e.g. if there is no free() in your code, then there is no double-free, etc.---As mentioned on the OSH landing page, YSH is also part of the Oils project, and you can upgrade with    shopt --set ysh:upgrade\n\nIf you want JSON and so forth, e.g.    ysh$ json read < x.json\n    ysh$ = _reply\n    (Dict)   {shell: \"ysh\", fun: true}\n\nYSH aims to be the \"ultimate glue language\" - https://oils.pub/ysh.html\n \nreply",
      "> It's also memory safe, e.g. if there is no free() in your code, then there is no double-free, etc.How does that work in practice? Is it an arena allocator per command execution? Fixed size preallocation  of space for shell variable names and values?\n \nreply",
      "Good question -- at first I thought we would use an arena, at least for the parser.  But we ended up writing a garbage collector for the subset of C++ we generate!That was completed in 2023: Pictures of a Working Garbage Collectorhttps://www.oilshell.org/blog/2023/01/garbage-collector.htmlhttps://news.ycombinator.com/item?id=34350260There are two reasons to have a GC:(1) The AST (aka \"lossless syntax tree\") is actually GRAPH - it is useful to share nodes.I think graphs are fairly common for ASTs.  Once a node can have multiple parents, then ownership becomes less clear, and errors using arenas can easily cause memory safety bugs.For example, in Rust, I think you start using Rc<T> and so forth, which is automatic memory management at runtime.(2) YSH is part of Oils, and it has nested dicts and lists like Python and JavaScript.Once you have nested dicts and lists, you need GC.  We actually have the GC at the \"meta-level\", and that (somewhat surprisingly) saves a ton of code and bugs.It's like writing a Python/Ruby interpreter in Java or Go, and re-using the platform GC, rather than writing one specific to your language.In particular, we don't have GC rooting, ownership, or anything like Py_INCREF/DECREF littered all over the codebase.  This makes our implementation like an executable spec!And that makes it very easy to contribute -- you write typed Python, and it's as fast as shells written in C.  I think it's the best of both worlds---Something I've pointed out over the years, and which many people find illuminating, is- bash doesn't have nested maps and arrays -- it only has flat ones.  Therefore it doesn't need GC- awk too - it doesn't need GC, because its data structures are limited - https://news.ycombinator.com/item?id=28785732- make and cmake too.I put all these \"weak glue\" languages in the \"string-ish\" category.  In contrast, YSH gains A LOT of power from having real data structures, and that requires GC.(Writing a GC was the hardest part of the project -- I think sh/awk/make/cmake left it out for a reason!  Even nushell has no GC I believe.  fish lacks a GC too.)YSH has more of the power of Python/JS/Ruby, which if you look at it historically, did \"replace\" shell and awk for a huge set of problems.  (Guido van Rossum specifically mentioned the \"hole\" between shell and C as a motivation for Python.)Garbage Collection Makes YSH Different - https://www.oilshell.org/blog/2024/09/gc.html---On a different note, if anyone really wants a shell buildable with the Rust toolchain, it would be worthwhile to TRANSLATE Oils to Rust.  This will definitely work, because Oils is translated to C++ (completed in early 2024).You would have to write the runtime, which is around 4K lines for the garbage collector, and 4K lines for the OS bindings.  That's a lot easier than writing a bash-compatible shell.That is, Oils has about 8K lines of hand-written C++ code.  Compare with bash which is 162K+ lines of C written from scratch -- it's 20x less.I think 8K lines of unsafe code is also comparable to Rust binaries.  e.g. prior to ~2018, Rust binaries used dlmalloc by default, which is 20-30K lines of C code.(What's important is that almost all PRs modify safe code only -- it is very easy to review typed Python)This would be a fun exercise for anybody interested in writing a GC in Rust (which is a hard challenge, with many nontrivial choices).  You can write a GC, and get a shell for free, etc.---Also, Brush and nushell are different projects ... Oils has both things -- OSH and YSH -- compatible and newSo you actually get 2 shells for free by doing that :-P  The \"executable spec\" strategy took a long time, but it actually worked!\n \nreply",
      "Since everyone is sharing shells written in Rust, I've become quite fond of Nushell: https://www.nushell.sh/I'd love to see more shell exploring things beyond POSIX. Text based stdin/stdout will always have its place, but having ways to express, serialize, and pass along data in more structured ways is quite nice.\n \nreply",
      "You should try powershell\n \nreply",
      "I have! Nutshell to me feels like a reimagined powershell without the focus on windows admin.\n \nreply",
      "Powershell doesn\u2019t \u201cfocus on windows admin\u201d any more than bash focuses on linux admin.What do you mean?\n \nreply",
      "It\u2019s available in other OSs now, but my exposure to it has mainly been windows server and endpoint environments.YMMV, I\u2019m mainly speaking from my own experience/history with powershell. For a long time it was the only way I knew of to manage various aspects of low level windows OS settings.\n \nreply",
      "Gotcha. Speaking as someone who isn\u2019t a fan of windows after about windows 2000, powershell is quite powerful, if not a touch verbose. Worth reading up on if you have a need to use it on any kind of cadence.\n \nreply",
      "+1 on nushell, it is incredible. Having all data typed and structured is an insane superpower.Common worries I hear from people that were non-issues in practice:- Not POSIX compatible: Nushell doesn't aim to replace POSIX, there's no problem with dropping back to bash to execute some shell snippets- You need to re-learn everything:  I'm not a huge fan of how the commands are organized, but I still didn't find it that difficult. nushell's prompt/readline comes with an inline help menu with fuzzy search. Hit CTRL+O to edit a command/pipeline in your IDE/editor of choice with LSP backed intellisense, type-checking and in-editor command docs/help. The syntax is very simple and intuitive.- Just use python: Sure, but python comes with a lot of disadvantages. It's slow and uses dynamic typing. Static typing in nushell catch typos in pipelines & scripts before they execute. It also makes in-shell and IDE LSP tab-completions very accurate. Large files process quickly though it will still consume more memory if you aren't able to process all the data in a streaming fasion. It's like having jq but with autocomplete and it works on all command output & shell variables. Though if you really like python, check out Oil/OSH/YSH: https://oils.pub/- All Unix commands output text, structured data is useless in a shell: `detect columns` (https://www.nushell.sh/commands/docs/detect_columns.html) - now it's structured. Or use `from <format>` if the command outputs CSV, JSON, INI, YAML, etc... Or don't, cause GNU tools work fine in nushell if you keep everything in text formatAnd there are other crazy features too.- Write nushell plugins in your language of choice, plugins can work with structured data- Plugins can run in the background and maintain state, nushell can automatically start a plugin when it is first used and stop the plugin when it is idle  - e.g. a plugin can open a SQL connection and use it across multiple commands. There's a built-in plugin for opening in-mem/on-disk SQLite databases\n\n- Data can carry metadata, e.g. binary data can carry its mime type, strings often carry metadata about which line and file the string was read from.- Closures, generators, ranges, errors/exceptions + try-catch- Ongoing work on DAP suppprt to allow debugging scripts from your IDE- Create your own hooks to customize how different types of data are displayed. Display structured data in table/tree form, display binary data in hex, etc...- Collect related commands/variables into modules. Load a module knowing that you can easily unload the whole module later, module contents don't pollute global state. Variable declarations, env vars and loaded modules are scoped to the current code block and disappear after the closing bracket, lowering the odds of a name collision.- Native support of Polars dataframes to work with even moar data- Complex parllelism: Message-passing/actor architecture background jobs. Turn-key parallelism: transform every element of a list in parallel - `par-each` (https://www.nushell.sh/commands/docs/par-each.html)The biggest downside of nushell is that it hasn't hit 1.0 yet so commands occasionally get renamed. Expect that you may occasionally need to tweak a script to get it working again. Definitely a pain point.\n \nreply"
    ],
    "link": "https://github.com/reubeno/brush",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        bash/POSIX-compatible shell implemented in Rust\n      \n\n\n\n\nbrush (Bo(u)rn(e) RUsty SHell) is a POSIX- and bash-compatible shell,\nimplemented in Rust. It's built and tested on Linux and macOS, with experimental support on Windows. (Its Linux build is fully supported running on Windows via WSL.)brush is functional for interactive use as a daily driver! It can execute most sh and bash scripts we've\nencountered. Known limitations are tracked with filed issues. Out of an abundance of caution,\nwe wouldn't recommend using it yet in production scenarios in case it doesn't behave identically\nto your existing stable shell. (If you do find any behavioral differences, though, please report them with an\nissue!)Contributions and feedback of all kinds are welcome! For more guidance, please consult our\ncontribution guidelines. For more technical de",
    "summary": "In the ever-spinning wheel of technology where reinventing the wheel is sport, Brush (Bo(u)rn(e) RUsty SHell) emerges as yet another shell conceived in the fevered dreams of developers passionate about memory safety and the letter 'R'. Built on Rust and promising POSIX and Bash compatibility, Brush ambitiously hits the scene with \"experimental\" Windows support, naturally via the WSL detour - because why make Windows users feel too at home? Commenters frolic in the gifts of this new shiny tool, debating the old \"shell philosophy\" while precariously balancing on the fine line between excitement and skepticism. As shells breed like rabbits in the Rust ecosystem, enthusiasts merrily exchange links to other contrived solutions to problems already solved decades ago. The flurried fascination with retrofitting everything with Rust isn\u2019t just a phase, it\u2019s a lifestyle. \ud83e\udd80\ud83d\udcbb"
  },
  {
    "title": "Claude's system prompt is over 24k tokens with tools (github.com/asgeirtj)",
    "points": 40,
    "submitter": "mike210",
    "submit_time": "2025-05-06T20:39:35 1746563975",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43909409",
    "comments": [
      "I like how there are IFs and ELSE IFs but those logical constructs aren't actually explicitly followed...and inside the IF instead of a dash as a bullet point there's an arrow.. that's the syntax? hah.. what if there were two lines of instructions, you'd make a new line starting with another arrow..?\n \nreply",
      "> Armed with a good understanding of the restrictions, I now need to review your current investment strategy to assess potential impacts. First, I'll find out where you work by reading your Gmail profile. [read_gmail_profile]> Notable discovery: you have significant positions in semiconductor manufacturers. This warrants checking for any internal analysis on the export restrictions [google_drive_search: export controls]Oh that's not creepy.  Are these supposed to be examples of tools usage available to enterprise customers or what exactly?\n \nreply",
      "As seen on r/LocalLlaMA here:  https://www.reddit.com/r/LocalLLaMA/comments/1kfkg29/For what it's worth I pasted this into a few tokenizers and got just over 24k tokens.  Seems like an enormously long manual of instructions, with a lot of very specific instructions embedded...\n \nreply",
      "I think it\u2019s feasible because of their token prefix prompt caching, available to everyone via API: https://docs.anthropic.com/en/docs/build-with-claude/prompt-...\n \nreply",
      "do tools like cursor get a special pass? Or do they do some magic?I'm always amazed at how well they deal with diffs. \nespecially when the response jank clearly points to a \"... + a change\", \nand cursor maps it back to a proper diff.\n \nreply",
      "Cursor for instance does lots of tricks to make applying janky diffs efficient, e.g. https://blog.getbind.co/2024/10/02/how-cursor-ai-implemented...\n \nreply",
      "my lord \u2026 does it work as some rule file?\n \nreply",
      "Maybe therein is why it rarely follows my own project prompt instructions. I tell it to give me the whole code (no snippets), and not to make up new features, and it still barfs up refactoring and \"optimizations\" I didn't ask for, as well as \"Put this into your script\" with no specifics where the snippet lives.Single tasks that are one-and-done are great, but when working on a project, it's exhausting the amount it just doesn't listen to you.\n \nreply",
      "is this claude the app or the api?\n \nreply"
    ],
    "link": "https://github.com/asgeirtj/system_prompts_leaks/blob/main/claude.txt",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          ",
    "summary": "Welcome to the debut of Claude's veritable codex of conditions for coding, a sprawling 24000-token testament to the adage: \"more is less.\" Developers gather 'round, deploying meticulously detailed feedback in the comments like, \"Ah, yes, the innovative use of arrows instead of dashes\u2014groundbreaking!\" Meanwhile, tech enthusiasts creatively confuse tools with wizards, suggesting that amid the arrays and loops, some form of digital sorcery automagically aligns their janky diffs. Are we developing tech, or casting spells? Only the arcane arrows know."
  },
  {
    "title": "Old Timey Code and Old Timey Mono Fonts (github.com/dse)",
    "points": 58,
    "submitter": "dsevil",
    "submit_time": "2025-05-04T04:10:09 1746331809",
    "num_comments": 9,
    "comments_url": "https://news.ycombinator.com/item?id=43884418",
    "comments": [
      "Based on Reproducing Typewriter, a letterpress typeface from c. 1906 used to create fake typewritten letters for promotional material. Where other monospace typefaces replicated actual typewriters, Reproducing Typewriter had improvements for better readability at smaller point sizes and/or where poor quality reproduction was an issue.I thought its features would make it the basis of a good coding font, too. Old Timey Mono is much closer to the original while Old Timey Code makes it an even better typeface for writing source code.It was the coding font used in the Turbo Pascal 3.0 user manual. I've not seen it elsewhere except old patents' cover pages.Enjoy and if you have any comments or questions, comment or enquire away.https://github.com/dse/old-timey-mono-fonthttps://webonastick.com/fonts/old-timey-mono/\n \nreply",
      "Thanks so much for making it! Usually fonts like this are a non-starter for me since they lack Cyrillic letters. Your inclusion of those symbols is much appreciated.\n \nreply",
      "Thank you so much for making and sharing this. I'm especially grateful that you included the code variant. I'm not a programmer but love monospace fonts but the lack of a slashed zero in so many otherwise lovely fonts has been a deal breaker for me!\n \nreply",
      "Based on the first link, it seems as though zero and upper-case \"O\" are very similar.  (My eyes cannot discern a difference, but I admit that my eyes are not top-notch.)\n \nreply",
      "In Old Timey Mono, lowercase \"L\" and the number \"1\" are also very similar.Old Timey Code fixes both of these--  it has a slashed zero and redraws the number 1 to be distinct (angles the top serif).\n \nreply",
      "The original typewriter had no \"0\" or \"1\" you were expected to use \"I\" or \"O\" in place. I suspect an authentic typeface should have trouble distinguishing them.and having said that, forget authenticity, I really appreciate typefaces that make an effort to distinguish all characters.\n \nreply",
      "It's gorgeous. Thanks for making and sharing it!\n \nreply",
      "This is pretty nice: thanks for including polytonic Greek and macrons over Latin vowels including over y.  I especially love how the breathing marks and accents look together over initial vowels in Greek, and I love zeta (\u03b6) and xi (\u03be) in this font.Might I make a few specific suggestions:- allow combining breve over Latin y as well: sometimes that's handy for indicating contrast- check the height of stacking diacritical marks: a perispomenos tonos or circumflex accent over a breathing mark over a vowel (like in \u03b5\u1f36\u03bd\u03b1\u03b9 e\u0129nai) ends up stacking up tall enough to intersect with descenders (like on \u03b6 zeta) from the line above- the circumflex over alpha (\u1fb6) looks really good, because it follows the curve of the alpha itself, but circumflex over eta (\u1fc6) looks off-center, because it left-aligns to the ear on the left of eta.  The same could be said for the iota subscript (\u1fb3\u1fc3\u1ff3): it looks great under alpha and omega, but it's a bit awkward under eta because of how far to the left it is.- have you considered adding a variation for the Porsonic or single-curve circumflex?\n \nreply",
      "I'm currently using an old copy of Letter Gothic 12 Pitch from an ancient Ventura Publisher CD, and like that style a lot, so definitely going to try this.\n \nreply"
    ],
    "link": "https://github.com/dse/old-timey-mono-font",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          It will be under Releases.https://webonastick.com/fonts/old-timey-mono/A clean monospace typeface based on Reproducing Typewriter which was\navailable as early as 1906.  Reproducing Typewriter was designed for\nsimulating typewritten letters in smaller point sizes for\nadvertisements, catalogs, etc., where readability and/or distortion\nresulting from duplication were issues with faces based on actual\ntypewriters at the time.\"Sight Saver\" typewriter faces designed for readability were also\navailable, though they were at larger sizes than ordinary pica or\nelite typefaces.I developed Old Timey Mono because I thought those characteristics\nwould make it a good basis for a coding font with an antique\naesthetic.  You may have seen Reproducing Typewriter used in the\nTurbo Pascal 3.0 product manual.At a size of 12pt (6 lines per inch), Old Timey Mono is",
    "summary": "**Old Timey Code and Old Timey Mono Fonts: A Retro Coder's Dream or Visual Nightmare?**\n\nIn a remarkable blast from the past, a valiant coder resurrects a 1906 typewriter font for the digital age, because clear character distinction was too mainstream for 2023. The GitHub warriors rally in the comments, tripping over themselves to thank the creator for adding Cyrillic letters and distinguishing the number \"1\" from a lowercase \"l\", as though discerning basic characters wasn't a feature of well-designed fonts since, like, ever. Someone even lamented the close resemblance between zero and uppercase \"O\", revealing more about the quality of their eyesight than the typeface. Meanwhile, another eager commenter, clearly a fontophile, praised the inclusion of Greek and Latin diacritics, while pitching tweaks for accents that surely five other people on earth might care about. With Old Timey Mono, step back in time and experience the joy of straining your eyes, just like your great-grandpa during his coding days on a mechanical typewriter\u2014no electricity required! \ud83e\uddd0\ud83d\udc53"
  },
  {
    "title": "The curse of knowing how, or; fixing everything (notashelf.dev)",
    "points": 964,
    "submitter": "Lunar5227",
    "submit_time": "2025-05-06T06:01:23 1746511283",
    "num_comments": 399,
    "comments_url": "https://news.ycombinator.com/item?id=43902212",
    "comments": [
      "There's a quote I learned when doing theatre, which I've seen attributed to either the stage magician Doug Henning or possibly Stanislavski, describing the process of art as taking something that's difficult and making it habit, then taking something that's habitual and making it easy, and then taking something that's easy and making it beautiful.For example, as an actor, you learn your lines by rote (they become habit), then you gain an understanding of the character's motivations (remembering the lines becomes easy, because of course that's what your character would say), then you work to tune your performance so the audience shares in the emotion and unspoken meaning of the lines (that's beautiful/art).As this relates to software, I think it goes something like: you learn the magic incantation to make the computer do what you want (solving a hard task becomes habit), then you learn why that incantation works (solving it becomes easy), then you figure out better ways to solve the problem, such that the original friction can be removed completely (you find a more beautiful way to solve it).\n \nreply",
      "Agreed on this view! Sharing some similar thoughtsParaphrasing a virtuous music band reflecting on their discography: \"the first album was about what we could; the second one was about what we should\"It also aligns with Gell's philosophy of art. Here's a wikipedia exerpt:> Gell argues that art in general acts on its users, i.e. achieves agency, through a sort of technical virtuosity. Art can enchant the viewer, who is always a blind viewer, because \"the technology of enchantment is founded on the enchantment of technology\"\n \nreply",
      "> \"the first album was about what we could; the second one was about what we should\"Funny enough, when you apply this to software it becomes the pejorative \"second system syndrome\" (Brooks, 1975)\n \nreply",
      "in the world of music known as Sophomore slump:  In the world of music, there is a common phenomenon known as the sophomore album curse/syndrome, where newly popular artists often struggle to replicate their initial success with their second album, which is often characterized by struggles in changing musical style\n\n\nhttps://en.wikipedia.org/wiki/Sophomore_slump\n \nreply",
      "> As this relates to software, I think it goes something like: you learn the magic incantation to make the computer do what you want (solving a hard task becomes habit), then you learn why that incantation works (solving it becomes easy), then you figure out better ways to solve the problem, such that the original friction can be removed completely (you find a more beautiful way to solve it).I've come to a less pleasant way of putting a similar workflow. If something hurts(in the sense that you dread doing it, even though it needs to be done), make it hurt as much as possible and keep doing it until it doesn't. Along the way you'll find the ways to make it easier, faster, more efficient, etc but if you put it off every time until you can find the fortitude to embrace the suck then it's never going to get noticeably better.Most of the time this is business process related, especially when inheriting legacy systems. Ideal outcome is that you understand it enough after really digging into it to cut out or replace entire swathes of the processes without losing confidence that it will continue to operate smoothly.\n \nreply",
      "In less poetic terms, the progression I talk about is \"copy, choose, create\". First, we learn to copy a solution. Later, we know enough solutions that we can choose the best for the situation and copy that one. Finally, we know enough to create our own solutions that are well adapted to the problem.\n \nreply",
      "For people wanting to dig into this idea some more, I'd recommend the book by Austin Kleon called \"Steal Like An Artist.\"  Also, there is some nuance in the book about copying and stealing, without being a thief.\n \nreply",
      "That quote is meaningful.In a similar context, Bruce Lee said about martial arts that  \"martial\" is to discover  the dangerous animal within us, and the \"art\" is to be able to tame that animal.\n \nreply",
      "> the \"art\" is to be able to mate that animal.I'm assuming (hoping?) that this was supposed to be \"tame\"? If not, I've got some questions about Bruce Lee.\n \nreply",
      "That was a beautiful quote, I can see some examples in my life where this applies.\n \nreply"
    ],
    "link": "https://notashelf.dev/posts/curse-of-knowing",
    "first_paragraph": "It starts innocently.You rename a batch of files with a ten-line Python script, or you alias a common\ngit command to shave off two keystrokes. Maybe you build a small shell\nfunction to format JSON from the clipboard.You\u2019re not even trying to be clever. You\u2019re just solving tiny problems. Making\nthe machine do what it should have done in the first place. And then something\nhappens. You cross a threshold. You look at your tools, your environment, your\noperating system\u2014even your editor\u2014and suddenly everything is fair game.You could rebuild that (if you wanted to).\nYou could improve that (if you wanted to).Then someone challenges you. As banter maybe, perhaps jokingly but also with a\ndash of hope. Then the air in the room changes.It suddenly becomes something else. It becomes:You should.And from that moment forward, the world is broken in new and specific ways that\nonly you can see.Before I could program, broken software was frustrating but ignorable. For years\nI\u2019ve simply \u201cused\u201d a computer",
    "summary": "**The Curse of Knowing Too Much, Or How I Learned to Stop Worrying and Fix Everything**\n\nToday on notashelf.dev, a tragic hero recounts their Prometheus-esque journey from renaming files to essentially becoming one with their computer, narrated with the suspense and melodrama typically reserved for an Apple product launch. \ud83c\udfad The comment section instantly transforms into a pseudo-philosophical slam poetry session, where everyone tries to out-quote each other, ranging from stage magicians to martial artists, blissfully unaware that they're really just discussing how to be less annoyed by repetitive tasks. As the debate about whether \"software development is art\" spirals into existential oblivion, one brave soul accidentally proposes \"mating\" with the metaphorical beast instead of \"taming\" it, leaving readers and Bruce Lee enthusiasts equally horrified and amused. Clearly, the *true* curse is having comment sections. \ud83d\ude31"
  },
  {
    "title": "Continue (YC S23) is hiring software engineers in San Francisco (ycombinator.com)",
    "points": 0,
    "submitter": "",
    "submit_time": "2025-05-06T21:01:57 1746565317",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.ycombinator.com/companies/continue/jobs/smcxRnM-software-engineer",
    "first_paragraph": "Create, share, and use custom AI code assistantsContinue is seeking an outstanding software engineer to help us build state-of-the-art autocomplete and codebase retrieval, who thinks rigorously and pays attention to the smallest details. In this role, you will work on fundamental, but highly open-ended problems where deliberate measurement, rapid experimentation, and empathy for users push forward the product.About youPlease keep in mind that we are describing the background that we imagine would best fit the role. If you don\u2019t meet all the requirements, but you are confident that you are up for the task, we absolutely want to get to know you!What you will doWe\u2019re a startup, so you\u2019ll have to be ready to do whatever is required to accomplish our mission. However, you can definitely expect to:We believe there is an opportunity to create a future where developers are amplified, not automated. This is why we are enabling developers to create, share, and use custom AI code assistants with ",
    "summary": "**Startup Seeks Magical Code Wizard to Save Silicon Valley (Again)**\n\nAre you neurotic about comma placements and worship at the altar of hyper-optimized code? *Continue* might just let you into their super-exclusive club of dreamers, hoping to revolutionize coding by essentially inventing glorified spellcheck. Expect to \"push the product forward\" through tasks undeniably pivotal to human advancement, like fine-tuning autocomplete suggestions that could change the world\u2014or at least make it *slightly* less frustrating to type. Commenters are already lining up to declare this the breakthrough of the century, clearly having solved all simpler life problems like climate change or global hunger. \ud83d\ude80\ud83d\udc69\u200d\ud83d\udcbb"
  },
  {
    "title": "Accents in latent spaces: How AI hears accent strength in English (boldvoice.com)",
    "points": 178,
    "submitter": "ilyausorov",
    "submit_time": "2025-05-06T14:07:57 1746540477",
    "num_comments": 92,
    "comments_url": "https://news.ycombinator.com/item?id=43905299",
    "comments": [
      "Victor's problem isn't really the vowels or pacing. The final consonants are soft or not really audible. I am not hearing the /\u014b/ of \"long\" as the most marked example. It sounds closer to \"law\". In his \"improved\" recording he hasn't fixed this.I sometimes see content on social media encouraging people to sound more native or improve their accent. But IMO it's perfectly ok to have an accent, as long as the speech meets some baseline of intelligibility. (So Victor needs to work on \"long\" but not \"days\".) I've even come across people who are trying to mimick a native accent but lose intelligibility, where they'd sound better with their foreign accent. (An example I've seen is a native Spanish speaker trying to imitate the American accent's intervocalic T and D, and I don't understand them. A Spanish /t/ or /d/ would be different from most English language accents, but be way more understandable.)\n \nreply",
      "A very important part of people trusting you is them being able to understand what you say without making extra efforts compared to a native speaker.An easy way to improve intonation and fluency is to imitate a native speaker. Copying things like the intervocalic T and D is a consequence of that. It would be easier for a native Spanish speaker to say the Spanish /t/ and /d/ but intonation and fluency would be impaired.The sounds don't \"flow\" as they should.\n \nreply",
      "\"If Victor wanted to move beyond this point, the sound-by-sound phonetic analysis available in the BoldVoice app would allow him to understand the patterns in pronunciation and stress that contribute to Eliza\u2019s accent and teach him how to apply them in his own speech.\"Indeed Victor would likely receive a personalized lesson and practice on the NG sound on the app.\n \nreply",
      "Thank you for pinpoints my confusion/disconnect on what lack of improvement that I was sensing. There was an improvement on pacing, and cadence, yes, but that was not the main challenge with Victors accent. Visually I'd say victor improved by at most 5% and not 50% as indicated by the visualization. In some regards it was even harder to understand than the original due to speed and cadence without improvement in core pronunciation.\n \nreply",
      "Yeah, as long as it\u2019s intelligible an accent is perfectly fineIt\u2019s also perfectly fine to want to sound like a native speaker - whether it be because they are self conscious, think it will benefit them in some way, or simply want to feel like they are speaking \u201ccorrectly\u201dSorry to pick on you, it\u2019s just amazing to me how sensitive we are to \u201cinclusivity\u201d to the point where we almost discourage people wanting to fit in\n \nreply",
      "That kind of implies that there's a \"correct\" accent for English, even though many countries and regions natively speak it. Someone from Glasgow is just as much of a native speaker as someone from Los Angeles even though the accents are wildly different.\n \nreply",
      "Hence in quotes, man\n \nreply",
      "Being legible also means to cater to your audience. I work in an English-speaking company in a country where English isn't the native language, with loads of non-native speakers from around the world. Sometimes the native/best English speakers are the ones being misunderstood, because they use idioms or advanced words. None of us are bad at English, and I don't mean that I need to \"dumb it down\" (if anything, verbally I'm one of the worser ones), but I don't feel like I'm missing out on speaking simple with an accent.\n \nreply",
      "Generalizing from my own experience, it\u2019s easier for me to understand a non-native Spanish speaker than a native Spanish speaker and I would guess that the same applies with ESL speakers. One thing I found really fascinating is that even though I\u2019d never studied French\u00b9, I actually had an easier time understanding a conversation between my ex-wife and her aunt in French than when they spoke Spanish in which I was functional (my skill in the language has gone up a great deal since then so that I now read fluently, and speak and listen reasonably well, albeit less well than I would like).\u2e3b1. Thanks to my kids studying French on Duolingo and my joining them, I can no longer say that I\u2019ve never studied it.\n \nreply",
      ">Generalizing from my own experience, it\u2019s easier for me to understand a non-native Spanish speaker than a native Spanish speaker and I would guess that the same applies with ESL speakers.You guessed right -- it's /usually/ easier to understand other non-native speakers, both because of accent and less idioms. That is unless the accent is really heavy and doesn't match your own.\n \nreply"
    ],
    "link": "https://accent-strength.boldvoice.com/",
    "first_paragraph": "",
    "summary": "**Accents in latent spaces: How AI hears accent strength in English (boldvoice.com)**\n\nIn another breathtaking display of technological solutionism, BoldVoice has decided to digitize the art of nitpicking accents, because surely what Victor needs is an app to tell him his \"long\" sounds too much like \"law\". Commenters get into the weeds about phonetics with the passion of an unpaid intern, while others champion the hot take that having an accent is okay\u2014if it's legible enough not to scare the locals. One brave soul even attempts to redefine intelligibility, suggesting that sounding like a melting pot of non-native speakers might be the new English standard. Forget fluency; as long you can hear the linguistic despair over the stripped down T's and D's, you're probably doing it right. \ud83c\udfad"
  },
  {
    "title": "Alignment is not free: How model upgrades can silence your confidence signals (variance.co)",
    "points": 3,
    "submitter": "karinemellata",
    "submit_time": "2025-05-06T23:22:49 1746573769",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://www.variance.co/post/alignment-is-not-free-how-a-model-silenced-our-confidence-signals",
    "first_paragraph": "View open positionsThe post-training process for LLMs can bias behavior for language models when they encounter content that violates their safety post-training guidelines. As mentioned by OpenAI\u2019s GPT-4 system card, model calibration rarely survives post-training, resulting in models that are extremely confident even when they\u2019re wrong.\u00b9 For our use case, we often see this behavior with the side effect of biasing language model outputs towards violations, which can result in wasted review times for human reviewers in an LLM-powered content moderation system.\u200dTake the below histogram of log probs sampled from a golden dataset of false positives against GPT-4o. We can see that almost all outputs have log p\u22480 nats (probability \u2248 1) for outputting \u201ctrue\u201d, indicating a true violation in this dataset. However, there are a few outliers in this dataset, almost all of which correspond to patterns of behavior we observed in our dataset when our model would stray away from formal grounded policy",
    "summary": "**The Mirage of AI Precision: A Comedy in Tech Minor**\n\nIn yet another groundbreaking exposition on why AI still can\u2019t get it right, <em>Variance.co</em> breathlessly reveals that post-training \"tweaks\" to language models like OpenAI's GPT-4 make it about as reliable as a drunken fortune teller. The models, now exuding the unfounded confidence of a toddler in a superhero cape, boldly assert truths where there are none, subsequently wasting the precious life moments of human reviewers forced to double-check their homework. Comment sections ignite with armchair experts who, despite not knowing their <i>log probs</i> from their logarithms, vehemently argue that they could solve AI alignment during a coffee break\u2014if only they weren't so busy commenting online. \ud83e\udd16\ud83d\udcac\ud83d\udeab"
  },
  {
    "title": "Nnd \u2013 a TUI debugger alternative to GDB, LLDB (github.com/al13n321)",
    "points": 210,
    "submitter": "zX41ZdbW",
    "submit_time": "2025-05-06T13:58:03 1746539883",
    "num_comments": 71,
    "comments_url": "https://news.ycombinator.com/item?id=43905185",
    "comments": [
      "Hi, author here :)Didn't expect it to be posted, readme maybe doesn't have enough context. It just says \"Essential features are there\". What are those? Most of what I've ever used in any debugger:* Showing code, disassembly, threads, stack traces, local variables.* Watches, with a little custom expression language. E.g. you can do pointer arithmetic, type casts, turn a pointer+length into an array, show as hex, etc. Access to local and global variables, thread-local variables, registers. Type introspection (e.g. sizeof and offsets of fields).* Pretty printers for most C++ and Rust standard library types. Probably fragile and version-dependent (e.g. fields names often changes across versions), please report when they don't work.* Automatically down-casting abstract classes to concrete classes.* Breakpoints, conditional breakpoints (but no data breakpoints yet).* Stepping: into/over/out a source code line, into/over a disassembly instruction, over a source code column (when there are multple statements one line, e.g. to skip evaluation of arguments of a function call). All places where control can stop (statements) are highlighted in the code, so you usually don't get surprised by where a step takes you. (...except when there's garbage in debug info, and you end up temporarily on line 0 or something. This happens frustratingly often, and there's not much I can do about it. I already added quite a few workarounds to make stepping less janky in such cases. If a step takes you to an unexpected place, it usually under-steps rather than over-steps, so you can just step again until you end up in the correct place.)* Various searches: file by name, function by name, function by address (like addr2line), type by name, global variable by name, thread by stack trace.* Debugging core dumps. There's also a gdump-like tool built in (`nnd --dump-core`) that makes core dump of a running process without killing it; it uses fork to minimize downtime (usually around a second even if there are tens of GB of memory to dump).* Customizable key bindings, see `nnd --help-files` or `nnd --help-state`.* TUI with mouse support, tooltips, etc.\n \nreply",
      "Looks great, reminds me of the GUD interface in emacs. I\u2019m unable to get it to find the code for a crate in a rust workspace. I\u2019ve tried pointing the -d argument at the crate subdirectory, but nothing shows up in the code window. Any tips for debugging this issue?\n \nreply",
      "As a curiosity, is there a more heuristic approach and/or toolchain integrated approach that could be used for disassembly of stdlib components?For example, a crate that could be linked in to provide some \"well-known\" object shapes (hashmaps, vec, hashset, etc) with marker values that could be heuristically analyzed to understand the debuggability of those objects?Alternatively, I'd love to have a crate with recognizers and/or heuristics that could be somewhat debugger-independent and could be worked on for the benefit of other users. I'm quite an experienced Rust developer, just not really with debuggers, happy to help if there's a sandbox project that this could be plugged into.\n \nreply",
      "Here's an overly long reply, sorry :)For custom pretty-printers, the long-term plan is to make the watch expression language rich enough that you can just write one-liners in the watches window to pretty-print your struct. E.g. `for entry in my_hashmap.entries_ptr.[my_hashmap.num_entries] { if entry.has_value { yield struct {key: &entry.key, value: &entry.value}; } }`. Then allow loading a collection of such printers from a file; I guess each pretty-printer would have a regex of type names for which to use it (e.g. `std:.*:unordered_(multi)?(set|map)`). There are not very many containers in standard libraries (like, 10-20?), and hopefully most of their pretty-printers can be trivial one-liners, so they would be easy enough to add and maintain that incompatibility with other debuggers wouldn't be a big concern. Currently nnd doesn't have anything like that (e.g. there are no loops in the watch expression language), I don't have a good design for the language yet, not sure if I'll ever get around to it.(Btw, \"pretty-printers\" is not a good name for what I'm talking about; rather, it transforms a value into another value, e.g. an std::vector into a slice, or an unordered_map into an array of pairs, which is then printed using a normal non-customizable printer. The transformed value ~fully replaces the original value, so you can e.g. do array indexing on std::vector as if it was a slice: `v[42]`. This seems like a better way to do it than a literal pretty-printer that outputs a string.)What kind of cooperation from library authors would help with container recognition... The current recognizers are just looking for fields begin/end (pointers) or data/len (pointer and number), etc (see src/pretty.rs, though it's not very good code). So just use those names for fields and it should work :) . I'm not sure any more formal/bureaucratic contract is needed. But it would be easy for the recognizer to also check e.g. typedefs inside the struct (I guess most languages have something that translates to typedefs in debug info? at least C++ and Rust do). E.g. maybe a convention would say that if `typedef int THIS_IS_A_VECTOR` is present inside the struct then the struct should be shown as a vector even if it has additional unrecognized fields apart from begin/end/[capacity]; or `typedef int THIS_IS_NOT_A_CONTAINER` would make the debugger show the struct plainly even if has begin+end and nothing else. That's just off the top of my head, I haven't thought in the direction of adding markup to the code.A maintained collection of recognizers (in some new declarative language?) for containers in various versions of various libraries sure sounds nice at least in theory (then maybe I wouldn't've needed to do all the terrible things that I did in `src/pretty.rs`). But I don't want to maintain such a thing myself, and don't have useful thoughts on how to go about doing it. Except maybe this: nnd got a lot of mileage from very loose duck-typed matching; it doesn't just look for fields \"begin\" and \"end\", it also (1) strips field names to remove common suffixes and prefixes: \"_M_begin_\", \"__begin_\", \"c_begin\" are all matched as \"begin\", (2) unwraps struct if it has just one field: `foo._M_t._M_head_impl._M_whatever_other_nonsense._M_actual_data` becomes just `foo._M_actual_data`; this transformation alone is enough to remove the need for any custom pretty-printer for std::unique_ptr - it just unwraps into a plain pointer automatically. Tricks like this cut down the number of different recognizers required by a large factor, but maybe would occasionally produce false positives (\"pretty-print\" something that's not a container).(Dump of thoughts about the expression language, probably not very readable: The maximally ambitious version of the language would have something like: (1) compile to bytecode or machine code for fast conditional breakpoints, (2) be able to inject the expression bytecode+interpreter (or machine code) into the debuggee for super fast conditional breakpoints, and maybe for debuggee function calls along the way, (3) have two address spaces: debuggee memory and script memory, with pointers tagged with address space id either at runtime or at compile time, ideally both (at compile time for good typechecking and error messages, at runtime for being able to do something like `let elem = if container.empty {&dummy_element} else {container.start}`; or maybe the latter is not important in practice, and the address space id should just be part of the pointer type? idk; I guess the correct way to do it is to write lots of pretty-printers for real containers in an imaginary language and see what comes up), (4) some kind of template functions for pretty-printing, (5) templates not only by type, but also maybe by address space, by whether the value's address is known (e.g. a debuggee variable may live on the stack at one point in the program and in register in another part), by variable locations if they're compiled into the bytecode (e.g. same as in the previous pair of parentheses), (6) use the same type system for the scripting language and the debugged program's types, but without RAII etc (e.g. the script would be able to create an std::vector and assign its fields, but it would be a \"dead\" version of the struct, with no constructor and destructor), (7) but there's at least one simplification: the script is always short-lived, so script memory allocations can just use an arena and never deallocate, so the language doesn't need RAII, GC, or even defer, just malloc. The design space of languages with multiple address spaces and tagged pointers doesn't seem very explored, at least by me (should look for prior art), so it'll take a bunch of thinking and rewriting. Probably the maximally ambitious version is too complex, and it's better to choose some simpler set of requirements, but it's not clear which one. If you somehow understood any of that and have thoughts, lmk :) )\n \nreply",
      "\"what we mean by fast\"I cannot tell you how much respect I feel for you\n \nreply",
      "For those interested in writing a debugger:\nThere are a series of tutorials on how to write a debugger from scratch\nfor Windows x86-64 using Rust [1].\nAdditionally, there is a book titled \"Building a Debugger - Write a Native x64 Debugger From Scratch\" by Sy Brand [2].[^1]: https://www.timdbg.com/posts/writing-a-debugger-from-scratch...\n[^2]: https://nostarch.com/building-a-debugger\n \nreply",
      "I recently found a super cool article[1] which also gives a good overview of how a debugger works.[1]:https://keowu.re/posts/Writing-a-Windows-ARM64-Debugger-for-...\n \nreply",
      "Cool! I did not know about that book. Added to [1]. :-)--1: https://github.com/munificent/craftinginterpreters/issues/92...\n \nreply",
      "Nice! Thanks for sharing :-)Another book added to my To-Read list\n \nreply",
      "I don't work with anything that would need this, but I love TUIs so I checked it out and saw this bit:Operations that can't be instantaneous (loading debug info, searching for functions and types) should be reasonably efficient, multi-threaded, asynchronous, cancellable, and have progress bars.I wish this were more common, especially the progress bars thing.\n \nreply"
    ],
    "link": "https://github.com/al13n321/nnd",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        A debugger for Linux\n      A debugger for Linux. Partially inspired by RemedyBG.Mom, can we have RAD Debugger on Linux?\nNo, we have debugger at home.\nDebugger at home:Properties:What we mean by \"fast\":Limitations:Development status:Distributed as a single 6 MB executable file with no dependencies.\"Installation\":Or build from source:Run nnd --help for documentation.\n        A debugger for Linux\n      ",
    "summary": "In an audacious attempt to reinvent the wheel, a valiant coder gifts the open-source world with <em>nnd</em>, a debugger that bravely promises to be what GDB and LLDB apparently weren't enough of: a debugger you can actually <i>'debug'</i> with. The documentation sets new standards in ambiguity, assuring that \"Essential features are there,\" a phrase that sends users into a scavenger hunt in a codebase maze with no winner. Over in comment land, the propeller heads congregate to swap war stories of failed installation attempts and feature requests that read more like a wishlist to Santa than legitimate software feedback. Meanwhile, the original poster cheerfully pops in, effectually 'surprised Pikachu face' about their own project\u2019s listing, contributing mightily to the eternal circular discussion of what could be done if anyone actually understood what was already done."
  },
  {
    "title": "VVVVVV Source Code (github.com/terrycavanagh)",
    "points": 12,
    "submitter": "radeeyate",
    "submit_time": "2025-05-06T23:22:08 1746573728",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://github.com/TerryCavanagh/VVVVVV",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        The source code to VVVVVV! http://thelettervsixtim.es/\nThis is the source code to VVVVVV, the 2010 indie game by Terry Cavanagh, with music by Magnus P\u00e5lsson. You can read the announcement of the source code release on Terry's blog!The source code for the desktop version is in this folder.VVVVVV is still commerically available at thelettervsixtim.es if you'd like to support it, but you are completely free to compile the game for your own personal use. If you're interested in distributing a compiled version of the game, see LICENSE.md for more information.Discussion about VVVVVV updates mainly happens on the \"unofficial\" VVVVVV discord, in the vvvvvv-code channel.\n        The source code to VVVVVV! http://thelettervsixtim.es/\n",
    "summary": "**VVVVVV Source Code Release: A Critical Reflexive Journey into Absolute Nerdery**\n\nCongrats, everyone! \ud83c\udf89 Developer Terry Cavanagh just dropped the source code of VVVVVV like it\u2019s hot \u2013 which it isn't, given that it\u2019s basically a philanthropic sprinkle on the decade-old indie game pie. Keen spectators, armed with a mastery of MISSINGNO exploits rather than actual coding knowledge, flood the comments. They're ready to unleash their \"groundbreaking\" tweaks: implementing gravity-defying features that defy both physics <em>and</em> good taste. Meanwhile, the \"unofficial\" Discord channel morphs into both a bug report landfill and a bizarre tech support group where ASCII art meets existential dread. Remember - licensing is cool, but scrolling through walls of self-assured \u201cexpert\u201d diatribes is cooler. \ud83d\udd79\ufe0f\ud83d\udcbb"
  },
  {
    "title": "Show HN: Plexe \u2013 ML Models from a Prompt (github.com/plexe-ai)",
    "points": 85,
    "submitter": "vaibhavdubey97",
    "submit_time": "2025-05-06T15:38:04 1746545884",
    "num_comments": 41,
    "comments_url": "https://news.ycombinator.com/item?id=43906346",
    "comments": [
      "Awesome work.Only watched demo, but judging from the fact there are several agent-decided steps in the whole model generation process, I think it'd be useful for Plexe to ask the user in-between if they're happy with the plan for the next steps, so it's more interactive and not just a single, large one-shot.E.g. telling the user what features the model plans to use, and the user being able to request any changes before that step is executed.Also wanted to ask how you plan to scale to more advanced (case-specific) models? I see this as a quick and easy way to get the more trivial models working especially for less ML-experienced people, but am curious what would change for more complicated models or demanding users?\n \nreply",
      "Agree. We've designed a mechanism to enable any of the agents to ask for input from the user, but we haven't implemented it yet. Especially for more complex use cases, or use cases where the datasets are large and training runs are long, being able to interrupt (or guide) the agents' work would really help avoid \"wasted\" one-shot runs.Regarding more complicated models and demanding users, I think we'd need:1. More visibility into the training runs; log more metrics to MLFlow, visualise the state of the multi-agent system so the user knows \"who is doing what\", etc.\n2. Give the user more control over the process, both before the building starts and during. Let the user override decisions made by the agents. This will require the mechanism I mentioned for letting both the user and the agents send each other messages during the build process.\n3. Run model experiments in parallel. Currently the whole thing is \"single thread\", but with better parallelism (and potentially launching the training jobs on a separate Ray cluster, which we've started working on) you could throw more compute at the problem.I'm sure there are many more things that would help here, but these are the first that come to mind off the top of my head.What are your thoughts? Anything in particular that you think a demanding user would want/need?\n \nreply",
      "This is a really interesting idea! I'll be honest, it took me a minute to really get what it was doing. The GitHub page video doesn't play with any audio, so it's not clear what's happening.Once I watched the video, I think I have a better understanding. One thing I would like to see is more of a breakdown of how this solves a problem that just a big model itself wouldn't.\n \nreply",
      "Thank you!Yeah we rushed to create a \"Plexe in action\" video for our Readme. We'll put a link to the YouTube video on the Readme so it's easier.Using large generative models enables fast prototyping, but runs into several issues: generic LLMs have high latency and cost, and fine-tuning/distilling doesn\u2019t address the fundamental size issue. Given these pain points, we realized the solution isn\u2019t bigger generic models (fine-tuned or not), but rather automating the creation, deployment, and management of lightweight models built on domain-specific data. An LLM can detect if an email is malicious, but a classifier built specifically for detecting malicious emails is orders of magnitude smaller and more efficient. Plus, it's easier to retrain with more data.\n \nreply",
      "I like the idea of trying multiple solutions.Does it decide based on data if it should make its own ML model or fine-tune a relevant one?Also, does it detect issues with the training data? When I was doing NLP ML models before LLMs, the tasks that took all my time were related to data cleaning, not the training or choosing the right approach.\n \nreply",
      "Currently it decides whether to make its own model or fine-tune a relevant one based primarily on the problem description. The agent's ability to analyse the data when making decisions is pretty limited right now, and something we're currently working on (i.e. let the agent look at the data whenever relevant, etc).I guess that kind of answers your second question, too: it does not currently detect issues with the training data. But it will after the next few pull requests we have lined up!And yes, completely agree about data cleaning vs. model building. We started from model building as that's the \"easier\" problem, but our aim is to add more agents to the system to also handle reviewing the data, reasoning about it, creating feature engineering jobs, etc.\n \nreply",
      "I don't want to hate, what you built is really cool and should save time in a data scientist's workflow, but... we did this. It won't \"automate most of the ML lifecycle.\" Back in ~2018 \"autoML\" was all the rage. It failed because creating boilerplate and training models are not the hard parts of ML. The hard parts are evaluating data quality, seeking out new data, designing features, making appropriate choices to prevent leakage, designing evaluation appropriate to the business problem, and knowing how this will all interact with the model design choices.\n \nreply",
      "Yes, this is the issue. In any reasonably-sized enterprise you\u2019re not going to have a clean CSV to plug in to a model generator. You\u2019re either going to have 1) 50 different excel spreadsheets to wrangle and combine somehow or 2) 50+ terabytes of messy logs to process.Creating something that can grok MNIST is certainly cool, but it\u2019s kind of the equivalent of saying leetcode is equivalent to software engineering.Second, and more practically speaking, you are automating (what I think of as) the most fun part of ML: the creativity of framing a problem and designing a model to solve that problem.\n \nreply",
      "Agree completely. We built Plexe with that first scenario in mind - the messy spreadsheet problem that's so common in enterprise. You can connect multiple data sources, and Plexe will identify what it needs based on the problem description. We're also gradually developing support for handling terabyte-scale data, though we're not there yet. We started by validating our approach on well-defined problems with clean datasets, but we've been systematically adding capabilities to handle increasingly complex scenarios.On your second point about automating the \"fun part\", we see Plexe as amplifying that creativity rather than automating it. We're trying to make it easier to design the experiments and evaluating results. But would love to hear your feedback on this!\n \nreply",
      "Hey, one of the authors here! I completely agree with your comment. Training ML models on a clean dataset is the \"easy\" and fun part of an ML engineer's job.While we do think our approach might have some advantages compared to \"2018-style\" AutoML (more flexibility, easier to use, potentially more intelligence solution space exploration), we know it suffers from the issue you highlighted. For the time being, this is aimed primarily at engineers who don't have ML expertise: someone who understands the business context, knows how to build data processing pipelines and web services, but might not know how to build the models.Our next focus area is trying to apply the same agentic approach to the \"data exploration\" and \"feature ETL engineering\" part of the ML project lifecycle. Think a \"data analyst agent\" or \"data engineering agent\", with the ability to run and deploy feature processing jobs. I know it's a grand vision, and it won't happen overnight, but it's what we'd like to accomplish!Would love to hear your thoughts :)\n \nreply"
    ],
    "link": "https://github.com/plexe-ai/plexe",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        \u2728 Build a machine learning model from a prompt\n      \nBuild machine learning models using natural language.Quickstart |\nFeatures |\nInstallation |\nDocumentationplexe lets you create machine learning models by describing them in plain language. Simply explain what you want,\nand the AI-powered system builds a fully functional model through an automated agentic approach. Also available as a\nmanaged cloud service.You can use plexe as a Python library to build and train machine learning models:Define models using plain English descriptions:The system uses a team of specialized AI agents to:Build complete models with a single method call:Plexe supports distributed model training and evaluation with Ray for faster parallel processing:Ray distributes your workload across available CPU cores, significantly speeding up model generation and eva",
    "summary": "**Show HN: Plexe \u2013 ML Models from a Prompt (github.com/plexe-ai)**\n\nWelcome to the future of AI, where typing \"make me a model\" into a GitHub repo can replace years of education in machine learning. Plexe - the latest attempt to jump on the automation bandwagon - promises to turn plain English into complex models with the ease of making microwave popcorn. Commenters, oscillating between naive optimism and crushing reality, are particularly jazzed about turning their copious, messy spreadsheets into sleek AI without understanding the monstrous complexity of the task. Here's to hoping Plexe can magically transform verbose documentation enthusiasts into capable machine learning engineers, because as we all know, reading a manual is exactly the same as earning a Ph.D. in Computer Science. \ud83d\ude80\ud83d\ude43"
  }
]