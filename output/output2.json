[
  {
    "title": "On the Nature of Time (stephenwolfram.com)",
    "points": 100,
    "submitter": "iamwil",
    "submit_time": "2024-10-08T22:42:01.000000Z",
    "num_comments": 46,
    "comments_url": "https://news.ycombinator.com/item?id=41782534",
    "comments": [
      "Do physicists think time actually exists? I wonder if someone has reasoned that time is an accounting method that humans have developed to make sense of their experienced change of systems.Wolfram uses the words progression and computation a lot in his essay, but there\u2019s an implicit bias there of assuming a process is deterministic, or has some state it\u2019s driving towards. But none of these \u201cprogressions\u201d mean anything, I think. It seems they are simply reactions subject to thermodynamics.If no one observed these system changes, then the trends, patterns, and periodicity of these systems would just be a consequence of physics. It seems what we call \u201ctime\u201d is more the accumulation of an effect rather than a separate aspect of physics.For example, I wonder what happens time in physics simulations is replaced by a measure of effect amplitude. I don\u2019t know, tbh, I am not a physicist so maybe this is all na\u00efve and nonsense.\n \nreply",
      "I think it\u2019s really interesting to see the similarities between what Wolfram is saying and the work of Julian Barbour on time being an emergent property. Both suggest a similar underlying ontology for the universe: a timeless, all-encompassing realm containing all possible states / configurations of everything. But what\u2019s really fascinating is that they reach this conclusion through different implementations of that same interface. Barbour talks about a static geometric landscape where time emerges objectively from the relational (I won\u2019t say causal) structures between configurations, independent of any observer. On the other hand, Wolfram\u2019s idea of the Ruliad is that there\u2019s a timeless computational structure, but time emerges due to our computational limitations as observers navigating this space.They\u2019ve both converged on a timeless \u201cfoundation\u201d for reality, but they\u2019re completely opposite in how they explain the emergence of time: objective geometry, vs. subjective computational experience\n \nreply",
      "So you are saying there is a version of me that is king of the universe in some timeline?\n \nreply",
      "I wrote up more or less the same idea ten years ago, but in what I think is a more accessible presentation:https://blog.rongarret.info/2014/10/parallel-universes-and-a...\n \nreply",
      "I have read and appreciated your writings going back to the comp.lang.lisp days, but a blog post that starts with \u201cif you haven\u2019t read the previous post, please do before reading the rest of this one\u201d is not what I would consider accessible. \u2026and that previous post then asks the reader to first read a paper or watch a video before proceeding. While a decade later than what you wrote, Wolfram\u2019s article is much more self contained and complete.\n \nreply",
      "Thank you so much for this.Whenever people criticize Wolfram the comeback is often, he\u2019s just trying to discuss big ideas that mainstream science won\u2019t talk about. Of course that\u2019s not the reason for the criticism at all and I think your work here shows that it\u2019s totally fine to speculate and get a little philosophical. The results can be interesting and thought provoking.There\u2019s a difference between big ideas and grandiosity. It also shows big ideas can stay scientifically grounded and don\u2019t require making up corny terminology (Ruliad? lol).\n \nreply",
      "I think he's a quack trying to torture an explanation of the universe out of his pet theory that uses a lot of words to say simple things but doesn't predict anything. If \"time is what progresses when one applies computational rules\" then how is the order in which the rules are applied defined in the first place?Computational irreducibility is a neat idea but i'm not sure its novel or something that explains the entire universe. My basic intro course on differential equations taught us that the vast majority of them cannot be solved analytically, they have to be approximated. I don't know if the irreducibility idea is anything fundamentally different than saying some problems are hard, whether its non analytical equations or NP hard problems.\n \nreply",
      "I think you\u2019re slightly misunderstanding his concept of computational irreducibility. It\u2019s more like the halting problem than anything: basically he\u2019s saying that dynamic systems can\u2019t be reduced to an equation that is easier to calculate and so you just have to simulate the entire system, run it, and watch what happens. This means we can\u2019t ever predict the future within these systems.\n \nreply",
      "Thought experiment on the nature of reality:- In a much larger universe, write down in a log book every event to every particle at every instant, from the Big Bang to the restaurant.- Put it on the fireplace mantle and leave it there.This is basically a log of a simulation. It exists in much the same way as an ongoing simulation would, except that its time dimension isn't shared with the simulating universe. But every observer within has had the same observations as if it did.\n \nreply",
      "Ok but the act of writing it down would always take longer than the actual unfolding of the universe itself. Just like the halting problem, we can\u2019t skip ahead at any point and we have no idea what will come next.\n \nreply"
    ],
    "link": "https://writings.stephenwolfram.com/2024/10/on-the-nature-of-time/",
    "first_paragraph": "Time is a central feature of human experience. But what actually is it? In traditional scientific accounts it\u2019s often represented as some kind of coordinate much like space (though a coordinate that for some reason is always systematically increasing for us). But while this may be a useful mathematical description, it\u2019s not telling us anything about what time in a sense \u201cintrinsically is\u201d. We get closer as soon as we start thinking in computational terms. Because then it\u2019s natural for us to think of successive states of the world as being computed one from the last by the progressive application of some computational rule. And this suggests that we can identify the progress of time with the \u201cprogressive doing of  computation by the universe\u201d. But does this just mean that we are replacing a \u201ctime coordinate\u201d with a \u201ccomputational step count\u201d? No. Because of the phenomenon of computational irreducibility. With the traditional mathematical idea of a time coordinate one typically imagines ",
    "summary": "<h1>The Timeless Art of Making Time More Complicated</h1>\n\nStephen Wolfram, modern alchemist of the digital era, tackles the enigma of time by essentially repainting the Mona Lisa with matrix algebra. \ud83d\udcbb\u23f3 In his grandiloquence, he morphs simple seconds into cosmic computation dramas, much to the thrill of those who religiously follow his digital breadcrumbs. The comment section, a tragicomic playset for amateur and half-baked physicists, turns into a Battle Royale of theories, with some claiming temporal monarchy and others waving diplomas as if they were time-travel tickets. Amidst overwritten explanations and misunderstood concepts, it\u2019s clear we\u2019re all still just watching the clock tick, none the wiser but infinitely more entertained. \ud83e\udd37\u200d\u2642\ufe0f\ud83d\udd70\ufe0f"
  },
  {
    "title": "My first game with Carimbo, my homemade engine (nullonerror.org)",
    "points": 172,
    "submitter": "delduca",
    "submit_time": "2024-10-08T17:13:16.000000Z",
    "num_comments": 87,
    "comments_url": "https://news.ycombinator.com/item?id=41779519",
    "comments": [
      "I think the best advice for making your own game is: do what you're most excited to do. Do you spend time thinking about making your own game engine? Then start making it. Be willing to change course later if it's too hard, your time will not be wasted.I got into gamedev by messing around with making my own engine, mostly focusing on low level graphics APIs, and that knowledge transferred well when I switched to a professional game engine. I knew about shaders and such and knew I was somewhat prepared to alter the engine I was using if needed.Or, the other way, if you start making a game in an engine and you hate it, your efforts are not wasted. The truth is like 10 or 20 thousand lines of game logic can make a lot of games, and that's really not much code to port to your own game engine compared to the rest of the engine. All the art and other assets can be ported too. Plus, if you know a professional game engine you can use it for tooling or get some good architecture ideas to use in your own engine.So, just get moving with whatever excites you most and be willing to change course.\n \nreply",
      "This gets at something I\u2019ve thought about a lot as a software dev.Making a game engine is a very concrete task, in that you can map out the steps and many of them are well-defined.On the other hand \u201cmake a good game\u201d kinda isn\u2019t.  Which I think is a big reason why coders gravitate towards the \u201cstart making an engine\u201d route and then fall down it :)The developers I admire and look up to a lot are the artists that fell into programming.  I think they\u2019re the best when it comes to being a successful lone / indie dev for games.  Everyone notices art, but you don\u2019t notice programming unless it goes wrong or you know the tech behind what you\u2019re looking at.\n \nreply",
      "I know this will get downvoted. I fully agree that if you want to make a game-engine then make one. It's fun! On the other hand, if you want to make a game, use an existing engine.Analogies: I want to write a novel so I'll first build a typewriter. I want to film a movie so I'll first build a camera and digital editing software. I want to cook food so I'll first build a stove, pots, pans, and knives.In all those other examples it hopefully clear, you just buy the tools you need and then make the thing the tools allow you to make. At this point, the same is true in games.If you like making a game engine than make one. Just like if you like making knives or cameras then make them. But, making a knife is not cooking, making a camera is not making a movie, and making a game-engine is not making a game.I also want to add, making a game-engine is easy compared to making a game. Why? Because all the things to you need are well known. You need a 2d renderer (UI), a 3d renderer (assuming you need 3d), image loaders, model loaders, sound players, music players, keyboard input, joypad input, entity systems. Etc. You build them and it seems like you're making progress and if your goal is to make a game-engine, then you are making progress. But, if your goal is to make game, you're mostly likely fooling yourself that you're making progress. Again, back to the analogies, if you're making a knife, you aren't making progress on cooking, your making progress on making a knife. If your making a camera, you aren't making progress on making a movie, you're making progress on making a camera.What makes making a game harder than a game-engine is the list things to do for a game is unknown.\n \nreply",
      "Another way to think of this is not making a \u201cgame engine\u201d but just \u201cmaking a game\u201d. Get rid of all the generic stuff and use some common patterns that fit the game exactly. No need to over-abstract!\n \nreply",
      "That\u2019s not what the person you replied to is saying though\u2026 They\u2019re saying just use an existing engine, and if you really need to, you can always write your own engine later if you really need it.\n \nreply",
      "Yeah I\u2019m just saying in general. I probably shouldn\u2019t have replied to anyone in particular\u2014just jumping in the conversation. =)\n \nreply",
      "> The truth is like 10 or 20 thousand lines of game logic can make a lot of games, and that's really not much code to port to your own game engine compared to the rest of the engine.I intuitively want to agree, but on the other hand I've also seen many, many hobby/indie/etc projects deadlock when they switched engines.  Or even engine versions (Unreal 4 -> 5).\n \nreply",
      "Probably because most of their logic is locked in \"no code\" systems, such as Unreal's blueprint or as Unity component data.A well- and hand-written game is easily portable, see the original Doom, where separation between core logic and side effects, such as drawing in a screen, is clear. Actually, this is a good architectural model for all types of software.\n \nreply",
      "Granted, but I don't think there are many people / teams who are capable of writing their own engine and then fail to port a few thousand lines of game logic. So I stand by my advice that if you decide later to write your own engine, porting the logic will not be the difficult part.\n \nreply",
      "I find that this is the best advice for life, not just making your own games. Do what you're most excited to do :)\n \nreply"
    ],
    "link": "https://nullonerror.org/2024/10/08/my-first-game-with-carimbo/",
    "first_paragraph": "",
    "summary": "**Homemade Game Engine Blues: Reinventing the Wheel for Fun and No Profit**\n\nIn a shocking turn of events, an ambitious hobbyist at nullonerror.org decides the best method to make a game is to chain themselves to the desk and code a game engine from scratch. Commenters chime in with pearls of wisdom, like comparing this act to crafting a typewriter before writing a novel. Groundbreaking! One particularly enthusiastic user suggests making games should really be about throwing out the \"generic stuff\" because who needs standards when you can make bespoke game-logic spaghetti? Meanwhile, a lone voice in the comment wilderness recommends just sticking with existing engines, but what\u2019s the craft in that? Stay tuned as these digital Sisyphuses continue pushing pixels uphill, convinced their homemade engine rocks will one day stop rolling back down. \ud83d\udcbb\ud83c\udfae\ud83d\ude05"
  },
  {
    "title": "Show HN: Winamp and other media players, rebuilt for the web with Web Components (player.style)",
    "points": 158,
    "submitter": "Heff",
    "submit_time": "2024-10-08T18:27:46.000000Z",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41780297",
    "comments": [
      "I love the idea behind this and thank you for making it MIT license.I just happen to be working on a media app (a video editor) and previously I have built a few video players (in both Flash and HTML/JS). We actually tried to use web components on one player (back in 2015-ish) and they were a constant pain that we eventually discarded in favor of plain old JavaScript. Strangely enough, for my current media app I've been using web components (e.g. a video editor timeline) and so far it is going very well. I'm not sure what changed or if it is just the case that the slow advancement of the web has brought compatibility far enough to make it viable.I've just skimmed the Media Chrome docs and have only taken a quick glance at the github repo, but I like your design principles and architecture notes. My main concerns about adopting something like this (especially since I have a lot of experience building exactly stuff like this from scratch) are extensibility (e.g. how hard would it be to modify my timeline component to fit into the MediaController paradigm) and file size. One advantage of doing everything oneself is that you have everything you need and nothing more. I'm sure Media Chrome has a lot of stuff I just won't need (but someone else will) - the questions is how much bloat I am taking on for things I won't ever use. And not just components I won't use, but unused features of the components I will use. Sometimes it is just a matter of existing unnecessary functionality getting in the way of a lower-level kind of extensibility.As an aside, your `media-elements` repo [1] does not have a license file. I see in the package.json that the elements are also MIT but having an explicit LICENSE file is always appreciated.That being said, this is a very tempting library. At the least I will probably steal the idea to wrap my components in a media-controller like element since I've been using the containing page so far to stich my elements together and I wanted a nicer abstraction.1. https://github.com/muxinc/media-elements\n \nreply",
      "Back around the same 2015 time frame I think I was being very optimistic and stubborn when it came to Web Components. I very much wanted them to work, but didn't really get anything into production until around 2020. There was a v2 of the web component spec between then and now, but I'm not an expert in what changed. Now I'm seeing web components everywhere, especially in media players. i.e. Apple's web player.I'm glad you like the controller architecture. The original version just had every element pointing directly to the media element, and the controller cleaned up a lot. Highly recommend it, at least compared to what I was doing.A video editor UI I think is natural extension of the Media Chrome suite. I'd love to hear what else might be helpful there if you want to post an issue in the repo.I can deeply empathize with your hesitation to adopt something like media chrome based on future flexibility and size. I'll give you 3 points that would sell me on it. :)\n  1. You can only include the UI components you need, which is at least a major difference from other web video players when it comes to size.\n  2. We have some of the most experienced player devs working on it, including for things like accessibility and upcoming internationalization.\n  3. We're working hard to make it super configurable between slots, css parts, and css vars.Of course we'll never beat the file size of completely custom software, but I feel like it'll come pretty close once all the basic features are built in.Thanks for the heads up on the elements license!\n \nreply",
      "The Mux marketing strategy is brilliant.Take over or create new open source projects so that every developer comes across your company in the search for a video package.Another example I noticed recently is https://github.com/cookpete/react-player\n \nreply",
      "Thanks! \"Brilliant\" might be giving us too much credit. We're mostly just paying attention to how devs are using video and trying to solve problems in that space. Next-video.dev is another example I'm proud of.Some of what you're seeing on the open source player front is that we already kind of have to support those projects anyway. We're player agnostic, so our customers use a lot of open source, including projects that aren't actively being maintained.I think we're benefitting right now from being one of a very few dev-focused video companies that's also actively contributing to open source.\n \nreply",
      "There's some weirdness around focus going on here, hopefully this comes across as constructive criticism. All of them have the same problem:When you click on the video itself, the left and right arrow keys work to scrub the video backwards and forwards. Up and down do nothing.When you click on the scrubber, the left and right arrow keys stop working. Also, the up and down arrow keys start working to rewind/advance the video a different amount of time.If you click in void space, e.g. on the Winamp example or the blue bar that looks like windows 98 on the Reelplay example - both of these controls stop working, as well as space to play/pause.Latest chrome on macos.\n \nreply",
      "Good feedback, thanks! There's a related issue in the media chrome repo here: https://github.com/muxinc/media-chrome/issues/957The situation is a little complex with \"hot keys\" for controlling the video in general (after clicking on the video), accessibility controls for each component, and then general accessibility expectations for the whole page. For example, should we capture the up and down arrows to always control volume when the player is in focus, or should we not do that because people expect that for scrolling the page.All that said, we definitely have some iteration ahead of us on this front so thanks again for the input.\n \nreply",
      "I love it. Just one kindness: could you add subtitle tracks to the wizard?\nThey are quite hard to add for now, since there is no documentation and media-chrome seems to use a different synthax.\n \nreply",
      "Thank you for the feedback! I'm not quite sure I'm following, though. By the wizard do you mean the framework/element picker within a theme? Would you want the wizard there to be something where you can put in a URL for a subtitle track and we'll add it to the generated tag?If you're seeing something weird around subtitle syntax then there's probably a documentation issue somewhere (or I'm misunderstanding your question). Subtitles themselves should work with a standard `<track>` in the media element, and the only other place we touch them is via the captions button/menu to toggle those tracks.\n \nreply",
      "About the wizard, it would help if you just had a \"Subtitles\" checkbox somewhere which then adds a blank <track> line to the script.As for the synthax question, my point is that your script looks like this  <video\n    slot=\"media\"\n    src=\"https://stream.mux.com/fXNzVtmtWuyz00xnSrJg4OJH6PyNo6D02UzmgeKGkP5YQ/low.mp4\"\n    playsinline\n    crossorigin\n  ></video>\n\nAnd media chorme looks like this<video slot=\"media\" src=\"./video.mp4\" crossOrigin playsInline><track label=\"English\" kind=\"captions\" srcLang=\"en\" src=\"./captions.vtt\"></track><track label=\"thumbnails\" default kind=\"metadata\" src=\"./thumbnails.vtt\"></track></video>And I just don't know how to interpolate the two(Edit, checking further, I did manage to mix the two, and I can play subtitles over your demo video, but not over mine so I guess a foolproof sample in the wizard is probably needed :D )\n \nreply",
      "Ahh ok, I see your point. I'll open an issue on the repo to track this, makes sense to me! Either way, helps to really drive the point home that \"it's just a normal media element in a slot.\"\n \nreply"
    ],
    "link": "https://player.style",
    "first_paragraph": "Find a player theme you lovePick your player and app frameworkCopy, paste, and you're doneCustomize any detail of the player UI using just HTML and CSSA sleek and modern theme lovingly named after our favorite SF TV antenna, which is neither sleek nor modern.Instaplay is a mobile-first theme inspired by playback experiences you can find in popular social media apps.A sleek and modern theme lovingly named after our favorite SF TV antenna, which is neither sleek nor modern.Everything but the big red N and long bus rides to Los Gatos.Reelplay is a nostalgic media player inspired by media players of a bygone eraA fresh take on the classic Vimeo player design.An homage to the modern, ubiquitous YouTube player. Recreated with web components, or at least as close as we could get.A slick, minimal audio player theme made with Tailwind CSS.A media player theme created for Demuxed 2022.This theme optimizes for shorter content that doesn't need the robust playback controls that longer content typi",
    "summary": "**Nostalgia as a Service: Web Edition**\n\nHacker News is once again at the forefront of unasked-for nostalgia, unveiling a repository where you can Frankenweenie old media players back to life, all with the glory of web components. Rejoice as you can now slap a \"modern\" VLC face onto your browser-only device, because everyone knows what the web truly lacks is the hundredth iteration of a media player skin. Commenters interchange their life stories of past heroic builds, weaving tales of brave encounters with early web components, and now, like an old war hero, proclaiming how they've tamed the beast in their media app nooks. Most are just excited to contribute to <em>another</em> open-source project they'll abandon when the next shiny thing comes along. Who needs streamlined content consumption when you can spend hours ensuring Winamp can whip a llama's ass on your HTML5 page? \ud83d\ude44"
  },
  {
    "title": "Differential Transformer (arxiv.org)",
    "points": 419,
    "submitter": "weirdcat",
    "submit_time": "2024-10-08T11:54:30.000000Z",
    "num_comments": 149,
    "comments_url": "https://news.ycombinator.com/item?id=41776324",
    "comments": [
      "I feel like I'm missing a key insight here. I understand the problem that regular softmax attention struggles to approach assigning zero attention to irrelevant stuff. And I get that having this subtraction formula makes it possible to assign exactly (or near) zero attention weight without having crazy outlier activations. But it seems like it also makes it very easy to have negative attention weight (which is equivalent to having positive attention weight on the negation of your value vectors). Intuitively, it just feels like a difficult balancing act to keep all the stuff you don't care about so close to zero.But Figure 1 clearly shows that it works, so I don't doubt that it is in fact possible. I'm just struggling to build a picture of how exactly the network accomplishes this.\n \nreply",
      "Regular softmax (and attention) has an error in it.softmax should be exp()/1+\u2211exp()Notice the 1 added to the denominator.The difference is at the negative limit, softmax can be 0, instead of some epsilon. The same could be done by adding an extra zero value in x.Downside is, you have to retrain your model from scratch to fix this.\n \nreply",
      "I've tried that in a small transformer that I trained from scratch and it didn't really make any difference. I also made a version where I made this trainable somehow, probably by replacing the 1 with a constant associated with the layer, and that didn't make any difference either.I didn't follow Miller's proposal quite as he wrote it though and I put the mechanism in all the layers rather than avoiding it at the end.My test doesn't absolutely rule out usefulness-- there's always different ways of applying something, but I saw no  indication of it.\n \nreply",
      "I guess the next step is to see if you're getting those mega activations as he describes.A/B test the two models and compare?Would be interesting to see if these activations only show up on larger models, or they're some relation to model size.\n \nreply",
      "> softmax should be exp()/1+\u2211exp()\n\nYou referring to Miller's blogpost?[0] There's not an error in attention. Adding the +1 actually makes it not attention because you no longer generate a probability distribution[1]. There's nothing really preventing attention to have a zero in any of the entries, the thing is that you probably won't get -inf (very large negative number) inside inner product and you're going to have a difficult time updating those weights via gradient descent.I've also tested it on many networks and different types of attention and I've yet to see a meaningful improvement (or even an improvement), even in generalization.It really is the training method...As to the paper, I'm also still at a big lost and honestly, if reviewing could not accept it. The results look good, but I can't tell why and there's some \"black magic\" going on here.  - Figure 3 has \"Transformer\" and doesn't specify. Is this StableLM-3B-4E1T?\n    - What fucking dataset is this on? Stable has a WandB link[2] for that project and I don't see any experiment with similar (presumably entropy?) loss values (come on... this is fucking research... label your fucking graphs...)\n  - Where the fuck is the ablation? (Yes, I saw Fig 6 and Sec 3.8)\n    - How do I know that (assuming this is Stable) that the difference isn't just hyperparemeters? Or worse, GPUs! (yes, number of GPUs can change results due to sharding and this changing the statistics)\n    - How do I know it isn't down to 1k warmup steps instead of 5k?\n    - What about hidden size, layers, heads, or FFN size? Stable has 32/2560/32/? and this has 28/3072/12/8192 (these all will mess with sharding statistics too). Is the head dimension the same?\n    - How do I know it isn't down to the tokenizer?\n  - What is this magic? `0.8 - 0.6 * math.exp(-0.3 * depth)`\n    - Was this learned? Hand picked? This is a huge factor\n    - Any information about the learned parameters? Their final values? Trajectories? \n  - The code does not seem to be the same as whats in the algos...\n\nObviously they improved something, but there is nothing in the paper that is convincing me that it is the differential attention. There are too many parameters at play and how am I supposed to know that the difference is by the thing they are proposing. And more importantly, how much it is improved by that specific thing and not by other things.  [0] https://www.evanmiller.org/attention-is-off-by-one.html\n\n  [1] This is a bit convoluted but without this condition many \"alternative forms\" you see would be equivalent to other architectures like linear layers or gated units. Term is not well defined, but this really appears to be the only agreed upon aspect, even if only implicitly stated. This is a much longer conversation though. \n\n  [2] https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo\n\n  [2.1] The config: https://github.com/Stability-AI/StableLM/blob/main/configs/stablelm-3b-4e1t.yml\n \nreply",
      "I feel like that blogpost was almost just ragebait for ai researchers. It goes between calling not including the +1 an error (which to me implies it would improve training losses, which it doesn't really https://news.ycombinator.com/item?id=36854613) and saying possibly it could help with some types of quantization (which could very well be true but is a much weaker statement) and the author provides basically no evidence for either.\n \nreply",
      "It's the stereotypical computer scientist who thinks they know something others don't and don't feel the need to prove their claim. Specifically when it disagrees with experts. And unsurprisingly it's been something others have already investigated and even written about. Definitely not all CS people, but it is a stereotype many other fields believe.I know he's an economist btw. I was also surprised he got a job at anthropic a few months after. I wonder if they're related.\n \nreply",
      ">I'm just struggling to build a picture of how exactly the network accomplishes this.I mean, intuitively it would be trivial for the model to just optimise lambda to zero during training. Then you essentially have built a vanilla transformer with an overcomplicated parameter pruning mechanism. Pruning is already pretty well established in the literature as something that works surprisingly good for reducing parameter counts up to (hold on to your papers)... about 40%. In practice the model probably doesn't work exactly like that, but I wouldn't be surprised if it just approximates the normal transformer in the end anyways.\n \nreply",
      "negative values can enhance the expressibility\n \nreply",
      "doubt is the seed of reason\n \nreply"
    ],
    "link": "https://arxiv.org/abs/2410.05258",
    "first_paragraph": "Help | Advanced SearcharXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\narXiv Operational Status \n                    Get status notifications via\n                    email\n                    or slack\n",
    "summary": "In the great halls of arXiv, where complexity meanders through the corridors of academia, a mystic manuscript titled \"Differential Transformer\" drops, sending *shivers* across the spines of those (all six!) who can decipher its sacred glyphs. One brave soul, armed with the sacred knowledge of the \"regular softmax attention,\" ponders the alchemical secrets locked within these ethereal pages, only to find themselves lost in a labyrinth of negative weights and near-zero attention spans. The commenters, equally befuddled but eternally optimistic, embark on a noble quest to transform confusion into \"constructive feedback,\" arguing fervently about the mystical power of a plus one in a denominator. Amid the cryptographic chorus, a revelation unfolds\u2014beyond the shadows of softmax, in the depths of tortured tensor math, lies the elusive insight: maybe, just maybe, this is merely another transformer wearing a fancy differential hat."
  },
  {
    "title": "Who died and left the US $7B? (sherwood.news)",
    "points": 132,
    "submitter": "jsnell",
    "submit_time": "2024-10-08T18:58:26.000000Z",
    "num_comments": 99,
    "comments_url": "https://news.ycombinator.com/item?id=41780569",
    "comments": [
      "Matt Levine touched on this briefly today, and I liked his two cents:>It\u2019s kind of cool? Like you could imagine a hierarchy, in roughly ascending order of wealth:>Too poor to pay taxes.>Rich enough to pay taxes.>Rich enough to not pay taxes.>Rich enough to not even bother with not paying taxes.\n \nreply",
      "Pretty obscene that somebody could have so much wealth that $7,000,000,000 is just the tax bill. Also weird that it's framed as a \"gift.\"\n \nreply",
      "The article highlights that it\u2019s not actually that hard for the ultra-wealthy to avoid a massive estate tax bill through proper tax planning and investment strategies. What\u2019s striking here is that this individual wasn\u2019t even the richest person to ever die, yet he paid the largest estate tax in history, likely by choice.\n \nreply",
      "Interestingly a lot of the larger philanthropic organizations are just as administration heavy as the US government and suffer from the same mission creep and the same obfuscated, bureaucratic decision making process, etc.  Not to mention the leadership is often richly compensated (i.e. $1M in salary) and non-elected.In fact we should probably celebrate gifts to the US government more than we do.\n \nreply",
      "My hunch is that taxes are the most efficient 'charity', even with the bloat, and everyone's too busy sniffing farts in their corner to see it.\n \nreply",
      "Why is that obscene? Presumably the person created something very valuable to the world. Sure there are zero-sum ways to generate wealth (e.g. suing people, theft, front running trades) but generally that kind of wealth comes from actually generating something that people value.\n \nreply",
      "The word \u201cPresumably\u201d is doing a lot of heavy lifting there. That Econ101 justification is harder and harder to keep up as you learn more about both economics and the real world.For detailed counter arguments, see Branko Milanovi\u0107 Global inequality: A New Approach for the Age of Globalization, James Kwak Economism: Bad Economics and the Rise of Inequality, Walt Bogdanich & Michael Forsythe When McKinsey Comes to Town: The Hidden Influence of the World's Most Powerful Consulting Firm, and many other books.\n \nreply",
      "The article says that he made it by stock trading. It is, at best, difficult to articulate how that could be creating value rather than capturing it. Many of the world's billionaires made their money that way.Doing something positive-sum is a way to become a billionaire, but many people are very handsomely paid to ensure that their clients are on the good side of zero-sum transactions.\n \nreply",
      "He did it through offering a _money management_ service to large institutions and wealthy individuals.That money management service includes, amongst other things, picking stocks. But that's the tactical \"what\". The value-added element is that he preserved and grew that wealth instead. It turns out that is hard to do, and is indeed a positive sum service to customers. They get to relax _and_ make money. Great outcome.\n \nreply",
      "Taking money away from mismanaged companies, and giving it to well-managed ones, is a net positive of stock trading. Another net positive of stock trading is making buyers and sellers available all day, for anything on the market.\n \nreply"
    ],
    "link": "https://sherwood.news/power/who-died-and-left-the-us-7-billion-fayez-sarofim/",
    "first_paragraph": "Billionaires are like black holes. We deduce their existence from the fundamental laws of capitalism, see their gravity pull politics into their orbit, even detect signals of their existence in the public markets.Determining exactly how rich anyone really is, though, can be a fool\u2019s errand. The most profound revelation in Thomas Piketty\u2019s era-defining \u201cCapital in the Twenty-First Century\u201d wasn\u2019t that we were experiencing Gilded Age levels of inequality, but his argument that Forbes\u2019 list of the ultrawealthy isn\u2019t accurate \u2014 and we don\u2019t really know the billionaires living among us, or the extent of their wealth.Last year, observers with the economic equivalent of a radio telescope detected a radiating anomaly on the February 28, 2023, daily balance sheet of the US Treasury Department: a $7 billion estate- and gift-tax payment.\u00a0John Ricco, now an analyst at Yale University\u2019s Budget Lab, first spotted the huge receipt while trying to answer a somewhat macabre question: how did senior-cit",
    "summary": "In a stunning display of capitalist necromancy, sherwood.news reveals that a secretive billionaire ghost dropped a cool $7 billion into Uncle Sam's begging bowl. Commenters leap into action, flexing their economics degrees from the University of Twitter, categorizing this cash splash between \"obscene\" and \"philanthropic,\" while completely missing the essence of how stock trading is essentially professional coin flipping. One user, attempting profundity, declares taxes as the top-tier charity, presumably while drafting their Nobel Prize acceptance speech in economic sciences. In the cacophony, the word \"presumably\" is saddled with so much work it's contemplating an early retirement. Meanwhile, the rest of us ponder if we're just too poor or clever enough yet not to pay taxes. \ud83e\udd11\ud83d\udcb8\ud83d\udc7b"
  },
  {
    "title": "Rabbit hole: stumbling across two Portuguese punched cards (jgc.org)",
    "points": 102,
    "submitter": "jgrahamc",
    "submit_time": "2024-10-08T17:19:18.000000Z",
    "num_comments": 31,
    "comments_url": "https://news.ycombinator.com/item?id=41779576",
    "comments": [
      "Found a reference to ENIASA - Instituto de Inform\u00e1tica de Engenharia SARL (computer science engeneering). Rereading your post, I'm not entirely sure if it was just an academic publishing from maybe the same group or if a new branch for computers derived from the mecanograph educational offers. Curious use of ordenador istead of computador as it is nowadays, makes me wonder if it was an early adoption of the term computer.It was submitted for registration and approved in 1970, according to Di\u00e1rio da Rep\u00fablica (similar to Federal Register in the US): https://files.dre.pt/gratuitos/3s/1970/09/1970d210s000.pdf , page 4, line 82 of that table. Or here: https://i.imgur.com/GyKPamu.png\n \nreply",
      "It's still \"ordenador\" in Spain and \"ordinateur\" in French. Interesting that we moved forward to computer over the years.\n \nreply",
      "Is this because these early computers were more often used to keep tabs and sort things (put things in order) rather than merely compute things?(I'm aware that in order to perform those tasks the processing unit will also have to perform arithmetic operations)\n \nreply",
      "Orden in Spanish means both command (mandate, instruction) and order (as from sort).\n \nreply",
      "Yeah, I found that too. But that's all I found.\n \nreply",
      "Read my comment below about the french language domination\n \nreply",
      "The book has a picture of the IBM 2321 Data Cell Drive, 1964 to 1975.[1]\nThat's an exotic peripheral for the original IBM System/360, a tape strip library. Before disks got big, there were various mechanical kludges to select storage media from a library and move them to a read/write unit.\nIBM had several such mechanical systems. This one was a commercial product with modest success.[1] https://en.wikipedia.org/wiki/IBM_2321_Data_Cell\n \nreply",
      "Look up a guy called Pedro Aniceto - he\u2019ll tell you so many stories of when those cards were current here (he used to courier them across town when he was a kid)\n \nreply",
      ";) Punching cards was in fact my first \"decent\" job. There were the \"punchers\" and \"the programmers\". A real social battle...\n \nreply",
      "I feel like you should submit your own blog posts here!\n \nreply"
    ],
    "link": "https://blog.jgc.org/2024/10/rabbit-hole-stumbling-across-two.html",
    "first_paragraph": "",
    "summary": "**Title: Stumbling Around in Technological Footprints**\n\nIn a burst of nostalgic technobabble, a blogger digs up two ancient Portuguese punched cards and tumbles down a rabbit hole lined with outdated Portuguese techno-jargon. The comment section turns into a makeshift history lesson where everyone is a part-time linguist and a full-time reminiscer. One commenter awkwardly swings nostalgia like a blunt weapon, reminiscing about how \"punching cards was my first decent job,\" sparking a flood of equally irrelevant memories. Meanwhile, another tries to connect language evolution to technological use, as if the shift from \"ordenador\" to \"computador\" was a pivotal moment in human history. \ud83d\ude44\n\nIcons by icons8."
  },
  {
    "title": "Trap \u00e2\u20ac\u201c Transformers in APL (github.com/bobmcdear)",
    "points": 28,
    "submitter": "tlack",
    "submit_time": "2024-10-07T19:36:33.000000Z",
    "num_comments": 6,
    "comments_url": "https://news.ycombinator.com/item?id=41770051",
    "comments": [
      "> Though APL may strike some as a strange language of choice for deep learning, it offers benefits that are especially suitable for this field: First, the only first-class data type in APL is the multi-dimensional array, which is one of the central object of deep learning in the form of tensors. This also signifies that APL is by nature data parallel and therefore particularly amenable to parallelization. Notably, the Co-dfns project compiles APL code for CPUs and GPUs, exploiting the data parallel essence of APL to achieve high performance. Second, APL also almost entirely dispenses with the software-specific \"noise\" that bloats code in other languages, so APL code can be directly mapped to algorithms or mathematical expressions on a blackboard and vice versa, which cannot be said of the majority of programming languages. Finally, APL is extremely terse; its density might be considered a defect by some that renders APL a cryptic write-once, read-never language, but it allows for incredibly concise implementations of most algorithms. Assuming a decent grasp on APL syntax, shorter programs mean less code to maintain, debug, and understand.This is really cool.  At about 150 lines, terse indeed.  And it makes sense that of course APL could work well with gpus, but I\u2019m kind of surprised there\u2019s enough of it still out in the wild so that there\u2019s already a reliable tool chain for doing this.\n \nreply",
      "> APL could work well with gpusI've seen at least an APL implementation running on top of Julia, thanks to macros.Julia has good GPU support, and it makes it easy to compose that support with any library.However, kdb+ and q, which are APL descendants, have good GPU support already: https://code.kx.com/q/interfaces/gpus. But licenses are not cheap...\n \nreply",
      "> APL code can be directly mapped to algorithms or mathematical expressions on a blackboard and vice versaAfter looking at the code, I find this claim questionable.\n \nreply",
      "APL was invented by Iverson as a blackboard notation because he felt the existing notation was awkward/insufficent for describing computation/algorithms\n \nreply",
      "After looking at HN comments for years, I find this low effort dismissal downvoteable.APL was originally a rewrite and normalisation of traditional math notation for use on blackboards. Before it was anything to do with computers it was linear algebra without all the bizarre precedence rules and with some common useful operations.\n \nreply",
      "Name checks out.\n \nreply"
    ],
    "link": "https://github.com/BobMcDear/trap",
    "first_paragraph": "We read every piece of feedback, and take your input very seriously.\n            To see all available qualifiers, see our documentation.\n          \n        Autoregressive transformers in APL\n      \u2022 Introduction\n\u2022 Usage\n\u2022 Performance\n\u2022 Acknowledgementstrap is an implementation of autoregressive transformers - namely, GPT2 - in APL. In addition to containing the complete definition of GPT, it also supports backpropagation and training with Adam, achieving parity with the PyTorch reference code.Existing transformer implementations generally fall under two broad categories: A predominant fraction depend on libraries carefully crafted by experts that provide a straightforward interface to common functionalities with cutting-edge performance - PyTorch, TensorFlow, JAX, etc. While relatively easy to develop, this class of implementations involves interacting with frameworks whose underlying code tends to be quite specialized and thus difficult to understand or tweak. Truly from-scratch imple",
    "summary": "**Summarizing Transformers with APL: An Exercise in Masochism**  \nIn a bold move that combines the obscurity of APL with the trendy world of machine learning, <em>Trap</em> on GitHub decides that rewriting GPT2 in a language designed for alien communication is exactly what the industry needs. Commenters, in a show of undying optimism, marvel at the mere 150 lines of code, blissfully ignoring the fact that each line is as comprehensible as quantum physics to the average programmer. One brave soul dares to question the practicality of this APL sorcery, only to be met with historical trivia about APL's origins, because nothing says \"useful programming discussion\" like a good old-fashioned detour into mid-20th century academic disputes. \ud83d\ude80\ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "Practices of Reliable Software Design (entropicthoughts.com)",
    "points": 35,
    "submitter": "fagnerbrack",
    "submit_time": "2024-10-08T21:01:22.000000Z",
    "num_comments": 2,
    "comments_url": "https://news.ycombinator.com/item?id=41781777",
    "comments": [
      "There is a bunch of good advice here, but it's missed the most useful principal in my experience, probably because the motivating example is too small in scope:The way to build reliable software systems is to have multiple independent paths to success.This is the Erlang \"let it crash\" strategy restated, but I've also found it embodied in things like the architecture of Google Search, Tandem Computer, Ethereum, RAID 5, the Space Shuttle, etc.  Basically, you achieve reliability through redundancy.  For any given task, compute the answer multiple times in parallel, ideally in multiple independent ways.  If the answer agrees, great, you're done.  If not, have some consensus mechanism to detect the true answer.  If you can't compute the answer in parallel, or you still don't get one back, retry.The reason for this is simply math.  If you have n different events that must all go right to achieve success, the chance of this happening is x1 * x2 * ... * xn.  This product goes to zero very quickly - if you have 20 components connected in series that are all 98% reliable, the chance of success is only 2/3.  If instead you have n different events where any one can go right to achieve success, the chance of success is 1 - (1 - y1) * (1 - y2) * ... * (1 - yn).  This inverse actually increases as the number of alternate pathways to success goes up and fast.  If you have 3 alternatives each of which has just an 80% chance of success, but any of the 3 will work, then doing them all in parallel has a 97% chance of success.This is why complex software systems that must stay up are built with redundancy, replicas, failover, retries, and other similar mechanisms in place.  And the presence of those mechanisms usually trumps anything you can do to increase the reliability of individual components, simply because you get diminishing returns to carefulness.  You might spend 100x more resources to go from 90% reliability to 99% reliability, but if you can identify a system boundary and correctness check, you can get that 99% reliability simply by having 2 teams each build a subsystem that is 90% reliable and checking that their answers agree.\n \nreply",
      "In the limit, there is a hard tradeoff between efficiency and reliability.Failovers, redundancies, and backups are all important for building systems that are resilient in the face of problems, for reasons you've pointed out.However, failovers, redundancies and backups are inefficient. Solving a problem with 1 thing is always going to be more efficient that solving the same problem with 10 things.It's interesting to see this tradeoff play out in real-life. We see people coalescing around one or two services because that's the most efficient path, and then we see them diversifying across multiple services once bad things happen to the centralised services.\n \nreply"
    ],
    "link": "https://entropicthoughts.com/practices-of-reliable-software-design",
    "first_paragraph": "\nI was nerd-sniped. Out of the blue, a friend asked me,\n\nIf you would build an in-memory cache, how would you do it?\n\nIt should have good performance and be able to hold many entries. Reads are more\ncommon than writes. I know how I would do it already, but I\u2019m curious about your\napproach.\n\nI couldn\u2019t not take the bait.\n\nIn the process of answering the question and writing the code, I discovered a\nlot of things happened in my thought processes that have come with experience.\nThese are things that make software engineering easier, but that I know I\nwouldn\u2019t have considered when I was less experienced.\n\nI started writing a long and sprawling article on it, but it didn\u2019t quite hit\nthe right notes, so this is a much abbreviated version. Time allowing, I might\nexpand on some of these practices in separate articles that can be more focused\nand concise.\n\nHere are all eight practices I have adopted with experience that I had use for\nduring the exercise of writing a fast, small, in-memory cache.",
    "summary": "<h1>Practices of Reliable Software Design: An Exercise in Overconfidence</h1>\n\nAt <em>entropicthoughts.com</em>, another software savant scribbles down what they perceive as enlightening wisdom on how to build a memory cache that neither you nor your cat asked for. Apparently, discovering how to cache statements is a profound journey worthy of a \"much abbreviated\" article, because everyone knows brevity is the soul of not winding up over-engineered like the Space Shuttle. The commenters, eager to prove they too can string jargon into sentences, discuss redundancy with the kind of fervor usually reserved for arguing about which Star Wars movie is the worst. Each contributor meticulously ignores the pragmatic simplicity of just turning it off and on again. \ud83d\ude44\u2728"
  },
  {
    "title": "Show HN: Kotlin Money (eriksen.com.br)",
    "points": 331,
    "submitter": "eriksencosta",
    "submit_time": "2024-10-08T12:59:52.000000Z",
    "num_comments": 185,
    "comments_url": "https://news.ycombinator.com/item?id=41776878",
    "comments": [
      "Cool stuff!The use of infix functions reads a bit weird to me.If I were to design an API like this in Kotlin, I think I would have gone for regular extensions for many cases and perhaps extension properties, think as such:    val fiveBucks = 5.usd\n    val fiveBucks = 5.money(\"USD\")\n    val tenPercent = 10.percent\n\nHow come you went for \"increaseBy\" and \"decreaseBy\" instead of overloading `plus` and `minus`? Just curious, preference is a valid answer.\n \nreply",
      "Nothing is preventing you from using it this way ? Infix functions are just syntactic sugar, some prefer it, some don't, but there's zero downsides to it (aside from your coworkers abusing it.) 5 money \"USD\" is literally the exact same thing as 5.money(\"USD\"), and Int.usd = this.money(\"USD\")+ and - are already overloaded (see val subtotal = price + shipping), increase/decreaseBy are for operating over percentages (and could be written as subtotal * (1 - discount), which is much less clear). As the other comment say, it has an actual, real life meaning that people understand clearly. Your price increased by 10 percent. the By convention is also already present in the Kotlin stdlib, although it's more for grouping operations, numeric operations are taking the Of suffix now (sumBy has been deprecated in favor of sumOf, increaseBy could become increaseOf without any loss of clarity)\n \nreply",
      "`decreaseBy` is a multiplication and subtraction combined, map naturally to commerce domain, and is more complex than plain addition / subtraction.\n \nreply",
      "> overloading `plus` and `minus`Perhaps I'm misunderstanding your idea, but what about  val total = price + 10.percent * 2\n\nIf the price is 10, then the total is 12 or 22?\n \nreply",
      ">val total = price + 10.percent * 2If the price is 10, I would expect total to be 10,2.(price + 10.percent) * 2 would be 22.I don't see any way to get 12.\n \nreply",
      "price + (10.percent * 2) looks like 12: If 10.percent = 1, 1*2 = 2. 10 + 2 = 12.Under the usual rules of operator precedence, that would be the expected answer (multiplication having higher precedence than addition).\n \nreply",
      "This does a better job of showing an uneasy feeling I have about Kotlin than anything I could say.- The infix is weird and footgun-y.- Extension methods on int/double serving as constructors smells funny.- Using infix operators as constructors but not using infix operators for addition/subtraction smells funny.In general, at least in a corporate environment switching off Java for Android, I found Kotlin a distracting step sideways.Code reviews tended to involve a lot of bikeshedding over how to make it Kotlin-y, and there's a sort of \"why not?\" approach to language features that creates much room for the bikeshedding.It left me feeling like we were unconciously choosing to have the same arguments C++ programmers had in 1990, all over again. Except it was even more destructive, because those arguments were centered, and conflated with \"proper\" coding in  the fancy new language.I'm not against new and shiny: I was the first to use Kotlin in the org., and I dove right into Swift. There's something alarming with this transition.I'm heartened by starting to see some debate in Android dev communities about whether Kotlin/Compose were a bridge to nowhere that shouldn't have been a focus for years.\n \nreply",
      "> The infix is weird and footgun-y.If it's a foot gun, then I'm not seeing how the gun is loaded?> Extension methods on int/double serving as constructors smells funny.It doesn't feel meaningfully different from the static factory pattern in traditional Java, where 10.percent() would be something like MoneyUtils.createPercent() or Percent.create() with several overloads. Under the hood, that is essentially what is happening. Only downside I can see is that it muddies what truly belongs to the class, but that's true of any extension function and Kotlin is intended to be used with IDEA's introspection anyway.The Kotlin standard library has lots of extension functions that construct new objects.It also reminds me of Ruby, which perhaps you're also not a fan of and that's OK.\n \nreply",
      "I hear what you are saying but there is the other side. Perhaps the opinion/feeling you are having is related to getting older.We've learned a lot about what works and what doesn't in programming languages. Goto considered harmful and all that kind of stuff. If you want to get a Lisp/Haskell fanatic really going point out how so many of the features in those languages have finally made their way into mainstream languages (lambdas, etc.).What we don't often consider are all of the language features that didn't make it.This process didn't stop sometime in the past. It is happening right now. That feeling of unease may not be an indication of the quality of the features you are considering. It may largely be uncertainty about what features will or will not stand the test of time.Perhaps as we get older, we want languages that have all of the good stuff we've learned from the past and none of the experimental stuff that we aren't too sure about yet. That might be because we are starting to notice that we won't have enough time left in our remaining days to sort all the new toys into the good/bad bin.\n \nreply",
      "Stirring call to action for creativity, but it is unclear how a comment that boils down to \"new things might be good\" applies in this context.as OP refers to, custom operators are considered harmful in languages ranging from C++ to Swift. It's a great contemporary example of goto.\n \nreply"
    ],
    "link": "https://blog.eriksen.com.br/en/introducing-kotlin-money",
    "first_paragraph": "On software development and delivery\nOct 8, 2024Manipulating monetary amounts is a common computing chore. However, no mainstream language has a first-class data type for representing money, it\u2019s up to programmers to code abstractions for it. This isn\u2019t an issue per se until dealing with rounding issues from operations like installment payments (e.g., buy now, pay later), foreign exchange, or even simple things like fee processing and tax collection.Inspired by my days at N26 dealing with these challenges, I introduce Money: a Kotlin library that makes monetary calculations and allocations easy:The library supports mathematical operations with monetary amounts, calculations with percentages, and allocation, making it simple to model use cases like those mentioned. Cryptocurrencies are also fully supported out of the box:One of the nicest features of the library is its allocation capability. Allocation allows the distribution of a monetary amount into parts while guaranteeing that the s",
    "summary": "**Kotlin Converts Currency:** Once again, a thoughtful soul has decided that dealing with money in software needs a \"unique\" solution, sparking unprecedented excitement (or heightened apathy, it\u2019s not clear) among aspiring Kotlinista commenters. Cue \"Money: A Kotlin library,\" because apparently money computations weren't fun enough in plain ol' Java or with boring, established libraries \u2014 now we need infix fun and extension syntax sugar to *truly* understand what $5 plus 10% looks like. Enthusiastic forum warriors engage in intense semantic battles over whether to 'increaseBy' or to simply overload `plus`, a discussion so critical that it threatens the very fabric of their beings. Adding spice, some advocate for this syntactic carnival as Kotlin's road to programming language nirvana, oblivious to the echoes of C++'s past hauntings. Every commenter leaves convinced of their superior grasp of both Kotlin and economics, while the world desperately pleads, \"Just use a calculator, please.\""
  },
  {
    "title": "Lunar Lake's iGPU: Debut of Intel's Xe2 Architecture (chipsandcheese.com)",
    "points": 34,
    "submitter": "mfiguiere",
    "submit_time": "2024-10-08T19:32:37.000000Z",
    "num_comments": 7,
    "comments_url": "https://news.ycombinator.com/item?id=41780929",
    "comments": [
      "> Xe2, Intel is looking to use the same graphics architecture across their product stack.. integrated GPUs as a springboard into the discrete GPU market.Linux support for Xe2 and power management will take time to mature, https://www.phoronix.com/forums/forum/linux-graphics-x-org-d...Intel dropped Xe1 SR-IOV graphics virtualization in the upstream i915 driver, but the OSS community has continued improvement in an LTS fork, making steady progress, https://github.com/strongtz/i915-sriov-dkms/commits/master/ & https://github.com/Upinel/PVE-Intel-vGPU?tab=readme-ov-file.  SR-IOV improves VM graphics performance.\n \nreply",
      "i wish they covered things like x264/x265/av1/etc encoding/decoding performance and other benefits that aren't just gaming.\n \nreply",
      "It\u2019s probably just not that interesting. There\u2019s generally a proprietary encode/decode pipeline on chip. It can generally handle most decode operations with CPU help and a very narrow encoding spec mostly built around being able to do it in realtime for broadcast.Most of the video you encode on a computer is actually all in software/CPU because the quality and efficiency is better.\n \nreply",
      ">>> It can generally handle most decode operations with CPU help and a very narrow encoding spec.This is so much spot on. Video coding specs are like a \"huge bunch of tools\" and encoders get to choose whatever subset-of-tools suits them. And than hardware gets frozen for a generation.\n \nreply",
      "I agree, I never really cared about QSV as an Intel feature until I started doing Livestreams, using Plex/Jellyfin/Emby, and virtualizing/homelab work.\n \nreply",
      "QuickSync passthrough should get you everything you need on i3+ chips. It's basically intel's only selling point in the homelab/home server space, and it's a big one.[Edit: I think I initially misread you - but I agree, it's a huge differentiator]\n \nreply",
      "100% agree with that. x265 transcoding gets done on my MBP regularly so I\u2019d like to see that as a comparison point.\n \nreply"
    ],
    "link": "https://chipsandcheese.com/p/lunar-lakes-igpu-debut-of-intels",
    "first_paragraph": "",
    "summary": "Welcome to yet another round of Intel\u2019s Silicon Soap Opera, where the stellar cast of over-promised and under-delivered GPU tech takes center stage. Intel attempts a **leap of faith** with Xe2 architecture, hoping to stick the landing in the discreet GPU marketplace, but all commenters seem to care about is encoding specs for their weekend Netflix binge projects. Not to disappoint, our tech-savvy audience quickly nerds out over Linux support timelines and VM performance boosts, promising a future where Intel\u2019s integration might actually work as intended... eventually. Meanwhile QuickSync stays in the spotlight as the aged starlet, continuing to promise high performance but only really shining in niche homelab setups. \ud83d\ude34\ud83d\udcbb\ud83c\udfad"
  },
  {
    "title": "A modest critique of Htmx (chrisdone.com)",
    "points": 140,
    "submitter": "wibwobble12333",
    "submit_time": "2024-10-08T20:19:57.000000Z",
    "num_comments": 32,
    "comments_url": "https://news.ycombinator.com/item?id=41781457",
    "comments": [
      "These all look reasonable to me.I especially go back and forth on attribute inheritance (it can be disabled via the htmx.config.disableInheritance option)Three of the criticisms boil down to the fact that client-side state doesn't always play well w/htmx swaps (especially the simple ones) which is absolutely true.And events can get crazy.  They are powerful, but crazy and at times hard to debug.  Such is event-driven life.The one thing I don't agree with is the default queuing mode: it is not to cancel an existing request and replace it.  Instead it is to keep the current request in flight and queue one and only one additional request.  I'd need to sit down w/them to see if they were misinterpreting something, using the hx-sync attribute to implement the behavior they mention, or if there is a bug.I would also like to take this opportunity to market our mug for people who don't like htmx:https://swag.htmx.org/products/htmx-sucks-mug\n \nreply",
      "Man, you\u2019re everywhere. I have a Montana\u2019s restaurant in my city and I\u2019m scared to go in because of you.\n \nreply",
      "you should be\n \nreply",
      "By queue only one additional request, do you mean cancel any existing queued request?\n \nreply",
      "the code is a little gnarly, but if you don't specify anything the default behavior is to keep the \"last\" event that comes in while a request is in flight:https://github.com/bigskysoftware/htmx/blob/1242977d11bebe56...and that dumps the existing request queue and puts request created by the last event in it by itself:https://github.com/bigskysoftware/htmx/blob/1242977d11bebe56...we don't cancel the current request or issue the next request until the current request finishesbut there could be a bug in the code, for sure, it's pretty crazy\n \nreply",
      ">https://github.com/bigskysoftware/htmx/blob/1242977d11bebe56...This seems to be a scenario where switch/case blocks could make the elif-trees a bit easier to read.Also, the code could use some care about not going so deep into the Vs:    // request headers\n    if (requestAttrValues.noHeaders) {\n    // ignore all headers\n    } else {\n      for (const header in headers) {\n        if (headers.hasOwnProperty(header)) {\n          const headerValue = headers[header]\n          safelySetHeaderValue(xhr, header, headerValue)\n        }\n      }\n    }\n\ncould just be:    // request headers\n    if (!requestAttrValues.noHeaders) {\n      Object.keys(headers)\n        .filter(hdr => headers.hasOwnProperty(hdr)) \n        .forEach(hdr => safelySetHeaderValue(xhr, hdr, headers[hdr]));\n    }\n\n\nNot even sure if that hasOwnProp check is needed, unless header keys are explicitly set to undef.\n \nreply",
      "yeah i prefer just plain ol' if statements, i find them easier to debug\n \nreply",
      "I thought they were complaining that any request is being cancelled by a subsequent one, since they wanted all the requests they made to go through (presumably the requests are altering state?) Probably I misunderstood what was meant by \u201closing work\u201d though.\n \nreply",
      "yeah i don't know if I understand what they were saying eitherregardless if you have a lot of events flying around then updating the UI via hypermedia exchanges is going to be a bad idea, as I mention here:https://htmx.org/essays/when-to-use-hypermedia/#if-your-ui-s...\n \nreply",
      "I\u2019ll definitely pile on with the inheritance causing issues. It made me feel like unsetting them constantly defensively.\n \nreply"
    ],
    "link": "https://chrisdone.com/posts/htmx-critique/",
    "first_paragraph": "",
    "summary": "Title: **Tech Enthusiasts Struggle Profoundly with Basic Configuration**\n\nIn the latest episode of <em>\"Trivial Complaints from the Frontlines of Web Development,\"</em> a developer takes a swipe at Htmx for features that are, well, entirely configurable. Commenters, in true form, rally to transform a molehill into Mt. Everest, while simultaneously showcasing their ability to copy-paste documentation URLs. One bright soul tries to sell a mug emblazoned with \"htmx sucks,\" presumably because irony is best served on porcelain. Amidst the chaos, suggestions for code elegance emerge \u2014 because why fix bugs when you can refactor into oblivion? \ud83d\ude44"
  },
  {
    "title": "European govt air-gapped systems breached using custom malware (welivesecurity.com)",
    "points": 52,
    "submitter": "tagyro",
    "submit_time": "2024-10-08T17:52:14.000000Z",
    "num_comments": 37,
    "comments_url": "https://news.ycombinator.com/item?id=41779952",
    "comments": [
      "The weak-point is the shared USB device that copies from one machine to another which seems to defeat the whole purpose of being air-gapped - you could have printed-and-OCR'd data three decades ago so the air-gapped machine is never reading anything from outside at all, these days a video stream and AI could probably automate that?\n \nreply",
      "I can definitely imagine use cases where a network is air gapped internally for security but bidirectional transfer still takes place. The point is that humans are supposed to be in control of exactly what is transferred, in both directions (not feasible with a network connection, to my knowledge).\n \nreply",
      "Surely some government has come up with physically-unidirectional data transmission mechanisms for getting data onto airgapped networks. There has to be something more sophisticated than single-use CD-ROMs, even if it's just a blinking LED on one end and a photosensor on the other end.\n \nreply",
      "> There has to be something more sophisticated than single-use CD-ROMsBut why, when a DVD-R handles most use cases at a cost of < $0.25 each, are reliable and ubiquitous, the hardware is likely already there (unless you are using Apple - caveat emptor) and they close the threat vector posed by read/write USB devices.Sometimes the simplest solution is the best solution.\n \nreply",
      "I would guess having a CD/DVD drive opens another attack surface. Similar to why people gluing their USB ports closed.\n \nreply",
      "Right \u2014 but the question isn\u2019t CD/DVD versus nothing. It\u2019s CD/DVD versus USB; and which has a smaller attack surface.I\u2019d argue that read-only CD/DVD has a smaller attack surface than USB, so of the two, it\u2019s preferable. I\u2019d further argue that a CD/DVD (ie, the actual object moved between systems) is easier to inspect than USB devices, to validate the behavior.\n \nreply",
      "Data diodes are commonly used: https://csrc.nist.gov/glossary/term/data_diodeI don't know if people class something connected using a data diode as airgapped or not.\n \nreply",
      "https://docs.aws.amazon.com/whitepapers/latest/cross-domain-...\n \nreply",
      "I have heard (on HN) of... 100 MBit ethernet with the transmit wires cut. Probably in the context of in-flight infotainment: plane data to infotainment yes, infotainment anything to plane control anything no. If it's stupid but it works...\n \nreply",
      "https://en.wikipedia.org/wiki/Unidirectional_network\n \nreply"
    ],
    "link": "https://www.welivesecurity.com/en/eset-research/mind-air-gap-goldenjackal-gooses-government-guardrails/",
    "first_paragraph": "\n      Award-winning news, views, and insight from the ESET security community\n    \n        Award-winning news, views, and insight from the ESET security community\n      ESET ResearchESET Research analyzed two separate toolsets for breaching air-gapped systems, used by a cyberespionage threat actor known as GoldenJackalMat\u00edas Porolli\n07 Oct 2024\n\u00a0\u2022\u00a0\n, \n40 min. read\nESET researchers discovered a series of attacks on a governmental organization in Europe using tools capable of targeting air-gapped systems. The campaign, which we attribute to GoldenJackal, a cyberespionage APT group that targets government and diplomatic entities, took place from May 2022 to March 2024. By analyzing the toolset deployed by the group, we were able to identify an attack GoldenJackal carried out earlier, in 2019, against a South Asian embassy in Belarus that, yet again, targeted the embassy\u2019s air-gapped systems with custom tools.This blogpost introduces previously undocumented tools that we attribute to Gold",
    "summary": "**European Governments Dupe Themselves with Fancy USB Sticks: Now with More Espionage!**\n\nIn an exhilarating 40-minute scroll-fest, ESET celebrates its own \"award-winning\" ability to state the obvious: air-gap security is as reliable as a papier-m\u00e2ch\u00e9 vault when underlings keep shoving spy-laden USBs into supposedly secluded systems. Enter GoldenJackal\u2014because all threat actors apparently need names cooler than a Bond villain\u2014to devise a scheme that makes breaking into European governmental \"air-gaps\" look as challenging as hacking a preschool's attendance sheet. Marvel as commenters engage in the digital equivalent of explaining rocket science to explain the awe-inspiring tech of CD-ROMs and their groundbreaking preference over USBs\u2014because what's espionage without a dose of nostalgia? Meanwhile, someone points out insanely complex solutions like \"data diodes,\" as others mildly suggest chopping Ethernet cables because, hey, if it's stupid but it works, it doesn't matter if it's still stupid. \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcbe\ud83d\udd13"
  },
  {
    "title": "Canvases versus Documents (kaiwenwang.com)",
    "points": 41,
    "submitter": "todsacerdoti",
    "submit_time": "2024-10-06T06:50:19.000000Z",
    "num_comments": 8,
    "comments_url": "https://news.ycombinator.com/item?id=41755303",
    "comments": [
      "> Rather than an FAQ section, I wonder if websites will integrate a \u201cLLM question\u201d section instead.FYI the point of FAQ sections is to \"prime\" your brain about certain ideas; they're not always questions that they're frequently asked. A blank canvas chat interface won't have the same effect.> There\u2019s too much slop on website homepagesSome of these homepages have been iterated by many teams of very smart people, and they're sloppy because they convert. That's it. It's not to convey meaning, like a Wikipedia article, but get someone to click \"sign up\" or \"buy.\"It's important to distinguish \"conveying meaning\" with \"conveying feeling\"> hacked together with Tailwind + Cursor + shadcnmost YC and other generic (but pretty) landing pages are usually webflow\n \nreply",
      "> Text is meant to be understood and quickly scanned. Websites are attempting a \u201ccanvas-style\u201d design, generally better for conveying feeling, but for text, which is meant to convey meaning and immersion.This might be, because lots of websites are not there to convey meaning but rather want to bring you in the right mood to do x. And it seems to be that humans - on average - can be far more easily brought into the right mood by manipulating `feelings` than through rational arguments.\n \nreply",
      "I think that's also why some politicians, who shall remain unnamed, are able to speak entire paragraphs of words that don't connect into logical sentences. They'll say all the right words - \"great\", \"success\", \"prosperity\", \"safe\", and also the other kinds of words \"immigrants\", \"violence\", etc, without actually committing to anything specific.I think the reason it works is the same reason poetry works - the sentences aren't meant to be English prose that can be parsed logically, but instead, meant to be verses that evoke certain emotions.it seems to work really well for getting elected.\n \nreply",
      "It's like the marketing department won. Who cares if there's a there there, just make everyone think there is.\n \nreply",
      "This dichotomy seems like a very useful tool for analyzing media.\n \nreply",
      "Is the article a canvas or a document?\n \nreply",
      "I'd really like the page to be a canvas, made up of many addressable sub objects such as documents. Doable with rdfa or microdata, where elements can declare urls for themselves, but there's nothing but semantic web researchers and some experimental browsers & extensions for it. The page itself could rock & extend this premise, if it wanted.\n \nreply",
      "Nototo, a now dismissed web app, really nailed this. I'm gonna miss it so much.\n \nreply"
    ],
    "link": "https://kaiwenwang.com/writing/canvases-versus-documents",
    "first_paragraph": "Much web design has reflected that of paper documents: finite with clearly defined edges. But by no means is the canvas style recent, as we can see in Space Jam.I feel a shift in style is ongoing: the big text on a clear background is now becoming bordered. See Zed, Hebbia, or Matrices.The non-bordered style is shown below.But it\u2019s not the front page that needs a border, it\u2019s the fact that a clearly defined container gives a sense of coherence to the sections underneath the top part (which I refer to as the storefront, or just front because it\u2019s the first thing you see), without which seem lost in space.There\u2019s another resizing issue I\u2019ve seen where the container size is big enough to fit the width of the screen, so there ends up being no padding on the left or right side, making the document weird. I\u2019m generally against using Tailwind\u2019s container class because it has multiple breakpoints, which is saying that there are 4-5 different sized documents that represent your website, and you",
    "summary": "Welcome to the latest web design existential crisis at \u2b50\ud83c\udfa8 kaiwenwang.com \u2b50\ud83c\udfa8, where screens are *canvases* not boring old *documents*. Rejoice as the blogosphere discovers what designers from the '90s learned when *Space Jam* premiered: big text and clear backgrounds are just a phase. The comments section is a chaotic blend of undigested naive technical musings and tech bro reminiscences, with gentle nods to dissolved startups and casual dismissal of solid UX practices. Meanwhile, another commenter floats the idea of replacing FAQ sections with chatbots because, why solve a problem cleanly when you can add another layer of complexity? Oh, the nostalgia of web design debates: an unending loop of rehashed ideas chasing modern twists."
  },
  {
    "title": "The Static Site Paradox (kristoff.it)",
    "points": 286,
    "submitter": "alraj",
    "submit_time": "2024-10-08T09:08:51.000000Z",
    "num_comments": 192,
    "comments_url": "https://news.ycombinator.com/item?id=41775238",
    "comments": [
      "The current state of web development engineering is largely the result of how startup economics have functioned over the past decade.A startup's market value is often closely tied to its number of employees. From an investor's perspective, a company with 1,000 employees is typically valued much higher than a small team of 37 programmers \u2014 regardless of the revenue generated per employee, or even if the company isn\u2019t generating revenue at all. This is largely because interest rates remained very low for a long time, making it reasonable to borrow investment funds for promising companies with large staffs.However, those employees need to be kept busy with something that appears useful, at least in theory. I believe this is one of the primary reasons we see such complex solutions for relatively simple tasks, which sometimes might not require a large team of advanced web developers or sophisticated technologies at all.\n \nreply",
      "We\u2019re dealing with this big time in Asheville now. When cell service came back at all, everyone had shitty intermittent 3G, and none of the websites we needed for basic survival information would load. A bunch of good people created some text only news sites, and today I noticed that the Buncombe county website finally has a low bandwidth site, but even then when I inspected it, it had 130k of bootstrap css and 50k of jQuery blocking rendering. It\u2019s great that people are doing this work, but citizens needed this a week and a half ago. By now, I\u2019ve figured out where to get water, food, non potable water, etc. Seeing tech fail so badly through all this has been eye opening for me, in a depressing way.\n \nreply",
      "Not as catastrophic, but during power outages in my neighborhood, I am usually left with a poor cell signal (no backups on cable internet boxes). Electric utility's outage map is hidden behind a login (!) and is rendered with fancy clustering and other UI features that are already slow even with a decent connection. So it takes quite a while to check the status or even report an outage.I could call the utlity company, but for some reason, they chose voice navigation (instead of keypad tones) for their menu, and it is not good at recognizing distorted voice (over bad 4G or 2G connection).\n \nreply",
      "Do you have links to the text only news sites?\n \nreply",
      "CNN is on https://lite.cnn.com/. The home page is a single 30KB HTML file (my browser also insists on downloading their 6KB favicon).A thing of beauty, compared to their normal home page with 90+ files of 12MB.\n \nreply",
      "http://frogfind.com/ Might be useful\n \nreply",
      "Yeah, I should have posted that. This is the one I use the most:https://text.bpr.org/I forgot to save the links to the others.\n \nreply",
      "Wow it's amazing how noticeably faster than is that every other website. We really messed up the internet.\n \nreply",
      "that situation  has had me thinking about getting an amateur radio license again. In a disaster like what happened to Western NC, which encompasses a much greater area than just Asheville, I wouldn't want to rely on anything based on the Internet. You want something with a long wavelength and low power. But it's so inaccessible, and I'm not sure if it's for good reason or not.Connectivity was knocked out from Black Mountain all the way to the Tennessee and Georgia borders. I'd be surprised if many people even have shitty 3G back yet. What I know is remaining in touch with people who live there has been hard.\n \nreply",
      "You can listen without a license. I was able to hear everything on a $15 handheld and it was immensely helpful.In an emergency you are allowed to transmit without a license. There were plenty of unlicensed calls going to the Mt. Mitchell repeater.All that being said, I am definitely getting my license once this is over.\n \nreply"
    ],
    "link": "https://kristoff.it/blog/static-site-paradox/",
    "first_paragraph": "\nOctober 08, 2024\n    \u2022\n    3\n    min read \u2022 by\n    Loris Cro\n\nIn front of you are two personal websites, each used as a blog and to display basic contact info of the owner:If you didn't know any better, you would expect almost all normal users to have [2] and professional engineers to have something like [1], but it's actually the inverse: only few professional software engineers can \"afford\" to have the second option as their personal website, and almost all normal users are stuck with overcomplicated solutions.Weird as it might be, it's not a great mystery why that is: it's easier to spin up a Wordpress blog than it is to figure out by yourself all the intermediate steps:And so, while we software engineers enjoy free hosting & custom domain support with GitHub Pages / Cloudflare Pages / etc, normal users are stuck with a bunch of greedy clowns that make them pay for every little thing, all while wasting ungodly amounts of computational power to render what could have been a static w",
    "summary": "### The Static Site Paradox: A Sarcasm-Infused Mockery\n\nIn the not-so-shocking revelation of the decade, *The Static Site Paradox* unveils that software engineers relish the luxuries of streamlined, static websites while the average Joe drowns in the morass of over-engineered blog solutions, courtesy of the digital overlords at Wordpress. In a dramatic twist that surprises absolutely no one, tech gurus bask in the nirvana of free hosting and custom domains, while everyone else falls prey to the menacing circus of 'greedy clowns' charging a premium to push every pixel over the web. \ud83d\ude44 Meanwhile, in the comments, a collective of trauma-stricken web users mourn the loss of their digital simplicity, reminiscing over a bygone era of sub-30KB HTML files and engaging in quiet revelations about the cosmos-altering speed of text-only sites. Ah, technology\u2014so close to making life easier, yet so committed to making it a Kafkaesque labyrinth!"
  },
  {
    "title": "Instant (YC S22) is hiring a founding engineer to help build a modern Firebase",
    "points": 0,
    "submitter": "",
    "submit_time": "2024-10-08T21:00:16.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "item?id=41781768",
    "first_paragraph": "",
    "summary": "Instant (YC S22), the startup that couldn't settle for the myriad of perfectly usable backend solutions, seeks a \"founding engineer\" to reinvent yet another wheel. The bold, innovative plan? A modern Firebase, because apparently, Firebase isn\u2019t modern enough. Commenters trip over themselves to either worship this needless duplication of technology or to demonstrate their profound detachment from reality by questioning if \"modern\" means it will run solely on blockchain. \ud83e\udd26\u200d\u2642\ufe0f"
  },
  {
    "title": "Don't let dicts spoil your code (roman.pt)",
    "points": 14,
    "submitter": "juniperplant",
    "submit_time": "2024-10-08T21:10:14.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://roman.pt/posts/dont-let-dicts-spoil-your-code/",
    "first_paragraph": "",
    "summary": "In a valiant attempt to save the world from the profound peril of poorly handled dictionaries in programming, a brave soul at roman.pt concocts an article that might have been mistaken for groundbreaking\u2014if it had been penned when dinosaurs roamed the Earth. Loaded with face-palming insights such as \"dictionaries are useful,\" and earth-shattering revelations about key-value pairs, the piece bravely battles the unseen menace of developer incompetence. Meanwhile, the commenters, in a heroic lack of self-awareness, vie for the crown of Captain Obvious, each offering anecdotes that underscore the dire unspoken crisis: too much internet access. Will dictionaries ever recover from such enthusiastic mundanity? Stay tuned\u2014or don't. \u2620\ufe0f\ud83d\udcbb"
  },
  {
    "title": "My search for the mysterious missing secretary who shaped chatbot history (theconversation.com)",
    "points": 33,
    "submitter": "severine",
    "submit_time": "2024-10-06T13:29:25.000000Z",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=41757034",
    "comments": [
      "Another small tidbit of lost computer history:  The first person who lost a game to a computer was an unnamed Los Alamos laboratory assistant who lost a game of simplified chess against a computer program in 1956 [1].  The assistant's name seems to be lost to time.[1].  https://en.wikipedia.org/wiki/Los_Alamos_chess#Los_Alamos_tr...\n \nreply",
      "In case anyone is wondering, the author hasn't found her yet; the article ends with:>The blizzard is worsening. The announcement rings out that the campus is closing early due to the weather. The missing secretary\u2019s voice still eludes me. For now, the history of talking machines remains one sided. It\u2019s a silence that haunts me as I trudge home through the muffled, snowbound streets.\n \nreply",
      "I feel the piece would have worked better backwards. Lead with the outcome that she wasn't found. Move through the effacement of the minor players (and potentially major contributors - the most interesting part). Close on the asking of how and why this happens. (Even the elimination of footsteps by falling snow could have been worked through it as a stronger metaphor)\n \nreply",
      "Thanks for saving me from the clickbait. I think this sort of social archaeology is overrated; this was a person who was explicitly protective of her privacy.\n \nreply",
      "The article is annoyingly written. Basically the author takes this little story from Weizenbaum's published accounts of \"Eliza\"\u2014> My secretary watched me work on this program over a long period of time. One day she asked to be permitted to talk with the system. Of course, she knew she was talking to a machine. Yet, after I watched her type in a few sentences she turned to me and said: \u2018Would you mind leaving the room, please?\u2019\u2014and wonders whether this story is true, and if so, what was Weizenbaum's secretary's name. It's not immediately clear to me that we should assume there was a secretary at all; Weizenbaum might have anonymized not only the participant's name but also her profession (and maybe her sex).The author says she made an effort to find out who was Weizenbaum's secretary circa 1966, but was (completely?) unsuccessful:> I work my way through Weizenbaum\u2019s yellowed papers. Surely, among the transcripts, code print outs, letters and notebooks there will be evidence? There are some clues, reference to a secretary in letters to and from Weizenbaum. But no name.> I broaden my hunt to administrative records. I look in department papers and the collections of Weizenbaum\u2019s workplace, Project MAC \u2013 the hallowed centre of computing innovation at MIT. No luck. I contact the HR office and MIT\u2019s alumni group. I stretch the patience of the ever-generous archivists. As my last day arrives, I still hear only silence.The Weizenbaum archives are partially online. On page 149 of this 150-page collection labeled \"SLIP, 1963 - 1967\" ( https://dome.mit.edu/handle/1721.3/201706 ), it's indicated that on November 5, 1963, someone with the initials \"jep\" was taking dictation from JW.Now, a single set of initials isn't remotely \"identification\" of JW's secretary (let alone identifying the participant from Weizenbaum's story, year unknown). But I feel like as a reward for reading all that, at least the author could have mentioned that she'd found those initials, and worked that into the tale she wanted to spin. As it is, it feels like she cared strictly more about spinning the tale than about finding the identity of the secretary. And if she doesn't care, why should the reader?\n \nreply"
    ],
    "link": "https://theconversation.com/my-search-for-the-mysterious-missing-secretary-who-shaped-chatbot-history-225602",
    "first_paragraph": "\n      Senior Lecturer in Contemporary Literature, University of Birmingham\n    Rebecca Roach's research was supported by a Leverhulme Trust Research Fellowship for the project \u201cMachine Talk: Literature, Computing and Conversation after 1945\u201d and facilitated by the expertise and patience of staff at MIT's Distinctive Collections.University of Birmingham provides funding as a founding partner of The Conversation UK.View all partnersThe Massachusetts Institute of Technology\u2019s (MIT) Distinctive Collections archive is quiet while the blizzard blows outside. Silence seems to be accumulating with the falling snow. I am the only researcher in the archive, but there is a voice that I am straining to hear.I am searching for someone \u2013 let\u2019s call her the missing secretary. She played a crucial role in the history of computing, but she has never been named. I\u2019m at MIT as part of my research into the history of talking machines. You might know them as \u201cchatbots\u201d \u2013 computer programmes and interfaces",
    "summary": "Title: A Goose Chase in a Blizzard for MIT's Mystery Secretary\n\nIn a thrilling twist of academia meets wild goose chase, an intrepid researcher braves the perils of *snow* and dusty *archives* to uncover the so-called \"missing secretary\" who may or may not have shaped the history of chatbots. Spoiler alert: after a riveting saga of parsing through mountains of yellowed papers and enduring the blizzard's wrath at MIT, our hero emerges, alas, nameless and empty-handed. Commenters, in a display of excessive gratitude, jeer at the fruitlessness of this icy escapade, sparing others the drudgery of clickbait while debating whether this anonymity was an ultimate act of privacy protection or just another historical oversight. In the end, everyone agrees that maybe, just maybe, some secrets are better served cold \u2013 like the weather outside."
  },
  {
    "title": "Nobel Prize in Physics awarded to John Hopfield and Geoffrey Hinton [pdf] (nobelprize.org)",
    "points": 757,
    "submitter": "drpossum",
    "submit_time": "2024-10-08T09:52:46.000000Z",
    "num_comments": 682,
    "comments_url": "https://news.ycombinator.com/item?id=41775463",
    "comments": [
      "Here is the reasoning: https://www.nobelprize.org/uploads/2024/09/advanced-physicsp...I'm surprised Terry Sejnowski isn't included, considering it seems to be for Hopfield Nets and Boltzmann machines, where Terry played a large role in the latter.\n \nreply",
      "I guess it makes sense to use that link above since it goes into much more detail. Changed from https://www.nobelprize.org/prizes/physics/2024/summary/. Thanks!\n \nreply",
      "He was probably considered since he is mentioned in the reasoning paper, still it could be one of those unfortunate omissions in the nobel history since those deciding the prize might have a hard time to measure impact.\n \nreply",
      "Then they shouldn't be trusted to give awards in an area they are not experts in.\n \nreply",
      "Trusted? The will of Alfred Nobel states that the Royal Swedish Academy of Sciences is the body that selects the winner, you can't change that.Also, I think the process looks fairly decent:https://www.nobelprize.org/nomination/physics/Gather nominations, make a shortlist, research the shortlist with actual field experts, present candidates, discuss, and vote.And in 50 years you'll be able to find out who the other candidates were!\n \nreply",
      "Impact is hard to quantify. There have been several occasions where someone who very well deserved a Nobel prize didn't get one. There are all kinds of reasons. Given he is mentioned in the reasoning he was probably considered. We can't know the reason he did not get the prize.I recently watched this quite video on the subject:  https://youtu.be/zS7sJJB7BUI?feature=shared and found it quite enjoyable.\n \nreply",
      "That would probably leave the prizes awarded in a very narrow field, also the prize is supposed to be given to the thing that has \"conferred the greatest benefit to humankind\".So in this case they picked something that might be viewed as only having a tangential connection to the field, but the impact has been so immense that they probably went outside their regular comfort zone (and how many prizes can we give for LHC work that really don't touch regular human lives in the foreseeable future anyhow?).\n \nreply",
      "This is where passive voice highlights a weak position.Trusted\u2026 by who?\n \nreply",
      "Shouldn't be trusted? They are a random swedish foundation. How is one going to change that? Or disallow it or what you want.\n \nreply",
      "Second time he gets overlooked (after turing award)\n \nreply"
    ],
    "link": "https://www.nobelprize.org/uploads/2024/09/advanced-physicsprize2024.pdf",
    "first_paragraph": "",
    "summary": "The Nobel Prize in Physics just got doled out like Halloween candy, with recipients John Hopfield and Geoffrey Hinton snagging the spotlight. Meanwhile, the comment section on nobelprize.org has morphed into the digital equivalent of a conspiracy theorists' meetup, where everyone's *shocked*\u2014<em>shocked!</em>\u2014that Terry Sejnowski didn\u2019t make the cut. Each keyboard warrior, suddenly an expert in Nobel-worthy achievements, dissects the intricacies of Hopfield Nets with the fervor typically reserved for arguing about the best Star Trek captain. No doubt, more about how these prize decisions are a covert operation carried out by the omnipotent Swedish cabal. \ud83d\udc7d\ud83c\udfc6"
  },
  {
    "title": "Shrinking augmented reality displays into eyeglasses to expand their use (phys.org)",
    "points": 6,
    "submitter": "PaulHoule",
    "submit_time": "2024-10-06T21:31:36.000000Z",
    "num_comments": 0,
    "comments_url": "",
    "comments": [
      "xxx"
    ],
    "link": "https://phys.org/news/2024-09-augmented-reality-displays-eyeglasses.html",
    "first_paragraph": "\n\n                  Click here to sign in with\n                  \n\n\n                  or\n                  \n\n\n\n\nForget Password?\n\nLearn more\nshare this!86TwitShareEmail\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tSeptember 25, 2024\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\tThis article has been reviewed according to Science\u00a0X's \n\t\t\t\t\t\t\t\t\t\t\t\t\teditorial process\n\t\t\t\t\t\t\t\t\t\t\t\t\tand policies.\n\t\t\t\t\t\t\t\t\t\t\t\t\tEditors have highlighted\n\t\t\t\t\t\t\t\t\t\t\t\t\tthe following attributes while ensuring the content's credibility:\n\t\t\t\t\t\t\t\t\t\t\t\t\n fact-checked\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n peer-reviewed publication\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n trusted source\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n proofread\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t by \t\t\t\t\t\t\t\t\t\t American Chemical Society\nAugmented reality (AR) takes digital images and superimposes them onto real-world views. But AR is more than a new way to play video games; it could transform surgery and self-driving cars. To make the technology easier to integrate into common personal devices, researchers report in ACS Photonics how to combine two optical techn",
    "summary": "In a breathtaking leap that no one asked for, researchers have somehow managed to make playing Pok\u00e9mon GO on your eyeglasses a looming reality. Yes, the latest article from phys.org waxes poetic about squeezing augmented reality displays into specs, promising to revolutionize everything from slicing up your innards to crashing cars autonomously. Commenters, engaged in a battle of wits, fervently discuss whether this will help their nearsighted grandmas see better or just bombard them with pop-up ads. This modern marvel, peer-reviewed and fact-checked into oblivion, will likely end up as another high-tech way to walk into lamp posts. \ud83e\udd13\ud83d\ude80"
  },
  {
    "title": "Building a robust data synchronization framework with Rails (pcreux.com)",
    "points": 46,
    "submitter": "pcreux",
    "submit_time": "2024-10-08T12:39:56.000000Z",
    "num_comments": 5,
    "comments_url": "https://news.ycombinator.com/item?id=41776706",
    "comments": [
      "Ruby reads like a charm and makes these examples so elegant.  It truly shows that the language was optimized for developer happiness.\n \nreply",
      "Does everyone that loves Rails also love Ruby? I'm not one of them. I personally think Ruby is awful but Rails is incredible.\n \nreply",
      "I'm curious how you can find rails incredible while simultaneously hating ruby? Rails takes a lot of its inspiration from the language itself (internals relying heavily on metaprogramming, lots of exploitation of ruby's quirks all over the place). Like, what do you like about rails and what do you hate about ruby and how do those two things not overlap?\n \nreply",
      "Really elegant stuff, thank you for sharing.\n \nreply",
      "Hey Philippe! Very nice to see this from you. Looks great.\n \nreply"
    ],
    "link": "https://pcreux.com/2024/10/07/rails-data-sync-service.html",
    "first_paragraph": "07 Oct 2024At Zipline, we offer a one-stop shop application for retail employees that handles everything from clocking to task management and team communication. To achieve this, we need to integrate with numerous third-party applications with unique requirements. We decided to develop a satellite service called ZipIO for data synchronization. In this post, I\u2019ll review our journey of building this robust and scalable system.Language and Framework: We chose Ruby on Rails for its simplicity and productivity. However, we structured our core logic as POROs (Plain Old Ruby Object) to decouple the core logic from the engine that runs it.Modular Architecture: We broke down the workflow into discrete steps: Signals, Commands, Outcomes, and Tasks. This modular approach allows for easy reuse of logic across different third-party integrations.Asynchronous Processing: While we support synchronous operations for specific use cases, 99% of our workflows run asynchronously for optimal performance and",
    "summary": "**Building Yet Another Magical Machine with Rails: a Satirical Opera in Several Acts**\n\nIn an astounding feat of typical engineering hubris, <em>Zipline</em> decides that adapting Ruby on Rails to handle the overwhelming complexity of \"employees clocking in\" justifies the creation of their brand new satellite service, <strong>ZipIO</strong>. The Rails community rejoices, heralding the modular breakdown of what essentially amounts to a glorified cron job as the pinnacle of software architecture, while feverishly arguing about the aesthetics of Ruby syntax in a way that suggests deep personal relationships with their code editors. Meanwhile, one brave soul dares to dislike Ruby while enjoying Rails, sparking existential crises and a flurry of condescending tech-splaining in the comments, as everyone else marvels at the sheer elegance of reinventing several wheels. \ud83d\ude80\ud83d\udcbb\ud83c\udfa2"
  }
]